<!DOCTYPE html>
<html lang="en">

<head>
    <title>MyArxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                MyArxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2023-03-23T00:00:00Z">2023-03-23</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">22</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning and Verification of Task Structure in Instructional Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13519v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13519v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Medhini Narasimhan, Licheng Yu, Sean Bell, Ning Zhang, Trevor Darrell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given the enormous number of instructional videos available online, learning
a diverse array of multi-step task models from videos is an appealing goal. We
introduce a new pre-trained video model, VideoTaskformer, focused on
representing the semantics and structure of instructional videos. We pre-train
VideoTaskformer using a simple and effective objective: predicting weakly
supervised textual labels for steps that are randomly masked out from an
instructional video (masked step modeling). Compared to prior work which learns
step representations locally, our approach involves learning them globally,
leveraging video of the entire surrounding task as context. From these learned
representations, we can verify if an unseen video correctly executes a given
task, as well as forecast which steps are likely to be taken after a given
step. We introduce two new benchmarks for detecting mistakes in instructional
videos, to verify if there is an anomalous step and if steps are executed in
the right order. We also introduce a long-term forecasting benchmark, where the
goal is to predict long-range future steps from a given step. Our method
outperforms previous baselines on these tasks, and we believe the tasks will be
a valuable way for the community to measure the quality of step
representations. Additionally, we evaluate VideoTaskformer on 3 existing
benchmarks -- procedural activity recognition, step classification, and step
forecasting -- and demonstrate on each that our method outperforms existing
baselines and achieves new state-of-the-art performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Wesbite at https://medhini.github.io/task_structure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoBIT: A Contrastive Bi-directional Image-Text Generation Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13455v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13455v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoxuan You, Mandy Guo, Zhecan Wang, Kai-Wei Chang, Jason Baldridge, Jiahui Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The field of vision and language has witnessed a proliferation of pre-trained
foundation models. Most existing methods are independently pre-trained with
contrastive objective like CLIP, image-to-text generative objective like PaLI,
or text-to-image generative objective like Parti. However, the three objectives
can be pre-trained on the same data, image-text pairs, and intuitively they
complement each other as contrasting provides global alignment capacity and
generation grants fine-grained understanding. In this work, we present a
Contrastive Bi-directional Image-Text generation model (CoBIT), which attempts
to unify the three pre-training objectives in one framework. Specifically,
CoBIT employs a novel unicoder-decoder structure, consisting of an image
unicoder, a text unicoder and a cross-modal decoder. The image/text unicoders
can switch between encoding and decoding in different tasks, enabling
flexibility and shared knowledge that benefits both image-to-text and
text-to-image generations. CoBIT achieves superior performance in image
understanding, image-text understanding (Retrieval, Captioning, VQA, SNLI-VE)
and text-based content creation, particularly in zero-shot scenarios. For
instance, 82.7% in zero-shot ImageNet classification, 9.37 FID score in
zero-shot text-to-image generation and 44.8 CIDEr in zero-shot captioning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Development and validation of a natural language processing algorithm to
  pseudonymize documents in the context of a clinical data warehouse 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13451v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13451v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xavier Tannier, Perceval Wajsbürt, Alice Calliger, Basile Dura, Alexandre Mouchet, Martin Hilka, Romain Bey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The objective of this study is to address the critical issue of
de-identification of clinical reports in order to allow access to data for
research purposes, while ensuring patient privacy. The study highlights the
difficulties faced in sharing tools and resources in this domain and presents
the experience of the Greater Paris University Hospitals (AP-HP) in
implementing a systematic pseudonymization of text documents from its Clinical
Data Warehouse. We annotated a corpus of clinical documents according to 12
types of identifying entities, and built a hybrid system, merging the results
of a deep learning model as well as manual rules. Our results show an overall
performance of 0.99 of F1-score. We discuss implementation choices and present
experiments to better understand the effort involved in such a task, including
dataset size, document types, language models, or rule addition. We share
guidelines and code under a 3-Clause BSD license.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Paraphrasing evades detectors of AI-generated text, but retrieval is an
  effective defense 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13408v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13408v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kalpesh Krishna, Yixiao Song, Marzena Karpinska, John Wieting, Mohit Iyyer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To detect the deployment of large language models for malicious use cases
(e.g., fake content creation or academic plagiarism), several approaches have
recently been proposed for identifying AI-generated text via watermarks or
statistical irregularities. How robust are these detection algorithms to
paraphrases of AI-generated text? To stress test these detectors, we first
train an 11B parameter paraphrase generation model (DIPPER) that can paraphrase
paragraphs, optionally leveraging surrounding text (e.g., user-written prompts)
as context. DIPPER also uses scalar knobs to control the amount of lexical
diversity and reordering in the paraphrases. Paraphrasing text generated by
three large language models (including GPT3.5-davinci-003) with DIPPER
successfully evades several detectors, including watermarking, GPTZero,
DetectGPT, and OpenAI's text classifier. For example, DIPPER drops the
detection accuracy of DetectGPT from 70.3% to 4.6% (at a constant false
positive rate of 1%), without appreciably modifying the input semantics. To
increase the robustness of AI-generated text detection to paraphrase attacks,
we introduce a simple defense that relies on retrieving semantically-similar
generations and must be maintained by a language model API provider. Given a
candidate text, our algorithm searches a database of sequences previously
generated by the API, looking for sequences that match the candidate text
within a certain threshold. We empirically verify our defense using a database
of 15M generations from a fine-tuned T5-XXL model and find that it can detect
80% to 97% of paraphrased generations across different settings, while only
classifying 1% of human-written sequences as AI-generated. We will open source
our code, model and data for future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint (27 pages). Code, models, data will be added to
  https://github.com/martiansideofthemoon/ai-detection-paraphrases</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Compositional Zero-Shot Domain Transfer with Text-to-Text Models <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13386v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13386v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangyu Liu, Qianchu Liu, Shruthi Bannur, Fernando Pérez-García, Naoto Usuyama, Sheng Zhang, Tristan Naumann, Aditya Nori, Hoifung Poon, Javier Alvarez-Valle, Ozan Oktay, Stephanie L. Hyland
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Label scarcity is a bottleneck for improving task performance in specialised
domains. We propose a novel compositional transfer learning framework (DoT5 -
domain compositional zero-shot T5) for zero-shot domain transfer. Without
access to in-domain labels, DoT5 jointly learns domain knowledge (from MLM of
unlabelled in-domain free text) and task knowledge (from task training on more
readily available general-domain data) in a multi-task manner. To improve the
transferability of task training, we design a strategy named NLGU: we
simultaneously train NLG for in-domain label-to-data generation which enables
data augmentation for self-finetuning and NLU for label prediction. We evaluate
DoT5 on the biomedical domain and the resource-lean subdomain of radiology,
focusing on NLI, text summarisation and embedding learning. DoT5 demonstrates
the effectiveness of compositional transfer learning through multi-task
learning. In particular, DoT5 outperforms the current SOTA in zero-shot
transfer by over 7 absolute points in accuracy on RadNLI. We validate DoT5 with
ablations and a case study demonstrating its ability to solve challenging NLI
examples requiring in-domain expertise.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at TACL, pre-MIT Press publication version. 16 pages, 4
  figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DBLP-QuAD: A Question Answering <span class="highlight-title">Dataset</span> over the DBLP Scholarly
  Knowledge Graph <span class="chip">ECIR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13351v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13351v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Debayan Banerjee, Sushil Awale, Ricardo Usbeck, Chris Biemann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work we create a question answering dataset over the DBLP scholarly
knowledge graph (KG). DBLP is an on-line reference for bibliographic
information on major computer science publications that indexes over 4.4
million publications published by more than 2.2 million authors. Our dataset
consists of 10,000 question answer pairs with the corresponding SPARQL queries
which can be executed over the DBLP KG to fetch the correct answer. DBLP-QuAD
is the largest scholarly question answering dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages ceur-ws 1 column accepted at International Bibliometric
  Information Retrieval Workshp @ ECIR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Swiss<span class="highlight-title">BERT</span>: The Multilingual Language Model for Switzerland 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13310v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13310v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jannis Vamvas, Johannes Graën, Rico Sennrich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present SwissBERT, a masked language model created specifically for
processing Switzerland-related text. SwissBERT is a pre-trained model that we
adapted to news articles written in the national languages of Switzerland --
German, French, Italian, and Romansh. We evaluate SwissBERT on natural language
understanding tasks related to Switzerland and find that it tends to outperform
previous models on these tasks, especially when processing contemporary news
and/or Romansh Grischun. Since SwissBERT uses language adapters, it may be
extended to Swiss German dialects in future work. The model and our open-source
code are publicly released at https://github.com/ZurichNLP/swissbert.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GETT-QA: Graph Embedding based T2T <span class="highlight-title">Transformer</span> for Knowledge Graph
  Question Answering <span class="chip">ESWC 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13284v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13284v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Debayan Banerjee, Pranav Ajit Nair, Ricardo Usbeck, Chris Biemann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we present an end-to-end Knowledge Graph Question Answering
(KGQA) system named GETT-QA. GETT-QA uses T5, a popular text-to-text
pre-trained language model. The model takes a question in natural language as
input and produces a simpler form of the intended SPARQL query. In the simpler
form, the model does not directly produce entity and relation IDs. Instead, it
produces corresponding entity and relation labels. The labels are grounded to
KG entity and relation IDs in a subsequent step. To further improve the
results, we instruct the model to produce a truncated version of the KG
embedding for each entity. The truncated KG embedding enables a finer search
for disambiguation purposes. We find that T5 is able to learn the truncated KG
embeddings without any change of loss function, improving KGQA performance. As
a result, we report strong results for LC-QuAD 2.0 and SimpleQuestions-Wikidata
datasets on end-to-end KGQA over Wikidata.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages single column format accepted at ESWC 2023 research track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visual-Language <span class="highlight-title">Prompt</span> Tuning with Knowledge-guided Context Optimization <span class="chip">CVPR23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13283v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13283v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hantao Yao, Rui Zhang, Changsheng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt tuning is an effective way to adapt the pre-trained visual-language
model (VLM) to the downstream task using task-related textual tokens.
Representative CoOp-based work combines the learnable textual tokens with the
class tokens to obtain specific textual knowledge. However, the specific
textual knowledge is the worse generalization to the unseen classes because it
forgets the essential general textual knowledge having a strong generalization
ability. To tackle this issue, we introduce a novel Knowledge-guided Context
Optimization (KgCoOp) to enhance the generalization ability of the learnable
prompt for unseen classes. The key insight of KgCoOp is that forgetting about
essential knowledge can be alleviated by reducing the discrepancy between the
learnable prompt and the hand-crafted prompt. Especially, KgCoOp minimizes the
discrepancy between the textual embeddings generated by learned prompts and the
hand-crafted prompts. Finally, adding the KgCoOp upon the contrastive loss can
make a discriminative prompt for both seen and unseen tasks. Extensive
evaluation of several benchmarks demonstrates that the proposed
Knowledge-guided Context Optimization is an efficient method for prompt tuning,
\emph{i.e.,} achieves better performance with less training time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by CVPR23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parameter-Efficient Sparse Retrievers and Rerankers using Adapters <span class="chip">ECIR'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13220v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13220v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vaishali Pal, Carlos Lassance, Hervé Déjean, Stéphane Clinchant
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parameter-Efficient transfer learning with Adapters have been studied in
Natural Language Processing (NLP) as an alternative to full fine-tuning.
Adapters are memory-efficient and scale well with downstream tasks by training
small bottle-neck layers added between transformer layers while keeping the
large pretrained language model (PLMs) frozen. In spite of showing promising
results in NLP, these methods are under-explored in Information Retrieval.
While previous studies have only experimented with dense retriever or in a
cross lingual retrieval scenario, in this paper we aim to complete the picture
on the use of adapters in IR. First, we study adapters for SPLADE, a sparse
retriever, for which adapters not only retain the efficiency and effectiveness
otherwise achieved by finetuning, but are memory-efficient and orders of
magnitude lighter to train. We observe that Adapters-SPLADE not only optimizes
just 2\% of training parameters, but outperforms fully fine-tuned counterpart
and existing parameter-efficient dense IR models on IR benchmark datasets.
Secondly, we address domain adaptation of neural retrieval thanks to adapters
on cross-domain BEIR datasets and TripClick. Finally, we also consider
knowledge sharing between rerankers and first stage rankers. Overall, our study
complete the examination of adapters for neural IR
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at ECIR'23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fairness-guided Few-shot <span class="highlight-title">Prompt</span>ing for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13217v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13217v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huan Ma, Changqing Zhang, Yatao Bian, Lemao Liu, Zhirui Zhang, Peilin Zhao, Shu Zhang, Huazhu Fu, Qinghua Hu, Bingzhe Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models have demonstrated surprising ability to perform
in-context learning, i.e., these models can be directly applied to solve
numerous downstream tasks by conditioning on a prompt constructed by a few
input-output examples. However, prior research has shown that in-context
learning can suffer from high instability due to variations in training
examples, example order, and prompt formats. Therefore, the construction of an
appropriate prompt is essential for improving the performance of in-context
learning. In this paper, we revisit this problem from the view of predictive
bias. Specifically, we introduce a metric to evaluate the predictive bias of a
fixed prompt against labels or a given attributes. Then we empirically show
that prompts with higher bias always lead to unsatisfactory predictive quality.
Based on this observation, we propose a novel search strategy based on the
greedy search to identify the near-optimal prompt for improving the performance
of in-context learning. We perform comprehensive experiments with
state-of-the-art mainstream models such as GPT-3 on various downstream tasks.
Our results indicate that our method can enhance the model's in-context
learning performance in an effective and interpretable manner.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Simple Explanation for the Phase Transition in Large Language Models
  with List Decoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13112v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13112v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng-Shang Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Various recent experimental results show that large language models (LLM)
exhibit emergent abilities that are not present in small models. System
performance is greatly improved after passing a certain critical threshold of
scale. In this letter, we provide a simple explanation for such a phase
transition phenomenon. For this, we model an LLM as a sequence-to-sequence
random function. Instead of using instant generation at each step, we use a
list decoder that keeps a list of candidate sequences at each step and defers
the generation of the output sequence at the end. We show that there is a
critical threshold such that the expected number of erroneous candidate
sequences remains bounded when an LLM is below the threshold, and it grows
exponentially when an LLM is above the threshold. Such a threshold is related
to the basic reproduction number in a contagious disease.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-View Zero-Shot Open Intent Induction from Dialogues: Multi Domain
  Batch and Proxy Gradient Transfer <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13099v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13099v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyukhun Koh, Haesung Pyun, Nakyeong Yang, Kyomin Jung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In Task Oriented Dialogue (TOD) system, detecting and inducing new intents
are two main challenges to apply the system in the real world. In this paper,
we suggest the semantic multi-view model to resolve these two challenges: (1)
SBERT for General Embedding (GE), (2) Multi Domain Batch (MDB) for dialogue
domain knowledge, and (3) Proxy Gradient Transfer (PGT) for cluster-specialized
semantic. MDB feeds diverse dialogue datasets to the model at once to tackle
the multi-domain problem by learning the multiple domain knowledge. We
introduce a novel method PGT, which employs the Siamese network to fine-tune
the model with a clustering method directly.Our model can learn how to cluster
dialogue utterances by using PGT. Experimental results demonstrate that our
multi-view model with MDB and PGT significantly improves the Open Intent
Induction performance compared to baseline systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 3 figures, ACL 2023 workshop (DSTC)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Universal <span class="highlight-title">Transformer</span>: block reusing with adaptor in <span class="highlight-title">Transformer</span>
  for automatic speech recognit 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13072v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13072v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyu Tang, Zhaoyi Liu, Chang Zeng, Xinfeng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based models have recently made significant achievements in the
application of end-to-end (E2E) automatic speech recognition (ASR). It is
possible to deploy the E2E ASR system on smart devices with the help of
Transformer-based models. While these models still have the disadvantage of
requiring a large number of model parameters. To overcome the drawback of
universal Transformer models for the application of ASR on edge devices, we
propose a solution that can reuse the block in Transformer models for the
occasion of the small footprint ASR system, which meets the objective of
accommodating resource limitations without compromising recognition accuracy.
Specifically, we design a novel block-reusing strategy for speech Transformer
(BRST) to enhance the effectiveness of parameters and propose an adapter module
(ADM) that can produce a compact and adaptable model with only a few additional
trainable parameters accompanying each reusing block. We conducted an
experiment with the proposed method on the public AISHELL-1 corpus, and the
results show that the proposed approach achieves the character error rate (CER)
of 9.3%/6.63% with only 7.6M/8.3M parameters without and with the ADM,
respectively. In addition, we also make a deeper analysis to show the effect of
ADM in the general block-reusing method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Retrieval-Augmented Classification with Decoupled Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13065v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13065v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinnian Liang, Shuangzhi Wu, Hui Huang, Jiaqi Bai, Chao Bian, Zhoujun Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pretrained language models (PLMs) have shown marvelous improvements across
various NLP tasks. Most Chinese PLMs simply treat an input text as a sequence
of characters, and completely ignore word information. Although Whole Word
Masking can alleviate this, the semantics in words is still not well
represented. In this paper, we revisit the segmentation granularity of Chinese
PLMs. We propose a mixed-granularity Chinese BERT (MigBERT) by considering both
characters and words. To achieve this, we design objective functions for
learning both character and word-level representations. We conduct extensive
experiments on various Chinese NLP tasks to evaluate existing PLMs as well as
the proposed MigBERT. Experimental results show that MigBERT achieves new SOTA
performance on all these tasks. Further analysis demonstrates that words are
semantically richer than characters. More interestingly, we show that MigBERT
also works with Japanese. Our code has been released
here~\footnote{\url{https://github.com/xnliang98/MigBERT}} and you can download
our model here~\footnote{\url{https://huggingface.co/xnliang/MigBERT-large/}}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SPeC: A Soft <span class="highlight-title">Prompt</span>-Based Calibration on Mitigating Performance
  Variability in Clinical Notes Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13035v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13035v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu-Neng Chuang, Ruixiang Tang, Xiaoqian Jiang, Xia Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electronic health records (EHRs) store an extensive array of patient
information, encompassing medical histories, diagnoses, treatments, and test
outcomes. These records are crucial for enabling healthcare providers to make
well-informed decisions regarding patient care. Summarizing clinical notes
further assists healthcare professionals in pinpointing potential health risks
and making better-informed decisions. This process contributes to reducing
errors and enhancing patient outcomes by ensuring providers have access to the
most pertinent and current patient data. Recent research has shown that
incorporating prompts with large language models (LLMs) substantially boosts
the efficacy of summarization tasks. However, we show that this approach also
leads to increased output variance, resulting in notably divergent outputs even
when prompts share similar meanings. To tackle this challenge, we introduce a
model-agnostic Soft Prompt-Based Calibration (SPeC) pipeline that employs soft
prompts to diminish variance while preserving the advantages of prompt-based
summarization. Experimental findings on multiple clinical note tasks and LLMs
indicate that our method not only bolsters performance but also effectively
curbs variance for various LLMs, providing a more uniform and dependable
solution for summarizing vital medical information.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ges<span class="highlight-title">GPT</span>: Speech Gesture Synthesis With Text Parsing from <span class="highlight-title">GPT</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13013v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13013v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nan Gao, Zeyu Zhao, Zhi Zeng, Shuwu Zhang, Dongdong Weng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gesture synthesis has gained significant attention as a critical research
area, focusing on producing contextually appropriate and natural gestures
corresponding to speech or textual input. Although deep learning-based
approaches have achieved remarkable progress, they often overlook the rich
semantic information present in the text, leading to less expressive and
meaningful gestures. We propose GesGPT, a novel approach to gesture generation
that leverages the semantic analysis capabilities of Large Language Models
(LLMs), such as GPT. By capitalizing on the strengths of LLMs for text
analysis, we design prompts to extract gesture-related information from textual
input. Our method entails developing prompt principles that transform gesture
generation into an intention classification problem based on GPT, and utilizing
a curated gesture library and integration module to produce semantically rich
co-speech gestures. Experimental results demonstrate that GesGPT effectively
generates contextually appropriate and expressive gestures, offering a new
perspective on semantic co-speech gesture generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Is Chat<span class="highlight-title">GPT</span> A Good Keyphrase Generator? A Preliminary Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13001v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13001v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyang Song, Haiyun Jiang, Shuming Shi, Songfang Yao, Shilong Lu, Yi Feng, Huafeng Liu, Liping Jing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emergence of ChatGPT has recently garnered significant attention from the
computational linguistics community. To demonstrate its capabilities as a
keyphrase generator, we conduct a preliminary evaluation of ChatGPT for the
keyphrase generation task. We evaluate its performance in various aspects,
including keyphrase generation prompts, keyphrase generation diversity,
multi-domain keyphrase generation, and long document understanding. Our
evaluation is based on six benchmark datasets, and we adopt the prompt
suggested by OpenAI while extending it to six candidate prompts. We find that
ChatGPT performs exceptionally well on all six candidate prompts, with minor
performance differences observed across the datasets. Based on our findings, we
conclude that ChatGPT has great potential for keyphrase generation. Moreover,
we discover that ChatGPT still faces challenges when it comes to generating
absent keyphrases. Meanwhile, in the final section, we also present some
limitations and future expansions of this report.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report, 7 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Using Context-to-Vector with Graph Retrofitting to Improve Word
  Embeddings <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.16848v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.16848v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiangbin Zheng, Yile Wang, Ge Wang, Jun Xia, Yufei Huang, Guojiang Zhao, Yue Zhang, Stan Z. Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although contextualized embeddings generated from large-scale pre-trained
models perform well in many tasks, traditional static embeddings (e.g.,
Skip-gram, Word2Vec) still play an important role in low-resource and
lightweight settings due to their low computational cost, ease of deployment,
and stability. In this paper, we aim to improve word embeddings by 1)
incorporating more contextual information from existing pre-trained models into
the Skip-gram framework, which we call Context-to-Vec; 2) proposing a
post-processing retrofitting method for static embeddings independent of
training by employing priori synonym knowledge and weighted vector
distribution. Through extrinsic and intrinsic tasks, our methods are well
proven to outperform the baselines by a large margin.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Offensive Language and Hate Speech Detection for Danish 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1908.04531v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1908.04531v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gudbjartur Ingi Sigurbergsson, Leon Derczynski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The presence of offensive language on social media platforms and the
implications this poses is becoming a major concern in modern society. Given
the enormous amount of content created every day, automatic methods are
required to detect and deal with this type of content. Until now, most of the
research has focused on solving the problem for the English language, while the
problem is multilingual.
  We construct a Danish dataset containing user-generated comments from
\textit{Reddit} and \textit{Facebook}. It contains user generated comments from
various social media platforms, and to our knowledge, it is the first of its
kind. Our dataset is annotated to capture various types and target of offensive
language. We develop four automatic classification systems, each designed to
work for both the English and the Danish language. In the detection of
offensive language in English, the best performing system achieves a macro
averaged F1-score of $0.74$, and the best performing system for Danish achieves
a macro averaged F1-score of $0.70$. In the detection of whether or not an
offensive post is targeted, the best performing system for English achieves a
macro averaged F1-score of $0.62$, while the best performing system for Danish
achieves a macro averaged F1-score of $0.73$. Finally, in the detection of the
target type in a targeted offensive post, the best performing system for
English achieves a macro averaged F1-score of $0.56$, and the best performing
system for Danish achieves a macro averaged F1-score of $0.63$.
  Our work for both the English and the Danish language captures the type and
targets of offensive language, and present automatic methods for detecting
different kinds of offensive language such as hate speech and cyberbullying.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of the Twelfth Language Resources and Evaluation
  Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Discriminating Between Similar Nordic Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2012.06431v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2012.06431v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        René Haas, Leon Derczynski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic language identification is a challenging problem. Discriminating
between closely related languages is especially difficult. This paper presents
a machine learning approach for automatic language identification for the
Nordic languages, which often suffer miscategorisation by existing
state-of-the-art tools. Concretely we will focus on discrimination between six
Nordic languages: Danish, Swedish, Norwegian (Nynorsk), Norwegian (Bokm{\aa}l),
Faroese and Icelandic.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of the Eighth Workshop on NLP for Similar Languages,
  Varieties and Dialects</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Memotion 3: <span class="highlight-title">Dataset</span> on Sentiment and Emotion Analysis of Codemixed
  Hindi-English Memes <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.09892v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.09892v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shreyash Mishra, S Suryavardan, Parth Patwa, Megha Chakraborty, Anku Rani, Aishwarya Reganti, Aman Chadha, Amitava Das, Amit Sheth, Manoj Chinnakotla, Asif Ekbal, Srijan Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Memes are the new-age conveyance mechanism for humor on social media sites.
Memes often include an image and some text. Memes can be used to promote
disinformation or hatred, thus it is crucial to investigate in details. We
introduce Memotion 3, a new dataset with 10,000 annotated memes. Unlike other
prevalent datasets in the domain, including prior iterations of Memotion,
Memotion 3 introduces Hindi-English Codemixed memes while prior works in the
area were limited to only the English memes. We describe the Memotion task, the
data collection and the dataset creation methodologies. We also provide a
baseline for the task. The baseline code and dataset will be made available at
https://github.com/Shreyashm16/Memotion-3.0
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Defactify2 @AAAI</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">150</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning and Verification of Task Structure in Instructional Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13519v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13519v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Medhini Narasimhan, Licheng Yu, Sean Bell, Ning Zhang, Trevor Darrell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given the enormous number of instructional videos available online, learning
a diverse array of multi-step task models from videos is an appealing goal. We
introduce a new pre-trained video model, VideoTaskformer, focused on
representing the semantics and structure of instructional videos. We pre-train
VideoTaskformer using a simple and effective objective: predicting weakly
supervised textual labels for steps that are randomly masked out from an
instructional video (masked step modeling). Compared to prior work which learns
step representations locally, our approach involves learning them globally,
leveraging video of the entire surrounding task as context. From these learned
representations, we can verify if an unseen video correctly executes a given
task, as well as forecast which steps are likely to be taken after a given
step. We introduce two new benchmarks for detecting mistakes in instructional
videos, to verify if there is an anomalous step and if steps are executed in
the right order. We also introduce a long-term forecasting benchmark, where the
goal is to predict long-range future steps from a given step. Our method
outperforms previous baselines on these tasks, and we believe the tasks will be
a valuable way for the community to measure the quality of step
representations. Additionally, we evaluate VideoTaskformer on 3 existing
benchmarks -- procedural activity recognition, step classification, and step
forecasting -- and demonstrate on each that our method outperforms existing
baselines and achieves new state-of-the-art performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Wesbite at https://medhini.github.io/task_structure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Three ways to improve feature alignment for open vocabulary detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13518v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13518v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Relja Arandjelović, Alex Andonian, Arthur Mensch, Olivier J. Hénaff, Jean-Baptiste Alayrac, Andrew Zisserman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The core problem in zero-shot open vocabulary detection is how to align
visual and text features, so that the detector performs well on unseen classes.
Previous approaches train the feature pyramid and detection head from scratch,
which breaks the vision-text feature alignment established during pretraining,
and struggles to prevent the language model from forgetting unseen classes.
  We propose three methods to alleviate these issues. Firstly, a simple scheme
is used to augment the text embeddings which prevents overfitting to a small
number of classes seen during training, while simultaneously saving memory and
computation. Secondly, the feature pyramid network and the detection head are
modified to include trainable gated shortcuts, which encourages vision-text
feature alignment and guarantees it at the start of detection training.
Finally, a self-training approach is used to leverage a larger corpus of
image-text pairs thus improving detection performance on classes with no human
annotated bounding boxes.
  Our three methods are evaluated on the zero-shot version of the LVIS
benchmark, each of them showing clear and significant benefits. Our final
network achieves the new stateof-the-art on the mAP-all metric and demonstrates
competitive performance for mAP-rare, as well as superior transfer to COCO and
Objects365.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ablating Concepts in Text-to-Image Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13516v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13516v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nupur Kumari, Bingliang Zhang, Sheng-Yu Wang, Eli Shechtman, Richard Zhang, Jun-Yan Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale text-to-image diffusion models can generate high-fidelity images
with powerful compositional ability. However, these models are typically
trained on an enormous amount of Internet data, often containing copyrighted
material, licensed images, and personal photos. Furthermore, they have been
found to replicate the style of various living artists or memorize exact
training samples. How can we remove such copyrighted concepts or images without
retraining the model from scratch? To achieve this goal, we propose an
efficient method of ablating concepts in the pretrained model, i.e., preventing
the generation of a target concept. Our algorithm learns to match the image
distribution for a target style, instance, or text prompt we wish to ablate to
the distribution corresponding to an anchor concept. This prevents the model
from generating target concepts given its text condition. Extensive experiments
show that our method can successfully prevent the generation of the ablated
concept while preserving closely related concepts in the model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>project website: https://www.cs.cmu.edu/~concept-ablation/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Persistent Nature: A Generative Model of Unbounded 3D Worlds <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13515v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13515v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucy Chai, Richard Tucker, Zhengqi Li, Phillip Isola, Noah Snavely
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite increasingly realistic image quality, recent 3D image generative
models often operate on 3D volumes of fixed extent with limited camera motions.
We investigate the task of unconditionally synthesizing unbounded nature
scenes, enabling arbitrarily large camera motion while maintaining a persistent
3D world model. Our scene representation consists of an extendable, planar
scene layout grid, which can be rendered from arbitrary camera poses via a 3D
decoder and volume rendering, and a panoramic skydome. Based on this
representation, we learn a generative world model solely from single-view
internet photos. Our method enables simulating long flights through 3D
landscapes, while maintaining global scene consistency--for instance, returning
to the starting point yields the same view of the scene. Our approach enables
scene extrapolation beyond the fixed bounds of current 3D generative models,
while also supporting a persistent, camera-independent world representation
that stands in contrast to auto-regressive 3D prediction models. Our project
page: https://chail.github.io/persistent-nature/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR camera ready version, project page:
  https://chail.github.io/persistent-nature/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SAOR: Single-View Articulated Object Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13514v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13514v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehmet Aygün, Oisin Mac Aodha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce SAOR, a novel approach for estimating the 3D shape, texture, and
viewpoint of an articulated object from a single image captured in the wild.
Unlike prior approaches that rely on pre-defined category-specific 3D templates
or tailored 3D skeletons, SAOR learns to articulate shapes from single-view
image collections with a skeleton-free part-based model without requiring any
3D object shape priors. To prevent ill-posed solutions, we propose a
cross-instance consistency loss that exploits disentangled object shape
deformation and articulation. This is helped by a new silhouette-based sampling
mechanism to enhance viewpoint diversity during training. Our method only
requires estimated object silhouettes and relative depth maps from
off-the-shelf pre-trained networks during training. At inference time, given a
single-view image, it efficiently outputs an explicit mesh representation. We
obtain improved qualitative and quantitative results on challenging quadruped
animals compared to relevant existing work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://mehmetaygun.github.io/saor</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Preset for Color Style Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13511v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13511v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhanghan Ke, Yuhao Liu, Lei Zhu, Nanxuan Zhao, Rynson W. H. Lau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a Neural Preset technique to address the
limitations of existing color style transfer methods, including visual
artifacts, vast memory requirement, and slow style switching speed. Our method
is based on two core designs. First, we propose Deterministic Neural Color
Mapping (DNCM) to consistently operate on each pixel via an image-adaptive
color mapping matrix, avoiding artifacts and supporting high-resolution inputs
with a small memory footprint. Second, we develop a two-stage pipeline by
dividing the task into color normalization and stylization, which allows
efficient style switching by extracting color styles as presets and reusing
them on normalized input images. Due to the unavailability of pairwise
datasets, we describe how to train Neural Preset via a self-supervised
strategy. Various advantages of Neural Preset over existing methods are
demonstrated through comprehensive evaluations. Besides, we show that our
trained model can naturally support multiple applications without fine-tuning,
including low-light image enhancement, underwater image correction, image
dehazing, and image harmonization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Position-Guided Point Cloud Panoptic Segmentation <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13509v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13509v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeqi Xiao, Wenwei Zhang, Tai Wang, Chen Change Loy, Dahua Lin, Jiangmiao Pang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  DEtection TRansformer (DETR) started a trend that uses a group of learnable
queries for unified visual perception. This work begins by applying this
appealing paradigm to LiDAR-based point cloud segmentation and obtains a simple
yet effective baseline. Although the naive adaptation obtains fair results, the
instance segmentation performance is noticeably inferior to previous works. By
diving into the details, we observe that instances in the sparse point clouds
are relatively small to the whole scene and often have similar geometry but
lack distinctive appearance for segmentation, which are rare in the image
domain. Considering instances in 3D are more featured by their positional
information, we emphasize their roles during the modeling and design a robust
Mixed-parameterized Positional Embedding (MPE) to guide the segmentation
process. It is embedded into backbone features and later guides the mask
prediction and query update processes iteratively, leading to Position-Aware
Segmentation (PA-Seg) and Masked Focal Attention (MFA). All these designs impel
the queries to attend to specific regions and identify various instances. The
method, named Position-guided Point cloud Panoptic segmentation transFormer
(P3Former), outperforms previous state-of-the-art methods by 3.4% and 1.2% PQ
on SemanticKITTI and nuScenes benchmark, respectively. The source code and
models are available at https://github.com/SmartBot-PJLab/P3Former .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://github.com/SmartBot-PJLab/P3Former</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MV-JAR: Masked Voxel Jigsaw and Reconstruction for LiDAR-Based
  <span class="highlight-title">Self-Supervised</span> <span class="highlight-title">Pre-Train</span>ing <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13510v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13510v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Runsen Xu, Tai Wang, Wenwei Zhang, Runjian Chen, Jinkun Cao, Jiangmiao Pang, Dahua Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces the Masked Voxel Jigsaw and Reconstruction (MV-JAR)
method for LiDAR-based self-supervised pre-training and a carefully designed
data-efficient 3D object detection benchmark on the Waymo dataset. Inspired by
the scene-voxel-point hierarchy in downstream 3D object detectors, we design
masking and reconstruction strategies accounting for voxel distributions in the
scene and local point distributions within the voxel. We employ a
Reversed-Furthest-Voxel-Sampling strategy to address the uneven distribution of
LiDAR points and propose MV-JAR, which combines two techniques for modeling the
aforementioned distributions, resulting in superior performance. Our
experiments reveal limitations in previous data-efficient experiments, which
uniformly sample fine-tuning splits with varying data proportions from each
LiDAR sequence, leading to similar data diversity across splits. To address
this, we propose a new benchmark that samples scene sequences for diverse
fine-tuning splits, ensuring adequate model convergence and providing a more
accurate evaluation of pre-training methods. Experiments on our Waymo benchmark
and the KITTI dataset demonstrate that MV-JAR consistently and significantly
improves 3D detection performance across various data scales, achieving up to a
6.3% increase in mAPH compared to training from scratch. Codes and the
benchmark will be available at https://github.com/SmartBot-PJLab/MV-JAR .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2023 with a carefully designed benchmark on Waymo.
  Codes and the benchmark will be available at
  https://github.com/SmartBot-PJLab/MV-JAR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DreamBooth3D: Subject-Driven Text-to-3D Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13508v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13508v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer, Nataniel Ruiz, Ben Mildenhall, Shiran Zada, Kfir Aberman, Michael Rubinstein, Jonathan Barron, Yuanzhen Li, Varun Jampani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present DreamBooth3D, an approach to personalize text-to-3D generative
models from as few as 3-6 casually captured images of a subject. Our approach
combines recent advances in personalizing text-to-image models (DreamBooth)
with text-to-3D generation (DreamFusion). We find that naively combining these
methods fails to yield satisfactory subject-specific 3D assets due to
personalized text-to-image models overfitting to the input viewpoints of the
subject. We overcome this through a 3-stage optimization strategy where we
jointly leverage the 3D consistency of neural radiance fields together with the
personalization capability of text-to-image models. Our method can produce
high-quality, subject-specific 3D assets with text-driven modifications such as
novel poses, colors and attributes that are not seen in any of the input images
of the subject.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page at https://dreambooth3d.github.io/ Video Summary at
  https://youtu.be/TFtaoAqSkEA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ReBotNet: Fast Real-time Video Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13504v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13504v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeya Maria Jose Valanarasu, Rahul Garg, Andeep Toor, Xin Tong, Weijuan Xi, Andreas Lugmayr, Vishal M. Patel, Anne Menini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most video restoration networks are slow, have high computational load, and
can't be used for real-time video enhancement. In this work, we design an
efficient and fast framework to perform real-time video enhancement for
practical use-cases like live video calls and video streams. Our proposed
method, called Recurrent Bottleneck Mixer Network (ReBotNet), employs a
dual-branch framework. The first branch learns spatio-temporal features by
tokenizing the input frames along the spatial and temporal dimensions using a
ConvNext-based encoder and processing these abstract tokens using a bottleneck
mixer. To further improve temporal consistency, the second branch employs a
mixer directly on tokens extracted from individual frames. A common decoder
then merges the features form the two branches to predict the enhanced frame.
In addition, we propose a recurrent training approach where the last frame's
prediction is leveraged to efficiently enhance the current frame while
improving temporal consistency. To evaluate our method, we curate two new
datasets that emulate real-world video call and streaming scenarios, and show
extensive results on multiple datasets where ReBotNet outperforms existing
approaches with lower computations, reduced memory requirements, and faster
inference time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Website: https://jeya-maria-jose.github.io/rebotnet-web/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Large-scale Study of Spatiotemporal Representation Learning with a New
  Benchmark on Action Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13505v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13505v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andong Deng, Taojiannan Yang, Chen Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of building a benchmark (suite of datasets) is to provide a unified
protocol for fair evaluation and thus facilitate the evolution of a specific
area. Nonetheless, we point out that existing protocols of action recognition
could yield partial evaluations due to several limitations. To comprehensively
probe the effectiveness of spatiotemporal representation learning, we introduce
BEAR, a new BEnchmark on video Action Recognition. BEAR is a collection of 18
video datasets grouped into 5 categories (anomaly, gesture, daily, sports, and
instructional), which covers a diverse set of real-world applications. With
BEAR, we thoroughly evaluate 6 common spatiotemporal models pre-trained by both
supervised and self-supervised learning. We also report transfer performance
via standard finetuning, few-shot finetuning, and unsupervised domain
adaptation. Our observation suggests that current state-of-the-art cannot
solidly guarantee high performance on datasets close to real-world
applications, and we hope BEAR can serve as a fair and challenging evaluation
benchmark to gain insights on building next-generation spatiotemporal learners.
Our dataset, code, and models are released at:
https://github.com/AndongDeng/BEAR
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Chordal Averaging on Flag Manifolds and Its Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13501v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13501v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathan Mankovich, Tolga Birdal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a new, provably-convergent algorithm for computing the
flag-mean and flag-median of a set of points on a flag manifold under the
chordal metric. The flag manifold is a mathematical space consisting of flags,
which are sequences of nested subspaces of a vector space that increase in
dimension. The flag manifold is a superset of a wide range of known matrix
groups, including Stiefel and Grassmanians, making it a general object that is
useful in a wide variety computer vision problems.
  To tackle the challenge of computing first order flag statistics, we first
transform the problem into one that involves auxiliary variables constrained to
the Stiefel manifold. The Stiefel manifold is a space of orthogonal frames, and
leveraging the numerical stability and efficiency of Stiefel-manifold
optimization enables us to compute the flag-mean effectively. Through a series
of experiments, we show the competence of our method in Grassmann and rotation
averaging, as well as principal component analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TriPlaneNet: An Encoder for EG3D Inversion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13497v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13497v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ananta R. Bhattarai, Matthias Nießner, Artem Sevastopolsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent progress in NeRF-based GANs has introduced a number of approaches for
high-resolution and high-fidelity generative modeling of human heads with a
possibility for novel view rendering. At the same time, one must solve an
inverse problem to be able to re-render or modify an existing image or video.
Despite the success of universal optimization-based methods for 2D GAN
inversion, those, applied to 3D GANs, may fail to produce 3D-consistent
renderings. Fast encoder-based techniques, such as those developed for
StyleGAN, may also be less appealing due to the lack of identity preservation.
In our work, we introduce a real-time method that bridges the gap between the
two approaches by directly utilizing the tri-plane representation introduced
for EG3D generative model. In particular, we build upon a feed-forward
convolutional encoder for the latent code and extend it with a
fully-convolutional predictor of tri-plane numerical offsets. As shown in our
work, the renderings are similar in quality to optimization-based techniques
and significantly outperform the baselines for novel view. As we empirically
prove, this is a consequence of directly operating in the tri-plane space, not
in the GAN parameter space, while making use of an encoder-based trainable
approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Video: https://youtu.be/GpmSswHMeWU Project page:
  https://anantarb.github.io/triplanenet</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The effectiveness of MAE pre-<span class="highlight-title">pretrain</span>ing for billion-scale <span class="highlight-title">pretrain</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13496v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13496v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mannat Singh, Quentin Duval, Kalyan Vasudev Alwala, Haoqi Fan, Vaibhav Aggarwal, Aaron Adcock, Armand Joulin, Piotr Dollár, Christoph Feichtenhofer, Ross Girshick, Rohit Girdhar, Ishan Misra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper revisits the standard pretrain-then-finetune paradigm used in
computer vision for visual recognition tasks. Typically, state-of-the-art
foundation models are pretrained using large scale (weakly) supervised datasets
with billions of images. We introduce an additional pre-pretraining stage that
is simple and uses the self-supervised MAE technique to initialize the model.
While MAE has only been shown to scale with the size of models, we find that it
scales with the size of the training dataset as well. Thus, our MAE-based
pre-pretraining scales with both model and data size making it applicable for
training foundation models. Pre-pretraining consistently improves both the
model convergence and the downstream transfer performance across a range of
model scales (millions to billions of parameters), and dataset sizes (millions
to billions of images). We measure the effectiveness of pre-pretraining on 10
different visual recognition tasks spanning image classification, video
recognition, object detection, low-shot classification and zero-shot
recognition. Our largest model achieves new state-of-the-art results on
iNaturalist-18 (91.3%), 1-shot ImageNet-1k (62.1%), and zero-shot transfer on
Food-101 (96.0%). Our study reveals that model initialization plays a
significant role, even for web-scale pretraining with billions of images.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ReVersion: Diffusion-Based Relation Inversion from Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13495v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13495v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqi Huang, Tianxing Wu, Yuming Jiang, Kelvin C. K. Chan, Ziwei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models gain increasing popularity for their generative
capabilities. Recently, there have been surging needs to generate customized
images by inverting diffusion models from exemplar images. However, existing
inversion methods mainly focus on capturing object appearances. How to invert
object relations, another important pillar in the visual world, remains
unexplored. In this work, we propose ReVersion for the Relation Inversion task,
which aims to learn a specific relation (represented as "relation prompt") from
exemplar images. Specifically, we learn a relation prompt from a frozen
pre-trained text-to-image diffusion model. The learned relation prompt can then
be applied to generate relation-specific images with new objects, backgrounds,
and styles. Our key insight is the "preposition prior" - real-world relation
prompts can be sparsely activated upon a set of basis prepositional words.
Specifically, we propose a novel relation-steering contrastive learning scheme
to impose two critical properties of the relation prompt: 1) The relation
prompt should capture the interaction between objects, enforced by the
preposition prior. 2) The relation prompt should be disentangled away from
object appearances. We further devise relation-focal importance sampling to
emphasize high-level interactions over low-level appearances (e.g., texture,
color). To comprehensively evaluate this new task, we contribute ReVersion
Benchmark, which provides various exemplar images with diverse relations.
Extensive experiments validate the superiority of our approach over existing
methods across a wide range of visual relations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>First two authors contributed equally. Project page:
  https://ziqihuangg.github.io/projects/reversion.html Code:
  https://github.com/ziqihuangg/ReVersion</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NS3D: Neuro-Symbolic Grounding of 3D Objects and Relations <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13483v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13483v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joy Hsu, Jiayuan Mao, Jiajun Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Grounding object properties and relations in 3D scenes is a prerequisite for
a wide range of artificial intelligence tasks, such as visually grounded
dialogues and embodied manipulation. However, the variability of the 3D domain
induces two fundamental challenges: 1) the expense of labeling and 2) the
complexity of 3D grounded language. Hence, essential desiderata for models are
to be data-efficient, generalize to different data distributions and tasks with
unseen semantic forms, as well as ground complex language semantics (e.g.,
view-point anchoring and multi-object reference). To address these challenges,
we propose NS3D, a neuro-symbolic framework for 3D grounding. NS3D translates
language into programs with hierarchical structures by leveraging large
language-to-code models. Different functional modules in the programs are
implemented as neural networks. Notably, NS3D extends prior neuro-symbolic
visual reasoning methods by introducing functional modules that effectively
reason about high-arity relations (i.e., relations among more than two
objects), key in disambiguating objects in complex 3D scenes. Modular and
compositional architecture enables NS3D to achieve state-of-the-art results on
the ReferIt3D view-dependence task, a 3D referring expression comprehension
benchmark. Importantly, NS3D shows significantly improved performance on
settings of data-efficiency and generalization, and demonstrate zero-shot
transfer to an unseen 3D question-answering task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TactoFind: A Tactile Only System for Object Retrieval <span class="chip">ICRA 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13482v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13482v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sameer Pai, Tao Chen, Megha Tippur, Edward Adelson, Abhishek Gupta, Pulkit Agrawal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of object retrieval in scenarios where visual sensing is
absent, object shapes are unknown beforehand and objects can move freely, like
grabbing objects out of a drawer. Successful solutions require localizing free
objects, identifying specific object instances, and then grasping the
identified objects, only using touch feedback. Unlike vision, where cameras can
observe the entire scene, touch sensors are local and only observe parts of the
scene that are in contact with the manipulator. Moreover, information gathering
via touch sensors necessitates applying forces on the touched surface which may
disturb the scene itself. Reasoning with touch, therefore, requires careful
exploration and integration of information over time -- a challenge we tackle.
We present a system capable of using sparse tactile feedback from fingertip
touch sensors on a dexterous hand to localize, identify and grasp novel objects
without any visual feedback. Videos are available at
https://taochenshh.github.io/projects/tactofind.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ICRA 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Prior-free Category-level Pose Estimation with Implicit Space
  Transformation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13479v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13479v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianhui Liu, Yukang Chen, Xiaoqing Ye, Xiaojuan Qi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Category-level 6D pose estimation aims to predict the poses and sizes of
unseen objects from a specific category. Thanks to prior deformation, which
explicitly adapts a category-specific 3D prior (i.e., a 3D template) to a given
object instance, prior-based methods attained great success and have become a
major research stream. However, obtaining category-specific priors requires
collecting a large amount of 3D models, which is labor-consuming and often not
accessible in practice. This motivates us to investigate whether priors are
necessary to make prior-based methods effective. Our empirical study shows that
the 3D prior itself is not the credit to the high performance. The keypoint
actually is the explicit deformation process, which aligns camera and world
coordinates supervised by world-space 3D models (also called canonical space).
Inspired by these observation, we introduce a simple prior-free implicit space
transformation network, namely IST-Net, to transform camera-space features to
world-space counterparts and build correspondence between them in an implicit
manner without relying on 3D priors. Besides, we design camera- and world-space
enhancers to enrich the features with pose-sensitive information and
geometrical constraints, respectively. Albeit simple, IST-Net becomes the first
prior-free method that achieves state-of-the-art performance, with top
inference speed on the REAL275 dataset. Our code and models will be publicly
available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TransPoser: <span class="highlight-title">Transformer</span> as an Optimizer for Joint Object Shape and Pose
  Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13477v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13477v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuta Yoshitake, Mai Nishimura, Shohei Nobuhara, Ko Nishino
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel method for joint estimation of shape and pose of rigid
objects from their sequentially observed RGB-D images. In sharp contrast to
past approaches that rely on complex non-linear optimization, we propose to
formulate it as a neural optimization that learns to efficiently estimate the
shape and pose. We introduce Deep Directional Distance Function (DeepDDF), a
neural network that directly outputs the depth image of an object given the
camera viewpoint and viewing direction, for efficient error computation in 2D
image space. We formulate the joint estimation itself as a Transformer which we
refer to as TransPoser. We fully leverage the tokenization and multi-head
attention to sequentially process the growing set of observations and to
efficiently update the shape and pose with a learned momentum, respectively.
Experimental results on synthetic and real data show that DeepDDF achieves high
accuracy as a category-level object shape representation and TransPoser
achieves state-of-the-art accuracy efficiently for joint shape and pose
estimation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Plotting Behind the Scenes: Towards Learnable Game Engines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13472v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13472v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Willi Menapace, Aliaksandr Siarohin, Stéphane Lathuilière, Panos Achlioptas, Vladislav Golyanik, Elisa Ricci, Sergey Tulyakov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Game engines are powerful tools in computer graphics. Their power comes at
the immense cost of their development. In this work, we present a framework to
train game-engine-like neural models, solely from monocular annotated videos.
The result-a Learnable Game Engine (LGE)-maintains states of the scene, objects
and agents in it, and enables rendering the environment from a controllable
viewpoint. Similarly to a game engine, it models the logic of the game and the
underlying rules of physics, to make it possible for a user to play the game by
specifying both high- and low-level action sequences. Most captivatingly, our
LGE unlocks the director's mode, where the game is played by plotting behind
the scenes, specifying high-level actions and goals for the agents in the form
of language and desired states. This requires learning "game AI", encapsulated
by our animation model, to navigate the scene using high-level constraints,
play against an adversary, devise the strategy to win a point. The key to
learning such game AI is the exploitation of a large and diverse text corpus,
collected in this work, describing detailed actions in a game and used to train
our animation model. To render the resulting state of the environment and its
agents, we use a compositional NeRF representation used in our synthesis model.
To foster future research, we present newly collected, annotated and calibrated
large-scale Tennis and Minecraft datasets. Our method significantly outperforms
existing neural video game simulators in terms of rendering quality. Besides,
our LGEs unlock applications beyond capabilities of the current state of the
art. Our framework, data, and models are available at
https://learnable-game-engines.github.io/lge-website.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Egocentric Audio-Visual Object Localization <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13471v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13471v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Huang, Yapeng Tian, Anurag Kumar, Chenliang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans naturally perceive surrounding scenes by unifying sound and sight in a
first-person view. Likewise, machines are advanced to approach human
intelligence by learning with multisensory inputs from an egocentric
perspective. In this paper, we explore the challenging egocentric audio-visual
object localization task and observe that 1) egomotion commonly exists in
first-person recordings, even within a short duration; 2) The out-of-view sound
components can be created while wearers shift their attention. To address the
first problem, we propose a geometry-aware temporal aggregation module to
handle the egomotion explicitly. The effect of egomotion is mitigated by
estimating the temporal geometry transformation and exploiting it to update
visual representations. Moreover, we propose a cascaded feature enhancement
module to tackle the second issue. It improves cross-modal localization
robustness by disentangling visually-indicated audio representation. During
training, we take advantage of the naturally available audio-visual temporal
synchronization as the ``free'' self-supervision to avoid costly labeling. We
also annotate and create the Epic Sounding Object dataset for evaluation
purposes. Extensive experiments show that our method achieves state-of-the-art
localization performance in egocentric videos and can be generalized to diverse
audio-visual scenes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoBIT: A Contrastive Bi-directional Image-Text Generation Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13455v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13455v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoxuan You, Mandy Guo, Zhecan Wang, Kai-Wei Chang, Jason Baldridge, Jiahui Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The field of vision and language has witnessed a proliferation of pre-trained
foundation models. Most existing methods are independently pre-trained with
contrastive objective like CLIP, image-to-text generative objective like PaLI,
or text-to-image generative objective like Parti. However, the three objectives
can be pre-trained on the same data, image-text pairs, and intuitively they
complement each other as contrasting provides global alignment capacity and
generation grants fine-grained understanding. In this work, we present a
Contrastive Bi-directional Image-Text generation model (CoBIT), which attempts
to unify the three pre-training objectives in one framework. Specifically,
CoBIT employs a novel unicoder-decoder structure, consisting of an image
unicoder, a text unicoder and a cross-modal decoder. The image/text unicoders
can switch between encoding and decoding in different tasks, enabling
flexibility and shared knowledge that benefits both image-to-text and
text-to-image generations. CoBIT achieves superior performance in image
understanding, image-text understanding (Retrieval, Captioning, VQA, SNLI-VE)
and text-based content creation, particularly in zero-shot scenarios. For
instance, 82.7% in zero-shot ImageNet classification, 9.37 FID score in
zero-shot text-to-image generation and 44.8 CIDEr in zero-shot captioning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Set-the-Scene: Global-Local Training for Generating Controllable NeRF
  Scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13450v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13450v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dana Cohen-Bar, Elad Richardson, Gal Metzer, Raja Giryes, Daniel Cohen-Or
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent breakthroughs in text-guided image generation have led to remarkable
progress in the field of 3D synthesis from text. By optimizing neural radiance
fields (NeRF) directly from text, recent methods are able to produce remarkable
results. Yet, these methods are limited in their control of each object's
placement or appearance, as they represent the scene as a whole. This can be a
major issue in scenarios that require refining or manipulating objects in the
scene. To remedy this deficit, we propose a novel GlobalLocal training
framework for synthesizing a 3D scene using object proxies. A proxy represents
the object's placement in the generated scene and optionally defines its coarse
geometry. The key to our approach is to represent each object as an independent
NeRF. We alternate between optimizing each NeRF on its own and as part of the
full scene. Thus, a complete representation of each object can be learned,
while also creating a harmonious scene with style and lighting match. We show
that using proxies allows a wide variety of editing options, such as adjusting
the placement of each independent object, removing objects from a scene, or
refining an object. Our results show that Set-the-Scene offers a powerful
solution for scene synthesis and manipulation, filling a crucial gap in
controllable text-to-3D synthesis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>project page at https://danacohen95.github.io/Set-the-Scene/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CLIP for All Things Zero-Shot Sketch-Based Image Retrieval, Fine-Grained
  or Not <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13440v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13440v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aneeshan Sain, Ayan Kumar Bhunia, Pinaki Nath Chowdhury, Subhadeep Koley, Tao Xiang, Yi-Zhe Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we leverage CLIP for zero-shot sketch based image retrieval
(ZS-SBIR). We are largely inspired by recent advances on foundation models and
the unparalleled generalisation ability they seem to offer, but for the first
time tailor it to benefit the sketch community. We put forward novel designs on
how best to achieve this synergy, for both the category setting and the
fine-grained setting ("all"). At the very core of our solution is a prompt
learning setup. First we show just via factoring in sketch-specific prompts, we
already have a category-level ZS-SBIR system that overshoots all prior arts, by
a large margin (24.8%) - a great testimony on studying the CLIP and ZS-SBIR
synergy. Moving onto the fine-grained setup is however trickier, and requires a
deeper dive into this synergy. For that, we come up with two specific designs
to tackle the fine-grained matching nature of the problem: (i) an additional
regularisation loss to ensure the relative separation between sketches and
photos is uniform across categories, which is not the case for the gold
standard standalone triplet loss, and (ii) a clever patch shuffling technique
to help establishing instance-level structural correspondences between
sketch-photo pairs. With these designs, we again observe significant
performance gains in the region of 26.9% over previous state-of-the-art. The
take-home message, if any, is the proposed CLIP and prompt learning paradigm
carries great promise in tackling other sketch-related tasks (not limited to
ZS-SBIR) where data scarcity remains a great challenge. Code and models will be
made available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in Computer Vision and Pattern Recognition (CVPR), 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video
  Generators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13439v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13439v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, Humphrey Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent text-to-video generation approaches rely on computationally heavy
training and require large-scale video datasets. In this paper, we introduce a
new task of zero-shot text-to-video generation and propose a low-cost approach
(without any training or optimization) by leveraging the power of existing
text-to-image synthesis methods (e.g., Stable Diffusion), making them suitable
for the video domain.
  Our key modifications include (i) enriching the latent codes of the generated
frames with motion dynamics to keep the global scene and the background time
consistent; and (ii) reprogramming frame-level self-attention using a new
cross-frame attention of each frame on the first frame, to preserve the
context, appearance, and identity of the foreground object.
  Experiments show that this leads to low overhead, yet high-quality and
remarkably consistent video generation. Moreover, our approach is not limited
to text-to-video synthesis but is also applicable to other tasks such as
conditional and content-specialized video generation, and Video
Instruct-Pix2Pix, i.e., instruction-guided video editing.
  As experiments show, our method performs comparably or sometimes better than
recent approaches, despite not being trained on additional video data. Our code
will be open sourced at: https://github.com/Picsart-AI-Research/Text2Video-Zero .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The project is available at:
  https://github.com/Picsart-AI-Research/Text2Video-Zero</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Patch-Mix <span class="highlight-title">Transformer</span> for Unsupervised Domain Adaptation: A Game
  Perspective <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13434v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13434v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinjing Zhu, Haotian Bai, Lin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Endeavors have been recently made to leverage the vision transformer (ViT)
for the challenging unsupervised domain adaptation (UDA) task. They typically
adopt the cross-attention in ViT for direct domain alignment. However, as the
performance of cross-attention highly relies on the quality of pseudo labels
for targeted samples, it becomes less effective when the domain gap becomes
large. We solve this problem from a game theory's perspective with the proposed
model dubbed as PMTrans, which bridges source and target domains with an
intermediate domain. Specifically, we propose a novel ViT-based module called
PatchMix that effectively builds up the intermediate domain, i.e., probability
distribution, by learning to sample patches from both domains based on the
game-theoretical models. This way, it learns to mix the patches from the source
and target domains to maximize the cross entropy (CE), while exploiting two
semi-supervised mixup losses in the feature and label spaces to minimize it. As
such, we interpret the process of UDA as a min-max CE game with three players,
including the feature extractor, classifier, and PatchMix, to find the Nash
Equilibria. Moreover, we leverage attention maps from ViT to re-weight the
label of each patch by its importance, making it possible to obtain more
domain-discriminative feature representations. We conduct extensive experiments
on four benchmark datasets, and the results show that PMTrans significantly
surpasses the ViT-based and CNN-based SoTA methods by +3.6% on Office-Home,
+1.4% on Office-31, and +17.7% on DomainNet, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2023 (Highlight)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Medical diffusion on a budget: textual inversion for medical image
  generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13430v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13430v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bram de Wilde, Anindo Saha, Richard P. G. ten Broek, Henkjan Huisman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion-based models for text-to-image generation have gained immense
popularity due to recent advancements in efficiency, accessibility, and
quality. Although it is becoming increasingly feasible to perform inference
with these systems using consumer-grade GPUs, training them from scratch still
requires access to large datasets and significant computational resources. In
the case of medical image generation, the availability of large, publicly
accessible datasets that include text reports is limited due to legal and
ethical concerns. While training a diffusion model on a private dataset may
address this issue, it is not always feasible for institutions lacking the
necessary computational resources. This work demonstrates that pre-trained
Stable Diffusion models, originally trained on natural images, can be adapted
to various medical imaging modalities by training text embeddings with textual
inversion. In this study, we conducted experiments using medical datasets
comprising only 100 samples from three medical modalities. Embeddings were
trained in a matter of hours, while still retaining diagnostic relevance in
image generation. Experiments were designed to achieve several objectives.
Firstly, we fine-tuned the training and inference processes of textual
inversion, revealing that larger embeddings and more examples are required.
Secondly, we validated our approach by demonstrating a 2\% increase in the
diagnostic accuracy (AUC) for detecting prostate cancer on MRI, which is a
challenging multi-modal imaging modality, from 0.78 to 0.80. Thirdly, we
performed simulations by interpolating between healthy and diseased states,
combining multiple pathologies, and inpainting to show embedding flexibility
and control of disease appearance. Finally, the embeddings trained in this
study are small (less than 1 MB), which facilitates easy sharing of medical
data with reduced privacy concerns.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Low-Light Image Enhancement by Learning Contrastive Representations in
  Spatial and Frequency Domains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13412v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13412v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Huang, Xiaoguang Tu, Gui Fu, Tingting Liu, Bokai Liu, Ming Yang, Ziliang Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Images taken under low-light conditions tend to suffer from poor visibility,
which can decrease image quality and even reduce the performance of the
downstream tasks. It is hard for a CNN-based method to learn generalized
features that can recover normal images from the ones under various unknow
low-light conditions. In this paper, we propose to incorporate the contrastive
learning into an illumination correction network to learn abstract
representations to distinguish various low-light conditions in the
representation space, with the purpose of enhancing the generalizability of the
network. Considering that light conditions can change the frequency components
of the images, the representations are learned and compared in both spatial and
frequency domains to make full advantage of the contrastive learning. The
proposed method is evaluated on LOL and LOL-V2 datasets, the results show that
the proposed method achieves better qualitative and quantitative results
compared with other state-of-the-arts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SC-MIL: Supervised Contrastive Multiple Instance Learning for Imbalanced
  Classification in Pathology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13405v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13405v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dinkar Juyal, Siddhant Shingi, Syed Ashar Javed, Harshith Padigela, Chintan Shah, Anand Sampat, Archit Khosla, John Abel, Amaro Taylor-Weiner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multiple Instance learning (MIL) models have been extensively used in
pathology to predict biomarkers and risk-stratify patients from gigapixel-sized
images. Machine learning problems in medical imaging often deal with rare
diseases, making it important for these models to work in a label-imbalanced
setting. Furthermore, these imbalances can occur in out-of-distribution (OOD)
datasets when the models are deployed in the real-world. We leverage the idea
that decoupling feature and classifier learning can lead to improved decision
boundaries for label imbalanced datasets. To this end, we investigate the
integration of supervised contrastive learning with multiple instance learning
(SC-MIL). Specifically, we propose a joint-training MIL framework in the
presence of label imbalance that progressively transitions from learning
bag-level representations to optimal classifier learning. We perform
experiments with different imbalance settings for two well-studied problems in
cancer pathology: subtyping of non-small cell lung cancer and subtyping of
renal cell carcinoma. SC-MIL provides large and consistent improvements over
other techniques on both in-distribution (ID) and OOD held-out sets across
multiple imbalanced settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MSFA-Frequency-Aware <span class="highlight-title">Transformer</span> for Hyperspectral Images Demosaicing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13404v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13404v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haijin Zeng, Kai Feng, Shaoguang Huang, Jiezhang Cao, Yongyong Chen, Hongyan Zhang, Hiep Luong, Wilfried Philips
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hyperspectral imaging systems that use multispectral filter arrays (MSFA)
capture only one spectral component in each pixel. Hyperspectral demosaicing is
used to recover the non-measured components. While deep learning methods have
shown promise in this area, they still suffer from several challenges,
including limited modeling of non-local dependencies, lack of consideration of
the periodic MSFA pattern that could be linked to periodic artifacts, and
difficulty in recovering high-frequency details. To address these challenges,
this paper proposes a novel de-mosaicing framework, the MSFA-frequency-aware
Transformer network (FDM-Net). FDM-Net integrates a novel MSFA-frequency-aware
multi-head self-attention mechanism (MaFormer) and a filter-based Fourier
zero-padding method to reconstruct high pass components with greater difficulty
and low pass components with relative ease, separately. The advantage of
Maformer is that it can leverage the MSFA information and non-local
dependencies present in the data. Additionally, we introduce a joint spatial
and frequency loss to transfer MSFA information and enhance training on
frequency components that are hard to recover. Our experimental results
demonstrate that FDM-Net outperforms state-of-the-art methods with 6dB PSNR,
and reconstructs high-fidelity details successfully.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimization and Optimizers for Adversarial Robustness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13401v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13401v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hengyue Liang, Buyun Liang, Le Peng, Ying Cui, Tim Mitchell, Ju Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Empirical robustness evaluation (RE) of deep learning models against
adversarial perturbations entails solving nontrivial constrained optimization
problems. Existing numerical algorithms that are commonly used to solve them in
practice predominantly rely on projected gradient, and mostly handle
perturbations modeled by the $\ell_1$, $\ell_2$ and $\ell_\infty$ distances. In
this paper, we introduce a novel algorithmic framework that blends a
general-purpose constrained-optimization solver PyGRANSO with Constraint
Folding (PWCF), which can add more reliability and generality to the
state-of-the-art RE packages, e.g., AutoAttack. Regarding reliability, PWCF
provides solutions with stationarity measures and feasibility tests to assess
the solution quality. For generality, PWCF can handle perturbation models that
are typically inaccessible to the existing projected gradient methods; the main
requirement is the distance metric to be almost everywhere differentiable.
Taking advantage of PWCF and other existing numerical algorithms, we further
explore the distinct patterns in the solutions found for solving these
optimization problems using various combinations of losses, perturbation
models, and optimization algorithms. We then discuss the implications of these
patterns on the current robustness evaluation and adversarial training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-granularity Interaction Simulation for Unsupervised Interactive
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13399v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13399v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kehan Li, Yian Zhao, Zhennan Wang, Zesen Cheng, Peng Jin, Xiangyang Ji, Li Yuan, Chang Liu, Jie Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interactive segmentation enables users to segment as needed by providing cues
of objects, which introduces human-computer interaction for many fields, such
as image editing and medical image analysis. Typically, massive and expansive
pixel-level annotations are spent to train deep models by object-oriented
interactions with manually labeled object masks. In this work, we reveal that
informative interactions can be made by simulation with semantic-consistent yet
diverse region exploration in an unsupervised paradigm. Concretely, we
introduce a Multi-granularity Interaction Simulation (MIS) approach to open up
a promising direction for unsupervised interactive segmentation. Drawing on the
high-quality dense features produced by recent self-supervised models, we
propose to gradually merge patches or regions with similar features to form
more extensive regions and thus, every merged region serves as a
semantic-meaningful multi-granularity proposal. By randomly sampling these
proposals and simulating possible interactions based on them, we provide
meaningful interaction at multiple granularities to teach the model to
understand interactions. Our MIS significantly outperforms non-deep learning
unsupervised methods and is even comparable with some previous deep-supervised
methods without any annotation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DDT: A Diffusion-Driven <span class="highlight-title">Transformer</span>-based Framework for Human Mesh
  Recovery from a Video 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13397v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13397v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ce Zheng, Guo-Jun Qi, Chen Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human mesh recovery (HMR) provides rich human body information for various
real-world applications such as gaming, human-computer interaction, and virtual
reality. Compared to single image-based methods, video-based methods can
utilize temporal information to further improve performance by incorporating
human body motion priors. However, many-to-many approaches such as VIBE suffer
from motion smoothness and temporal inconsistency. While many-to-one approaches
such as TCMR and MPS-Net rely on the future frames, which is non-causal and
time inefficient during inference. To address these challenges, a novel
Diffusion-Driven Transformer-based framework (DDT) for video-based HMR is
presented. DDT is designed to decode specific motion patterns from the input
sequence, enhancing motion smoothness and temporal consistency. As a
many-to-many approach, the decoder of our DDT outputs the human mesh of all the
frames, making DDT more viable for real-world applications where time
efficiency is crucial and a causal model is desired. Extensive experiments are
conducted on the widely used datasets (Human3.6M, MPI-INF-3DHP, and 3DPW),
which demonstrated the effectiveness and efficiency of our DDT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zero-guidance Segmentation Using Zero Segment Labels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13396v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13396v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pitchaporn Rewatbowornwong, Nattanat Chatthee, Ekapol Chuangsuwanich, Supasorn Suwajanakorn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  CLIP has enabled new and exciting joint vision-language applications, one of
which is open-vocabulary segmentation, which can locate any segment given an
arbitrary text query. In our research, we ask whether it is possible to
discover semantic segments without any user guidance in the form of text
queries or predefined classes, and label them using natural language
automatically? We propose a novel problem zero-guidance segmentation and the
first baseline that leverages two pre-trained generalist models, DINO and CLIP,
to solve this problem without any fine-tuning or segmentation dataset. The
general idea is to first segment an image into small over-segments, encode them
into CLIP's visual-language space, translate them into text labels, and merge
semantically similar segments together. The key challenge, however, is how to
encode a visual segment into a segment-specific embedding that balances global
and local context information, both useful for recognition. Our main
contribution is a novel attention-masking technique that balances the two
contexts by analyzing the attention layers inside CLIP. We also introduce
several metrics for the evaluation of this new task. With CLIP's innate
knowledge, our method can precisely locate the Mona Lisa painting among a
museum crowd. Project page: https://zero-guide-seg.github.io/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Xplainer: From X-Ray Observations to Explainable Zero-Shot Diagnosis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13391v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13391v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chantal Pellegrini, Matthias Keicher, Ege Özsoy, Petra Jiraskova, Rickmer Braren, Nassir Navab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated diagnosis prediction from medical images is a valuable resource to
support clinical decision-making. However, such systems usually need to be
trained on large amounts of annotated data, which often is scarce in the
medical domain. Zero-shot methods address this challenge by allowing a flexible
adaption to new settings with different clinical findings without relying on
labeled data. Further, to integrate automated diagnosis in the clinical
workflow, methods should be transparent and explainable, increasing medical
professionals' trust and facilitating correctness verification. In this work,
we introduce Xplainer, a novel framework for explainable zero-shot diagnosis in
the clinical setting. Xplainer adapts the classification-by-description
approach of contrastive vision-language models to the multi-label medical
diagnosis task. Specifically, instead of directly predicting a diagnosis, we
prompt the model to classify the existence of descriptive observations, which a
radiologist would look for on an X-Ray scan, and use the descriptor
probabilities to estimate the likelihood of a diagnosis. Our model is
explainable by design, as the final diagnosis prediction is directly based on
the prediction of the underlying descriptors. We evaluate Xplainer on two chest
X-ray datasets, CheXpert and ChestX-ray14, and demonstrate its effectiveness in
improving the performance and explainability of zero-shot diagnosis. Our
results suggest that Xplainer provides a more detailed understanding of the
decision-making process and can be a valuable tool for clinical diagnosis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 2 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Plug-and-Play Regulators for Image-Text Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13371v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13371v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haiwen Diao, Ying Zhang, Wei Liu, Xiang Ruan, Huchuan Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Exploiting fine-grained correspondence and visual-semantic alignments has
shown great potential in image-text matching. Generally, recent approaches
first employ a cross-modal attention unit to capture latent region-word
interactions, and then integrate all the alignments to obtain the final
similarity. However, most of them adopt one-time forward association or
aggregation strategies with complex architectures or additional information,
while ignoring the regulation ability of network feedback. In this paper, we
develop two simple but quite effective regulators which efficiently encode the
message output to automatically contextualize and aggregate cross-modal
representations. Specifically, we propose (i) a Recurrent Correspondence
Regulator (RCR) which facilitates the cross-modal attention unit progressively
with adaptive attention factors to capture more flexible correspondence, and
(ii) a Recurrent Aggregation Regulator (RAR) which adjusts the aggregation
weights repeatedly to increasingly emphasize important alignments and dilute
unimportant ones. Besides, it is interesting that RCR and RAR are
plug-and-play: both of them can be incorporated into many frameworks based on
cross-modal interaction to obtain significant benefits, and their cooperation
achieves further improvements. Extensive experiments on MSCOCO and Flickr30K
datasets validate that they can bring an impressive and consistent R@1 gain on
multiple models, confirming the general effectiveness and generalization
ability of the proposed methods. Code and pre-trained models are available at:
https://github.com/Paranioar/RCAR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 9 figures, Accepted by TIP2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ POTTER: Pooling Attention <span class="highlight-title">Transformer</span> for Efficient Human Mesh Recovery <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13357v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13357v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ce Zheng, Xianpeng Liu, Guo-Jun Qi, Chen Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer architectures have achieved SOTA performance on the human mesh
recovery (HMR) from monocular images. However, the performance gain has come at
the cost of substantial memory and computational overhead. A lightweight and
efficient model to reconstruct accurate human mesh is needed for real-world
applications. In this paper, we propose a pure transformer architecture named
POoling aTtention TransformER (POTTER) for the HMR task from single images.
Observing that the conventional attention module is memory and computationally
expensive, we propose an efficient pooling attention module, which
significantly reduces the memory and computational cost without sacrificing
performance. Furthermore, we design a new transformer architecture by
integrating a High-Resolution (HR) stream for the HMR task. The high-resolution
local and global features from the HR stream can be utilized for recovering
more accurate human mesh. Our POTTER outperforms the SOTA method METRO by only
requiring 7% of total parameters and 14% of the Multiply-Accumulate Operations
on the Human3.6M (PA-MPJPE metric) and 3DPW (all three metrics) datasets. The
project webpage is https://zczcwh.github.io/potter_page.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Increasing Textual Context Size Boosts Medical Image-Text Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13340v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13340v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Idan Glassberg, Tom Hope
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This short technical report demonstrates a simple technique that yields state
of the art results in medical image-text matching tasks. We analyze the use of
OpenAI's CLIP, a general image-text matching model, and observe that CLIP's
limited textual input size has negative impact on downstream performance in the
medical domain where encoding longer textual contexts is often required. We
thus train and release ClipMD, which is trained with a simple sliding window
technique to encode textual captions. ClipMD was tested on two medical
image-text datasets and compared with other image-text matching models. The
results show that ClipMD outperforms other models on both datasets by a large
margin. We make our code and pretrained model publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Clinically Relevant Latent Space Embedding of Cancer Histopathology
  Slides through Variational Autoencoder Based Image Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13332v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13332v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Sadegh Nasr, Amir Hajighasemi, Paul Koomey, Parisa Boodaghi Malidarreh, Michael Robben, Jillur Rahman Saurav, Helen H. Shang, Manfred Huber, Jacob M. Luber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce a Variational Autoencoder (VAE) based training
approach that can compress and decompress cancer pathology slides at a
compression ratio of 1:512, which is better than the previously reported state
of the art (SOTA) in the literature, while still maintaining accuracy in
clinical validation tasks. The compression approach was tested on more common
computer vision datasets such as CIFAR10, and we explore which image
characteristics enable this compression ratio on cancer imaging data but not
generic images. We generate and visualize embeddings from the compressed latent
space and demonstrate how they are useful for clinical interpretation of data,
and how in the future such latent embeddings can be used to accelerate search
of clinical imaging data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DARE-GRAM : Unsupervised Domain Adaptation Regression by Aligning
  Inverse Gram Matrices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13325v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13325v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ismail Nejjar, Qin Wang, Olga Fink
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised Domain Adaptation Regression (DAR) aims to bridge the domain gap
between a labeled source dataset and an unlabelled target dataset for
regression problems. Recent works mostly focus on learning a deep feature
encoder by minimizing the discrepancy between source and target features. In
this work, we present a different perspective for the DAR problem by analyzing
the closed-form ordinary least square~(OLS) solution to the linear regressor in
the deep domain adaptation context. Rather than aligning the original feature
embedding space, we propose to align the inverse Gram matrix of the features,
which is motivated by its presence in the OLS solution and the Gram matrix's
ability to capture the feature correlations. Specifically, we propose a simple
yet effective DAR method which leverages the pseudo-inverse low-rank property
to align the scale and angle in a selected subspace generated by the
pseudo-inverse Gram matrix of the two domains. We evaluate our method on three
domain adaptation regression benchmarks. Experimental results demonstrate that
our method achieves state-of-the-art performance. Our code is available at
https://github.com/ismailnejjar/DARE-GRAM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Generalization with Domain Convex Game <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13297v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13297v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangrui Lv, Jian Liang, Shuang Li, Jinming Zhang, Di Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain generalization (DG) tends to alleviate the poor generalization
capability of deep neural networks by learning model with multiple source
domains. A classical solution to DG is domain augmentation, the common belief
of which is that diversifying source domains will be conducive to the
out-of-distribution generalization. However, these claims are understood
intuitively, rather than mathematically. Our explorations empirically reveal
that the correlation between model generalization and the diversity of domains
may be not strictly positive, which limits the effectiveness of domain
augmentation. This work therefore aim to guarantee and further enhance the
validity of this strand. To this end, we propose a new perspective on DG that
recasts it as a convex game between domains. We first encourage each
diversified domain to enhance model generalization by elaborately designing a
regularization term based on supermodularity. Meanwhile, a sample filter is
constructed to eliminate low-quality samples, thereby avoiding the impact of
potentially harmful information. Our framework presents a new avenue for the
formal analysis of DG, heuristic analysis and extensive experiments demonstrate
the rationality and effectiveness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Considerations on the Evaluation of Biometric Quality Assessment
  Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13294v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13294v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Torsten Schlett, Christian Rathgeb, Juan Tapia, Christoph Busch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quality assessment algorithms can be used to estimate the utility of a
biometric sample for the purpose of biometric recognition. "Error versus
Discard Characteristic" (EDC) plots, and "partial Area Under Curve" (pAUC)
values of curves therein, are generally used by researchers to evaluate the
predictive performance of such quality assessment algorithms. An EDC curve
depends on an error type such as the "False Non Match Rate" (FNMR), a quality
assessment algorithm, a biometric recognition system, a set of comparisons each
corresponding to a biometric sample pair, and a comparison score threshold
corresponding to a starting error. To compute an EDC curve, comparisons are
progressively discarded based on the associated samples' lowest quality scores,
and the error is computed for the remaining comparisons. Additionally, a
discard fraction limit or range must be selected to compute pAUC values, which
can then be used to quantitatively rank quality assessment algorithms.
  This paper discusses and analyses various details for this kind of quality
assessment algorithm evaluation, including general EDC properties,
interpretability improvements for pAUC values based on a hard lower error limit
and a soft upper error limit, the use of relative instead of discrete rankings,
stepwise vs. linear curve interpolation, and normalisation of quality scores to
a [0, 100] integer range. We also analyse the stability of quantitative quality
assessment algorithm rankings based on pAUC values across varying pAUC discard
fraction limits and starting errors, concluding that higher pAUC discard
fraction limits should be preferred. The analyses are conducted both with
synthetic data and with real data for a face image quality assessment scenario,
with a focus on general modality-independent conclusions for EDC evaluations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LABRAD-OR: Lightweight Memory Scene Graphs for Accurate Bimodal
  Reasoning in Dynamic Operating Rooms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13293v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13293v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ege Özsoy, Tobias Czempiel, Felix Holm, Chantal Pellegrini, Nassir Navab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern surgeries are performed in complex and dynamic settings, including
ever-changing interactions between medical staff, patients, and equipment. The
holistic modeling of the operating room (OR) is, therefore, a challenging but
essential task, with the potential to optimize the performance of surgical
teams and aid in developing new surgical technologies to improve patient
outcomes. The holistic representation of surgical scenes as semantic scene
graphs (SGG), where entities are represented as nodes and relations between
them as edges, is a promising direction for fine-grained semantic OR
understanding. We propose, for the first time, the use of temporal information
for more accurate and consistent holistic OR modeling. Specifically, we
introduce memory scene graphs, where the scene graphs of previous time steps
act as the temporal representation guiding the current prediction. We design an
end-to-end architecture that intelligently fuses the temporal information of
our lightweight memory scene graphs with the visual information from point
clouds and images. We evaluate our method on the 4D-OR dataset and demonstrate
that integrating temporality leads to more accurate and consistent results
achieving an +5% increase and a new SOTA of 0.88 in macro F1. This work opens
the path for representing the entire surgery history with memory scene graphs
and improves the holistic understanding in the OR. Introducing scene graphs as
memory representations can offer a valuable tool for many temporal
understanding tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised Deep Probabilistic Approach for Partial Point Cloud
  Registration <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13290v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13290v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guofeng Mei, Hao Tang, Xiaoshui Huang, Weijie Wang, Juan Liu, Jian Zhang, Luc Van Gool, Qiang Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep point cloud registration methods face challenges to partial overlaps and
rely on labeled data. To address these issues, we propose UDPReg, an
unsupervised deep probabilistic registration framework for point clouds with
partial overlaps. Specifically, we first adopt a network to learn posterior
probability distributions of Gaussian mixture models (GMMs) from point clouds.
To handle partial point cloud registration, we apply the Sinkhorn algorithm to
predict the distribution-level correspondences under the constraint of the
mixing weights of GMMs. To enable unsupervised learning, we design three
distribution consistency-based losses: self-consistency, cross-consistency, and
local contrastive. The self-consistency loss is formulated by encouraging GMMs
in Euclidean and feature spaces to share identical posterior distributions. The
cross-consistency loss derives from the fact that the points of two partially
overlapping point clouds belonging to the same clusters share the cluster
centroids. The cross-consistency loss allows the network to flexibly learn a
transformation-invariant posterior distribution of two aligned point clouds.
The local contrastive loss facilitates the network to extract discriminative
local features. Our UDPReg achieves competitive performance on the
3DMatch/3DLoMatch and ModelNet/ModelLoNet benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visual-Language <span class="highlight-title">Prompt</span> Tuning with Knowledge-guided Context Optimization <span class="chip">CVPR23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13283v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13283v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hantao Yao, Rui Zhang, Changsheng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt tuning is an effective way to adapt the pre-trained visual-language
model (VLM) to the downstream task using task-related textual tokens.
Representative CoOp-based work combines the learnable textual tokens with the
class tokens to obtain specific textual knowledge. However, the specific
textual knowledge is the worse generalization to the unseen classes because it
forgets the essential general textual knowledge having a strong generalization
ability. To tackle this issue, we introduce a novel Knowledge-guided Context
Optimization (KgCoOp) to enhance the generalization ability of the learnable
prompt for unseen classes. The key insight of KgCoOp is that forgetting about
essential knowledge can be alleviated by reducing the discrepancy between the
learnable prompt and the hand-crafted prompt. Especially, KgCoOp minimizes the
discrepancy between the textual embeddings generated by learned prompts and the
hand-crafted prompts. Finally, adding the KgCoOp upon the contrastive loss can
make a discriminative prompt for both seen and unseen tasks. Extensive
evaluation of several benchmarks demonstrates that the proposed
Knowledge-guided Context Optimization is an efficient method for prompt tuning,
\emph{i.e.,} achieves better performance with less training time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by CVPR23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improved Anisotropic Gaussian Filters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13278v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13278v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex Keilmann, Michael Godehardt, Ali Moghiseh, Claudia Redenbach, Katja Schladitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Elongated anisotropic Gaussian filters are used for the orientation
estimation of fibers. In cases where computed tomography images are noisy,
roughly resolved, and of low contrast, they are the method of choice even if
being efficient only in virtual 2D slices. However, minor inaccuracies in the
anisotropic Gaussian filters can carry over to the orientation estimation.
Therefore, we propose a modified algorithm for 2D anisotropic Gaussian filters
and show that this improves their precision. Applied to synthetic images of
fiber bundles, it is more accurate and robust to noise. Finally, we demonstrate
the effectiveness of our approach by applying it to real-world images of sheet
molding compounds.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SINE: Semantic-driven Image-based NeRF Editing with Prior-guided Editing
  Field <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13277v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13277v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, Zhaopeng Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the great success in 2D editing using user-friendly tools, such as
Photoshop, semantic strokes, or even text prompts, similar capabilities in 3D
areas are still limited, either relying on 3D modeling skills or allowing
editing within only a few categories.In this paper, we present a novel
semantic-driven NeRF editing approach, which enables users to edit a neural
radiance field with a single image, and faithfully delivers edited novel views
with high fidelity and multi-view consistency.To achieve this goal, we propose
a prior-guided editing field to encode fine-grained geometric and texture
editing in 3D space, and develop a series of techniques to aid the editing
process, including cyclic constraints with a proxy mesh to facilitate geometric
supervision, a color compositing mechanism to stabilize semantic-driven texture
editing, and a feature-cluster-based regularization to preserve the irrelevant
content unchanged.Extensive experiments and editing examples on both real-world
and synthetic data demonstrate that our method achieves photo-realistic 3D
editing using only a single edited image, pushing the bound of semantic-driven
editing in 3D real-world scenes. Our project webpage:
https://zju3dv.github.io/sine/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2023. Project Page: https://zju3dv.github.io/sine/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TAPS3D: Text-Guided 3D Textured Shape Generation from Pseudo Supervision <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13273v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13273v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiacheng Wei, Hao Wang, Jiashi Feng, Guosheng Lin, Kim-Hui Yap
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we investigate an open research task of generating
controllable 3D textured shapes from the given textual descriptions. Previous
works either require ground truth caption labeling or extensive optimization
time. To resolve these issues, we present a novel framework, TAPS3D, to train a
text-guided 3D shape generator with pseudo captions. Specifically, based on
rendered 2D images, we retrieve relevant words from the CLIP vocabulary and
construct pseudo captions using templates. Our constructed captions provide
high-level semantic supervision for generated 3D shapes. Further, in order to
produce fine-grained textures and increase geometry diversity, we propose to
adopt low-level image regularization to enable fake-rendered images to align
with the real ones. During the inference phase, our proposed model can generate
3D textured shapes from the given text without any additional optimization. We
conduct extensive experiments to analyze each of our proposed components and
show the efficacy of our framework in generating high-fidelity 3D textured and
text-relevant shapes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Disguise without Disruption: Utility-Preserving Face De-Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13269v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13269v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zikui Cai, Zhongpai Gao, Benjamin Planche, Meng Zheng, Terrence Chen, M. Salman Asif, Ziyan Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increasing ubiquity of cameras and smart sensors, humanity is
generating data at an exponential rate. Access to this trove of information,
often covering yet-underrepresented use-cases (e.g., AI in medical settings)
could fuel a new generation of deep-learning tools. However, eager data
scientists should first provide satisfying guarantees w.r.t. the privacy of
individuals present in these untapped datasets. This is especially important
for images or videos depicting faces, as their biometric information is the
target of most identification methods. While a variety of solutions have been
proposed to de-identify such images, they often corrupt other non-identifying
facial attributes that would be relevant for downstream tasks. In this paper,
we propose Disguise, a novel algorithm to seamlessly de-identify facial images
while ensuring the usability of the altered data. Unlike prior arts, we ground
our solution in both differential privacy and ensemble-learning research
domains. Our method extracts and swaps depicted identities with fake ones,
synthesized via variational mechanisms to maximize obfuscation and
non-invertibility; while leveraging the supervision from a mixture-of-experts
to disentangle and preserve other utility attributes. We extensively evaluate
our method on multiple datasets, demonstrating higher de-identification rate
and superior consistency than prior art w.r.t. various downstream tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>paper + supplementary material</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Bag-of-Prototypes Representation for <span class="highlight-title">Dataset</span>-Level Applications <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13251v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13251v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weijie Tu, Weijian Deng, Tom Gedeon, Liang Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work investigates dataset vectorization for two dataset-level tasks:
assessing training set suitability and test set difficulty. The former measures
how suitable a training set is for a target domain, while the latter studies
how challenging a test set is for a learned model. Central to the two tasks is
measuring the underlying relationship between datasets. This needs a desirable
dataset vectorization scheme, which should preserve as much discriminative
dataset information as possible so that the distance between the resulting
dataset vectors can reflect dataset-to-dataset similarity. To this end, we
propose a bag-of-prototypes (BoP) dataset representation that extends the
image-level bag consisting of patch descriptors to dataset-level bag consisting
of semantic prototypes. Specifically, we develop a codebook consisting of K
prototypes clustered from a reference dataset. Given a dataset to be encoded,
we quantize each of its image features to a certain prototype in the codebook
and obtain a K-dimensional histogram. Without assuming access to dataset
labels, the BoP representation provides a rich characterization of the dataset
semantic distribution. Furthermore, BoP representations cooperate well with
Jensen-Shannon divergence for measuring dataset-to-dataset similarity. Although
very simple, BoP consistently shows its advantage over existing representations
on a series of benchmarks for two dataset-level tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023 camera-ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CrOC: Cross-View Online Clustering for Dense Visual Representation
  Learning <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13245v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13245v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Stegmüller, Tim Lebailly, Behzad Bozorgtabar, Tinne Tuytelaars, Jean-Philippe Thiran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning dense visual representations without labels is an arduous task and
more so from scene-centric data. We propose to tackle this challenging problem
by proposing a Cross-view consistency objective with an Online Clustering
mechanism (CrOC) to discover and segment the semantics of the views. In the
absence of hand-crafted priors, the resulting method is more generalizable and
does not require a cumbersome pre-processing step. More importantly, the
clustering algorithm conjointly operates on the features of both views, thereby
elegantly bypassing the issue of content not represented in both views and the
ambiguous matching of objects from one crop to the other. We demonstrate
excellent performance on linear and unsupervised segmentation transfer tasks on
various datasets and similarly for video object segmentation. Our code and
pre-trained models are publicly available at https://github.com/stegmuel/CrOC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2023, * denotes equal contribution</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 6D Object Pose Estimation from Approximate 3D Models for Orbital
  Robotics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13241v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13241v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maximilian Ulmer, Maximilian Durner, Martin Sundermeyer, Manuel Stoiber, Rudolph Triebel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel technique to estimate the 6D pose of objects from single
images where the 3D geometry of the object is only given approximately and not
as a precise 3D model. To achieve this, we employ a dense 2D-to-3D
correspondence predictor that regresses 3D model coordinates for every pixel.
In addition to the 3D coordinates, our model also estimates the pixel-wise
coordinate error to discard correspondences that are likely wrong. This allows
us to generate multiple 6D pose hypotheses of the object, which we then refine
iteratively using a highly efficient region-based approach. We also introduce a
novel pixel-wise posterior formulation by which we can estimate the probability
for each hypothesis and select the most likely one. As we show in experiments,
our approach is capable of dealing with extreme visual conditions including
overexposure, high contrast, or low signal-to-noise ratio. This makes it a
powerful technique for the particularly challenging task of estimating the pose
of tumbling satellites for in-orbit robotic applications. Our method achieves
state-of-the-art performance on the SPEED+ dataset and has won the SPEC2021
post-mortem competition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visually-<span class="highlight-title">Prompt</span>ed Language Model for Fine-Grained Scene Graph Generation
  in an Open World 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13233v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13233v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qifan Yu, Juncheng Li, Yu Wu, Siliang Tang, Wei Ji, Yueting Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scene Graph Generation (SGG) aims to extract <subject, predicate, object>
relationships in images for vision understanding. Although recent works have
made steady progress on SGG, they still suffer long-tail distribution issues
that tail-predicates are more costly to train and hard to distinguish due to a
small amount of annotated data compared to frequent predicates. Existing
re-balancing strategies try to haddle it via prior rules but are still confined
to pre-defined conditions, which are not scalable for various models and
datasets. In this paper, we propose a Cross-modal prediCate boosting (CaCao)
framework, where a visually-prompted language model is learned to generate
diverse fine-grained predicates in a low-resource way. The proposed CaCao can
be applied in a plug-and-play fashion and automatically strengthen existing SGG
to tackle the long-tailed problem. Based on that, we further introduce a novel
Entangled cross-modal prompt approach for open-world predicate scene graph
generation (Epic), where models can generalize to unseen predicates in a
zero-shot manner. Comprehensive experiments on three benchmark datasets show
that CaCao consistently boosts the performance of multiple scene graph
generation models in a model-agnostic way. Moreover, our Epic achieves
competitive performance on open-world predicate prediction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 16 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transforming Radiance Field with Lipschitz Network for Photorealistic 3D
  Scene Stylization <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13232v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13232v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zicheng Zhang, Yinglu Liu, Congying Han, Yingwei Pan, Tiande Guo, Ting Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in 3D scene representation and novel view synthesis have
witnessed the rise of Neural Radiance Fields (NeRFs). Nevertheless, it is not
trivial to exploit NeRF for the photorealistic 3D scene stylization task, which
aims to generate visually consistent and photorealistic stylized scenes from
novel views. Simply coupling NeRF with photorealistic style transfer (PST) will
result in cross-view inconsistency and degradation of stylized view syntheses.
Through a thorough analysis, we demonstrate that this non-trivial task can be
simplified in a new light: When transforming the appearance representation of a
pre-trained NeRF with Lipschitz mapping, the consistency and photorealism
across source views will be seamlessly encoded into the syntheses. That
motivates us to build a concise and flexible learning framework namely LipRF,
which upgrades arbitrary 2D PST methods with Lipschitz mapping tailored for the
3D scene. Technically, LipRF first pre-trains a radiance field to reconstruct
the 3D scene, and then emulates the style on each view by 2D PST as the prior
to learn a Lipschitz network to stylize the pre-trained appearance. In view of
that Lipschitz condition highly impacts the expressivity of the neural network,
we devise an adaptive regularization to balance the reconstruction and
stylization. A gradual gradient aggregation strategy is further introduced to
optimize LipRF in a cost-efficient manner. We conduct extensive experiments to
show the high quality and robust performance of LipRF on both photorealistic 3D
stylization and object appearance editing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023, Highlight</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Confidence-Aware and <span class="highlight-title">Self-Supervised</span> Image Anomaly Localisation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13227v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13227v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Johanna P. Müller, Matthew Baugh, Jeremy Tan, Mischa Dombrowski, Bernhard Kainz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Universal anomaly detection still remains a challenging problem in machine
learning and medical image analysis. It is possible to learn an expected
distribution from a single class of normative samples, e.g., through epistemic
uncertainty estimates, auto-encoding models, or from synthetic anomalies in a
self-supervised way. The performance of self-supervised anomaly detection
approaches is still inferior compared to methods that use examples from known
unknown classes to shape the decision boundary. However, outlier exposure
methods often do not identify unknown unknowns. Here we discuss an improved
self-supervised single-class training strategy that supports the approximation
of probabilistic inference with loosen feature locality constraints. We show
that up-scaling of gradients with histogram-equalised images is beneficial for
recently proposed self-supervision tasks. Our method is integrated into several
out-of-distribution (OOD) detection models and we show evidence that our method
outperforms the state-of-the-art on various benchmark datasets. Source code
will be publicly available by the time of the conference.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Structured Semantic Prior for Multi Label Recognition with
  Incomplete Labels <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13223v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13223v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixuan Ding, Ao Wang, Hui Chen, Qiang Zhang, Pengzhang Liu, Yongjun Bao, Weipeng Yan, Jungong Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-label recognition (MLR) with incomplete labels is very challenging.
Recent works strive to explore the image-to-label correspondence in the
vision-language model, \ie, CLIP~\cite{radford2021clip}, to compensate for
insufficient annotations. In spite of promising performance, they generally
overlook the valuable prior about the label-to-label correspondence. In this
paper, we advocate remedying the deficiency of label supervision for the MLR
with incomplete labels by deriving a structured semantic prior about the
label-to-label correspondence via a semantic prior prompter. We then present a
novel Semantic Correspondence Prompt Network (SCPNet), which can thoroughly
explore the structured semantic prior. A Prior-Enhanced Self-Supervised
Learning method is further introduced to enhance the use of the prior.
Comprehensive experiments and analyses on several widely used benchmark
datasets show that our method significantly outperforms existing methods on all
datasets, well demonstrating the effectiveness and the superiority of our
method. Our code will be available at https://github.com/jameslahm/SCPNet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explore the Power of Synthetic Data on Few-shot Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13221v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13221v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaobo Lin, Kun Wang, Xingyu Zeng, Rui Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-shot object detection (FSOD) aims to expand an object detector for novel
categories given only a few instances for training. The few training samples
restrict the performance of FSOD model. Recent text-to-image generation models
have shown promising results in generating high-quality images. How applicable
these synthetic images are for FSOD tasks remains under-explored. This work
extensively studies how synthetic images generated from state-of-the-art
text-to-image generators benefit FSOD tasks. We focus on two perspectives: (1)
How to use synthetic data for FSOD? (2) How to find representative samples from
the large-scale synthetic dataset? We design a copy-paste-based pipeline for
using synthetic data. Specifically, saliency object detection is applied to the
original generated image, and the minimum enclosing box is used for cropping
the main object based on the saliency map. After that, the cropped object is
randomly pasted on the image, which comes from the base dataset. We also study
the influence of the input text of text-to-image generator and the number of
synthetic images used. To construct a representative synthetic training
dataset, we maximize the diversity of the selected images via a sample-based
and cluster-based method. However, the severe problem of high false positives
(FP) ratio of novel categories in FSOD can not be solved by using synthetic
data. We propose integrating CLIP, a zero-shot recognition model, into the FSOD
pipeline, which can filter 90% of FP by defining a threshold for the similarity
score between the detected object and the text of the predicted category.
Extensive experiments on PASCAL VOC and MS COCO validate the effectiveness of
our method, in which performance gain is up to 21.9% compared to the few-shot
baseline.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Simple and Generic Framework for Feature Distillation via Channel-wise
  Transformation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13212v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13212v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziwei Liu, Yongtao Wang, Xiaojie Chu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge distillation is a popular technique for transferring the knowledge
from a large teacher model to a smaller student model by mimicking. However,
distillation by directly aligning the feature maps between teacher and student
may enforce overly strict constraints on the student thus degrade the
performance of the student model. To alleviate the above feature misalignment
issue, existing works mainly focus on spatially aligning the feature maps of
the teacher and the student, with pixel-wise transformation. In this paper, we
newly find that aligning the feature maps between teacher and student along the
channel-wise dimension is also effective for addressing the feature
misalignment issue. Specifically, we propose a learnable nonlinear channel-wise
transformation to align the features of the student and the teacher model.
Based on it, we further propose a simple and generic framework for feature
distillation, with only one hyper-parameter to balance the distillation loss
and the task specific loss. Extensive experimental results show that our method
achieves significant performance improvements in various computer vision tasks
including image classification (+3.28% top-1 accuracy for MobileNetV1 on
ImageNet-1K), object detection (+3.9% bbox mAP for ResNet50-based Faster-RCNN
on MS COCO), instance segmentation (+2.8% Mask mAP for ResNet50-based
Mask-RCNN), and semantic segmentation (+4.66% mIoU for ResNet18-based PSPNet in
semantic segmentation on Cityscapes), which demonstrates the effectiveness and
the versatility of the proposed method. The code will be made publicly
available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Don't FREAK Out: A Frequency-Inspired Approach to Detecting Backdoor
  Poisoned Samples in DNNs <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13211v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13211v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hasan Abed Al Kader Hammoud, Adel Bibi, Philip H. S. Torr, Bernard Ghanem
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we investigate the frequency sensitivity of Deep Neural
Networks (DNNs) when presented with clean samples versus poisoned samples. Our
analysis shows significant disparities in frequency sensitivity between these
two types of samples. Building on these findings, we propose FREAK, a
frequency-based poisoned sample detection algorithm that is simple yet
effective. Our experimental results demonstrate the efficacy of FREAK not only
against frequency backdoor attacks but also against some spatial attacks. Our
work is just the first step in leveraging these insights. We believe that our
analysis and proposed defense mechanism will provide a foundation for future
research and development of backdoor defenses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPRW (The Art of Robustness)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Taking A Closer Look at Visual Relation: Unbiased Video Scene Graph
  Generation with Decoupled Label Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13209v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13209v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenqing Wang, Yawei Luo, Zhiqing Chen, Tao Jiang, Lei Chen, Yi Yang, Jun Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current video-based scene graph generation (VidSGG) methods have been found
to perform poorly on predicting predicates that are less represented due to the
inherent biased distribution in the training data. In this paper, we take a
closer look at the predicates and identify that most visual relations (e.g.
sit_above) involve both actional pattern (sit) and spatial pattern (above),
while the distribution bias is much less severe at the pattern level. Based on
this insight, we propose a decoupled label learning (DLL) paradigm to address
the intractable visual relation prediction from the pattern-level perspective.
Specifically, DLL decouples the predicate labels and adopts separate
classifiers to learn actional and spatial patterns respectively. The patterns
are then combined and mapped back to the predicate. Moreover, we propose a
knowledge-level label decoupling method to transfer non-target knowledge from
head predicates to tail predicates within the same pattern to calibrate the
distribution of tail classes. We validate the effectiveness of DLL on the
commonly used VidSGG benchmark, i.e. VidVRD. Extensive experiments demonstrate
that the DLL offers a remarkably simple but highly effective solution to the
long-tailed problem, achieving the state-of-the-art VidSGG performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Confident Labelling Strategy Based on Deep Learning for Improving
  Early Detection of Knee OsteoArthritis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13203v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13203v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhe Wang, Aladine Chetouani, Rachid Jennane
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knee OsteoArthritis (KOA) is a prevalent musculoskeletal disorder that causes
decreased mobility in seniors. The diagnosis provided by physicians is
subjective, however, as it relies on personal experience and the
semi-quantitative Kellgren-Lawrence (KL) scoring system. KOA has been
successfully diagnosed by Computer-Aided Diagnostic (CAD) systems that use deep
learning techniques like Convolutional Neural Networks (CNN). In this paper, we
propose a novel Siamese-based network, and we introduce a new hybrid loss
strategy for the early detection of KOA. The model extends the classical
Siamese network by integrating a collection of Global Average Pooling (GAP)
layers for feature extraction at each level. Then, to improve the
classification performance, a novel training strategy that partitions each
training batch into low-, medium- and high-confidence subsets, and a specific
hybrid loss function are used for each new label attributed to each sample. The
final loss function is then derived by combining the latter loss functions with
optimized weights. Our test results demonstrate that our proposed approach
significantly improves the detection performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ First Session Adaptation: A Strong Replay-Free Baseline for
  Class-Incremental Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13199v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13199v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aristeidis Panos, Yuriko Kobe, Daniel Olmeda Reino, Rahaf Aljundi, Richard E. Turner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In Class-Incremental Learning (CIL) an image classification system is exposed
to new classes in each learning session and must be updated incrementally.
Methods approaching this problem have updated both the classification head and
the feature extractor body at each session of CIL. In this work, we develop a
baseline method, First Session Adaptation (FSA), that sheds light on the
efficacy of existing CIL approaches and allows us to assess the relative
performance contributions from head and body adaption. FSA adapts a pre-trained
neural network body only on the first learning session and fixes it thereafter;
a head based on linear discriminant analysis (LDA), is then placed on top of
the adapted body, allowing exact updates through CIL. FSA is replay-free
i.e.~it does not memorize examples from previous sessions of continual
learning. To empirically motivate FSA, we first consider a diverse selection of
22 image-classification datasets, evaluating different heads and body
adaptation techniques in high/low-shot offline settings. We find that the LDA
head performs well and supports CIL out-of-the-box. We also find that
Featurewise Layer Modulation (FiLM) adapters are highly effective in the
few-shot setting, and full-body adaption in the high-shot setting. Second, we
empirically investigate various CIL settings including high-shot CIL and
few-shot CIL, including settings that have previously been used in the
literature. We show that FSA significantly improves over the state-of-the-art
in 15 of the 16 settings considered. FSA with FiLM adapters is especially
performant in the few-shot setting. These results indicate that current
approaches to continuous body adaptation are not working as expected. Finally,
we propose a measure that can be applied to a set of unlabelled inputs which is
predictive of the benefits of body adaptation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Complementary Pseudo Multimodal Feature for Point Cloud Anomaly
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13194v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13194v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunkang Cao, Xiaohao Xu, Weiming Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Point cloud (PCD) anomaly detection steadily emerges as a promising research
area. This study aims to improve PCD anomaly detection performance by combining
handcrafted PCD descriptions with powerful pre-trained 2D neural networks. To
this end, this study proposes Complementary Pseudo Multimodal Feature (CPMF)
that incorporates local geometrical information in 3D modality using
handcrafted PCD descriptors and global semantic information in the generated
pseudo 2D modality using pre-trained 2D neural networks. For global semantics
extraction, CPMF projects the origin PCD into a pseudo 2D modality containing
multi-view images. These images are delivered to pre-trained 2D neural networks
for informative 2D modality feature extraction. The 3D and 2D modality features
are aggregated to obtain the CPMF for PCD anomaly detection. Extensive
experiments demonstrate the complementary capacity between 2D and 3D modality
features and the effectiveness of CPMF, with 95.15% image-level AU-ROC and
92.93% pixel-level PRO on the MVTec3D benchmark. Code is available on
https://github.com/caoyunkang/CPMF.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to Pattern Recognition. Code is available on
  https://github.com/caoyunkang/CPMF</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VADER: Video Alignment Differencing and Retrieval <span class="chip">ICCV2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13193v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13193v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Black, Simon Jenni, Tu Bui, Md. Mehrab Tanjim, Stefano Petrangeli, Ritwik Sinha, Viswanathan Swaminathan, John Collomosse
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose VADER, a spatio-temporal matching, alignment, and change
summarization method to help fight misinformation spread via manipulated
videos. VADER matches and coarsely aligns partial video fragments to candidate
videos using a robust visual descriptor and scalable search over adaptively
chunked video content. A transformer-based alignment module then refines the
temporal localization of the query fragment within the matched video. A
space-time comparator module identifies regions of manipulation between aligned
content, invariant to any changes due to any residual temporal misalignments or
artifacts arising from non-editorial changes of the content. Robustly matching
video to a trusted source enables conclusions to be drawn on video provenance,
enabling informed trust decisions on content encountered.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICCV2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Marching-Primitives: Shape Abstraction from Signed Distance Function <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13190v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13190v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weixiao Liu, Yuwei Wu, Sipu Ruan, Gregory S. Chirikjian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Representing complex objects with basic geometric primitives has long been a
topic in computer vision. Primitive-based representations have the merits of
compactness and computational efficiency in higher-level tasks such as physics
simulation, collision checking, and robotic manipulation. Unlike previous works
which extract polygonal meshes from a signed distance function (SDF), in this
paper, we present a novel method, named Marching-Primitives, to obtain a
primitive-based abstraction directly from an SDF. Our method grows geometric
primitives (such as superquadrics) iteratively by analyzing the connectivity of
voxels while marching at different levels of signed distance. For each valid
connected volume of interest, we march on the scope of voxels from which a
primitive is able to be extracted in a probabilistic sense and simultaneously
solve for the parameters of the primitive to capture the underlying local
geometry. We evaluate the performance of our method on both synthetic and
real-world datasets. The results show that the proposed method outperforms the
state-of-the-art in terms of accuracy, and is directly generalizable among
different categories and scales. The code is open-sourced at
https://github.com/ChirikjianLab/Marching-Primitives.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR2023 Highlight</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ScanERU: Interactive 3D Visual Grounding based on Embodied Reference
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13186v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13186v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyang Lu, Yunqiang Pei, Guoqing Wang, Yang Yang, Zheng Wang, Heng Tao Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aiming to link natural language descriptions to specific regions in a 3D
scene represented as 3D point clouds, 3D visual grounding is a very fundamental
task for human-robot interaction. The recognition errors can significantly
impact the overall accuracy and then degrade the operation of AI systems.
Despite their effectiveness, existing methods suffer from the difficulty of low
recognition accuracy in cases of multiple adjacent objects with similar
appearances.To address this issue, this work intuitively introduces the
human-robot interaction as a cue to facilitate the development of 3D visual
grounding. Specifically, a new task termed Embodied Reference Understanding
(ERU) is first designed for this concern. Then a new dataset called ScanERU is
constructed to evaluate the effectiveness of this idea. Different from existing
datasets, our ScanERU is the first to cover semi-synthetic scene integration
with textual, real-world visual, and synthetic gestural information.
Additionally, this paper formulates a heuristic framework based on attention
mechanisms and human body movements to enlighten the research of ERU.
Experimental results demonstrate the superiority of the proposed method,
especially in the recognition of multiple identical objects. Our codes and
dataset are ready to be available publicly.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CMG-Net: An End-to-End Contact-Based Multi-Finger Dexterous Grasping
  Network <span class="chip">ICRA 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13182v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13182v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingze Wei, Yaomin Huang, Zhiyuan Xu, Ning Liu, Zhengping Che, Xinyu Zhang, Chaomin Shen, Feifei Feng, Chun Shan, Jian Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a novel representation for grasping using contacts
between multi-finger robotic hands and objects to be manipulated. This
representation significantly reduces the prediction dimensions and accelerates
the learning process. We present an effective end-to-end network, CMG-Net, for
grasping unknown objects in a cluttered environment by efficiently predicting
multi-finger grasp poses and hand configurations from a single-shot point
cloud. Moreover, we create a synthetic grasp dataset that consists of five
thousand cluttered scenes, 80 object categories, and 20 million annotations. We
perform a comprehensive empirical study and demonstrate the effectiveness of
our grasping representation and CMG-Net. Our work significantly outperforms the
state-of-the-art for three-finger robotic hands. We also demonstrate that the
model trained using synthetic data performs very well for real robots.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors are with equal contributions. Paper accepted by
  ICRA 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancement of theColor Image Compression Using a New Algorithm based on
  Discrete Hermite Wavelet Transform 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13175v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13175v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hassan Mohamed Muhi-Aldeen, Asma A. Abdulrahman, Jabbar Abed Eleiwy, Fouad S. Tahir, Yurii Khlaponin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Internet has turned the entire world into a small village;this is because
it has made it possible to share millions of images and videos. However,
sending and receiving a huge amount of data is considered to be a main
challenge. To address this issue, a new algorithm is required to reduce image
bits and represent the data in a compressed form. Nevertheless, image
compression is an important application for transferring large files and
images. This requires appropriate and efficient transfers in this field to
achieve the task and reach the best results. In this work, we propose a new
algorithm based on discrete Hermite wavelets transformation (DHWT) that shows
the efficiency and quality of the color images. By compressing the color image,
this method analyzes it and divides it into approximate coefficients and detail
coefficients after adding the wavelets into MATLAB. With Multi-Resolution
Analyses (MRA), the appropriate filter is derived, and the mathematical aspects
prove to be validated by testing a new filter and performing its operation.
After the decomposition of the rows and upon the process of the reconstruction,
taking the inverse of the filter and dealing with the columns of the matrix,
the original matrix is improved by measuring the parameters of the image to
achieve the best quality of the resulting image, such as the peak
signal-to-noise ratio (PSNR), compression ratio (CR), bits per pixel (BPP), and
mean square error (MSE).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3D-POP -- An automated annotation approach to facilitate markerless
  2D-3D tracking of freely moving birds with marker-based motion capture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13174v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13174v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hemal Naik, Alex Hoi Hang Chan, Junran Yang, Mathilde Delacoux, Iain D. Couzin, Fumihiro Kano, Máté Nagy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in machine learning and computer vision are revolutionizing
the field of animal behavior by enabling researchers to track the poses and
locations of freely moving animals without any marker attachment. However,
large datasets of annotated images of animals for markerless pose tracking,
especially high-resolution images taken from multiple angles with accurate 3D
annotations, are still scant. Here, we propose a method that uses a motion
capture (mo-cap) system to obtain a large amount of annotated data on animal
movement and posture (2D and 3D) in a semi-automatic manner. Our method is
novel in that it extracts the 3D positions of morphological keypoints (e.g
eyes, beak, tail) in reference to the positions of markers attached to the
animals. Using this method, we obtained, and offer here, a new dataset - 3D-POP
with approximately 300k annotated frames (4 million instances) in the form of
videos having groups of one to ten freely moving birds from 4 different camera
views in a 3.6m x 4.2m area. 3D-POP is the first dataset of flocking birds with
accurate keypoint annotations in 2D and 3D along with bounding box and
individual identities and will facilitate the development of solutions for
problems of 2D to 3D markerless pose, trajectory tracking, and identification
in birds.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Take 5: Interpretable Image Classification with a Handful of Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13166v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13166v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Norrenbrock, Marco Rudolph, Bodo Rosenhahn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Neural Networks use thousands of mostly incomprehensible features to
identify a single class, a decision no human can follow. We propose an
interpretable sparse and low dimensional final decision layer in a deep neural
network with measurable aspects of interpretability and demonstrate it on
fine-grained image classification. We argue that a human can only understand
the decision of a machine learning model, if the features are interpretable and
only very few of them are used for a single decision. For that matter, the
final layer has to be sparse and, to make interpreting the features feasible,
low dimensional. We call a model with a Sparse Low-Dimensional Decision
SLDD-Model. We show that a SLDD-Model is easier to interpret locally and
globally than a dense high-dimensional decision layer while being able to
maintain competitive accuracy. Additionally, we propose a loss function that
improves a model's feature diversity and accuracy. Our more interpretable
SLDD-Model only uses 5 out of just 50 features per class, while maintaining 97%
to 100% of the accuracy on four common benchmark datasets compared to the
baseline model with 2048 features.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improvement of Color Image Analysis Using a New Hybrid Face Recognition
  Algorithm based on Discrete Wavelets and Chebyshev Polynomials 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13158v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13158v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hassan Mohamed Muhi-Aldeen, Maha Ammar Mustafa, Asma A. Abdulrahman, Jabbar Abed Eleiwy, Fouad S. Tahir, Yurii Khlaponin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work is unique in the use of discrete wavelets that were built from or
derived from Chebyshev polynomials of the second and third kind, filter the
Discrete Second Chebyshev Wavelets Transform (DSCWT), and derive two effective
filters. The Filter Discrete Third Chebyshev Wavelets Transform (FDTCWT) is
used in the process of analyzing color images and removing noise and impurities
that accompany the image, as well as because of the large amount of data that
makes up the image as it is taken. These data are massive, making it difficult
to deal with each other during transmission. However to address this issue, the
image compression technique is used, with the image not losing information due
to the readings that were obtained, and the results were satisfactory. Mean
Square Error (MSE), Peak Signal Noise Ratio (PSNR), Bit Per Pixel (BPP), and
Compression Ratio (CR) Coronavirus is the initial treatment, while the
processing stage is done with network training for Convolutional Neural
Networks (CNN) with Discrete Second Chebeshev Wavelets Convolutional Neural
Network (DSCWCNN) and Discrete Third Chebeshev Wavelets Convolutional Neural
Network (DTCWCNN) to create an efficient algorithm for face recognition, and
the best results were achieved in accuracy and in the least amount of time. Two
samples of color images that were made or implemented were used. The proposed
theory was obtained with fast and good results; the results are evident shown
in the tables below.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Calibrated Out-of-Distribution Detection with a Generic Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13148v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13148v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tomas Vojir, Jan Sochman, Rahaf Aljundi, Jiri Matas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Out-of-distribution detection is a common issue in deploying vision models in
practice and solving it is an essential building block in safety critical
applications. Existing OOD detection solutions focus on improving the OOD
robustness of a classification model trained exclusively on in-distribution
(ID) data. In this work, we take a different approach and propose to leverage
generic pre-trained representations. We first investigate the behaviour of
simple classifiers built on top of such representations and show striking
performance gains compared to the ID trained representations. We propose a
novel OOD method, called GROOD, that achieves excellent performance, predicated
by the use of a good generic representation. Only a trivial training process is
required for adapting GROOD to a particular problem. The method is simple,
general, efficient, calibrated and with only a few hyper-parameters. The method
achieves state-of-the-art performance on a number of OOD benchmarks, reaching
near perfect performance on several of them. The source code is available at
https://github.com/vojirt/GROOD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, submitted to conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative Image Inpainting with Segmentation Confusion Adversarial
  Training and Contrastive Learning <span class="chip">AAAI2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13133v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13133v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiwen Zuo, Lei Zhao, Ailin Li, Zhizhong Wang, Zhanjie Zhang, Jiafu Chen, Wei Xing, Dongming Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a new adversarial training framework for image inpainting
with segmentation confusion adversarial training (SCAT) and contrastive
learning. SCAT plays an adversarial game between an inpainting generator and a
segmentation network, which provides pixel-level local training signals and can
adapt to images with free-form holes. By combining SCAT with standard global
adversarial training, the new adversarial training framework exhibits the
following three advantages simultaneously: (1) the global consistency of the
repaired image, (2) the local fine texture details of the repaired image, and
(3) the flexibility of handling images with free-form holes. Moreover, we
propose the textural and semantic contrastive learning losses to stabilize and
improve our inpainting model's training by exploiting the feature
representation space of the discriminator, in which the inpainting images are
pulled closer to the ground truth images but pushed farther from the corrupted
images. The proposed contrastive losses better guide the repaired images to
move from the corrupted image data points to the real image data points in the
feature representation space, resulting in more realistic completed images. We
conduct extensive experiments on two benchmark datasets, demonstrating our
model's effectiveness and superiority both qualitatively and quantitatively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI2023, Oral</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Masked Image Training for Generalizable Deep Image Denoising <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13132v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13132v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyu Chen, Jinjin Gu, Yihao Liu, Salma Abdel Magid, Chao Dong, Qiong Wang, Hanspeter Pfister, Lei Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When capturing and storing images, devices inevitably introduce noise.
Reducing this noise is a critical task called image denoising. Deep learning
has become the de facto method for image denoising, especially with the
emergence of Transformer-based models that have achieved notable
state-of-the-art results on various image tasks. However, deep learning-based
methods often suffer from a lack of generalization ability. For example, deep
models trained on Gaussian noise may perform poorly when tested on other noise
distributions. To address this issue, we present a novel approach to enhance
the generalization performance of denoising networks, known as masked training.
Our method involves masking random pixels of the input image and reconstructing
the missing information during training. We also mask out the features in the
self-attention layers to avoid the impact of training-testing inconsistency.
Our approach exhibits better generalization ability than other deep learning
models and is directly applicable to real-world scenarios. Additionally, our
interpretability analysis demonstrates the superiority of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Watch Out for the Confusing Faces: Detecting Face Swapping with the
  Probability Distribution of Face Identification Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13131v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13131v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Duan, Xuhong Zhang, Chuer Yu, Zonghui Wang, Shouling Ji, Wenzhi Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, face swapping has been developing rapidly and achieved a surprising
reality, raising concerns about fake content. As a countermeasure, various
detection approaches have been proposed and achieved promising performance.
However, most existing detectors struggle to maintain performance on unseen
face swapping methods and low-quality images. Apart from the generalization
problem, current detection approaches have been shown vulnerable to evasion
attacks crafted by detection-aware manipulators. Lack of robustness under
adversary scenarios leaves threats for applying face swapping detection in real
world. In this paper, we propose a novel face swapping detection approach based
on face identification probability distributions, coined as IdP_FSD, to improve
the generalization and robustness. IdP_FSD is specially designed for detecting
swapped faces whose identities belong to a finite set, which is meaningful in
real-world applications. Compared with previous general detection methods, we
make use of the available real faces with concerned identities and require no
fake samples for training. IdP_FSD exploits face swapping's common nature that
the identity of swapped face combines that of two faces involved in swapping.
We reflect this nature with the confusion of a face identification model and
measure the confusion with the maximum value of the output probability
distribution. What's more, to defend our detector under adversary scenarios, an
attention-based finetuning scheme is proposed for the face identification
models used in IdP_FSD. Extensive experiments show that the proposed IdP_FSD
not only achieves high detection performance on different benchmark datasets
and image qualities but also raises the bar for manipulators to evade the
detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Task-Oriented Human-Object Interactions Generation with Implicit Neural
  Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13129v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13129v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quanzhou Li, Jingbo Wang, Chen Change Loy, Bo Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Digital human motion synthesis is a vibrant research field with applications
in movies, AR/VR, and video games. Whereas methods were proposed to generate
natural and realistic human motions, most only focus on modeling humans and
largely ignore object movements. Generating task-oriented human-object
interaction motions in simulation is challenging. For different intents of
using the objects, humans conduct various motions, which requires the human
first to approach the objects and then make them move consistently with the
human instead of staying still. Also, to deploy in downstream applications, the
synthesized motions are desired to be flexible in length, providing options to
personalize the predicted motions for various purposes. To this end, we propose
TOHO: Task-Oriented Human-Object Interactions Generation with Implicit Neural
Representations, which generates full human-object interaction motions to
conduct specific tasks, given only the task type, the object, and a starting
human status. TOHO generates human-object motions in three steps: 1) it first
estimates the keyframe poses of conducting a task given the task type and
object information; 2) then, it infills the keyframes and generates continuous
motions; 3) finally, it applies a compact closed-form object motion estimation
to generate the object motion. Our method generates continuous motions that are
parameterized only by the temporal coordinate, which allows for upsampling or
downsampling of the sequence to arbitrary frames and adjusting the motion
speeds by designing the temporal coordinate vector. We demonstrate the
effectiveness of our method, both qualitatively and quantitatively. This work
takes a step further toward general human-scene interaction simulation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MagicFusion: Boosting Text-to-Image Generation Performance by Fusing
  Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13126v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13126v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Zhao, Heliang Zheng, Chaoyue Wang, Long Lan, Wenjing Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of open-source AI communities has produced a cornucopia of
powerful text-guided diffusion models that are trained on various datasets.
While few explorations have been conducted on ensembling such models to combine
their strengths. In this work, we propose a simple yet effective method called
Saliency-aware Noise Blending (SNB) that can empower the fused text-guided
diffusion models to achieve more controllable generation. Specifically, we
experimentally find that the responses of classifier-free guidance are highly
related to the saliency of generated images. Thus we propose to trust different
models in their areas of expertise by blending the predicted noises of two
diffusion models in a saliency-aware manner. SNB is training-free and can be
completed within a DDIM sampling process. Additionally, it can automatically
align the semantics of two noise spaces without requiring additional
annotations such as masks. Extensive experiments show the impressive
effectiveness of SNB in various applications. Project page is available at
https://magicfusion.github.io/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Laplacian Segmentation Networks: Improved Epistemic Uncertainty from
  Spatial Aleatoric Uncertainty 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13123v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13123v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kilian Zepf, Selma Wanna, Marco Miani, Juston Moore, Jes Frellsen, Søren Hauberg, Aasa Feragen, Frederik Warburg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Out of distribution (OOD) medical images are frequently encountered, e.g.
because of site- or scanner differences, or image corruption. OOD images come
with a risk of incorrect image segmentation, potentially negatively affecting
downstream diagnoses or treatment. To ensure robustness to such incorrect
segmentations, we propose Laplacian Segmentation Networks (LSN) that jointly
model epistemic (model) and aleatoric (data) uncertainty in image segmentation.
We capture data uncertainty with a spatially correlated logit distribution. For
model uncertainty, we propose the first Laplace approximation of the weight
posterior that scales to large neural networks with skip connections that have
high-dimensional outputs. Empirically, we demonstrate that modelling spatial
pixel correlation allows the Laplacian Segmentation Network to successfully
assign high epistemic uncertainty to out-of-distribution objects appearing
within images.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Visual <span class="highlight-title">Prompt</span>s for Whole Slide Image Classification with
  Multiple Instance Learning <span class="chip">MICCAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13122v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13122v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Lin, Zhongchen Zhao, Zhengjie ZHU, Lisheng Wang, Kwang-Ting Cheng, Hao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multiple instance learning (MIL) has emerged as a popular method for
classifying histopathology whole slide images (WSIs). However, existing
approaches typically rely on pre-trained models from large natural image
datasets, such as ImageNet, to generate instance features, which can be
sub-optimal due to the significant differences between natural images and
histopathology images that lead to a domain shift. In this paper, we present a
novel, simple yet effective method for learning domain-specific knowledge
transformation from pre-trained models to histopathology images. Our approach
entails using a prompt component to assist the pre-trained model in discerning
differences between the pre-trained dataset and the target histopathology
dataset, resulting in improved performance of MIL models. We validate our
method on two publicly available datasets, Camelyon16 and TCGA-NSCLC. Extensive
experimental results demonstrate the significant performance improvement of our
method for different MIL models and backbones. Upon publication of this paper,
we will release the source code for our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to MICCAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DetOFA: Efficient Training of Once-for-All Networks for Object Detection
  by Using <span class="highlight-title">Pre-train</span>ed Supernet and Path Filter 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13121v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13121v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuiko Sakuma, Masato Ishii, Takuya Narihira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address the challenge of training a large supernet for the object
detection task, using a relatively small amount of training data. Specifically,
we propose an efficient supernet-based neural architecture search (NAS) method
that uses transfer learning and search space pruning. First, the supernet is
pre-trained on a classification task, for which large datasets are available.
Second, the search space defined by the supernet is pruned by removing
candidate models that are predicted to perform poorly. To effectively remove
the candidates over a wide range of resource constraints, we particularly
design a performance predictor, called path filter, which can accurately
predict the relative performance of the models that satisfy similar resource
constraints. Hence, supernet training is more focused on the best-performing
candidates. Our path filter handles prediction for paths with different
resource budgets. Compared to once-for-all, our proposed method reduces the
computational cost of the optimal network architecture by 30% and 63%, while
yielding better accuracy-floating point operations Pareto front (0.85 and 0.45
points of improvement on average precision for Pascal VOC and COCO,
respectively).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Regularization for Class-Incremental Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13113v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13113v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elif Ceren Gok, Murat Onur Yildirim, Mert Kilickaya, Joaquin Vanschoren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Class-Incremental Learning updates a deep classifier with new categories
while maintaining the previously observed class accuracy. Regularizing the
neural network weights is a common method to prevent forgetting previously
learned classes while learning novel ones. However, existing regularizers use a
constant magnitude throughout the learning sessions, which may not reflect the
varying levels of difficulty of the tasks encountered during incremental
learning. This study investigates the necessity of adaptive regularization in
Class-Incremental Learning, which dynamically adjusts the regularization
strength according to the complexity of the task at hand. We propose a Bayesian
Optimization-based approach to automatically determine the optimal
regularization magnitude for each learning task. Our experiments on two
datasets via two regularizers demonstrate the importance of adaptive
regularization for achieving accurate and less forgetful visual incremental
learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Permutable Hybrid Network for Volumetric Medical Image Segmentation <span class="chip">MICCAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13111v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13111v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Lin, Xiao Fang, Dong Zhang, Kwang-Ting Cheng, Hao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of Vision Transformer (ViT) has brought substantial advancements
in 3D volumetric benchmarks, particularly in 3D medical image segmentation.
Concurrently, Multi-Layer Perceptron (MLP) networks have regained popularity
among researchers due to their comparable results to ViT, albeit with the
exclusion of the heavy self-attention module. This paper introduces a
permutable hybrid network for volumetric medical image segmentation, named
PHNet, which exploits the advantages of convolution neural network (CNN) and
MLP. PHNet addresses the intrinsic isotropy problem of 3D volumetric data by
utilizing both 2D and 3D CNN to extract local information. Besides, we propose
an efficient Multi-Layer Permute Perceptron module, named MLPP, which enhances
the original MLP by obtaining long-range dependence while retaining positional
information. Extensive experimental results validate that PHNet outperforms the
state-of-the-art methods on two public datasets, namely, COVID-19-20 and
Synapse. Moreover, the ablation study demonstrates the effectiveness of PHNet
in harnessing the strengths of both CNN and MLP. The code will be accessible to
the public upon acceptance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to MICCAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OCELOT: Overlapped Cell on Tissue <span class="highlight-title">Dataset</span> for Histopathology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13110v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13110v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeongun Ryu, Aaron Valero Puche, JaeWoong Shin, Seonwook Park, Biagio Brattoli, Jinhee Lee, Wonkyung Jung, Soo Ick Cho, Kyunghyun Paeng, Chan-Young Ock, Donggeun Yoo, Sérgio Pereira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cell detection is a fundamental task in computational pathology that can be
used for extracting high-level medical information from whole-slide images. For
accurate cell detection, pathologists often zoom out to understand the
tissue-level structures and zoom in to classify cells based on their morphology
and the surrounding context. However, there is a lack of efforts to reflect
such behaviors by pathologists in the cell detection models, mainly due to the
lack of datasets containing both cell and tissue annotations with overlapping
regions. To overcome this limitation, we propose and publicly release OCELOT, a
dataset purposely dedicated to the study of cell-tissue relationships for cell
detection in histopathology. OCELOT provides overlapping cell and tissue
annotations on images acquired from multiple organs. Within this setting, we
also propose multi-task learning approaches that benefit from learning both
cell and tissue tasks simultaneously. When compared against a model trained
only for the cell detection task, our proposed approaches improve cell
detection performance on 3 datasets: proposed OCELOT, public TIGER, and
internal CARP datasets. On the OCELOT test set in particular, we show up to
6.79 improvement in F1-score. We believe the contributions of this paper,
including the release of the OCELOT dataset at
https://lunit-io.github.io/research/publications/ocelot are a crucial starting
point toward the important research direction of incorporating cell-tissue
relationships in computation pathology.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Keypoint-Guided Optimal Transport 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13102v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13102v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Gu, Yucheng Yang, Wei Zeng, Jian Sun, Zongben Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing Optimal Transport (OT) methods mainly derive the optimal transport
plan/matching under the criterion of transport cost/distance minimization,
which may cause incorrect matching in some cases. In many applications,
annotating a few matched keypoints across domains is reasonable or even
effortless in annotation burden. It is valuable to investigate how to leverage
the annotated keypoints to guide the correct matching in OT. In this paper, we
propose a novel KeyPoint-Guided model by ReLation preservation (KPG-RL) that
searches for the optimal matching (i.e., transport plan) guided by the
keypoints in OT. To impose the keypoints in OT, first, we propose a mask-based
constraint of the transport plan that preserves the matching of keypoint pairs.
Second, we propose to preserve the relation of each data point to the keypoints
to guide the matching. The proposed KPG-RL model can be solved by Sinkhorn's
algorithm and is applicable even when distributions are supported in different
spaces. We further utilize the relation preservation constraint in the
Kantorovich Problem and Gromov-Wasserstein model to impose the guidance of
keypoints in them. Meanwhile, the proposed KPG-RL model is extended to the
partial OT setting. Moreover, we deduce the dual formulation of the KPG-RL
model, which is solved using deep learning techniques. Based on the learned
transport plan from dual KPG-RL, we propose a novel manifold barycentric
projection to transport source data to the target domain. As applications, we
apply the proposed KPG-RL model to the heterogeneous domain adaptation and
image-to-image translation. Experiments verified the effectiveness of the
proposed approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MMFormer: Multimodal <span class="highlight-title">Transformer</span> Using Multiscale Self-Attention for
  Remote Sensing Image Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13101v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13101v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Zhang, Zuheng Ming, Wei Feng, Yaqian Liu, Liang He, Kaixing Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To benefit the complementary information between heterogeneous data, we
introduce a new Multimodal Transformer (MMFormer) for Remote Sensing (RS) image
classification using Hyperspectral Image (HSI) accompanied by another source of
data such as Light Detection and Ranging (LiDAR). Compared with traditional
Vision Transformer (ViT) lacking inductive biases of convolutions, we first
introduce convolutional layers to our MMFormer to tokenize patches from
multimodal data of HSI and LiDAR. Then we propose a Multi-scale Multi-head
Self-Attention (MSMHSA) module to address the problem of compatibility which
often limits to fuse HSI with high spectral resolution and LiDAR with
relatively low spatial resolution. The proposed MSMHSA module can incorporate
HSI to LiDAR data in a coarse-to-fine manner enabling us to learn a
fine-grained representation. Extensive experiments on widely used benchmarks
(e.g., Trento and MUUFL) demonstrate the effectiveness and superiority of our
proposed MMFormer for RS image classification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PointGame: Geometrically and Adaptively Masked Auto-Encoder on Point
  Clouds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13100v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13100v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yun Liu, Xuefeng Yan, Zhilei Chen, Zhiqi Li, Zeyong Wei, Mingqiang Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning is attracting large attention in point cloud
understanding. However, exploring discriminative and transferable features
still remains challenging due to their nature of irregularity and sparsity. We
propose a geometrically and adaptively masked auto-encoder for self-supervised
learning on point clouds, termed \textit{PointGame}. PointGame contains two
core components: GATE and EAT. GATE stands for the geometrical and adaptive
token embedding module; it not only absorbs the conventional wisdom of
geometric descriptors that captures the surface shape effectively, but also
exploits adaptive saliency to focus on the salient part of a point cloud. EAT
stands for the external attention-based Transformer encoder with linear
computational complexity, which increases the efficiency of the whole pipeline.
Unlike cutting-edge unsupervised learning models, PointGame leverages geometric
descriptors to perceive surface shapes and adaptively mines discriminative
features from training data. PointGame showcases clear advantages over its
competitors on various downstream tasks under both global and local fine-tuning
strategies. The code and pre-trained models will be publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CP$^3$: Channel Pruning Plug-in for Point-based Networks <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13097v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13097v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaomin Huang, Ning Liu, Zhengping Che, Zhiyuan Xu, Chaomin Shen, Yaxin Peng, Guixu Zhang, Xinmei Liu, Feifei Feng, Jian Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Channel pruning can effectively reduce both computational cost and memory
footprint of the original network while keeping a comparable accuracy
performance. Though great success has been achieved in channel pruning for 2D
image-based convolutional networks (CNNs), existing works seldom extend the
channel pruning methods to 3D point-based neural networks (PNNs). Directly
implementing the 2D CNN channel pruning methods to PNNs undermine the
performance of PNNs because of the different representations of 2D images and
3D point clouds as well as the network architecture disparity. In this paper,
we proposed CP$^3$, which is a Channel Pruning Plug-in for Point-based network.
CP$^3$ is elaborately designed to leverage the characteristics of point clouds
and PNNs in order to enable 2D channel pruning methods for PNNs. Specifically,
it presents a coordinate-enhanced channel importance metric to reflect the
correlation between dimensional information and individual channel features,
and it recycles the discarded points in PNN's sampling process and reconsiders
their potentially-exclusive information to enhance the robustness of channel
pruning. Experiments on various PNN architectures show that CP$^3$ constantly
improves state-of-the-art 2D CNN pruning approaches on different point cloud
tasks. For instance, our compressed PointNeXt-S on ScanObjectNN achieves an
accuracy of 88.52% with a pruning rate of 57.8%, outperforming the baseline
pruning methods with an accuracy gain of 1.94%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Yaomin Huang and Ning Liu are with equal contributions. This paper
  has been accepted by CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modeling Entities as Semantic Points for Visual Information Extraction
  in the Wild 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13095v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13095v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhibo Yang, Rujiao Long, Pengfei Wang, Sibo Song, Humen Zhong, Wenqing Cheng, Xiang Bai, Cong Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, Visual Information Extraction (VIE) has been becoming increasingly
important in both the academia and industry, due to the wide range of
real-world applications. Previously, numerous works have been proposed to
tackle this problem. However, the benchmarks used to assess these methods are
relatively plain, i.e., scenarios with real-world complexity are not fully
represented in these benchmarks. As the first contribution of this work, we
curate and release a new dataset for VIE, in which the document images are much
more challenging in that they are taken from real applications, and
difficulties such as blur, partial occlusion, and printing shift are quite
common. All these factors may lead to failures in information extraction.
Therefore, as the second contribution, we explore an alternative approach to
precisely and robustly extract key information from document images under such
tough conditions. Specifically, in contrast to previous methods, which usually
either incorporate visual information into a multi-modal architecture or train
text spotting and information extraction in an end-to-end fashion, we
explicitly model entities as semantic points, i.e., center points of entities
are enriched with semantic information describing the attributes and
relationships of different entities, which could largely benefit entity
labeling and linking. Extensive experiments on standard benchmarks in this
field as well as the proposed dataset demonstrate that the proposed method can
achieve significantly enhanced performance on entity labeling and linking,
compared with previous state-of-the-art models. Dataset is available at
https://www.modelscope.cn/datasets/damo/SIBR/summary.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Orthogonal Annotation Benefits Barely-supervised Medical Image
  Segmentation <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13090v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13090v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heng Cai, Shumeng Li, Lei Qi, Qian Yu, Yinghuan Shi, Yang Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent trends in semi-supervised learning have significantly boosted the
performance of 3D semi-supervised medical image segmentation. Compared with 2D
images, 3D medical volumes involve information from different directions, e.g.,
transverse, sagittal, and coronal planes, so as to naturally provide
complementary views. These complementary views and the intrinsic similarity
among adjacent 3D slices inspire us to develop a novel annotation way and its
corresponding semi-supervised model for effective segmentation. Specifically,
we firstly propose the orthogonal annotation by only labeling two orthogonal
slices in a labeled volume, which significantly relieves the burden of
annotation. Then, we perform registration to obtain the initial pseudo labels
for sparsely labeled volumes. Subsequently, by introducing unlabeled volumes,
we propose a dual-network paradigm named Dense-Sparse Co-training (DeSCO) that
exploits dense pseudo labels in early stage and sparse labels in later stage
and meanwhile forces consistent output of two networks. Experimental results on
three benchmark datasets validated our effectiveness in performance and
efficiency in annotation. For example, with only 10 annotated slices, our
method reaches a Dice up to 86.93% on KiTS19 dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Box-Level Active Detection <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13089v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13089v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengyao Lyu, Jundong Zhou, Hui Chen, Yijie Huang, Dongdong Yu, Yaqian Li, Yandong Guo, Yuchen Guo, Liuyu Xiang, Guiguang Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Active learning selects informative samples for annotation within budget,
which has proven efficient recently on object detection. However, the widely
used active detection benchmarks conduct image-level evaluation, which is
unrealistic in human workload estimation and biased towards crowded images.
Furthermore, existing methods still perform image-level annotation, but equally
scoring all targets within the same image incurs waste of budget and redundant
labels. Having revealed above problems and limitations, we introduce a
box-level active detection framework that controls a box-based budget per
cycle, prioritizes informative targets and avoids redundancy for fair
comparison and efficient application.
  Under the proposed box-level setting, we devise a novel pipeline, namely
Complementary Pseudo Active Strategy (ComPAS). It exploits both human
annotations and the model intelligence in a complementary fashion: an efficient
input-end committee queries labels for informative objects only; meantime
well-learned targets are identified by the model and compensated with
pseudo-labels. ComPAS consistently outperforms 10 competitors under 4 settings
in a unified codebase. With supervision from labeled data only, it achieves
100% supervised performance of VOC0712 with merely 19% box annotations. On the
COCO dataset, it yields up to 4.3% mAP improvement over the second-best method.
ComPAS also supports training with the unlabeled pool, where it surpasses 90%
COCO supervised performance with 85% label reduction. Our source code is
publicly available at https://github.com/lyumengyao/blad.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023 highlight</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Generalization against Photon-Limited Corruptions via Worst-Case
  Sharpness Minimization <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13087v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13087v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuo Huang, Miaoxi Zhu, Xiaobo Xia, Li Shen, Jun Yu, Chen Gong, Bo Han, Bo Du, Tongliang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robust generalization aims to tackle the most challenging data distributions
which are rare in the training set and contain severe noises, i.e.,
photon-limited corruptions. Common solutions such as distributionally robust
optimization (DRO) focus on the worst-case empirical risk to ensure low
training error on the uncommon noisy distributions. However, due to the
over-parameterized model being optimized on scarce worst-case data, DRO fails
to produce a smooth loss landscape, thus struggling on generalizing well to the
test set. Therefore, instead of focusing on the worst-case risk minimization,
we propose SharpDRO by penalizing the sharpness of the worst-case distribution,
which measures the loss changes around the neighbor of learning parameters.
Through worst-case sharpness minimization, the proposed method successfully
produces a flat loss curve on the corrupted distributions, thus achieving
robust generalization. Moreover, by considering whether the distribution
annotation is available, we apply SharpDRO to two problem settings and design a
worst-case selection process for robust generalization. Theoretically, we show
that SharpDRO has a great convergence guarantee. Experimentally, we simulate
photon-limited corruptions using CIFAR10/100 and ImageNet30 datasets and show
that SharpDRO exhibits a strong generalization ability against severe
corruptions and exceeds well-known baseline methods with large performance
gains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving the Performance of Spiking Neural Networks on Event-based
  <span class="highlight-title">Dataset</span>s with Knowledge Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13077v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13077v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang He, Dongcheng Zhao, Yang Li, Guobin Shen, Qingqun Kong, Yi Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spiking neural networks (SNNs) have rich spatial-temporal dynamics, which are
suitable for processing neuromorphic, event-based data. However, event-based
datasets are usually less annotated than static datasets used in traditional
deep learning. Small data scale makes SNNs prone to overfitting and limits the
performance of the SNN. To enhance the generalizability of SNNs on event-based
datasets, we propose a knowledge-transfer framework that leverages static
images to assist in the training on neuromorphic datasets. Our method proposes
domain loss and semantic loss to exploit both domain-invariant and unique
features of these two domains, providing SNNs with more generalized knowledge
for subsequent targeted training on neuromorphic data. Specifically, domain
loss aligns the feature space and aims to capture common features between
static and event-based images, while semantic loss emphasizes that the
differences between samples from different categories should be as large as
possible. Experimental results demonstrate that our method outperforms existing
methods on all mainstream neuromorphic vision datasets. In particular, we
achieve significant performance improvement of 2.7\% and 9.8\% when using only
10\% training data of CIFAR10-DVS and N-Caltech 101 datasets, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CORA: Adapting CLIP for Open-Vocabulary Detection with Region <span class="highlight-title">Prompt</span>ing
  and Anchor Pre-Matching <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13076v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13076v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoshi Wu, Feng Zhu, Rui Zhao, Hongsheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-vocabulary detection (OVD) is an object detection task aiming at
detecting objects from novel categories beyond the base categories on which the
detector is trained. Recent OVD methods rely on large-scale visual-language
pre-trained models, such as CLIP, for recognizing novel objects. We identify
the two core obstacles that need to be tackled when incorporating these models
into detector training: (1) the distribution mismatch that happens when
applying a VL-model trained on whole images to region recognition tasks; (2)
the difficulty of localizing objects of unseen classes. To overcome these
obstacles, we propose CORA, a DETR-style framework that adapts CLIP for
Open-vocabulary detection by Region prompting and Anchor pre-matching. Region
prompting mitigates the whole-to-region distribution gap by prompting the
region features of the CLIP-based region classifier. Anchor pre-matching helps
learning generalizable object localization by a class-aware matching mechanism.
We evaluate CORA on the COCO OVD benchmark, where we achieve 41.7 AP50 on novel
classes, which outperforms the previous SOTA by 2.4 AP50 even without resorting
to extra training data. When extra training data is available, we train
CORA$^+$ on both ground-truth base-category annotations and additional pseudo
bounding box labels computed by CORA. CORA$^+$ achieves 43.1 AP50 on the COCO
OVD benchmark and 28.1 box APr on the LVIS OVD benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 4 figures. Accepted by CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PanoHead: Geometry-Aware 3D Full-Head Synthesis in 360$^{\circ}$ <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13071v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13071v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sizhe An, Hongyi Xu, Yichun Shi, Guoxian Song, Umit Ogras, Linjie Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synthesis and reconstruction of 3D human head has gained increasing interests
in computer vision and computer graphics recently. Existing state-of-the-art 3D
generative adversarial networks (GANs) for 3D human head synthesis are either
limited to near-frontal views or hard to preserve 3D consistency in large view
angles. We propose PanoHead, the first 3D-aware generative model that enables
high-quality view-consistent image synthesis of full heads in $360^\circ$ with
diverse appearance and detailed geometry using only in-the-wild unstructured
images for training. At its core, we lift up the representation power of recent
3D GANs and bridge the data alignment gap when training from in-the-wild images
with widely distributed views. Specifically, we propose a novel two-stage
self-adaptive image alignment for robust 3D GAN training. We further introduce
a tri-grid neural volume representation that effectively addresses front-face
and back-head feature entanglement rooted in the widely-adopted tri-plane
formulation. Our method instills prior knowledge of 2D image segmentation in
adversarial learning of 3D neural scene structures, enabling compositable head
synthesis in diverse backgrounds. Benefiting from these designs, our method
significantly outperforms previous 3D GANs, generating high-quality 3D heads
with accurate geometry and diverse appearances, even with long wavy and afro
hairstyles, renderable from arbitrary poses. Furthermore, we show that our
system can reconstruct full 3D heads from single input images for personalized
realistic 3D avatars.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023. Project Page:https://sizhean.github.io/panohead</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Human Guided Ground-truth Generation for Realistic Image
  Super-resolution <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13069v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13069v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Du Chen, Jie Liang, Xindong Zhang, Ming Liu, Hui Zeng, Lei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How to generate the ground-truth (GT) image is a critical issue for training
realistic image super-resolution (Real-ISR) models. Existing methods mostly
take a set of high-resolution (HR) images as GTs and apply various degradations
to simulate their low-resolution (LR) counterparts. Though great progress has
been achieved, such an LR-HR pair generation scheme has several limitations.
First, the perceptual quality of HR images may not be high enough, limiting the
quality of Real-ISR outputs. Second, existing schemes do not consider much
human perception in GT generation, and the trained models tend to produce
over-smoothed results or unpleasant artifacts. With the above considerations,
we propose a human guided GT generation scheme. We first elaborately train
multiple image enhancement models to improve the perceptual quality of HR
images, and enable one LR image having multiple HR counterparts. Human subjects
are then involved to annotate the high quality regions among the enhanced HR
images as GTs, and label the regions with unpleasant artifacts as negative
samples. A human guided GT image dataset with both positive and negative
samples is then constructed, and a loss function is proposed to train the
Real-ISR models. Experiments show that the Real-ISR models trained on our
dataset can produce perceptually more realistic results with less artifacts.
Dataset and codes can be found at https://github.com/ChrisDud0257/HGGT
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages. Already accpted by 2023 IEEE/CVF Conference on Computer
  Vision and Pattern Recognition (CVPR)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SIEDOB: Semantic Image Editing by Disentangling Object and Background <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13062v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13062v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wuyang Luo, Su Yang, Xinjian Zhang, Weishan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic image editing provides users with a flexible tool to modify a given
image guided by a corresponding segmentation map. In this task, the features of
the foreground objects and the backgrounds are quite different. However, all
previous methods handle backgrounds and objects as a whole using a monolithic
model. Consequently, they remain limited in processing content-rich images and
suffer from generating unrealistic objects and texture-inconsistent
backgrounds. To address this issue, we propose a novel paradigm,
\textbf{S}emantic \textbf{I}mage \textbf{E}diting by \textbf{D}isentangling
\textbf{O}bject and \textbf{B}ackground (\textbf{SIEDOB}), the core idea of
which is to explicitly leverages several heterogeneous subnetworks for objects
and backgrounds. First, SIEDOB disassembles the edited input into background
regions and instance-level objects. Then, we feed them into the dedicated
generators. Finally, all synthesized parts are embedded in their original
locations and utilize a fusion network to obtain a harmonized result. Moreover,
to produce high-quality edited images, we propose some innovative designs,
including Semantic-Aware Self-Propagation Module, Boundary-Anchored Patch
Discriminator, and Style-Diversity Object Generator, and integrate them into
SIEDOB. We conduct extensive experiments on Cityscapes and ADE20K-Room datasets
and exhibit that our method remarkably outperforms the baselines, especially in
synthesizing realistic and diverse objects and texture-consistent backgrounds.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023 highlight paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffPattern: Layout Pattern Generation via Discrete Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13060v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13060v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixiao Wang, Yunheng Shen, Wenqian Zhao, Yang Bai, Guojin Chen, Farzan Farnia, Bei Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep generative models dominate the existing literature in layout pattern
generation. However, leaving the guarantee of legality to an inexplicable
neural network could be problematic in several applications. In this paper, we
propose \tool{DiffPattern} to generate reliable layout patterns.
\tool{DiffPattern} introduces a novel diverse topology generation method via a
discrete diffusion model with compute-efficiently lossless layout pattern
representation. Then a white-box pattern assessment is utilized to generate
legal patterns given desired design rules. Our experiments on several benchmark
settings show that \tool{DiffPattern} significantly outperforms existing
baselines and is capable of synthesizing reliable layout patterns.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>DAC2023 Accepted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical Semantic Contrast for Scene-aware Video Anomaly Detection <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13051v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13051v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengyang Sun, Xiaojin Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Increasing scene-awareness is a key challenge in video anomaly detection
(VAD). In this work, we propose a hierarchical semantic contrast (HSC) method
to learn a scene-aware VAD model from normal videos. We first incorporate
foreground object and background scene features with high-level semantics by
taking advantage of pre-trained video parsing models. Then, building upon the
autoencoder-based reconstruction framework, we introduce both scene-level and
object-level contrastive learning to enforce the encoded latent features to be
compact within the same semantic classes while being separable across different
classes. This hierarchical semantic contrast strategy helps to deal with the
diversity of normal patterns and also increases their discrimination ability.
Moreover, for the sake of tackling rare normal activities, we design a
skeleton-based motion augmentation to increase samples and refine the model
further. Extensive experiments on three public datasets and scene-dependent
mixture datasets validate the effectiveness of our proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Top-Down Visual Attention from Analysis by Synthesis <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13043v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13043v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baifeng Shi, Trevor Darrell, Xin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current attention algorithms (e.g., self-attention) are stimulus-driven and
highlight all the salient objects in an image. However, intelligent agents like
humans often guide their attention based on the high-level task at hand,
focusing only on task-related objects. This ability of task-guided top-down
attention provides task-adaptive representation and helps the model generalize
to various tasks. In this paper, we consider top-down attention from a classic
Analysis-by-Synthesis (AbS) perspective of vision. Prior work indicates a
functional equivalence between visual attention and sparse reconstruction; we
show that an AbS visual system that optimizes a similar sparse reconstruction
objective modulated by a goal-directed top-down signal naturally simulates
top-down attention. We further propose Analysis-by-Synthesis Vision Transformer
(AbSViT), which is a top-down modulated ViT model that variationally
approximates AbS, and achieves controllable top-down attention. For real-world
applications, AbSViT consistently improves over baselines on Vision-Language
tasks such as VQA and zero-shot retrieval where language guides the top-down
attention. AbSViT can also serve as a general backbone, improving performance
on classification, semantic segmentation, and model robustness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR2023 highlight; Project page:
  https://sites.google.com/view/absvit</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Open-Vocabulary Object Detection using Pseudo Caption Labels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13040v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13040v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han-Cheol Cho, Won Young Jhoo, Wooyoung Kang, Byungseok Roh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent open-vocabulary detection methods aim to detect novel objects by
distilling knowledge from vision-language models (VLMs) trained on a vast
amount of image-text pairs. To improve the effectiveness of these methods,
researchers have utilized datasets with a large vocabulary that contains a
large number of object classes, under the assumption that such data will enable
models to extract comprehensive knowledge on the relationships between various
objects and better generalize to unseen object classes. In this study, we argue
that more fine-grained labels are necessary to extract richer knowledge about
novel objects, including object attributes and relationships, in addition to
their names. To address this challenge, we propose a simple and effective
method named Pseudo Caption Labeling (PCL), which utilizes an image captioning
model to generate captions that describe object instances from diverse
perspectives. The resulting pseudo caption labels offer dense samples for
knowledge distillation. On the LVIS benchmark, our best model trained on the
de-duplicated VisualGenome dataset achieves an AP of 34.5 and an APr of 30.6,
comparable to the state-of-the-art performance. PCL's simplicity and
flexibility are other notable features, as it is a straightforward
pre-processing technique that can be used with any image captioning model
without imposing any restrictions on model architecture or training process.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Federated Uncertainty-Aware Aggregation for Fundus Diabetic Retinopathy
  Staging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13033v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13033v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meng Wang, Lianyu Wang, Xinxing Xu, Ke Zou, Yiming Qian, Rick Siow Mong Goh, Yong Liu, Huazhu Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning models have shown promising performance in the field of
diabetic retinopathy (DR) staging. However, collaboratively training a DR
staging model across multiple institutions remains a challenge due to non-iid
data, client reliability, and confidence evaluation of the prediction. To
address these issues, we propose a novel federated uncertainty-aware
aggregation paradigm (FedUAA), which considers the reliability of each client
and produces a confidence estimation for the DR staging. In our FedUAA, an
aggregated encoder is shared by all clients for learning a global
representation of fundus images, while a novel temperature-warmed uncertainty
head (TWEU) is utilized for each client for local personalized staging
criteria. Our TWEU employs an evidential deep layer to produce the uncertainty
score with the DR staging results for client reliability evaluation.
Furthermore, we developed a novel uncertainty-aware weighting module (UAW) to
dynamically adjust the weights of model aggregation based on the uncertainty
score distribution of each client. In our experiments, we collect five publicly
available datasets from different institutions to conduct a dataset for
federated DR staging to satisfy the real non-iid condition. The experimental
results demonstrate that our FedUAA achieves better DR staging performance with
higher reliability compared to other federated learning methods. Our proposed
FedUAA paradigm effectively addresses the challenges of collaboratively
training DR staging models across multiple institutions, and provides a robust
and reliable solution for the deployment of DR diagnosis models in real-world
clinical scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning a Practical SDR-to-HDRTV Up-conversion using New <span class="highlight-title">Dataset</span> and
  Degradation Models <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13031v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13031v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng Guo, Leidong Fan, Ziyu Xue, and Xiuhua Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In media industry, the demand of SDR-to-HDRTV up-conversion arises when users
possess HDR-WCG (high dynamic range-wide color gamut) TVs while most
off-the-shelf footage is still in SDR (standard dynamic range). The research
community has started tackling this low-level vision task by learning-based
approaches. When applied to real SDR, yet, current methods tend to produce dim
and desaturated result, making nearly no improvement on viewing experience.
Different from other network-oriented methods, we attribute such deficiency to
training set (HDR-SDR pair). Consequently, we propose new HDRTV dataset (dubbed
HDRTV4K) and new HDR-to-SDR degradation models. Then, it's used to train a
luminance-segmented network (LSN) consisting of a global mapping trunk, and two
Transformer branches on bright and dark luminance range. We also update
assessment criteria by tailored metrics and subjective experiment. Finally,
ablation studies are conducted to prove the effectiveness. Our work is
available at: https://github.com/AndreGuo/HDRTVDM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comprehensive Analysis of AI Biases in DeepFake Detection With
  Massively Annotated Databases 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.05845v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.05845v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ying Xu, Philipp Terhörst, Kiran Raja, Marius Pedersen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, image and video manipulations with Deepfake have become a
severe concern for security and society. Many detection models and datasets
have been proposed to detect Deepfake data reliably. However, there is an
increased concern that these models and training databases might be biased and,
thus, cause Deepfake detectors to fail. In this work, we investigate the bias
issue caused by public Deepfake datasets by (a) providing large-scale
demographic and non-demographic attribute annotations of 47 different
attributes for five popular Deepfake datasets and (b) comprehensively analysing
AI-bias of three state-of-the-art Deepfake detection backbone models on these
datasets. The investigation analyses the influence of a large variety of
distinctive attributes (from over 65M labels) on the detection performance,
including demographic (age, gender, ethnicity) and non-demographic (hair, skin,
accessories, etc.) information. The results indicate that investigated
databases lack diversity and, more importantly, show that the utilised Deepfake
detection backbone models are strongly biased towards many investigated
attributes. The Deepfake detection backbone methods, which are trained with
biased datasets, might output incorrect detection results, thereby leading to
generalisability, fairness, and security issues. We hope that the findings of
this study and the annotation databases will help to evaluate and mitigate bias
in future Deepfake detection techniques. The annotation datasets are publicly
available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Real-time event simulation with frame-based cameras <span class="chip">ICRA 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.04634v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.04634v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreas Ziegler, Daniel Teigland, Jonas Tebbe, Thomas Gossard, Andreas Zell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event cameras are becoming increasingly popular in robotics and computer
vision due to their beneficial properties, e.g., high temporal resolution, high
bandwidth, almost no motion blur, and low power consumption. However, these
cameras remain expensive and scarce in the market, making them inaccessible to
the majority. Using event simulators minimizes the need for real event cameras
to develop novel algorithms. However, due to the computational complexity of
the simulation, the event streams of existing simulators cannot be generated in
real-time but rather have to be pre-calculated from existing video sequences or
pre-rendered and then simulated from a virtual 3D scene. Although these offline
generated event streams can be used as training data for learning tasks, all
response time dependent applications cannot benefit from these simulators yet,
as they still require an actual event camera. This work proposes simulation
methods that improve the performance of event simulation by two orders of
magnitude (making them real-time capable) while remaining competitive in the
quality assessment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for 2023 IEEE International Conference on Robotics and
  Automation (ICRA 2023). Project web page:
  https://cogsys-tuebingen.github.io/realtime_event_simulator/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multiple Appropriate Facial Reaction Generation in Dyadic Interaction
  Settings: What, Why and How? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.06514v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.06514v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyang Song, Micol Spitale, Yiming Luo, Batuhan Bal, Hatice Gunes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  According to the Stimulus Organism Response (SOR) theory, all human
behavioral reactions are stimulated by context, where people will process the
received stimulus and produce an appropriate reaction. This implies that in a
specific context for a given input stimulus, a person can react differently
according to their internal state and other contextual factors. Analogously, in
dyadic interactions, humans communicate using verbal and nonverbal cues, where
a broad spectrum of listeners' non-verbal reactions might be appropriate for
responding to a specific speaker behaviour. There already exists a body of work
that investigated the problem of automatically generating an appropriate
reaction for a given input. However, none attempted to automatically generate
multiple appropriate reactions in the context of dyadic interactions and
evaluate the appropriateness of those reactions using objective measures. This
paper starts by defining the facial Multiple Appropriate Reaction Generation
(fMARG) task for the first time in the literature and proposes a new set of
objective evaluation metrics to evaluate the appropriateness of the generated
reactions. The paper subsequently introduces a framework to predict, generate,
and evaluate multiple appropriate facial reactions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Student Engagement Detection Using Emotion Analysis, Eye Tracking and
  Head Movement with Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1909.12913v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1909.12913v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prabin Sharma, Shubham Joshi, Subash Gautam, Sneha Maharjan, Salik Ram Khanal, Manuel Cabral Reis, João Barroso, Vítor Manuel de Jesus Filipe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increase of distance learning, in general, and e-learning, in
particular, having a system capable of determining the engagement of students
is of primordial importance, and one of the biggest challenges, both for
teachers, researchers and policy makers. Here, we present a system to detect
the engagement level of the students. It uses only information provided by the
typical built-in web-camera present in a laptop computer, and was designed to
work in real time. We combine information about the movements of the eyes and
head, and facial emotions to produce a concentration index with three classes
of engagement: "very engaged", "nominally engaged" and "not engaged at all".
The system was tested in a typical e-learning scenario, and the results show
that it correctly identifies each period of time where students were "very
engaged", "nominally engaged" and "not engaged at all". Additionally, the
results also show that the students with best scores also have higher
concentration indexes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 9 Figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ActMAD: Activation Matching to Align Distributions for
  Test-Time-Training <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.12870v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.12870v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Jehanzeb Mirza, Pol Jané Soneira, Wei Lin, Mateusz Kozinski, Horst Possegger, Horst Bischof
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Test-Time-Training (TTT) is an approach to cope with out-of-distribution
(OOD) data by adapting a trained model to distribution shifts occurring at
test-time. We propose to perform this adaptation via Activation Matching
(ActMAD): We analyze activations of the model and align activation statistics
of the OOD test data to those of the training data. In contrast to existing
methods, which model the distribution of entire channels in the ultimate layer
of the feature extractor, we model the distribution of each feature in multiple
layers across the network. This results in a more fine-grained supervision and
makes ActMAD attain state of the art performance on CIFAR-100C and Imagenet-C.
ActMAD is also architecture- and task-agnostic, which lets us go beyond image
classification, and score 15.4% improvement over previous approaches when
evaluating a KITTI-trained object detector on KITTI-Fog. Our experiments
highlight that ActMAD can be applied to online adaptation in realistic
scenarios, requiring little data to attain its full performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023 - Project Page: https://jmiemirza.github.io/ActMAD/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Planning-oriented Autonomous Driving <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.10156v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.10156v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, Wenhai Wang, Lewei Lu, Xiaosong Jia, Qiang Liu, Jifeng Dai, Yu Qiao, Hongyang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern autonomous driving system is characterized as modular tasks in
sequential order, i.e., perception, prediction, and planning. In order to
perform a wide diversity of tasks and achieve advanced-level intelligence,
contemporary approaches either deploy standalone models for individual tasks,
or design a multi-task paradigm with separate heads. However, they might suffer
from accumulative errors or deficient task coordination. Instead, we argue that
a favorable framework should be devised and optimized in pursuit of the
ultimate goal, i.e., planning of the self-driving car. Oriented at this, we
revisit the key components within perception and prediction, and prioritize the
tasks such that all these tasks contribute to planning. We introduce Unified
Autonomous Driving (UniAD), a comprehensive framework up-to-date that
incorporates full-stack driving tasks in one network. It is exquisitely devised
to leverage advantages of each module, and provide complementary feature
abstractions for agent interaction from a global perspective. Tasks are
communicated with unified query interfaces to facilitate each other toward
planning. We instantiate UniAD on the challenging nuScenes benchmark. With
extensive ablations, the effectiveness of using such a philosophy is proven by
substantially outperforming previous state-of-the-arts in all aspects. Code and
models are public.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023 award candidate. Project page:
  https://opendrivelab.github.io/UniAD/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FeatER: An Efficient Network for Human Reconstruction via Feature
  Map-Based <span class="highlight-title">TransformER</span> <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.15448v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.15448v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ce Zheng, Matias Mendieta, Taojiannan Yang, Guo-Jun Qi, Chen Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, vision transformers have shown great success in a set of human
reconstruction tasks such as 2D human pose estimation (2D HPE), 3D human pose
estimation (3D HPE), and human mesh reconstruction (HMR) tasks. In these tasks,
feature map representations of the human structural information are often
extracted first from the image by a CNN (such as HRNet), and then further
processed by transformer to predict the heatmaps (encodes each joint's location
into a feature map with a Gaussian distribution) for HPE or HMR. However,
existing transformer architectures are not able to process these feature map
inputs directly, forcing an unnatural flattening of the location-sensitive
human structural information. Furthermore, much of the performance benefit in
recent HPE and HMR methods has come at the cost of ever-increasing computation
and memory needs. Therefore, to simultaneously address these problems, we
propose FeatER, a novel transformer design that preserves the inherent
structure of feature map representations when modeling attention while reducing
memory and computational costs. Taking advantage of FeatER, we build an
efficient network for a set of human reconstruction tasks including 2D HPE, 3D
HPE, and HMR. A feature map reconstruction module is applied to improve the
performance of the estimated human pose and mesh. Extensive experiments
demonstrate the effectiveness of FeatER on various human pose and mesh
datasets. For instance, FeatER outperforms the SOTA method MeshGraphormer by
requiring 5% of Params and 16% of MACs on Human3.6M and 3DPW datasets. The
project webpage is https://zczcwh.github.io/feater_page/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cyclical Compound Domain Test-time Adaptation via Continual
  Domain-Matching Algorithm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08356v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08356v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junha Song, Kwanyong Park, InKyu Shin, Sanghyun Woo, Chaoning Zhang, In So Kweon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Test-time adaptation (TTA), a key component of lifelong learning in edge
devices, refers to the ability of a pre-trained model to adapt itself to new
environments during test time. Due to its practical ability, TTA has attracted
significant attention and experienced a rapid performance boost these days. In
this paper, we present an under-explored yet more realistic TTA scenario and
provide a strong baseline favorable to this scenario, named cyclical compound
domain (CCD). The CCD represents the real-world scenario in which the target
domain contains multiple sub-target domains (i.e., compound domain due to
weather or time change) and the sub-target domains are likely to rise
cyclically. Unfortunately, existing works do not faithfully account for this
plausible scenario, only focusing on adapting to the current sub-target domain
while discarding the past knowledge acquired from repeated sub-target domains.
Therefore, we first propose a lightweight domain-matching algorithm that allows
the TTA model to manage knowledge from the compound domain. This algorithm
identifies the type of domain among sub-target domains by continuously matching
the current image's distribution with reference domain points. Moreover, our
newly proposed regularization method compares the present distribution with
source one in order to regularize the adaptation pace according to each data in
sub-target domains. Qualitatively, we demonstrate that our simple-yet-effective
approach improves the adaptation performance on various benchmarks, including
image classification on ImageNet-C and semantic segmentation on GTA5, C-driving
datasets, and Cityscapes with corruptions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ECON: Explicit Clothed humans Optimized via Normal integration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07422v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07422v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuliang Xiu, Jinlong Yang, Xu Cao, Dimitrios Tzionas, Michael J. Black
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The combination of deep learning, artist-curated scans, and Implicit
Functions (IF), is enabling the creation of detailed, clothed, 3D humans from
images. However, existing methods are far from perfect. IF-based methods
recover free-form geometry, but produce disembodied limbs or degenerate shapes
for novel poses or clothes. To increase robustness for these cases, existing
work uses an explicit parametric body model to constrain surface
reconstruction, but this limits the recovery of free-form surfaces such as
loose clothing that deviates from the body. What we want is a method that
combines the best properties of implicit representation and explicit body
regularization. To this end, we make two key observations: (1) current networks
are better at inferring detailed 2D maps than full-3D surfaces, and (2) a
parametric model can be seen as a "canvas" for stitching together detailed
surface patches. Based on these, our method, ECON, has three main steps: (1) It
infers detailed 2D normal maps for the front and back side of a clothed person.
(2) From these, it recovers 2.5D front and back surfaces, called d-BiNI, that
are equally detailed, yet incomplete, and registers these w.r.t. each other
with the help of a SMPL-X body mesh recovered from the image. (3) It "inpaints"
the missing geometry between d-BiNI surfaces. If the face and hands are noisy,
they can optionally be replaced with the ones of SMPL-X. As a result, ECON
infers high-fidelity 3D humans even in loose clothes and challenging poses.
This goes beyond previous methods, according to the quantitative evaluation on
the CAPE and Renderpeople datasets. Perceptual studies also show that ECON's
perceived realism is better by a large margin. Code and models are available
for research purposes at econ.is.tue.mpg.de
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Homepage: https://xiuyuliang.cn/econ Code:
  https://github.com/YuliangXiu/ECON</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ShadowNeuS: Neural SDF Reconstruction by Shadow Ray Supervision <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.14086v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.14086v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingwang Ling, Zhibo Wang, Feng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  By supervising camera rays between a scene and multi-view image planes, NeRF
reconstructs a neural scene representation for the task of novel view
synthesis. On the other hand, shadow rays between the light source and the
scene have yet to be considered. Therefore, we propose a novel shadow ray
supervision scheme that optimizes both the samples along the ray and the ray
location. By supervising shadow rays, we successfully reconstruct a neural SDF
of the scene from single-view images under multiple lighting conditions. Given
single-view binary shadows, we train a neural network to reconstruct a complete
scene not limited by the camera's line of sight. By further modeling the
correlation between the image colors and the shadow rays, our technique can
also be effectively extended to RGB inputs. We compare our method with previous
works on challenging tasks of shape reconstruction from single-view binary
shadow or RGB images and observe significant improvements. The code and data
are available at https://github.com/gerwang/ShadowNeuS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023. Project page: https://gerwang.github.io/shadowneus/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Low-Rank Simplicity Bias in Deep Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2103.10427v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2103.10427v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minyoung Huh, Hossein Mobahi, Richard Zhang, Brian Cheung, Pulkit Agrawal, Phillip Isola
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern deep neural networks are highly over-parameterized compared to the
data on which they are trained, yet they often generalize remarkably well. A
flurry of recent work has asked: why do deep networks not overfit to their
training data? In this work, we make a series of empirical observations that
investigate and extend the hypothesis that deeper networks are inductively
biased to find solutions with lower effective rank embeddings. We conjecture
that this bias exists because the volume of functions that maps to low
effective rank embedding increases with depth. We show empirically that our
claim holds true on finite width linear and non-linear models on practical
learning paradigms and show that on natural data, these are often the solutions
that generalize well. We then show that the simplicity bias exists at both
initialization and after training and is resilient to hyper-parameters and
learning methods. We further demonstrate how linear over-parameterization of
deep non-linear models can be used to induce low-rank bias, improving
generalization performance on CIFAR and ImageNet without changing the modeling
capacity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Unified Arbitrary Style Transfer Framework via Adaptive Contrastive
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12710v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12710v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Zhang, Fan Tang, Weiming Dong, Haibin Huang, Chongyang Ma, Tong-Yee Lee, Changsheng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Unified Contrastive Arbitrary Style Transfer (UCAST), a novel
style representation learning and transfer framework, which can fit in most
existing arbitrary image style transfer models, e.g., CNN-based, ViT-based, and
flow-based methods. As the key component in image style transfer tasks, a
suitable style representation is essential to achieve satisfactory results.
Existing approaches based on deep neural network typically use second-order
statistics to generate the output. However, these hand-crafted features
computed from a single image cannot leverage style information sufficiently,
which leads to artifacts such as local distortions and style inconsistency. To
address these issues, we propose to learn style representation directly from a
large amount of images based on contrastive learning, by taking the
relationships between specific styles and the holistic style distribution into
account. Specifically, we present an adaptive contrastive learning scheme for
style transfer by introducing an input-dependent temperature. Our framework
consists of three key components, i.e., a parallel contrastive learning scheme
for style representation and style transfer, a domain enhancement module for
effective learning of style distribution, and a generative network for style
transfer. We carry out qualitative and quantitative evaluations to show that
our approach produces superior results than those obtained via state-of-the-art
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2205.09542</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding and Improving Visual <span class="highlight-title">Prompt</span>ing: A Label-Mapping
  Perspective <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.11635v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.11635v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aochuan Chen, Yuguang Yao, Pin-Yu Chen, Yihua Zhang, Sijia Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We revisit and advance visual prompting (VP), an input prompting technique
for vision tasks. VP can reprogram a fixed, pre-trained source model to
accomplish downstream tasks in the target domain by simply incorporating
universal prompts (in terms of input perturbation patterns) into downstream
data points. Yet, it remains elusive why VP stays effective even given a
ruleless label mapping (LM) between the source classes and the target classes.
Inspired by the above, we ask: How is LM interrelated with VP? And how to
exploit such a relationship to improve its accuracy on target tasks? We peer
into the influence of LM on VP and provide an affirmative answer that a better
'quality' of LM (assessed by mapping precision and explanation) can
consistently improve the effectiveness of VP. This is in contrast to the prior
art where the factor of LM was missing. To optimize LM, we propose a new VP
framework, termed ILM-VP (iterative label mapping-based visual prompting),
which automatically re-maps the source labels to the target labels and
progressively improves the target task accuracy of VP. Further, when using a
contrastive language-image pretrained (CLIP) model, we propose to integrate an
LM process to assist the text prompt selection of CLIP and to improve the
target task accuracy. Extensive experiments demonstrate that our proposal
significantly outperforms state-of-the-art VP methods. As highlighted below, we
show that when reprogramming an ImageNet-pretrained ResNet-18 to 13 target
tasks, our method outperforms baselines by a substantial margin, e.g., 7.9% and
6.7% accuracy improvements in transfer learning to the target Flowers102 and
CIFAR100 datasets. Besides, our proposal on CLIP-based VP provides 13.7% and
7.1% accuracy improvements on Flowers102 and DTD respectively. Our code is
available at https://github.com/OPTML-Group/ILM-VP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Instant Volumetric Head Avatars <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.12499v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.12499v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wojciech Zielonka, Timo Bolkart, Justus Thies
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Instant Volumetric Head Avatars (INSTA), a novel approach for
reconstructing photo-realistic digital avatars instantaneously. INSTA models a
dynamic neural radiance field based on neural graphics primitives embedded
around a parametric face model. Our pipeline is trained on a single monocular
RGB portrait video that observes the subject under different expressions and
views. While state-of-the-art methods take up to several days to train an
avatar, our method can reconstruct a digital avatar in less than 10 minutes on
modern GPU hardware, which is orders of magnitude faster than previous
solutions. In addition, it allows for the interactive rendering of novel poses
and expressions. By leveraging the geometry prior of the underlying parametric
face model, we demonstrate that INSTA extrapolates to unseen poses. In
quantitative and qualitative studies on various subjects, INSTA outperforms
state-of-the-art methods regarding rendering quality and training time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Website: https://zielon.github.io/insta/ Video:
  https://youtu.be/HOgaeWTih7Q Accepted to CVPR2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Modeling Inter-Class and Intra-Class Constraints in Novel Class
  Discovery <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.03591v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.03591v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenbin Li, Zhichen Fan, Jing Huo, Yang Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Novel class discovery (NCD) aims at learning a model that transfers the
common knowledge from a class-disjoint labelled dataset to another unlabelled
dataset and discovers new classes (clusters) within it. Many methods, as well
as elaborate training pipelines and appropriate objectives, have been proposed
and considerably boosted performance on NCD tasks. Despite all this, we find
that the existing methods do not sufficiently take advantage of the essence of
the NCD setting. To this end, in this paper, we propose to model both
inter-class and intra-class constraints in NCD based on the symmetric
Kullback-Leibler divergence (sKLD). Specifically, we propose an inter-class
sKLD constraint to effectively exploit the disjoint relationship between
labelled and unlabelled classes, enforcing the separability for different
classes in the embedding space. In addition, we present an intra-class sKLD
constraint to explicitly constrain the intra-relationship between a sample and
its augmentations and ensure the stability of the training process at the same
time. We conduct extensive experiments on the popular CIFAR10, CIFAR100 and
ImageNet benchmarks and successfully demonstrate that our method can establish
a new state of the art and can achieve significant performance improvements,
e.g., 3.5%/3.7% clustering accuracy improvements on CIFAR100-50 dataset split
under the task-aware/-agnostic evaluation protocol, over previous
state-of-the-art methods. Code is available at
https://github.com/FanZhichen/NCD-IIC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-view object pose estimation from correspondence distributions and
  epipolar geometry <span class="chip">ICRA 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.00924v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.00924v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rasmus Laurvig Haugaard, Thorbjørn Mosekjær Iversen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many automation tasks involving manipulation of rigid objects, the poses
of the objects must be acquired. Vision-based pose estimation using a single
RGB or RGB-D sensor is especially popular due to its broad applicability.
However, single-view pose estimation is inherently limited by depth ambiguity
and ambiguities imposed by various phenomena like occlusion, self-occlusion,
reflections, etc. Aggregation of information from multiple views can
potentially resolve these ambiguities, but the current state-of-the-art
multi-view pose estimation method only uses multiple views to aggregate
single-view pose estimates, and thus rely on obtaining good single-view
estimates. We present a multi-view pose estimation method which aggregates
learned 2D-3D distributions from multiple views for both the initial estimate
and optional refinement. Our method performs probabilistic sampling of 3D-3D
correspondences under epipolar constraints using learned 2D-3D correspondence
distributions which are implicitly trained to respect visual ambiguities such
as symmetry. Evaluation on the T-LESS dataset shows that our method reduces
pose estimation errors by 80-91% compared to the best single-view method, and
we present state-of-the-art results on T-LESS with four views, even compared
with methods using five and eight views.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 2 figures, 1 table, ICRA 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DepGraph: Towards Any Structural Pruning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.12900v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.12900v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gongfan Fang, Xinyin Ma, Mingli Song, Michael Bi Mi, Xinchao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Structural pruning enables model acceleration by removing
structurally-grouped parameters from neural networks. However, the
parameter-grouping patterns vary widely across different models, making
architecture-specific pruners, which rely on manually-designed grouping
schemes, non-generalizable to new architectures. In this work, we study a
highly-challenging yet barely-explored task, any structural pruning, to tackle
general structural pruning of arbitrary architecture like CNNs, RNNs, GNNs and
Transformers. The most prominent obstacle towards this goal lies in the
structural coupling, which not only forces different layers to be pruned
simultaneously, but also expects all removed parameters to be consistently
unimportant, thereby avoiding structural issues and significant performance
degradation after pruning. To address this problem, we propose a general and
{fully automatic} method, \emph{Dependency Graph} (DepGraph), to explicitly
model the dependency between layers and comprehensively group coupled
parameters for pruning. In this work, we extensively evaluate our method on
several architectures and tasks, including ResNe(X)t, DenseNet, MobileNet and
Vision transformer for images, GAT for graph, DGCNN for 3D point cloud,
alongside LSTM for language, and demonstrate that, even with a simple
norm-based criterion, the proposed method consistently yields gratifying
performances.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi PILOT: Learned Feasible Multiple Acquisition Trajectories for
  Dynamic MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07150v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07150v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tamir Shor, Tomer Weiss, Dor Noti, Alex Bronstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dynamic Magnetic Resonance Imaging (MRI) is known to be a powerful and
reliable technique for the dynamic imaging of internal organs and tissues,
making it a leading diagnostic tool. A major difficulty in using MRI in this
setting is the relatively long acquisition time (and, hence, increased cost)
required for imaging in high spatio-temporal resolution, leading to the
appearance of related motion artifacts and decrease in resolution. Compressed
Sensing (CS) techniques have become a common tool to reduce MRI acquisition
time by subsampling images in the k-space according to some acquisition
trajectory. Several studies have particularly focused on applying deep learning
techniques to learn these acquisition trajectories in order to attain better
image reconstruction, rather than using some predefined set of trajectories. To
the best of our knowledge, learning acquisition trajectories has been only
explored in the context of static MRI. In this study, we consider acquisition
trajectory learning in the dynamic imaging setting. We design an end-to-end
pipeline for the joint optimization of multiple per-frame acquisition
trajectories along with a reconstruction neural network, and demonstrate
improved image reconstruction quality in shorter acquisition times. The code
for reproducing all experiments is accessible at
https://github.com/tamirshor7/MultiPILOT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted For MIDL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Real-Time Evaluation in Online Continual Learning: A New Hope <span class="chip">CVPR'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.01047v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.01047v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yasir Ghunaim, Adel Bibi, Kumail Alhamoud, Motasem Alfarra, Hasan Abed Al Kader Hammoud, Ameya Prabhu, Philip H. S. Torr, Bernard Ghanem
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current evaluations of Continual Learning (CL) methods typically assume that
there is no constraint on training time and computation. This is an unrealistic
assumption for any real-world setting, which motivates us to propose: a
practical real-time evaluation of continual learning, in which the stream does
not wait for the model to complete training before revealing the next data for
predictions. To do this, we evaluate current CL methods with respect to their
computational costs. We conduct extensive experiments on CLOC, a large-scale
dataset containing 39 million time-stamped images with geolocation labels. We
show that a simple baseline outperforms state-of-the-art CL methods under this
evaluation, questioning the applicability of existing methods in realistic
settings. In addition, we explore various CL components commonly used in the
literature, including memory sampling strategies and regularization approaches.
We find that all considered methods fail to be competitive against our simple
baseline. This surprisingly suggests that the majority of existing CL
literature is tailored to a specific class of streams that is not practical. We
hope that the evaluation we provide will be the first step towards a paradigm
shift to consider the computational cost in the development of online continual
learning methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR'23 as Highlight (Top 2.5%)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning 3D-aware Image Synthesis with Unknown Pose Distribution <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07702v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07702v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zifan Shi, Yujun Shen, Yinghao Xu, Sida Peng, Yiyi Liao, Sheng Guo, Qifeng Chen, Dit-Yan Yeung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing methods for 3D-aware image synthesis largely depend on the 3D pose
distribution pre-estimated on the training set. An inaccurate estimation may
mislead the model into learning faulty geometry. This work proposes PoF3D that
frees generative radiance fields from the requirements of 3D pose priors. We
first equip the generator with an efficient pose learner, which is able to
infer a pose from a latent code, to approximate the underlying true pose
distribution automatically. We then assign the discriminator a task to learn
pose distribution under the supervision of the generator and to differentiate
real and synthesized images with the predicted pose as the condition. The
pose-free generator and the pose-aware discriminator are jointly trained in an
adversarial manner. Extensive results on a couple of datasets confirm that the
performance of our approach, regarding both image quality and geometry quality,
is on par with state of the art. To our best knowledge, PoF3D demonstrates the
feasibility of learning high-quality 3D-aware image synthesis without using 3D
pose priors for the first time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023. Project page: https://vivianszf.github.io/pof3d/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CVT-SLR: Contrastive Visual-Textual Transformation for Sign Language
  Recognition with Variational Alignment <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05725v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05725v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiangbin Zheng, Yile Wang, Cheng Tan, Siyuan Li, Ge Wang, Jun Xia, Yidong Chen, Stan Z. Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sign language recognition (SLR) is a weakly supervised task that annotates
sign videos as textual glosses. Recent studies show that insufficient training
caused by the lack of large-scale available sign datasets becomes the main
bottleneck for SLR. Most SLR works thereby adopt pretrained visual modules and
develop two mainstream solutions. The multi-stream architectures extend
multi-cue visual features, yielding the current SOTA performances but requiring
complex designs and might introduce potential noise. Alternatively, the
advanced single-cue SLR frameworks using explicit cross-modal alignment between
visual and textual modalities are simple and effective, potentially competitive
with the multi-cue framework. In this work, we propose a novel contrastive
visual-textual transformation for SLR, CVT-SLR, to fully explore the pretrained
knowledge of both the visual and language modalities. Based on the single-cue
cross-modal alignment framework, we propose a variational autoencoder (VAE) for
pretrained contextual knowledge while introducing the complete pretrained
language module. The VAE implicitly aligns visual and textual modalities while
benefiting from pretrained contextual knowledge as the traditional contextual
module. Meanwhile, a contrastive cross-modal alignment algorithm is designed to
explicitly enhance the consistency constraints. Extensive experiments on public
datasets (PHOENIX-2014 and PHOENIX-2014T) demonstrate that our proposed CVT-SLR
consistently outperforms existing single-cue methods and even outperforms SOTA
multi-cue methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2023 (Highlight Paper Top 2.5%)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diffusion Models in Vision: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.04747v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.04747v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, Mubarak Shah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Denoising diffusion models represent a recent emerging topic in computer
vision, demonstrating remarkable results in the area of generative modeling. A
diffusion model is a deep generative model that is based on two stages, a
forward diffusion stage and a reverse diffusion stage. In the forward diffusion
stage, the input data is gradually perturbed over several steps by adding
Gaussian noise. In the reverse stage, a model is tasked at recovering the
original input data by learning to gradually reverse the diffusion process,
step by step. Diffusion models are widely appreciated for the quality and
diversity of the generated samples, despite their known computational burdens,
i.e. low speeds due to the high number of steps involved during sampling. In
this survey, we provide a comprehensive review of articles on denoising
diffusion models applied in vision, comprising both theoretical and practical
contributions in the field. First, we identify and present three generic
diffusion modeling frameworks, which are based on denoising diffusion
probabilistic models, noise conditioned score networks, and stochastic
differential equations. We further discuss the relations between diffusion
models and other deep generative models, including variational auto-encoders,
generative adversarial networks, energy-based models, autoregressive models and
normalizing flows. Then, we introduce a multi-perspective categorization of
diffusion models applied in computer vision. Finally, we illustrate the current
limitations of diffusion models and envision some interesting directions for
future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in IEEE Transactions on Pattern Analysis and Machine
  Intelligence. 25 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Correspondence-Free Domain Alignment for Unsupervised Cross-Domain Image
  Retrieval <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.06081v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.06081v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xu Wang, Dezhong Peng, Ming Yan, Peng Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-domain image retrieval aims at retrieving images across different
domains to excavate cross-domain classificatory or correspondence
relationships. This paper studies a less-touched problem of cross-domain image
retrieval, i.e., unsupervised cross-domain image retrieval, considering the
following practical assumptions: (i) no correspondence relationship, and (ii)
no category annotations. It is challenging to align and bridge distinct domains
without cross-domain correspondence. To tackle the challenge, we present a
novel Correspondence-free Domain Alignment (CoDA) method to effectively
eliminate the cross-domain gap through In-domain Self-matching Supervision
(ISS) and Cross-domain Classifier Alignment (CCA). To be specific, ISS is
presented to encapsulate discriminative information into the latent common
space by elaborating a novel self-matching supervision mechanism. To alleviate
the cross-domain discrepancy, CCA is proposed to align distinct domain-specific
classifiers. Thanks to the ISS and CCA, our method could encode the
discrimination into the domain-invariant embedding space for unsupervised
cross-domain image retrieval. To verify the effectiveness of the proposed
method, extensive experiments are conducted on four benchmark datasets compared
with six state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Better Teacher Better Student: Dynamic Prior Knowledge for Knowledge
  Distillation <span class="chip">ICLR'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.06067v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.06067v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zengyu Qiu, Xinzhu Ma, Kunlin Yang, Chunya Liu, Jun Hou, Shuai Yi, Wanli Ouyang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge distillation (KD) has shown very promising capabilities in
transferring learning representations from large models (teachers) to small
models (students). However, as the capacity gap between students and teachers
becomes larger, existing KD methods fail to achieve better results. Our work
shows that the `prior knowledge' is vital to KD, especially when applying large
teachers. Particularly, we propose the dynamic prior knowledge (DPK), which
integrates part of teacher's features as the prior knowledge before the feature
distillation. This means that our method also takes the teacher's feature as
`input', not just `target'. Besides, we dynamically adjust the ratio of the
prior knowledge during the training phase according to the feature gap, thus
guiding the student in an appropriate difficulty. To evaluate the proposed
method, we conduct extensive experiments on two image classification benchmarks
(i.e. CIFAR100 and ImageNet) and an object detection benchmark (i.e. MS COCO.
The results demonstrate the superiority of our method in performance under
varying settings. Besides, our DPK makes the performance of the student model
positively correlated with that of the teacher model, which means that we can
further boost the accuracy of students by applying larger teachers. More
importantly, DPK provides a fast solution in teacher model selection for any
given model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR'23 accepted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cross Aggregation <span class="highlight-title">Transformer</span> for Image Restoration <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.13654v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.13654v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Chen, Yulun Zhang, Jinjin Gu, Yongbing Zhang, Linghe Kong, Xin Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, Transformer architecture has been introduced into image restoration
to replace convolution neural network (CNN) with surprising results.
Considering the high computational complexity of Transformer with global
attention, some methods use the local square window to limit the scope of
self-attention. However, these methods lack direct interaction among different
windows, which limits the establishment of long-range dependencies. To address
the above issue, we propose a new image restoration model, Cross Aggregation
Transformer (CAT). The core of our CAT is the Rectangle-Window Self-Attention
(Rwin-SA), which utilizes horizontal and vertical rectangle window attention in
different heads parallelly to expand the attention area and aggregate the
features cross different windows. We also introduce the Axial-Shift operation
for different window interactions. Furthermore, we propose the Locality
Complementary Module to complement the self-attention mechanism, which
incorporates the inductive bias of CNN (e.g., translation invariance and
locality) into Transformer, enabling global-local coupling. Extensive
experiments demonstrate that our CAT outperforms recent state-of-the-art
methods on several image restoration applications. The code and models are
available at https://github.com/zhengchen1999/CAT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2022. Code is available at
  https://github.com/zhengchen1999/CAT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PlaneDepth: <span class="highlight-title">Self-supervised</span> Depth Estimation via Orthogonal Planes <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.01612v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.01612v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruoyu Wang, Zehao Yu, Shenghua Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multiple near frontal-parallel planes based depth representation demonstrated
impressive results in self-supervised monocular depth estimation (MDE).
Whereas, such a representation would cause the discontinuity of the ground as
it is perpendicular to the frontal-parallel planes, which is detrimental to the
identification of drivable space in autonomous driving. In this paper, we
propose the PlaneDepth, a novel orthogonal planes based presentation, including
vertical planes and ground planes. PlaneDepth estimates the depth distribution
using a Laplacian Mixture Model based on orthogonal planes for an input image.
These planes are used to synthesize a reference view to provide the
self-supervision signal. Further, we find that the widely used resizing and
cropping data augmentation breaks the orthogonality assumptions, leading to
inferior plane predictions. We address this problem by explicitly constructing
the resizing cropping transformation to rectify the predefined planes and
predicted camera pose. Moreover, we propose an augmented self-distillation loss
supervised with a bilateral occlusion mask to boost the robustness of
orthogonal planes representation for occlusions. Thanks to our orthogonal
planes representation, we can extract the ground plane in an unsupervised
manner, which is important for autonomous driving. Extensive experiments on the
KITTI dataset demonstrate the effectiveness and efficiency of our method. The
code is available at https://github.com/svip-lab/PlaneDepth.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2023. Code and models are available at:
  https://github.com/svip-lab/PlaneDepth</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CTCNet: A CNN-<span class="highlight-title">Transformer</span> Cooperation Network for Face Image
  Super-Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.08696v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.08696v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangwei Gao, Zixiang Xu, Juncheng Li, Jian Yang, Tieyong Zeng, Guo-Jun Qi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, deep convolution neural networks (CNNs) steered face
super-resolution methods have achieved great progress in restoring degraded
facial details by jointly training with facial priors. However, these methods
have some obvious limitations. On the one hand, multi-task joint learning
requires additional marking on the dataset, and the introduced prior network
will significantly increase the computational cost of the model. On the other
hand, the limited receptive field of CNN will reduce the fidelity and
naturalness of the reconstructed facial images, resulting in suboptimal
reconstructed images. In this work, we propose an efficient CNN-Transformer
Cooperation Network (CTCNet) for face super-resolution tasks, which uses the
multi-scale connected encoder-decoder architecture as the backbone.
Specifically, we first devise a novel Local-Global Feature Cooperation Module
(LGCM), which is composed of a Facial Structure Attention Unit (FSAU) and a
Transformer block, to promote the consistency of local facial detail and global
facial structure restoration simultaneously. Then, we design an efficient
Feature Refinement Module (FRM) to enhance the encoded features. Finally, to
further improve the restoration of fine facial details, we present a
Multi-scale Feature Fusion Unit (MFFU) to adaptively fuse the features from
different stages in the encoder procedure. Extensive evaluations on various
datasets have assessed that the proposed CTCNet can outperform other
state-of-the-art methods significantly. Source code will be available at
https://github.com/IVIPLab/CTCNet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE Transactions on Image Processing, 12 figures, 9 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Hierarchical Hybrid Learning Framework for Multi-agent Trajectory
  Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12274v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12274v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujun Jiao, Mingze Miao, Zhishuai Yin, Chunyuan Lei, Xu Zhu, Linzhen Nie, Bo Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate and robust trajectory prediction of neighboring agents is critical
for autonomous vehicles traversing in complex scenes. Most methods proposed in
recent years are deep learning-based due to their strength in encoding complex
interactions. However, unplausible predictions are often generated since they
rely heavily on past observations and cannot effectively capture the transient
and contingency interactions from sparse samples. In this paper, we propose a
hierarchical hybrid framework of deep learning (DL) and reinforcement learning
(RL) for multi-agent trajectory prediction, to cope with the challenge of
predicting motions shaped by multi-scale interactions. In the DL stage, the
traffic scene is divided into multiple intermediate-scale heterogenous graphs
based on which Transformer-style GNNs are adopted to encode heterogenous
interactions at intermediate and global levels. In the RL stage, we divide the
traffic scene into local sub-scenes utilizing the key future points predicted
in the DL stage. To emulate the motion planning procedure so as to produce
trajectory predictions, a Transformer-based Proximal Policy Optimization (PPO)
incorporated with a vehicle kinematics model is devised to plan motions under
the dominant influence of microscopic interactions. A multi-objective reward is
designed to balance between agent-centric accuracy and scene-wise
compatibility. Experimental results show that our proposal matches the
state-of-the-arts on the Argoverse forecasting benchmark. It's also revealed by
the visualized results that the hierarchical learning framework captures the
multi-scale interactions and improves the feasibility and compliance of the
predicted trajectories.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Feature-Conditioned Cascaded Video Diffusion Models for Precise
  Echocardiogram Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12644v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12644v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hadrien Reynaud, Mengyun Qiao, Mischa Dombrowski, Thomas Day, Reza Razavi, Alberto Gomez, Paul Leeson, Bernhard Kainz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image synthesis is expected to provide value for the translation of machine
learning methods into clinical practice. Fundamental problems like model
robustness, domain transfer, causal modelling, and operator training become
approachable through synthetic data. Especially, heavily operator-dependant
modalities like Ultrasound imaging require robust frameworks for image and
video generation. So far, video generation has only been possible by providing
input data that is as rich as the output data, e.g., image sequence plus
conditioning in, video out. However, clinical documentation is usually scarce
and only single images are reported and stored, thus retrospective
patient-specific analysis or the generation of rich training data becomes
impossible with current approaches. In this paper, we extend elucidated
diffusion models for video modelling to generate plausible video sequences from
single images and arbitrary conditioning with clinical parameters. We explore
this idea within the context of echocardiograms by looking into the variation
of the Left Ventricle Ejection Fraction, the most essential clinical metric
gained from these examinations. We use the publicly available EchoNet-Dynamic
dataset for all our experiments. Our image to sequence approach achieves an
$R^2$ score of 93%, which is 38 points higher than recently proposed sequence
to sequence generation methods. Code and models will be available at:
https://github.com/HReynaud/EchoDiffusion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A New Benchmark: On the Utility of Synthetic Data with Blender for Bare
  Supervised Learning and Downstream Domain Adaptation <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.09165v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.09165v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hui Tang, Kui Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning in computer vision has achieved great success with the price of
large-scale labeled training data. However, exhaustive data annotation is
impracticable for each task of all domains of interest, due to high labor costs
and unguaranteed labeling accuracy. Besides, the uncontrollable data collection
process produces non-IID training and test data, where undesired duplication
may exist. All these nuisances may hinder the verification of typical theories
and exposure to new findings. To circumvent them, an alternative is to generate
synthetic data via 3D rendering with domain randomization. We in this work push
forward along this line by doing profound and extensive research on bare
supervised learning and downstream domain adaptation. Specifically, under the
well-controlled, IID data setting enabled by 3D rendering, we systematically
verify the typical, important learning insights, e.g., shortcut learning, and
discover the new laws of various data regimes and network architectures in
generalization. We further investigate the effect of image formation factors on
generalization, e.g., object scale, material texture, illumination, camera
viewpoint, and background in a 3D scene. Moreover, we use the
simulation-to-reality adaptation as a downstream task for comparing the
transferability between synthetic and real data when used for pre-training,
which demonstrates that synthetic data pre-training is also promising to
improve real test results. Lastly, to promote future research, we develop a new
large-scale synthetic-to-real benchmark for image classification, termed S2RDA,
which provides more significant challenges for transfer from simulation to
reality. The code and datasets are available at
https://github.com/huitangtang/On_the_Utility_of_Synthetic_Data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 14 figures, 5 tables, accepted by the IEEE/CVF Conference
  on Computer Vision and Pattern Recognition (CVPR), 2023. The proposed new
  synthetic-to-real benchmark S2RDA is available at
  https://pan.baidu.com/s/1fHHaqrEHbUZLXEg9XKpgSg?pwd=w9wa</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Patch of Invisibility: Naturalistic Black-Box Adversarial Attacks on
  Object Detectors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04238v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04238v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raz Lapid, Moshe Sipper
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial attacks on deep-learning models have been receiving increased
attention in recent years. Work in this area has mostly focused on
gradient-based techniques, so-called white-box attacks, wherein the attacker
has access to the targeted model's internal parameters; such an assumption is
usually unrealistic in the real world. Some attacks additionally use the entire
pixel space to fool a given model, which is neither practical nor physical
(i.e., real-world). On the contrary, we propose herein a gradient-free method
that uses the learned image manifold of a pretrained generative adversarial
network (GAN) to generate naturalistic physical adversarial patches for object
detectors. We show that our proposed method works both digitally and
physically.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ I2Edit: Towards Multi-turn Interactive Image Editing via Dialogue 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11108v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11108v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xing Cui, Zekun Li, Peipei Li, Yibo Hu, Hailin Shi, Zhaofeng He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although there have been considerable research efforts on controllable facial
image editing, the desirable interactive setting where the users can interact
with the system to adjust their requirements dynamically hasn't been well
explored. This paper focuses on facial image editing via dialogue and
introduces a new benchmark dataset, Multi-turn Interactive Image Editing
(I2Edit), for evaluating image editing quality and interaction ability in
real-world interactive facial editing scenarios. The dataset is constructed
upon the CelebA-HQ dataset with images annotated with a multi-turn dialogue
that corresponds to the user editing requirements. I2Edit is challenging, as it
needs to 1) track the dynamically updated user requirements and edit the images
accordingly, as well as 2) generate the appropriate natural language response
to communicate with the user. To address these challenges, we propose a
framework consisting of a dialogue module and an image editing module. The
former is for user edit requirements tracking and generating the corresponding
indicative responses, while the latter edits the images conditioned on the
tracked user edit requirements. In contrast to previous works that simply treat
multi-turn interaction as a sequence of single-turn interactions, we extract
the user edit requirements from the whole dialogue history instead of the
current single turn. The extracted global user edit requirements enable us to
directly edit the input raw image to avoid error accumulation and attribute
forgetting issues. Extensive quantitative and qualitative experiments on the
I2Edit dataset demonstrate the advantage of our proposed framework over the
previous single-turn methods. We believe our new dataset could serve as a
valuable resource to push forward the exploration of real-world, complex
interactive image editing. Code and data will be made public.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BBDM: Image-to-image Translation with Brownian Bridge Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.07680v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.07680v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Li, Kaitao Xue, Bin Liu, Yu-Kun Lai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image-to-image translation is an important and challenging problem in
computer vision and image processing. Diffusion models (DM) have shown great
potentials for high-quality image synthesis, and have gained competitive
performance on the task of image-to-image translation. However, most of the
existing diffusion models treat image-to-image translation as conditional
generation processes, and suffer heavily from the gap between distinct domains.
In this paper, a novel image-to-image translation method based on the Brownian
Bridge Diffusion Model (BBDM) is proposed, which models image-to-image
translation as a stochastic Brownian bridge process, and learns the translation
between two domains directly through the bidirectional diffusion process rather
than a conditional generation process. To the best of our knowledge, it is the
first work that proposes Brownian Bridge diffusion process for image-to-image
translation. Experimental results on various benchmarks demonstrate that the
proposed BBDM model achieves competitive performance through both visual
inspection and measurable metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fine-Grained Face Swapping via Regional GAN Inversion <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.14068v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.14068v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhian Liu, Maomao Li, Yong Zhang, Cairong Wang, Qi Zhang, Jue Wang, Yongwei Nie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel paradigm for high-fidelity face swapping that faithfully
preserves the desired subtle geometry and texture details. We rethink face
swapping from the perspective of fine-grained face editing, \textit{i.e.,
``editing for swapping'' (E4S)}, and propose a framework that is based on the
explicit disentanglement of the shape and texture of facial components.
Following the E4S principle, our framework enables both global and local
swapping of facial features, as well as controlling the amount of partial
swapping specified by the user. Furthermore, the E4S paradigm is inherently
capable of handling facial occlusions by means of facial masks. At the core of
our system lies a novel Regional GAN Inversion (RGI) method, which allows the
explicit disentanglement of shape and texture. It also allows face swapping to
be performed in the latent space of StyleGAN. Specifically, we design a
multi-scale mask-guided encoder to project the texture of each facial component
into regional style codes. We also design a mask-guided injection module to
manipulate the feature maps with the style codes. Based on the disentanglement,
face swapping is reformulated as a simplified problem of style and mask
swapping. Extensive experiments and comparisons with current state-of-the-art
methods demonstrate the superiority of our approach in preserving texture and
shape details, as well as working with high resolution images. The project page
is http://e4s2022.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Weakly-Supervised Text Instance Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10848v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10848v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyan Zu, Haiyang Yu, Bin Li, Xiangyang Xue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text segmentation is a challenging vision task with many downstream
applications. Current text segmentation methods require pixel-level
annotations, which are expensive in the cost of human labor and limited in
application scenarios. In this paper, we take the first attempt to perform
weakly-supervised text instance segmentation by bridging text recognition and
text segmentation. The insight is that text recognition methods provide precise
attention position of each text instance, and the attention location can feed
to both a text adaptive refinement head (TAR) and a text segmentation head.
Specifically, the proposed TAR generates pseudo labels by performing two-stage
iterative refinement operations on the attention location to fit the accurate
boundaries of the corresponding text instance. Meanwhile, the text segmentation
head takes the rough attention location to predict segmentation masks which are
supervised by the aforementioned pseudo labels. In addition, we design a
mask-augmented contrastive learning by treating our segmentation result as an
augmented version of the input text image, thus improving the visual
representation and further enhancing the performance of both recognition and
segmentation. The experimental results demonstrate that the proposed method
significantly outperforms weakly-supervised instance segmentation methods on
ICDAR13-FST (18.95$\%$ improvement) and TextSeg (17.80$\%$ improvement)
benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-modal Gated Mixture of Local-to-Global Experts for Dynamic Image
  Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.01392v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.01392v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Sun, Bing Cao, Pengfei Zhu, Qinghua Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Infrared and visible image fusion aims to integrate comprehensive information
from multiple sources to achieve superior performances on various practical
tasks, such as detection, over that of a single modality. However, most
existing methods directly combined the texture details and object contrast of
different modalities, ignoring the dynamic changes in reality, which diminishes
the visible texture in good lighting conditions and the infrared contrast in
low lighting conditions. To fill this gap, we propose a dynamic image fusion
framework with a multi-modal gated mixture of local-to-global experts, termed
MoE-Fusion, to dynamically extract effective and comprehensive information from
the respective modalities. Our model consists of a Mixture of Local Experts
(MoLE) and a Mixture of Global Experts (MoGE) guided by a multi-modal gate. The
MoLE performs specialized learning of multi-modal local features, prompting the
fused images to retain the local information in a sample-adaptive manner, while
the MoGE focuses on the global information that complements the fused image
with overall texture detail and contrast. Extensive experiments show that our
MoE-Fusion outperforms state-of-the-art methods in preserving multi-modal image
texture and contrast through the local-to-global dynamic learning paradigm, and
also achieves superior performance on detection tasks. Our code will be
available: https://github.com/SunYM2020/MoE-Fusion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unsupervised Domain Generalization for Person Re-identification: A
  Domain-specific Adaptive Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.15077v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.15077v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Qi, Jiaqi Liu, Lei Wang, Yinghuan Shi, Xin Geng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain generalization (DG) has attracted much attention in person
re-identification (ReID) recently. It aims to make a model trained on multiple
source domains generalize to an unseen target domain. Although achieving
promising progress, existing methods usually need the source domains to be
labeled, which could be a significant burden for practical ReID tasks. In this
paper, we turn to investigate unsupervised domain generalization for ReID, by
assuming that no label is available for any source domains.
  To address this challenging setting, we propose a simple and efficient
domain-specific adaptive framework, and realize it with an adaptive
normalization module designed upon the batch and instance normalization
techniques. In doing so, we successfully yield reliable pseudo-labels to
implement training and also enhance the domain generalization capability of the
model as required. In addition, we show that our framework can even be applied
to improve person ReID under the settings of supervised domain generalization
and unsupervised domain adaptation, demonstrating competitive performance with
respect to relevant methods. Extensive experimental study on benchmark datasets
is conducted to validate the proposed framework. A significance of our work
lies in that it shows the potential of unsupervised domain generalization for
person ReID and sets a strong baseline for the further research on this topic.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Pattern Recognition (PR)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Versatile Diffusion: Text, Images and Variations All in One Diffusion
  Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.08332v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.08332v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingqian Xu, Zhangyang Wang, Eric Zhang, Kai Wang, Humphrey Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in diffusion models have set an impressive milestone in many
generation tasks, and trending works such as DALL-E2, Imagen, and Stable
Diffusion have attracted great interest. Despite the rapid landscape changes,
recent new approaches focus on extensions and performance rather than capacity,
thus requiring separate models for separate tasks. In this work, we expand the
existing single-flow diffusion pipeline into a multi-task multimodal network,
dubbed Versatile Diffusion (VD), that handles multiple flows of text-to-image,
image-to-text, and variations in one unified model. The pipeline design of VD
instantiates a unified multi-flow diffusion framework, consisting of sharable
and swappable layer modules that enable the crossmodal generality beyond images
and text. Through extensive experiments, we demonstrate that VD successfully
achieves the following: a) VD outperforms the baseline approaches and handles
all its base tasks with competitive quality; b) VD enables novel extensions
such as disentanglement of style and semantics, dual- and multi-context
blending, etc.; c) The success of our multi-flow multimodal framework over
images and text may inspire further diffusion-based universal AI research. Our
code and models are open-sourced at
https://github.com/SHI-Labs/Versatile-Diffusion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Github link: https://github.com/SHI-Labs/Versatile-Diffusion</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quality-aware <span class="highlight-title">Pre-train</span>ed Models for Blind Image Quality Assessment <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.00521v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.00521v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Zhao, Kun Yuan, Ming Sun, Mading Li, Xing Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Blind image quality assessment (BIQA) aims to automatically evaluate the
perceived quality of a single image, whose performance has been improved by
deep learning-based methods in recent years. However, the paucity of labeled
data somewhat restrains deep learning-based BIQA methods from unleashing their
full potential. In this paper, we propose to solve the problem by a pretext
task customized for BIQA in a self-supervised learning manner, which enables
learning representations from orders of magnitude more data. To constrain the
learning process, we propose a quality-aware contrastive loss based on a simple
assumption: the quality of patches from a distorted image should be similar,
but vary from patches from the same image with different degradations and
patches from different images. Further, we improve the existing degradation
process and form a degradation space with the size of roughly $2\times10^7$.
After pre-trained on ImageNet using our method, models are more sensitive to
image quality and perform significantly better on downstream BIQA tasks.
Experimental results show that our method obtains remarkable improvements on
popular BIQA datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revisiting <span class="highlight-title">Transformer</span> for Point Cloud-based 3D Scene Graph Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11048v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11048v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changsheng Lv, Mengshi Qi, Xia Li, Zhengyuan Yang, Huadong Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose the semantic graph Transformer (SGT) for 3D scene
graph generation. The task aims to parse a cloud point-based scene into a
semantic structural graph, with the core challenge of modeling the complex
global structure. Existing methods based on graph convolutional networks (GCNs)
suffer from the over-smoothing dilemma and could only propagate information
from limited neighboring nodes. In contrast, our SGT uses Transformer layers as
the base building block to allow global information passing, with two types of
proposed Transformer layers tailored for the 3D scene graph generation task.
Specifically, we introduce the graph embedding layer to best utilize the global
information in graph edges while maintaining comparable computation costs.
Additionally, we propose the semantic injection layer to leverage categorical
text labels and visual object knowledge. We benchmark our SGT on the
established 3DSSG benchmark and achieve a 35.9% absolute improvement in
relationship prediction's R@50 and an 80.4% boost on the subset with complex
scenes over the state-of-the-art. Our analyses further show SGT's superiority
in the long-tailed and zero-shot scenarios. We will release the code and model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DiffMIC: Dual-Guidance Diffusion Network for Medical Image
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10610v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10610v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yijun Yang, Huazhu Fu, Angelica I. Aviles-Rivero, Carola-Bibiane Schönlieb, Lei Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion Probabilistic Models have recently shown remarkable performance in
generative image modeling, attracting significant attention in the computer
vision community. However, while a substantial amount of diffusion-based
research has focused on generative tasks, few studies have applied diffusion
models to general medical image classification. In this paper, we propose the
first diffusion-based model (named DiffMIC) to address general medical image
classification by eliminating unexpected noise and perturbations in medical
images and robustly capturing semantic representation. To achieve this goal, we
devise a dual conditional guidance strategy that conditions each diffusion step
with multiple granularities to improve step-wise regional attention.
Furthermore, we propose learning the mutual information in each granularity by
enforcing Maximum-Mean Discrepancy regularization during the diffusion forward
process. We evaluate the effectiveness of our DiffMIC on three medical
classification tasks with different image modalities, including placental
maturity grading on ultrasound images, skin lesion classification using
dermatoscopic images, and diabetic retinopathy grading using fundus images. Our
experimental results demonstrate that DiffMIC outperforms state-of-the-art
methods by a significant margin, indicating the universality and effectiveness
of the proposed model. Our code will be publicly available at
https://github.com/scott-yjyang/DiffMIC.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Renderable Neural Radiance Map for Visual Navigation <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.00304v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.00304v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Obin Kwon, Jeongho Park, Songhwai Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel type of map for visual navigation, a renderable neural
radiance map (RNR-Map), which is designed to contain the overall visual
information of a 3D environment. The RNR-Map has a grid form and consists of
latent codes at each pixel. These latent codes are embedded from image
observations, and can be converted to the neural radiance field which enables
image rendering given a camera pose. The recorded latent codes implicitly
contain visual information about the environment, which makes the RNR-Map
visually descriptive. This visual information in RNR-Map can be a useful
guideline for visual localization and navigation. We develop localization and
navigation frameworks that can effectively utilize the RNR-Map. We evaluate the
proposed frameworks on camera tracking, visual localization, and image-goal
navigation. Experimental results show that the RNR-Map-based localization
framework can find the target location based on a single query image with fast
speed and competitive accuracy compared to other baselines. Also, this
localization framework is robust to environmental changes, and even finds the
most visually similar places when a query image from a different environment is
given. The proposed navigation framework outperforms the existing image-goal
navigation methods in difficult scenarios, under odometry and actuation noises.
The navigation framework shows 65.7% success rate in curved scenarios of the
NRNS dataset, which is an improvement of 18.6% over the current
state-of-the-art. Project page: https://rllab-snu.github.io/projects/RNR-Map/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint version. CVPR 2023 accepted, highlight paper. Project page:
  https://rllab-snu.github.io/projects/RNR-Map/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Masked Motion Encoding for <span class="highlight-title">Self-Supervised</span> Video Representation Learning <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.06096v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.06096v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Sun, Peihao Chen, Liangwei Chen, Changhao Li, Thomas H. Li, Mingkui Tan, Chuang Gan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How to learn discriminative video representation from unlabeled videos is
challenging but crucial for video analysis. The latest attempts seek to learn a
representation model by predicting the appearance contents in the masked
regions. However, simply masking and recovering appearance contents may not be
sufficient to model temporal clues as the appearance contents can be easily
reconstructed from a single frame. To overcome this limitation, we present
Masked Motion Encoding (MME), a new pre-training paradigm that reconstructs
both appearance and motion information to explore temporal clues. In MME, we
focus on addressing two critical challenges to improve the representation
performance: 1) how to well represent the possible long-term motion across
multiple frames; and 2) how to obtain fine-grained temporal clues from sparsely
sampled videos. Motivated by the fact that human is able to recognize an action
by tracking objects' position changes and shape changes, we propose to
reconstruct a motion trajectory that represents these two kinds of change in
the masked regions. Besides, given the sparse video input, we enforce the model
to reconstruct dense motion trajectories in both spatial and temporal
dimensions. Pre-trained with our MME paradigm, the model is able to anticipate
long-term and fine-grained motion details. Code is available at
https://github.com/XinyuSun/MME.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023 camera-ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Correlational Image Modeling for <span class="highlight-title">Self-Supervised</span> Visual <span class="highlight-title">Pre-Train</span>ing <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12670v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12670v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Li, Jiahao Xie, Chen Change Loy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Correlational Image Modeling (CIM), a novel and surprisingly
effective approach to self-supervised visual pre-training. Our CIM performs a
simple pretext task: we randomly crop image regions (exemplars) from an input
image (context) and predict correlation maps between the exemplars and the
context. Three key designs enable correlational image modeling as a nontrivial
and meaningful self-supervisory task. First, to generate useful
exemplar-context pairs, we consider cropping image regions with various scales,
shapes, rotations, and transformations. Second, we employ a bootstrap learning
framework that involves online and target encoders. During pre-training, the
former takes exemplars as inputs while the latter converts the context. Third,
we model the output correlation maps via a simple cross-attention block, within
which the context serves as queries and the exemplars offer values and keys. We
show that CIM performs on par or better than the current state of the art on
self-supervised and transfer benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adjust Sample Imbalance and Exclude Similar Object in Underwater Object
  Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.01482v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.01482v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunfeng Li, Bo Wang, Ye Li, Wei Huo, Zhuoyan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although modern trackers have competitive performance when dealing with
underwater image degradation, there are still two problems when applying them
to Underwater Object Tracking (UOT). On the one hand, the single object tracker
is trained on the open-air datasets, which means that the tracker has a serious
sample imbalance between underwater objects and open-air objects when applied
to UOT. On the other hand, underwater targets such as fish and dolphins usually
have a similar appearance, it is challenging for the model itself to
discriminate the weak discriminative features. The existing detection-based
post processing is hard to distinguish the tracked target among similar
objects. In this paper, we propose UOSTrack, which consists of Underwater
images and Open-air sequences Hybrid Training (UOHT) and Motion-based Post
Processing (MBPP). UOHT is designed to adjust the sample imbalance underwater
tracker. Specifically, Underwater Object Detection (UOD) image is converted
into imag pairs through customized data augmentation, so that the tracker has
more underwater domain training samples and learn the feature expression of
underwater objects. MBPP is proposed to exclude similar objects around the
target. Specifically, it uses the estimation box predicted by the Kalman Filter
and candidate boxes in each frame to reconfirm the target that is hidden in the
candidate area when the target is lost. UOSTrack has an average performance
improvement of 3.5% over OSTrack on Similar Object challenge of the UOT100 and
UTB180 datasets. The average performance improvement of UOSTrack on UOT100 and
UTB180 is 1% and 3%, respectively. Experiments on two UOT benchmarks
demonstrate the effectiveness of UOHT and MBPP, and the generalization and
applicability of MBPP for UOT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Feature Distillation for Zero-shot Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12145v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12145v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuoming Liu, Xuefeng Hu, Ram Nevatia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The large-scale vision-language models (e.g., CLIP) are leveraged by
different methods to detect unseen objects. However, most of these works
require additional captions or images for training, which is not feasible in
the context of zero-shot detection. In contrast, the distillation-based method
is an extra-data-free method, but it has its limitations. Specifically,
existing work creates distillation regions that are biased to the base
categories, which limits the distillation of novel category information and
harms the distillation efficiency. Furthermore, directly using the raw feature
from CLIP for distillation neglects the domain gap between the training data of
CLIP and the detection datasets, which makes it difficult to learn the mapping
from the image region to the vision-language feature space - an essential
component for detecting unseen objects. As a result, existing
distillation-based methods require an excessively long training schedule. To
solve these problems, we propose Efficient feature distillation for Zero-Shot
Detection (EZSD). Firstly, EZSD adapts the CLIP's feature space to the target
detection domain by re-normalizing CLIP to bridge the domain gap; Secondly,
EZSD uses CLIP to generate distillation proposals with potential novel
instances, to avoid the distillation being overly biased to the base
categories. Finally, EZSD takes advantage of semantic meaning for regression to
further improve the model performance. As a result, EZSD achieves
state-of-the-art performance in the COCO zero-shot benchmark with a much
shorter training schedule and outperforms previous work by 4% in LVIS overall
setting with 1/10 training time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Discontinuity for Video Frame Interpolation <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.07291v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.07291v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sangjin Lee, Hyeongmin Lee, Chajin Shin, Hanbin Son, Sangyoun Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video frame interpolation (VFI) is the task that synthesizes the intermediate
frame given two consecutive frames. Most of the previous studies have focused
on appropriate frame warping operations and refinement modules for the warped
frames. These studies have been conducted on natural videos containing only
continuous motions. However, many practical videos contain various unnatural
objects with discontinuous motions such as logos, user interfaces and
subtitles. We propose three techniques to make the existing deep learning-based
VFI architectures robust to these elements. First is a novel data augmentation
strategy called figure-text mixing (FTM) which can make the models learn
discontinuous motions during training stage without any extra dataset. Second,
we propose a simple but effective module that predicts a map called
discontinuity map (D-map), which densely distinguishes between areas of
continuous and discontinuous motions. Lastly, we propose loss functions to give
supervisions of the discontinuous motion areas which can be applied along with
FTM and D-map. We additionally collect a special test benchmark called
Graphical Discontinuous Motion (GDM) dataset consisting of some mobile games
and chatting videos. Applied to the various state-of-the-art VFI networks, our
method significantly improves the interpolation qualities on the videos from
not only GDM dataset, but also the existing benchmarks containing only
continuous motions such as Vimeo90K, UCF101, and DAVIS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>highlight at CVPR 2023 (10% of accepted papers)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Empirical Study of End-to-End Video-Language <span class="highlight-title">Transformer</span>s with Masked
  Visual Modeling <span class="chip">CVPR'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.01540v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.01540v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tsu-Jui Fu, Linjie Li, Zhe Gan, Kevin Lin, William Yang Wang, Lijuan Wang, Zicheng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Masked visual modeling (MVM) has been recently proven effective for visual
pre-training. While similar reconstructive objectives on video inputs (e.g.,
masked frame modeling) have been explored in video-language (VidL)
pre-training, previous studies fail to find a truly effective MVM strategy that
can largely benefit the downstream performance. In this work, we systematically
examine the potential of MVM in the context of VidL learning. Specifically, we
base our study on a fully end-to-end VIdeO-LanguagE Transformer (VIOLET), where
the supervision from MVM training can be backpropagated to the video pixel
space. In total, eight different reconstructive targets of MVM are explored,
from low-level pixel values and oriented gradients to high-level depth maps,
optical flow, discrete visual tokens, and latent visual features. We conduct
comprehensive experiments and provide insights into the factors leading to
effective MVM training, resulting in an enhanced model VIOLETv2. Empirically,
we show VIOLETv2 pre-trained with MVM objective achieves notable improvements
on 13 VidL benchmarks, ranging from video question answering, video captioning,
to text-to-video retrieval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR'23; the first two authors contributed equally; code is available
  at https://github.com/tsujuifu/pytorch_empirical-mvm</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">7</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Why and When: Understanding System Initiative during Conversational
  Collaborative Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13484v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13484v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sandeep Avula, Bogeum Choi, Jaime Arguello
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the last decade, conversational search has attracted considerable
attention. However, most research has focused on systems that can support a
\emph{single} searcher. In this paper, we explore how systems can support
\emph{multiple} searchers collaborating over an instant messaging platform
(i.e., Slack). We present a ``Wizard of Oz'' study in which 27 participant
pairs collaborated on three information-seeking tasks over Slack. Participants
were unable to search on their own and had to gather information by interacting
with a \emph{searchbot} directly from the Slack channel. The role of the
searchbot was played by a reference librarian. Conversational search systems
must be capable of engaging in \emph{mixed-initiative} interaction by taking
and relinquishing control of the conversation to fulfill different objectives.
Discourse analysis research suggests that conversational agents can take
\emph{two} levels of initiative: dialog- and task-level initiative. Agents take
dialog-level initiative to establish mutual belief between agents and
task-level initiative to influence the goals of the other agents. During the
study, participants were exposed to three experimental conditions in which the
searchbot could take different levels of initiative: (1) no initiative, (2)
only dialog-level initiative, and (3) both dialog- and task-level initiative.
In this paper, we focus on understanding the Wizard's actions. Specifically, we
focus on understanding the Wizard's motivations for taking initiative and their
rationale for the timing of each intervention. To gain insights about the
Wizard's actions, we conducted a stimulated recall interview with the Wizard.
We present findings from a qualitative analysis of this interview data and
discuss implications for designing conversational search systems to support
collaborative search.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modular Retrieval for Generalization and Interpretation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13419v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13419v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juhao Liang, Chen Zhang, Zhengyang Tang, Jie Fu, Dawei Song, Benyou Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  New retrieval tasks have always been emerging, thus urging the development of
new retrieval models. However, instantiating a retrieval model for each new
retrieval task is resource-intensive and time-consuming, especially for a
retrieval model that employs a large-scale pre-trained language model. To
address this issue, we shift to a novel retrieval paradigm called modular
retrieval, which aims to solve new retrieval tasks by instead composing
multiple existing retrieval modules. Built upon the paradigm, we propose a
retrieval model with modular prompt tuning named REMOP. It constructs retrieval
modules subject to task attributes with deep prompt tuning, and yields
retrieval models subject to tasks with module composition. We validate that,
REMOP inherently with modularity not only has appealing generalizability and
interpretability in preliminary explorations, but also achieves comparable
performance to state-of-the-art retrieval models on a zero-shot retrieval
benchmark.\footnote{Our code is available at
\url{https://github.com/FreedomIntelligence/REMOP}}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Unified Framework for Learned Sparse Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13416v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13416v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thong Nguyen, Sean MacAvaney, Andrew Yates
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learned sparse retrieval (LSR) is a family of first-stage retrieval methods
that are trained to generate sparse lexical representations of queries and
documents for use with an inverted index. Many LSR methods have been recently
introduced, with Splade models achieving state-of-the-art performance on
MSMarco. Despite similarities in their model architectures, many LSR methods
show substantial differences in effectiveness and efficiency. Differences in
the experimental setups and configurations used make it difficult to compare
the methods and derive insights. In this work, we analyze existing LSR methods
and identify key components to establish an LSR framework that unifies all LSR
methods under the same perspective. We then reproduce all prominent methods
using a common codebase and re-train them in the same environment, which allows
us to quantify how components of the framework affect effectiveness and
efficiency. We find that (1) including document term weighting is most
important for a method's effectiveness, (2) including query weighting has a
small positive impact, and (3) document expansion and query expansion have a
cancellation effect. As a result, we show how removing query expansion from a
state-of-the-art model can reduce latency significantly while maintaining
effectiveness on MSMarco and TripClick benchmarks. Our code is publicly
available at https://github.com/thongnt99/learned-sparse-retrieval
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GETT-QA: Graph Embedding based T2T <span class="highlight-title">Transformer</span> for Knowledge Graph
  Question Answering <span class="chip">ESWC 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13284v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13284v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Debayan Banerjee, Pranav Ajit Nair, Ricardo Usbeck, Chris Biemann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we present an end-to-end Knowledge Graph Question Answering
(KGQA) system named GETT-QA. GETT-QA uses T5, a popular text-to-text
pre-trained language model. The model takes a question in natural language as
input and produces a simpler form of the intended SPARQL query. In the simpler
form, the model does not directly produce entity and relation IDs. Instead, it
produces corresponding entity and relation labels. The labels are grounded to
KG entity and relation IDs in a subsequent step. To further improve the
results, we instruct the model to produce a truncated version of the KG
embedding for each entity. The truncated KG embedding enables a finer search
for disambiguation purposes. We find that T5 is able to learn the truncated KG
embeddings without any change of loss function, improving KGQA performance. As
a result, we report strong results for LC-QuAD 2.0 and SimpleQuestions-Wikidata
datasets on end-to-end KGQA over Wikidata.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages single column format accepted at ESWC 2023 research track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parameter-Efficient Sparse Retrievers and Rerankers using Adapters <span class="chip">ECIR'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13220v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13220v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vaishali Pal, Carlos Lassance, Hervé Déjean, Stéphane Clinchant
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parameter-Efficient transfer learning with Adapters have been studied in
Natural Language Processing (NLP) as an alternative to full fine-tuning.
Adapters are memory-efficient and scale well with downstream tasks by training
small bottle-neck layers added between transformer layers while keeping the
large pretrained language model (PLMs) frozen. In spite of showing promising
results in NLP, these methods are under-explored in Information Retrieval.
While previous studies have only experimented with dense retriever or in a
cross lingual retrieval scenario, in this paper we aim to complete the picture
on the use of adapters in IR. First, we study adapters for SPLADE, a sparse
retriever, for which adapters not only retain the efficiency and effectiveness
otherwise achieved by finetuning, but are memory-efficient and orders of
magnitude lighter to train. We observe that Adapters-SPLADE not only optimizes
just 2\% of training parameters, but outperforms fully fine-tuned counterpart
and existing parameter-efficient dense IR models on IR benchmark datasets.
Secondly, we address domain adaptation of neural retrieval thanks to adapters
on cross-domain BEIR datasets and TripClick. Finally, we also consider
knowledge sharing between rerankers and first stage rankers. Overall, our study
complete the examination of adapters for neural IR
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at ECIR'23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Limits of Predictability in Top-N Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13091v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13091v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        En Xu, Zhiwen Yu, Ying Zhang, Bin Guo, Lina Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Top-N recommendation aims to recommend each consumer a small set of N items
from a large collection of items, and its accuracy is one of the most common
indexes to evaluate the performance of a recommendation system. While a large
number of algorithms are proposed to push the Top-N accuracy by learning the
user preference from their history purchase data, a predictability question is
naturally raised - whether there is an upper limit of such Top-N accuracy. This
work investigates such predictability by studying the degree of regularity from
a specific set of user behavior data. Quantifying the predictability of Top-N
recommendations requires simultaneously quantifying the limits on the accuracy
of the N behaviors with the highest probability. This greatly increases the
difficulty of the problem. To achieve this, we firstly excavate the
associations among N behaviors with the highest probability and describe the
user behavior distribution based on the information theory. Then, we adopt the
Fano inequality to scale and obtain the Top-N predictability. Extensive
experiments are conducted on the real-world data where significant improvements
are observed compared to the state-of-the-art methods. We have not only
completed the predictability calculation for N targets but also obtained
predictability that is much closer to the true value than existing methods. We
expect our results to assist these research areas where the quantitative
requirement of Top-N predictability is required.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncertainty Calibration for Counterfactual Propensity Estimation in
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12973v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12973v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenbo Hu, Xin Sun, Qiang liu, Shu Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recommendation systems, a large portion of the ratings are missing due to
the selection biases, which is known as Missing Not At Random. The
counterfactual inverse propensity scoring (IPS) was used to weight the
imputation error of every observed rating. Although effective in multiple
scenarios, we argue that the performance of IPS estimation is limited due to
the uncertainty miscalibration of propensity estimation. In this paper, we
propose the uncertainty calibration for the propensity estimation in
recommendation systems with multiple representative uncertainty calibration
techniques. Theoretical analysis on the bias and generalization bound shows the
superiority of the calibrated IPS estimator over the uncalibrated one.
Experimental results on the coat and yahoo datasets shows that the uncertainty
calibration is improved and hence brings the better recommendation results.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">108</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning and Verification of Task Structure in Instructional Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13519v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13519v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Medhini Narasimhan, Licheng Yu, Sean Bell, Ning Zhang, Trevor Darrell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given the enormous number of instructional videos available online, learning
a diverse array of multi-step task models from videos is an appealing goal. We
introduce a new pre-trained video model, VideoTaskformer, focused on
representing the semantics and structure of instructional videos. We pre-train
VideoTaskformer using a simple and effective objective: predicting weakly
supervised textual labels for steps that are randomly masked out from an
instructional video (masked step modeling). Compared to prior work which learns
step representations locally, our approach involves learning them globally,
leveraging video of the entire surrounding task as context. From these learned
representations, we can verify if an unseen video correctly executes a given
task, as well as forecast which steps are likely to be taken after a given
step. We introduce two new benchmarks for detecting mistakes in instructional
videos, to verify if there is an anomalous step and if steps are executed in
the right order. We also introduce a long-term forecasting benchmark, where the
goal is to predict long-range future steps from a given step. Our method
outperforms previous baselines on these tasks, and we believe the tasks will be
a valuable way for the community to measure the quality of step
representations. Additionally, we evaluate VideoTaskformer on 3 existing
benchmarks -- procedural activity recognition, step classification, and step
forecasting -- and demonstrate on each that our method outperforms existing
baselines and achieves new state-of-the-art performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Wesbite at https://medhini.github.io/task_structure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Three ways to improve feature alignment for open vocabulary detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13518v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13518v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Relja Arandjelović, Alex Andonian, Arthur Mensch, Olivier J. Hénaff, Jean-Baptiste Alayrac, Andrew Zisserman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The core problem in zero-shot open vocabulary detection is how to align
visual and text features, so that the detector performs well on unseen classes.
Previous approaches train the feature pyramid and detection head from scratch,
which breaks the vision-text feature alignment established during pretraining,
and struggles to prevent the language model from forgetting unseen classes.
  We propose three methods to alleviate these issues. Firstly, a simple scheme
is used to augment the text embeddings which prevents overfitting to a small
number of classes seen during training, while simultaneously saving memory and
computation. Secondly, the feature pyramid network and the detection head are
modified to include trainable gated shortcuts, which encourages vision-text
feature alignment and guarantees it at the start of detection training.
Finally, a self-training approach is used to leverage a larger corpus of
image-text pairs thus improving detection performance on classes with no human
annotated bounding boxes.
  Our three methods are evaluated on the zero-shot version of the LVIS
benchmark, each of them showing clear and significant benefits. Our final
network achieves the new stateof-the-art on the mAP-all metric and demonstrates
competitive performance for mAP-rare, as well as superior transfer to COCO and
Objects365.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ablating Concepts in Text-to-Image Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13516v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13516v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nupur Kumari, Bingliang Zhang, Sheng-Yu Wang, Eli Shechtman, Richard Zhang, Jun-Yan Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale text-to-image diffusion models can generate high-fidelity images
with powerful compositional ability. However, these models are typically
trained on an enormous amount of Internet data, often containing copyrighted
material, licensed images, and personal photos. Furthermore, they have been
found to replicate the style of various living artists or memorize exact
training samples. How can we remove such copyrighted concepts or images without
retraining the model from scratch? To achieve this goal, we propose an
efficient method of ablating concepts in the pretrained model, i.e., preventing
the generation of a target concept. Our algorithm learns to match the image
distribution for a target style, instance, or text prompt we wish to ablate to
the distribution corresponding to an anchor concept. This prevents the model
from generating target concepts given its text condition. Extensive experiments
show that our method can successfully prevent the generation of the ablated
concept while preserving closely related concepts in the model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>project website: https://www.cs.cmu.edu/~concept-ablation/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Persistent Nature: A Generative Model of Unbounded 3D Worlds <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13515v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13515v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucy Chai, Richard Tucker, Zhengqi Li, Phillip Isola, Noah Snavely
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite increasingly realistic image quality, recent 3D image generative
models often operate on 3D volumes of fixed extent with limited camera motions.
We investigate the task of unconditionally synthesizing unbounded nature
scenes, enabling arbitrarily large camera motion while maintaining a persistent
3D world model. Our scene representation consists of an extendable, planar
scene layout grid, which can be rendered from arbitrary camera poses via a 3D
decoder and volume rendering, and a panoramic skydome. Based on this
representation, we learn a generative world model solely from single-view
internet photos. Our method enables simulating long flights through 3D
landscapes, while maintaining global scene consistency--for instance, returning
to the starting point yields the same view of the scene. Our approach enables
scene extrapolation beyond the fixed bounds of current 3D generative models,
while also supporting a persistent, camera-independent world representation
that stands in contrast to auto-regressive 3D prediction models. Our project
page: https://chail.github.io/persistent-nature/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR camera ready version, project page:
  https://chail.github.io/persistent-nature/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Preset for Color Style Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13511v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13511v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhanghan Ke, Yuhao Liu, Lei Zhu, Nanxuan Zhao, Rynson W. H. Lau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a Neural Preset technique to address the
limitations of existing color style transfer methods, including visual
artifacts, vast memory requirement, and slow style switching speed. Our method
is based on two core designs. First, we propose Deterministic Neural Color
Mapping (DNCM) to consistently operate on each pixel via an image-adaptive
color mapping matrix, avoiding artifacts and supporting high-resolution inputs
with a small memory footprint. Second, we develop a two-stage pipeline by
dividing the task into color normalization and stylization, which allows
efficient style switching by extracting color styles as presets and reusing
them on normalized input images. Due to the unavailability of pairwise
datasets, we describe how to train Neural Preset via a self-supervised
strategy. Various advantages of Neural Preset over existing methods are
demonstrated through comprehensive evaluations. Besides, we show that our
trained model can naturally support multiple applications without fine-tuning,
including low-light image enhancement, underwater image correction, image
dehazing, and image harmonization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Quantization Model of Neural Scaling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13506v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13506v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eric J. Michaud, Ziming Liu, Uzay Girit, Max Tegmark
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose the $\textit{Quantization Model}$ of neural scaling laws,
explaining both the observed power law dropoff of loss with model and data
size, and also the sudden emergence of new capabilities with scale. We derive
this model from what we call the $\textit{Quantization Hypothesis}$, where
learned network capabilities are quantized into discrete chunks
($\textit{quanta}$). We show that when quanta are learned in order of
decreasing use frequency, then a power law in use frequencies explains observed
power law scaling of loss. We validate this prediction on toy datasets, then
study how scaling curves decompose for large language models. Using language
model internals, we auto-discover diverse model capabilities (quanta) and find
tentative evidence that the distribution over corresponding subproblems in the
prediction of natural text is compatible with the power law predicted from the
neural scaling exponent as predicted from our theory.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Chordal Averaging on Flag Manifolds and Its Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13501v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13501v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathan Mankovich, Tolga Birdal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a new, provably-convergent algorithm for computing the
flag-mean and flag-median of a set of points on a flag manifold under the
chordal metric. The flag manifold is a mathematical space consisting of flags,
which are sequences of nested subspaces of a vector space that increase in
dimension. The flag manifold is a superset of a wide range of known matrix
groups, including Stiefel and Grassmanians, making it a general object that is
useful in a wide variety computer vision problems.
  To tackle the challenge of computing first order flag statistics, we first
transform the problem into one that involves auxiliary variables constrained to
the Stiefel manifold. The Stiefel manifold is a space of orthogonal frames, and
leveraging the numerical stability and efficiency of Stiefel-manifold
optimization enables us to compute the flag-mean effectively. Through a series
of experiments, we show the competence of our method in Grassmann and rotation
averaging, as well as principal component analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Closer Look at Model Adaptation using Feature Distortion and
  Simplicity Bias <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13500v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13500v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Puja Trivedi, Danai Koutra, Jayaraman J. Thiagarajan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advances in the expressivity of pretrained models have increased interest in
the design of adaptation protocols which enable safe and effective transfer
learning. Going beyond conventional linear probing (LP) and fine tuning (FT)
strategies, protocols that can effectively control feature distortion, i.e.,
the failure to update features orthogonal to the in-distribution, have been
found to achieve improved out-of-distribution generalization (OOD). In order to
limit this distortion, the LP+FT protocol, which first learns a linear probe
and then uses this initialization for subsequent FT, was proposed. However, in
this paper, we find when adaptation protocols (LP, FT, LP+FT) are also
evaluated on a variety of safety objectives (e.g., calibration, robustness,
etc.), a complementary perspective to feature distortion is helpful to explain
protocol behavior. To this end, we study the susceptibility of protocols to
simplicity bias (SB), i.e. the well-known propensity of deep neural networks to
rely upon simple features, as SB has recently been shown to underlie several
problems in robust generalization. Using a synthetic dataset, we demonstrate
the susceptibility of existing protocols to SB. Given the strong effectiveness
of LP+FT, we then propose modified linear probes that help mitigate SB, and
lead to better initializations for subsequent FT. We verify the effectiveness
of the proposed LP+FT variants for decreasing SB in a controlled setting, and
their ability to improve OOD generalization and safety on three adaptation
datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2023 as notable-25% (spotlight)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The effectiveness of MAE pre-<span class="highlight-title">pretrain</span>ing for billion-scale <span class="highlight-title">pretrain</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13496v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13496v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mannat Singh, Quentin Duval, Kalyan Vasudev Alwala, Haoqi Fan, Vaibhav Aggarwal, Aaron Adcock, Armand Joulin, Piotr Dollár, Christoph Feichtenhofer, Ross Girshick, Rohit Girdhar, Ishan Misra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper revisits the standard pretrain-then-finetune paradigm used in
computer vision for visual recognition tasks. Typically, state-of-the-art
foundation models are pretrained using large scale (weakly) supervised datasets
with billions of images. We introduce an additional pre-pretraining stage that
is simple and uses the self-supervised MAE technique to initialize the model.
While MAE has only been shown to scale with the size of models, we find that it
scales with the size of the training dataset as well. Thus, our MAE-based
pre-pretraining scales with both model and data size making it applicable for
training foundation models. Pre-pretraining consistently improves both the
model convergence and the downstream transfer performance across a range of
model scales (millions to billions of parameters), and dataset sizes (millions
to billions of images). We measure the effectiveness of pre-pretraining on 10
different visual recognition tasks spanning image classification, video
recognition, object detection, low-shot classification and zero-shot
recognition. Our largest model achieves new state-of-the-art results on
iNaturalist-18 (91.3%), 1-shot ImageNet-1k (62.1%), and zero-shot transfer on
Food-101 (96.0%). Our study reveals that model initialization plays a
significant role, even for web-scale pretraining with billions of images.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boosting Reinforcement Learning and Planning with Demonstrations: A
  <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13489v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13489v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tongzhou Mu, Hao Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although reinforcement learning has seen tremendous success recently, this
kind of trial-and-error learning can be impractical or inefficient in complex
environments. The use of demonstrations, on the other hand, enables agents to
benefit from expert knowledge rather than having to discover the best action to
take through exploration. In this survey, we discuss the advantages of using
demonstrations in sequential decision making, various ways to apply
demonstrations in learning-based decision making paradigms (for example,
reinforcement learning and planning in the learned models), and how to collect
the demonstrations in various scenarios. Additionally, we exemplify a practical
pipeline for generating and utilizing demonstrations in the recently proposed
ManiSkill robot learning benchmark.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TactoFind: A Tactile Only System for Object Retrieval <span class="chip">ICRA 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13482v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13482v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sameer Pai, Tao Chen, Megha Tippur, Edward Adelson, Abhishek Gupta, Pulkit Agrawal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of object retrieval in scenarios where visual sensing is
absent, object shapes are unknown beforehand and objects can move freely, like
grabbing objects out of a drawer. Successful solutions require localizing free
objects, identifying specific object instances, and then grasping the
identified objects, only using touch feedback. Unlike vision, where cameras can
observe the entire scene, touch sensors are local and only observe parts of the
scene that are in contact with the manipulator. Moreover, information gathering
via touch sensors necessitates applying forces on the touched surface which may
disturb the scene itself. Reasoning with touch, therefore, requires careful
exploration and integration of information over time -- a challenge we tackle.
We present a system capable of using sparse tactile feedback from fingertip
touch sensors on a dexterous hand to localize, identify and grasp novel objects
without any visual feedback. Videos are available at
https://taochenshh.github.io/projects/tactofind.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ICRA 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalization with quantum geometry for learning unitaries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13462v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13462v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tobias Haug, M. S. Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generalization is the ability of quantum machine learning models to make
accurate predictions on new data by learning from training data. Here, we
introduce the data quantum Fisher information metric (DQFIM) to determine when
a model can generalize. For variational learning of unitaries, the DQFIM
quantifies the amount of circuit parameters and training data needed to
successfully train and generalize. We apply the DQFIM to explain when a
constant number of training states and polynomial number of parameters are
sufficient for generalization. Further, we can improve generalization by
removing symmetries from training data. Finally, we show that
out-of-distribution generalization, where training and testing data are drawn
from different data distributions, can be better than using the same
distribution. Our work opens up new approaches to improve generalization in
quantum machine learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimization Dynamics of Equivariant and Augmented Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13458v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13458v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Axel Flinth, Fredrik Ohlsson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the optimization of multilayer perceptrons on symmetric data.
We compare the strategy of constraining the architecture to be equivariant to
that of using augmentation. We show that, under natural assumptions on the loss
and non-linearities, the sets of equivariant stationary points are identical
for the two strategies, and that the set of equivariant layers is invariant
under the gradient flow for augmented models. Finally, we show that stationary
points may be unstable for augmented training although they are stable for the
equivariant models
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Human Behavior in the Time of COVID-19: Learning from Big Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13452v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13452v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanjia Lyu, Arsal Imtiaz, Yufei Zhao, Jiebo Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since the World Health Organization (WHO) characterized COVID-19 as a
pandemic in March 2020, there have been over 600 million confirmed cases of
COVID-19 and more than six million deaths as of October 2022. The relationship
between the COVID-19 pandemic and human behavior is complicated. On one hand,
human behavior is found to shape the spread of the disease. On the other hand,
the pandemic has impacted and even changed human behavior in almost every
aspect. To provide a holistic understanding of the complex interplay between
human behavior and the COVID-19 pandemic, researchers have been employing big
data techniques such as natural language processing, computer vision, audio
signal processing, frequent pattern mining, and machine learning. In this
study, we present an overview of the existing studies on using big data
techniques to study human behavior in the time of the COVID-19 pandemic. In
particular, we categorize these studies into three groups - using big data to
measure, model, and leverage human behavior, respectively. The related tasks,
data, and methods are summarized accordingly. To provide more insights into how
to fight the COVID-19 pandemic and future global catastrophes, we further
discuss challenges and potential opportunities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in the Horizons in Big Data 2022 article
  collection of Frontiers in Big Data</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Set-the-Scene: Global-Local Training for Generating Controllable NeRF
  Scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13450v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13450v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dana Cohen-Bar, Elad Richardson, Gal Metzer, Raja Giryes, Daniel Cohen-Or
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent breakthroughs in text-guided image generation have led to remarkable
progress in the field of 3D synthesis from text. By optimizing neural radiance
fields (NeRF) directly from text, recent methods are able to produce remarkable
results. Yet, these methods are limited in their control of each object's
placement or appearance, as they represent the scene as a whole. This can be a
major issue in scenarios that require refining or manipulating objects in the
scene. To remedy this deficit, we propose a novel GlobalLocal training
framework for synthesizing a 3D scene using object proxies. A proxy represents
the object's placement in the generated scene and optionally defines its coarse
geometry. The key to our approach is to represent each object as an independent
NeRF. We alternate between optimizing each NeRF on its own and as part of the
full scene. Thus, a complete representation of each object can be learned,
while also creating a harmonious scene with style and lighting match. We show
that using proxies allows a wide variety of editing options, such as adjusting
the placement of each independent object, removing objects from a scene, or
refining an object. Our results show that Set-the-Scene offers a powerful
solution for scene synthesis and manipulation, filling a crucial gap in
controllable text-to-3D synthesis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>project page at https://danacohen95.github.io/Set-the-Scene/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GiveMeLabeledIssues: An Open Source Issue Recommendation System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13418v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13418v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joseph Vargovich, Fabio Santos, Jacob Penney, Marco A. Gerosa, Igor Steinmacher
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developers often struggle to navigate an Open Source Software (OSS) project's
issue-tracking system and find a suitable task. Proper issue labeling can aid
task selection, but current tools are limited to classifying the issues
according to their type (e.g., bug, question, good first issue, feature, etc.).
In contrast, this paper presents a tool (GiveMeLabeledIssues) that mines
project repositories and labels issues based on the skills required to solve
them. We leverage the domain of the APIs involved in the solution (e.g., User
Interface (UI), Test, Databases (DB), etc.) as a proxy for the required skills.
GiveMeLabeledIssues facilitates matching developers' skills to tasks, reducing
the burden on project maintainers. The tool obtained a precision of 83.9% when
predicting the API domains involved in the issues. The replication package
contains instructions on executing the tool and including new projects. A demo
video is available at https://www.youtube.com/watch?v=ic2quUue7i8
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>MSR Data and Tool Showcase 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Paraphrasing evades detectors of AI-generated text, but retrieval is an
  effective defense 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13408v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13408v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kalpesh Krishna, Yixiao Song, Marzena Karpinska, John Wieting, Mohit Iyyer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To detect the deployment of large language models for malicious use cases
(e.g., fake content creation or academic plagiarism), several approaches have
recently been proposed for identifying AI-generated text via watermarks or
statistical irregularities. How robust are these detection algorithms to
paraphrases of AI-generated text? To stress test these detectors, we first
train an 11B parameter paraphrase generation model (DIPPER) that can paraphrase
paragraphs, optionally leveraging surrounding text (e.g., user-written prompts)
as context. DIPPER also uses scalar knobs to control the amount of lexical
diversity and reordering in the paraphrases. Paraphrasing text generated by
three large language models (including GPT3.5-davinci-003) with DIPPER
successfully evades several detectors, including watermarking, GPTZero,
DetectGPT, and OpenAI's text classifier. For example, DIPPER drops the
detection accuracy of DetectGPT from 70.3% to 4.6% (at a constant false
positive rate of 1%), without appreciably modifying the input semantics. To
increase the robustness of AI-generated text detection to paraphrase attacks,
we introduce a simple defense that relies on retrieving semantically-similar
generations and must be maintained by a language model API provider. Given a
candidate text, our algorithm searches a database of sequences previously
generated by the API, looking for sequences that match the candidate text
within a certain threshold. We empirically verify our defense using a database
of 15M generations from a fine-tuned T5-XXL model and find that it can detect
80% to 97% of paraphrased generations across different settings, while only
classifying 1% of human-written sequences as AI-generated. We will open source
our code, model and data for future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint (27 pages). Code, models, data will be added to
  https://github.com/martiansideofthemoon/ai-detection-paraphrases</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Endpointing with Deep Contextual Multi-armed Bandits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13407v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13407v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Do June Min, Andreas Stolcke, Anirudh Raju, Colin Vaz, Di He, Venkatesh Ravichandran, Viet Anh Trinh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current endpointing (EP) solutions learn in a supervised framework, which
does not allow the model to incorporate feedback and improve in an online
setting. Also, it is a common practice to utilize costly grid-search to find
the best configuration for an endpointing model. In this paper, we aim to
provide a solution for adaptive endpointing by proposing an efficient method
for choosing an optimal endpointing configuration given utterance-level audio
features in an online setting, while avoiding hyperparameter grid-search. Our
method does not require ground truth labels, and only uses online learning from
reward signals without requiring annotated labels. Specifically, we propose a
deep contextual multi-armed bandit-based approach, which combines the
representational power of neural networks with the action exploration behavior
of Thompson modeling algorithms. We compare our approach to several baselines,
and show that our deep bandit models also succeed in reducing early cutoff
errors while maintaining low latency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SC-MIL: Supervised Contrastive Multiple Instance Learning for Imbalanced
  Classification in Pathology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13405v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13405v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dinkar Juyal, Siddhant Shingi, Syed Ashar Javed, Harshith Padigela, Chintan Shah, Anand Sampat, Archit Khosla, John Abel, Amaro Taylor-Weiner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multiple Instance learning (MIL) models have been extensively used in
pathology to predict biomarkers and risk-stratify patients from gigapixel-sized
images. Machine learning problems in medical imaging often deal with rare
diseases, making it important for these models to work in a label-imbalanced
setting. Furthermore, these imbalances can occur in out-of-distribution (OOD)
datasets when the models are deployed in the real-world. We leverage the idea
that decoupling feature and classifier learning can lead to improved decision
boundaries for label imbalanced datasets. To this end, we investigate the
integration of supervised contrastive learning with multiple instance learning
(SC-MIL). Specifically, we propose a joint-training MIL framework in the
presence of label imbalance that progressively transitions from learning
bag-level representations to optimal classifier learning. We perform
experiments with different imbalance settings for two well-studied problems in
cancer pathology: subtyping of non-small cell lung cancer and subtyping of
renal cell carcinoma. SC-MIL provides large and consistent improvements over
other techniques on both in-distribution (ID) and OOD held-out sets across
multiple imbalanced settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimization and Optimizers for Adversarial Robustness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13401v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13401v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hengyue Liang, Buyun Liang, Le Peng, Ying Cui, Tim Mitchell, Ju Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Empirical robustness evaluation (RE) of deep learning models against
adversarial perturbations entails solving nontrivial constrained optimization
problems. Existing numerical algorithms that are commonly used to solve them in
practice predominantly rely on projected gradient, and mostly handle
perturbations modeled by the $\ell_1$, $\ell_2$ and $\ell_\infty$ distances. In
this paper, we introduce a novel algorithmic framework that blends a
general-purpose constrained-optimization solver PyGRANSO with Constraint
Folding (PWCF), which can add more reliability and generality to the
state-of-the-art RE packages, e.g., AutoAttack. Regarding reliability, PWCF
provides solutions with stationarity measures and feasibility tests to assess
the solution quality. For generality, PWCF can handle perturbation models that
are typically inaccessible to the existing projected gradient methods; the main
requirement is the distance metric to be almost everywhere differentiable.
Taking advantage of PWCF and other existing numerical algorithms, we further
explore the distinct patterns in the solutions found for solving these
optimization problems using various combinations of losses, perturbation
models, and optimization algorithms. We then discuss the implications of these
patterns on the current robustness evaluation and adversarial training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Xplainer: From X-Ray Observations to Explainable Zero-Shot Diagnosis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13391v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13391v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chantal Pellegrini, Matthias Keicher, Ege Özsoy, Petra Jiraskova, Rickmer Braren, Nassir Navab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated diagnosis prediction from medical images is a valuable resource to
support clinical decision-making. However, such systems usually need to be
trained on large amounts of annotated data, which often is scarce in the
medical domain. Zero-shot methods address this challenge by allowing a flexible
adaption to new settings with different clinical findings without relying on
labeled data. Further, to integrate automated diagnosis in the clinical
workflow, methods should be transparent and explainable, increasing medical
professionals' trust and facilitating correctness verification. In this work,
we introduce Xplainer, a novel framework for explainable zero-shot diagnosis in
the clinical setting. Xplainer adapts the classification-by-description
approach of contrastive vision-language models to the multi-label medical
diagnosis task. Specifically, instead of directly predicting a diagnosis, we
prompt the model to classify the existence of descriptive observations, which a
radiologist would look for on an X-Ray scan, and use the descriptor
probabilities to estimate the likelihood of a diagnosis. Our model is
explainable by design, as the final diagnosis prediction is directly based on
the prediction of the underlying descriptors. We evaluate Xplainer on two chest
X-ray datasets, CheXpert and ChestX-ray14, and demonstrate its effectiveness in
improving the performance and explainability of zero-shot diagnosis. Our
results suggest that Xplainer provides a more detailed understanding of the
decision-making process and can be a valuable tool for clinical diagnosis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 2 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Compositional Zero-Shot Domain Transfer with Text-to-Text Models <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13386v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13386v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangyu Liu, Qianchu Liu, Shruthi Bannur, Fernando Pérez-García, Naoto Usuyama, Sheng Zhang, Tristan Naumann, Aditya Nori, Hoifung Poon, Javier Alvarez-Valle, Ozan Oktay, Stephanie L. Hyland
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Label scarcity is a bottleneck for improving task performance in specialised
domains. We propose a novel compositional transfer learning framework (DoT5 -
domain compositional zero-shot T5) for zero-shot domain transfer. Without
access to in-domain labels, DoT5 jointly learns domain knowledge (from MLM of
unlabelled in-domain free text) and task knowledge (from task training on more
readily available general-domain data) in a multi-task manner. To improve the
transferability of task training, we design a strategy named NLGU: we
simultaneously train NLG for in-domain label-to-data generation which enables
data augmentation for self-finetuning and NLU for label prediction. We evaluate
DoT5 on the biomedical domain and the resource-lean subdomain of radiology,
focusing on NLI, text summarisation and embedding learning. DoT5 demonstrates
the effectiveness of compositional transfer learning through multi-task
learning. In particular, DoT5 outperforms the current SOTA in zero-shot
transfer by over 7 absolute points in accuracy on RadNLI. We validate DoT5 with
ablations and a case study demonstrating its ability to solve challenging NLI
examples requiring in-domain expertise.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at TACL, pre-MIT Press publication version. 16 pages, 4
  figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FS-Real: Towards Real-World Cross-Device Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13363v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13363v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daoyuan Chen, Dawei Gao, Yuexiang Xie, Xuchen Pan, Zitao Li, Yaliang Li, Bolin Ding, Jingren Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) aims to train high-quality models in collaboration
with distributed clients while not uploading their local data, which attracts
increasing attention in both academia and industry. However, there is still a
considerable gap between the flourishing FL research and real-world scenarios,
mainly caused by the characteristics of heterogeneous devices and its scales.
Most existing works conduct evaluations with homogeneous devices, which are
mismatched with the diversity and variability of heterogeneous devices in
real-world scenarios. Moreover, it is challenging to conduct research and
development at scale with heterogeneous devices due to limited resources and
complex software stacks. These two key factors are important yet underexplored
in FL research as they directly impact the FL training dynamics and final
performance, making the effectiveness and usability of FL algorithms unclear.
To bridge the gap, in this paper, we propose an efficient and scalable
prototyping system for real-world cross-device FL, FS-Real. It supports
heterogeneous device runtime, contains parallelism and robustness enhanced FL
server, and provides implementations and extensibility for advanced FL utility
features such as personalization, communication compression and asynchronous
aggregation. To demonstrate the usability and efficiency of FS-Real, we conduct
extensive experiments with various device distributions, quantify and analyze
the effect of the heterogeneous device and various scales, and further provide
insights and open discussions about real-world FL scenarios. Our system is
released to help to pave the way for further real-world FL research and broad
applications involving diverse devices and scales.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Increasing Textual Context Size Boosts Medical Image-Text Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13340v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13340v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Idan Glassberg, Tom Hope
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This short technical report demonstrates a simple technique that yields state
of the art results in medical image-text matching tasks. We analyze the use of
OpenAI's CLIP, a general image-text matching model, and observe that CLIP's
limited textual input size has negative impact on downstream performance in the
medical domain where encoding longer textual contexts is often required. We
thus train and release ClipMD, which is trained with a simple sliding window
technique to encode textual captions. ClipMD was tested on two medical
image-text datasets and compared with other image-text matching models. The
results show that ClipMD outperforms other models on both datasets by a large
margin. We make our code and pretrained model publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Audio Diffusion Model for Speech Synthesis: A <span class="highlight-title">Survey</span> on Text To Speech
  and Speech Enhancement in Generative AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13336v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13336v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenshuang Zhang, Chaoning Zhang, Sheng Zheng, Mengchun Zhang, Maryam Qamar, Sung-Ho Bae, In So Kweon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative AI has demonstrated impressive performance in various fields,
among which speech synthesis is an interesting direction. With the diffusion
model as the most popular generative model, numerous works have attempted two
active tasks: text to speech and speech enhancement. This work conducts a
survey on audio diffusion model, which is complementary to existing surveys
that either lack the recent progress of diffusion-based speech synthesis or
highlight an overall picture of applying diffusion model in multiple fields.
Specifically, this work first briefly introduces the background of audio and
diffusion model. As for the text-to-speech task, we divide the methods into
three categories based on the stage where diffusion model is adopted: acoustic
model, vocoder and end-to-end framework. Moreover, we categorize various speech
enhancement tasks by either certain signals are removed or added into the input
speech. Comparisons of experimental results and discussions are also covered in
this survey.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decentralized Adversarial Training over Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13326v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13326v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ying Cao, Elsa Rizk, Stefan Vlaski, Ali H. Sayed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The vulnerability of machine learning models to adversarial attacks has been
attracting considerable attention in recent years. Most existing studies focus
on the behavior of stand-alone single-agent learners. In comparison, this work
studies adversarial training over graphs, where individual agents are subjected
to perturbations of varied strength levels across space. It is expected that
interactions by linked agents, and the heterogeneity of the attack models that
are possible over the graph, can help enhance robustness in view of the
coordination power of the group. Using a min-max formulation of diffusion
learning, we develop a decentralized adversarial training framework for
multi-agent systems. We analyze the convergence properties of the proposed
scheme for both convex and non-convex environments, and illustrate the enhanced
robustness to adversarial attacks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2303.01936</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Generative Multi-Agent Imitation Model as a Computational Benchmark
  for Evaluating Human Performance in Complex Interactive Tasks: A Case Study
  in Football 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13323v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13323v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaoyi Gu, Varuna De Silva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating the performance of human is a common need across many
applications, such as in engineering and sports. When evaluating human
performance in completing complex and interactive tasks, the most common way is
to use a metric having been proved efficient for that context, or to use
subjective measurement techniques. However, this can be an error prone and
unreliable process since static metrics cannot capture all the complex contexts
associated with such tasks and biases exist in subjective measurement. The
objective of our research is to create data-driven AI agents as computational
benchmarks to evaluate human performance in solving difficult tasks involving
multiple humans and contextual factors. We demonstrate this within the context
of football performance analysis. We train a generative model based on
Conditional Variational Recurrent Neural Network (VRNN) Model on a large player
and ball tracking dataset. The trained model is used to imitate the
interactions between two teams and predict the performance from each team. Then
the trained Conditional VRNN Model is used as a benchmark to evaluate team
performance. The experimental results on Premier League football dataset
demonstrates the usefulness of our method to existing state-of-the-art static
metric used in football analytics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reckoning with the Disagreement Problem: Explanation Consensus as a
  Training Objective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13299v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13299v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Avi Schwarzschild, Max Cembalest, Karthik Rao, Keegan Hines, John Dickerson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As neural networks increasingly make critical decisions in high-stakes
settings, monitoring and explaining their behavior in an understandable and
trustworthy manner is a necessity. One commonly used type of explainer is post
hoc feature attribution, a family of methods for giving each feature in an
input a score corresponding to its influence on a model's output. A major
limitation of this family of explainers in practice is that they can disagree
on which features are more important than others. Our contribution in this
paper is a method of training models with this disagreement problem in mind. We
do this by introducing a Post hoc Explainer Agreement Regularization (PEAR)
loss term alongside the standard term corresponding to accuracy, an additional
term that measures the difference in feature attribution between a pair of
explainers. We observe on three datasets that we can train a model with this
loss term to improve explanation consensus on unseen data, and see improved
consensus between explainers other than those used in the loss term. We examine
the trade-off between improved consensus and model performance. And finally, we
study the influence our method has on feature attribution explanations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Generalization with Domain Convex Game <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13297v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13297v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangrui Lv, Jian Liang, Shuang Li, Jinming Zhang, Di Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain generalization (DG) tends to alleviate the poor generalization
capability of deep neural networks by learning model with multiple source
domains. A classical solution to DG is domain augmentation, the common belief
of which is that diversifying source domains will be conducive to the
out-of-distribution generalization. However, these claims are understood
intuitively, rather than mathematically. Our explorations empirically reveal
that the correlation between model generalization and the diversity of domains
may be not strictly positive, which limits the effectiveness of domain
augmentation. This work therefore aim to guarantee and further enhance the
validity of this strand. To this end, we propose a new perspective on DG that
recasts it as a convex game between domains. We first encourage each
diversified domain to enhance model generalization by elaborately designing a
regularization term based on supermodularity. Meanwhile, a sample filter is
constructed to eliminate low-quality samples, thereby avoiding the impact of
potentially harmful information. Our framework presents a new avenue for the
formal analysis of DG, heuristic analysis and extensive experiments demonstrate
the rationality and effectiveness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enriching Neural Network Training <span class="highlight-title">Dataset</span> to Improve Worst-Case
  Performance Guarantees 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13228v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13228v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rahul Nellikkath, Spyros Chatzivasileiadis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning algorithms, especially Neural Networks (NNs), are a valuable
tool used to approximate non-linear relationships, like the AC-Optimal Power
Flow (AC-OPF), with considerable accuracy -- and achieving a speedup of several
orders of magnitude when deployed for use. Often in power systems literature,
the NNs are trained with a fixed dataset generated prior to the training
process. In this paper, we show that adapting the NN training dataset during
training can improve the NN performance and substantially reduce its worst-case
violations. This paper proposes an algorithm that identifies and enriches the
training dataset with critical datapoints that reduce the worst-case violations
and deliver a neural network with improved worst-case performance guarantees.
We demonstrate the performance of our algorithm in four test power systems,
ranging from 39-buses to 162-buses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2212.10930</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Don't FREAK Out: A Frequency-Inspired Approach to Detecting Backdoor
  Poisoned Samples in DNNs <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13211v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13211v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hasan Abed Al Kader Hammoud, Adel Bibi, Philip H. S. Torr, Bernard Ghanem
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we investigate the frequency sensitivity of Deep Neural
Networks (DNNs) when presented with clean samples versus poisoned samples. Our
analysis shows significant disparities in frequency sensitivity between these
two types of samples. Building on these findings, we propose FREAK, a
frequency-based poisoned sample detection algorithm that is simple yet
effective. Our experimental results demonstrate the efficacy of FREAK not only
against frequency backdoor attacks but also against some spatial attacks. Our
work is just the first step in leveraging these insights. We believe that our
analysis and proposed defense mechanism will provide a foundation for future
research and development of backdoor defenses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPRW (The Art of Robustness)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ It is all Connected: A New Graph Formulation for Spatio-Temporal
  Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13177v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13177v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lars Ødegaard Bentsen, Narada Dilp Warakagoda, Roy Stenbro, Paal Engelstad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With an ever-increasing number of sensors in modern society, spatio-temporal
time series forecasting has become a de facto tool to make informed decisions
about the future. Most spatio-temporal forecasting models typically comprise
distinct components that learn spatial and temporal dependencies. A common
methodology employs some Graph Neural Network (GNN) to capture relations
between spatial locations, while another network, such as a recurrent neural
network (RNN), learns temporal correlations. By representing every recorded
sample as its own node in a graph, rather than all measurements for a
particular location as a single node, temporal and spatial information is
encoded in a similar manner. In this setting, GNNs can now directly learn both
temporal and spatial dependencies, jointly, while also alleviating the need for
additional temporal networks. Furthermore, the framework does not require
aligned measurements along the temporal dimension, meaning that it also
naturally facilitates irregular time series, different sampling frequencies or
missing data, without the need for data imputation. To evaluate the proposed
methodology, we consider wind speed forecasting as a case study, where our
proposed framework outperformed other spatio-temporal models using GNNs with
either Transformer or LSTM networks as temporal update functions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Pre-print</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Take 5: Interpretable Image Classification with a Handful of Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13166v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13166v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Norrenbrock, Marco Rudolph, Bodo Rosenhahn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Neural Networks use thousands of mostly incomprehensible features to
identify a single class, a decision no human can follow. We propose an
interpretable sparse and low dimensional final decision layer in a deep neural
network with measurable aspects of interpretability and demonstrate it on
fine-grained image classification. We argue that a human can only understand
the decision of a machine learning model, if the features are interpretable and
only very few of them are used for a single decision. For that matter, the
final layer has to be sparse and, to make interpreting the features feasible,
low dimensional. We call a model with a Sparse Low-Dimensional Decision
SLDD-Model. We show that a SLDD-Model is easier to interpret locally and
globally than a dense high-dimensional decision layer while being able to
maintain competitive accuracy. Additionally, we propose a loss function that
improves a model's feature diversity and accuracy. Our more interpretable
SLDD-Model only uses 5 out of just 50 features per class, while maintaining 97%
to 100% of the accuracy on four common benchmark datasets compared to the
baseline model with 2048 features.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adiabatic replay for continual learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13157v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13157v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Krawczyk, Alexander Gepperth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conventional replay-based approaches to continual learning (CL) require, for
each learning phase with new data, the replay of samples representing all of
the previously learned knowledge in order to avoid catastrophic forgetting.
Since the amount of learned knowledge grows over time in CL problems,
generative replay spends an increasing amount of time just re-learning what is
already known. In this proof-of-concept study, we propose a replay-based CL
strategy that we term adiabatic replay (AR), which derives its efficiency from
the (reasonable) assumption that each new learning phase is adiabatic, i.e.,
represents only a small addition to existing knowledge. Each new learning phase
triggers a sampling process that selectively replays, from the body of existing
knowledge, just such samples that are similar to the new data, in contrast to
replaying all of it. Complete replay is not required since AR represents the
data distribution by GMMs, which are capable of selectively updating their
internal representation only where data statistics have changed. As long as
additions are adiabatic, the amount of to-be-replayed samples need not to
depend on the amount of previously acquired knowledge at all. We verify
experimentally that AR is superior to state-of-the-art deep generative replay
using VAEs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FedGH: Heterogeneous Federated Learning with Generalized Global Header 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13137v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13137v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liping Yi, Gang Wang, Xiaoguang Liu, Zhuan Shi, Han Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) is an emerging machine learning paradigm that allows
multiple parties to train a shared model collaboratively in a
privacy-preserving manner. Existing horizontal FL methods generally assume that
the FL server and clients hold the same model structure. However, due to system
heterogeneity and the need for personalization, enabling clients to hold models
with diverse structures has become an important direction. Existing
model-heterogeneous FL approaches often require publicly available datasets and
incur high communication and/or computational costs, which limit their
performances. To address these limitations, we propose the Federated Global
prediction Header (FedGH) approach. It is a communication and
computation-efficient model-heterogeneous FL framework which trains a shared
generalized global prediction header with representations extracted by
heterogeneous extractors for clients' models at the FL server. The trained
generalized global prediction header learns from different clients. The
acquired global knowledge is then transferred to clients to substitute each
client's local prediction header. We derive the non-convex convergence rate of
FedGH. Extensive experiments on two real-world datasets demonstrate that FedGH
achieves significantly more advantageous performance in both model-homogeneous
and -heterogeneous FL scenarios compared to seven state-of-the-art personalized
FL models, beating the best-performing baseline by up to 8.87% (for
model-homogeneous FL) and 1.83% (for model-heterogeneous FL) in terms of
average test accuracy, while saving up to 85.53% of communication overhead.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Laplacian Segmentation Networks: Improved Epistemic Uncertainty from
  Spatial Aleatoric Uncertainty 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13123v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13123v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kilian Zepf, Selma Wanna, Marco Miani, Juston Moore, Jes Frellsen, Søren Hauberg, Aasa Feragen, Frederik Warburg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Out of distribution (OOD) medical images are frequently encountered, e.g.
because of site- or scanner differences, or image corruption. OOD images come
with a risk of incorrect image segmentation, potentially negatively affecting
downstream diagnoses or treatment. To ensure robustness to such incorrect
segmentations, we propose Laplacian Segmentation Networks (LSN) that jointly
model epistemic (model) and aleatoric (data) uncertainty in image segmentation.
We capture data uncertainty with a spatially correlated logit distribution. For
model uncertainty, we propose the first Laplace approximation of the weight
posterior that scales to large neural networks with skip connections that have
high-dimensional outputs. Empirically, we demonstrate that modelling spatial
pixel correlation allows the Laplacian Segmentation Network to successfully
assign high epistemic uncertainty to out-of-distribution objects appearing
within images.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RLOR: A Flexible Framework of Deep Reinforcement Learning for Operation
  Research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13117v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13117v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ching Pui Wan, Tung Li, Jason Min Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning has been applied in operation research and has shown
promise in solving large combinatorial optimization problems. However, existing
works focus on developing neural network architectures for certain problems.
These works lack the flexibility to incorporate recent advances in
reinforcement learning, as well as the flexibility of customizing model
architectures for operation research problems. In this work, we analyze the
end-to-end autoregressive models for vehicle routing problems and show that
these models can benefit from the recent advances in reinforcement learning
with a careful re-implementation of the model architecture. In particular, we
re-implemented the Attention Model and trained it with Proximal Policy
Optimization (PPO) in CleanRL, showing at least 8 times speed up in training
time. We hereby introduce RLOR, a flexible framework for Deep Reinforcement
Learning for Operation Research. We believe that a flexible framework is key to
developing deep reinforcement learning models for operation research problems.
The code of our work is publicly available at https://github.com/cpwan/RLOR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Regularization for Class-Incremental Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13113v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13113v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elif Ceren Gok, Murat Onur Yildirim, Mert Kilickaya, Joaquin Vanschoren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Class-Incremental Learning updates a deep classifier with new categories
while maintaining the previously observed class accuracy. Regularizing the
neural network weights is a common method to prevent forgetting previously
learned classes while learning novel ones. However, existing regularizers use a
constant magnitude throughout the learning sessions, which may not reflect the
varying levels of difficulty of the tasks encountered during incremental
learning. This study investigates the necessity of adaptive regularization in
Class-Incremental Learning, which dynamically adjusts the regularization
strength according to the complexity of the task at hand. We propose a Bayesian
Optimization-based approach to automatically determine the optimal
regularization magnitude for each learning task. Our experiments on two
datasets via two regularizers demonstrate the importance of adaptive
regularization for achieving accurate and less forgetful visual incremental
learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Keypoint-Guided Optimal Transport 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13102v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13102v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Gu, Yucheng Yang, Wei Zeng, Jian Sun, Zongben Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing Optimal Transport (OT) methods mainly derive the optimal transport
plan/matching under the criterion of transport cost/distance minimization,
which may cause incorrect matching in some cases. In many applications,
annotating a few matched keypoints across domains is reasonable or even
effortless in annotation burden. It is valuable to investigate how to leverage
the annotated keypoints to guide the correct matching in OT. In this paper, we
propose a novel KeyPoint-Guided model by ReLation preservation (KPG-RL) that
searches for the optimal matching (i.e., transport plan) guided by the
keypoints in OT. To impose the keypoints in OT, first, we propose a mask-based
constraint of the transport plan that preserves the matching of keypoint pairs.
Second, we propose to preserve the relation of each data point to the keypoints
to guide the matching. The proposed KPG-RL model can be solved by Sinkhorn's
algorithm and is applicable even when distributions are supported in different
spaces. We further utilize the relation preservation constraint in the
Kantorovich Problem and Gromov-Wasserstein model to impose the guidance of
keypoints in them. Meanwhile, the proposed KPG-RL model is extended to the
partial OT setting. Moreover, we deduce the dual formulation of the KPG-RL
model, which is solved using deep learning techniques. Based on the learned
transport plan from dual KPG-RL, we propose a novel manifold barycentric
projection to transport source data to the target domain. As applications, we
apply the proposed KPG-RL model to the heterogeneous domain adaptation and
image-to-image translation. Experiments verified the effectiveness of the
proposed approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Probabilistic Stability of Stochastic Gradient Descent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13093v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13093v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liu Ziyin, Botao Li, Tomer Galanti, Masahito Ueda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A fundamental open problem in deep learning theory is how to define and
understand the stability of stochastic gradient descent (SGD) close to a fixed
point. Conventional literature relies on the convergence of statistical
moments, esp., the variance, of the parameters to quantify the stability. We
revisit the definition of stability for SGD and use the \textit{convergence in
probability} condition to define the \textit{probabilistic stability} of SGD.
The proposed stability directly answers a fundamental question in deep learning
theory: how SGD selects a meaningful solution for a neural network from an
enormous number of solutions that may overfit badly. To achieve this, we show
that only under the lens of probabilistic stability does SGD exhibit rich and
practically relevant phases of learning, such as the phases of the complete
loss of stability, incorrect learning, convergence to low-rank saddles, and
correct learning. When applied to a neural network, these phase diagrams imply
that SGD prefers low-rank saddles when the underlying gradient is noisy,
thereby improving the learning performance. This result is in sharp contrast to
the conventional wisdom that SGD prefers flatter minima to sharp ones, which we
find insufficient to explain the experimental data. We also prove that the
probabilistic stability of SGD can be quantified by the Lyapunov exponents of
the SGD dynamics, which can easily be measured in practice. Our work
potentially opens a new venue for addressing the fundamental question of how
the learning algorithm affects the learning outcome in deep learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Box-Level Active Detection <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13089v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13089v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengyao Lyu, Jundong Zhou, Hui Chen, Yijie Huang, Dongdong Yu, Yaqian Li, Yandong Guo, Yuchen Guo, Liuyu Xiang, Guiguang Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Active learning selects informative samples for annotation within budget,
which has proven efficient recently on object detection. However, the widely
used active detection benchmarks conduct image-level evaluation, which is
unrealistic in human workload estimation and biased towards crowded images.
Furthermore, existing methods still perform image-level annotation, but equally
scoring all targets within the same image incurs waste of budget and redundant
labels. Having revealed above problems and limitations, we introduce a
box-level active detection framework that controls a box-based budget per
cycle, prioritizes informative targets and avoids redundancy for fair
comparison and efficient application.
  Under the proposed box-level setting, we devise a novel pipeline, namely
Complementary Pseudo Active Strategy (ComPAS). It exploits both human
annotations and the model intelligence in a complementary fashion: an efficient
input-end committee queries labels for informative objects only; meantime
well-learned targets are identified by the model and compensated with
pseudo-labels. ComPAS consistently outperforms 10 competitors under 4 settings
in a unified codebase. With supervision from labeled data only, it achieves
100% supervised performance of VOC0712 with merely 19% box annotations. On the
COCO dataset, it yields up to 4.3% mAP improvement over the second-best method.
ComPAS also supports training with the unlabeled pool, where it surpasses 90%
COCO supervised performance with 85% label reduction. Our source code is
publicly available at https://github.com/lyumengyao/blad.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023 highlight</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MSAT: Biologically Inspired Multi-Stage Adaptive Threshold for
  Conversion of Spiking Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13080v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13080v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang He, Yang Li, Dongcheng Zhao, Qingqun Kong, Yi Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spiking Neural Networks (SNNs) can do inference with low power consumption
due to their spike sparsity. ANN-SNN conversion is an efficient way to achieve
deep SNNs by converting well-trained Artificial Neural Networks (ANNs).
However, the existing methods commonly use constant threshold for conversion,
which prevents neurons from rapidly delivering spikes to deeper layers and
causes high time delay. In addition, the same response for different inputs may
result in information loss during the information transmission. Inspired by the
biological model mechanism, we propose a multi-stage adaptive threshold (MSAT).
Specifically, for each neuron, the dynamic threshold varies with firing history
and input properties and is positively correlated with the average membrane
potential and negatively correlated with the rate of depolarization. The
self-adaptation to membrane potential and input allows a timely adjustment of
the threshold to fire spike faster and transmit more information. Moreover, we
analyze the Spikes of Inactivated Neurons error which is pervasive in early
time steps and propose spike confidence accordingly as a measurement of
confidence about the neurons that correctly deliver spikes. We use such spike
confidence in early time steps to determine whether to elicit spike to
alleviate this error. Combined with the proposed method, we examine the
performance on non-trivial datasets CIFAR-10, CIFAR-100, and ImageNet. We also
conduct sentiment classification and speech recognition experiments on the IDBM
and Google speech commands datasets respectively. Experiments show
near-lossless and lower latency ANN-SNN conversion. To the best of our
knowledge, this is the first time to build a biologically inspired multi-stage
adaptive threshold for converted SNN, with comparable performance to
state-of-the-art methods while improving energy efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predicting the Initial Conditions of the Universe using Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13056v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13056v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vaibhav Jindal, Drew Jamieson, Albert Liang, Aarti Singh, Shirley Ho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Finding the initial conditions that led to the current state of the universe
is challenging because it involves searching over a vast input space of initial
conditions, along with modeling their evolution via tools such as N-body
simulations which are computationally expensive. Deep learning has emerged as
an alternate modeling tool that can learn the mapping between the linear input
of an N-body simulation and the final nonlinear displacements at redshift zero,
which can significantly accelerate the forward modeling. However, this does not
help reduce the search space for initial conditions. In this paper, we
demonstrate for the first time that a deep learning model can be trained for
the reverse mapping. We train a V-Net based convolutional neural network, which
outputs the linear displacement of an N-body system, given the current time
nonlinear displacement and the cosmological parameters of the system. We
demonstrate that this neural network accurately recovers the initial linear
displacement field over a wide range of scales ($<1$-$2\%$ error up to nearly
$k = 1\ \mathrm{Mpc}^{-1}\,h$), despite the ill-defined nature of the inverse
problem at smaller scales. Specifically, smaller scales are dominated by
nonlinear effects which makes the backward dynamics much more susceptible to
numerical and computational errors leading to highly divergent backward
trajectories and a one-to-many backward mapping. The results of our method
motivate that neural network based models can act as good approximators of the
initial linear states and their predictions can serve as good starting points
for sampling-based methods to infer the initial states of the universe.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reimagining Application User Interface (UI) Design using Deep Learning
  Methods: Challenges and Opportunities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13055v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13055v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Subtain Malik, Muhammad Tariq Saeed, Marya Jabeen Zia, Shahzad Rasool, Liaquat Ali Khan, Mian Ilyas Ahmed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a review of the recent work in deep learning
methods for user interface design. The survey encompasses well known deep
learning techniques (deep neural networks, convolutional neural networks,
recurrent neural networks, autoencoders, and generative adversarial networks)
and datasets widely used to design user interface applications. We highlight
important problems and emerging research frontiers in this field. We believe
that the use of deep learning for user interface design automation tasks could
be one of the high potential fields for the advancement of the software
development industry.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A review paper on studies of UI design techniques and deep learning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Better Dynamic Graph Learning: New Architecture and Unified
  Library 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13047v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13047v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Le Yu, Leilei Sun, Bowen Du, Weifeng Lv
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose DyGFormer, a new Transformer-based architecture for dynamic graph
learning that solely learns from the sequences of nodes' historical first-hop
interactions. DyGFormer incorporates two distinct designs: a neighbor
co-occurrence encoding scheme that explores the correlations of the source node
and destination node based on their sequences; a patching technique that
divides each sequence into multiple patches and feeds them to Transformer,
allowing the model to effectively and efficiently benefit from longer
histories. We also introduce DyGLib, a unified library with standard training
pipelines, extensible coding interfaces, and comprehensive evaluating protocols
to promote reproducible, scalable, and credible dynamic graph learning
research. By performing extensive experiments on thirteen datasets from various
domains for transductive/inductive dynamic link prediction and dynamic node
classification tasks, we observe that: DyGFormer achieves state-of-the-art
performance on most of the datasets, demonstrating the effectiveness of
capturing nodes' correlations and long-term temporal dependencies; the results
of baselines vary across different datasets and some findings are inconsistent
with previous reports, which may be caused by their diverse pipelines and
problematic implementations. We hope our work can provide new insights and
facilitate the development of the dynamic graph learning field. All the
resources including datasets, data loaders, algorithms, and executing scripts
are publicly available at https://github.com/yule-BUAA/DyGLib.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SPeC: A Soft <span class="highlight-title">Prompt</span>-Based Calibration on Mitigating Performance
  Variability in Clinical Notes Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13035v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13035v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu-Neng Chuang, Ruixiang Tang, Xiaoqian Jiang, Xia Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electronic health records (EHRs) store an extensive array of patient
information, encompassing medical histories, diagnoses, treatments, and test
outcomes. These records are crucial for enabling healthcare providers to make
well-informed decisions regarding patient care. Summarizing clinical notes
further assists healthcare professionals in pinpointing potential health risks
and making better-informed decisions. This process contributes to reducing
errors and enhancing patient outcomes by ensuring providers have access to the
most pertinent and current patient data. Recent research has shown that
incorporating prompts with large language models (LLMs) substantially boosts
the efficacy of summarization tasks. However, we show that this approach also
leads to increased output variance, resulting in notably divergent outputs even
when prompts share similar meanings. To tackle this challenge, we introduce a
model-agnostic Soft Prompt-Based Calibration (SPeC) pipeline that employs soft
prompts to diminish variance while preserving the advantages of prompt-based
summarization. Experimental findings on multiple clinical note tasks and LLMs
indicate that our method not only bolsters performance but also effectively
curbs variance for various LLMs, providing a more uniform and dependable
solution for summarizing vital medical information.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Preference-Aware Constrained Multi-Objective Bayesian Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13034v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13034v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alaleh Ahmadianshalchi, Syrine Belakaria, Janardhan Rao Doppa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the problem of constrained multi-objective optimization
over black-box objective functions with practitioner-specified preferences over
the objectives when a large fraction of the input space is infeasible (i.e.,
violates constraints). This problem arises in many engineering design problems
including analog circuits and electric power system design. Our overall goal is
to approximate the optimal Pareto set over the small fraction of feasible input
designs. The key challenges include the huge size of the design space, multiple
objectives and large number of constraints, and the small fraction of feasible
input designs which can be identified only after performing expensive
simulations. We propose a novel and efficient preference-aware constrained
multi-objective Bayesian optimization approach referred to as PAC-MOO to
address these challenges. The key idea is to learn surrogate models for both
output objectives and constraints, and select the candidate input for
evaluation in each iteration that maximizes the information gained about the
optimal constrained Pareto front while factoring in the preferences over
objectives. Our experiments on two real-world analog circuit design
optimization problems demonstrate the efficacy of PAC-MOO over prior methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2110.06980</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Self-Supervised</span> Clustering of Multivariate Time-Series Data for
  Identifying TBI Physiological States 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13024v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13024v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hamid Ghaderi, Brandon Foreman, Amin Nayebi, Sindhu Tipirneni, Chandan K. Reddy, Vignesh Subbian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Determining clinically relevant physiological states from multivariate time
series data with missing values is essential for providing appropriate
treatment for acute conditions such as Traumatic Brain Injury (TBI),
respiratory failure, and heart failure. Utilizing non-temporal clustering or
data imputation and aggregation techniques may lead to loss of valuable
information and biased analyses. In our study, we apply the SLAC-Time
algorithm, an innovative self-supervision-based approach that maintains data
integrity by avoiding imputation or aggregation, offering a more useful
representation of acute patient states. By using SLAC-Time to cluster data in a
large research dataset, we identified three distinct TBI physiological states
and their specific feature profiles. We employed various clustering evaluation
metrics and incorporated input from a clinical domain expert to validate and
interpret the identified physiological states. Further, we discovered how
specific clinical events and interventions can influence patient states and
state transitions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 7 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ENVIDR: Implicit Differentiable Renderer with Neural Environment
  Lighting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13022v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13022v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruofan Liang, Huiting Chen, Chunlin Li, Fan Chen, Selvakumar Panneer, Nandita Vijaykumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in neural rendering have shown great potential for
reconstructing scenes from multiview images. However, accurately representing
objects with glossy surfaces remains a challenge for existing methods. In this
work, we introduce ENVIDR, a rendering and modeling framework for high-quality
rendering and reconstruction of surfaces with challenging specular reflections.
To achieve this, we first propose a novel neural renderer with decomposed
rendering components to learn the interaction between surface and environment
lighting. This renderer is trained using existing physically based renderers
and is decoupled from actual scene representations. We then propose an
SDF-based neural surface model that leverages this learned neural renderer to
represent general scenes. Our model additionally synthesizes indirect
illuminations caused by inter-reflections from shiny surfaces by marching
surface-reflected rays. We demonstrate that our method outperforms state-of-art
methods on challenging shiny scenes, providing high-quality rendering of
specular reflections while also enabling material editing and scene relighting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://nexuslrf.github.io/ENVIDR/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Failure-tolerant Distributed Learning for Anomaly Detection in Wireless
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13015v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13015v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marc Katzef, Andrew C. Cullen, Tansu Alpcan, Christopher Leckie, Justin Kopacz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The analysis of distributed techniques is often focused upon their
efficiency, without considering their robustness (or lack thereof). Such a
consideration is particularly important when devices or central servers can
fail, which can potentially cripple distributed systems. When such failures
arise in wireless communications networks, important services that they
use/provide (like anomaly detection) can be left inoperable and can result in a
cascade of security problems. In this paper, we present a novel method to
address these risks by combining both flat- and star-topologies, combining the
performance and reliability benefits of both. We refer to this method as
"Tol-FL", due to its increased failure-tolerance as compared to the technique
of Federated Learning. Our approach both limits device failure risks while
outperforming prior methods by up to 8% in terms of anomaly detection AUROC in
a range of realistic settings that consider client as well as server failure,
all while reducing communication costs. This performance demonstrates that
Tol-FL is a highly suitable method for distributed model training for anomaly
detection, especially in the domain of wireless networks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semantic Image Attack for Visual Model Diagnosis <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13010v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13010v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinqi Luo, Zhaoning Wang, Chen Henry Wu, Dong Huang, Fernando De la Torre
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In practice, metric analysis on a specific train and test dataset does not
guarantee reliable or fair ML models. This is partially due to the fact that
obtaining a balanced, diverse, and perfectly labeled dataset is typically
expensive, time-consuming, and error-prone. Rather than relying on a carefully
designed test set to assess ML models' failures, fairness, or robustness, this
paper proposes Semantic Image Attack (SIA), a method based on the adversarial
attack that provides semantic adversarial images to allow model diagnosis,
interpretability, and robustness. Traditional adversarial training is a popular
methodology for robustifying ML models against attacks. However, existing
adversarial methods do not combine the two aspects that enable the
interpretation and analysis of the model's flaws: semantic traceability and
perceptual quality. SIA combines the two features via iterative gradient ascent
on a predefined semantic attribute space and the image space. We illustrate the
validity of our approach in three scenarios for keypoint detection and
classification. (1) Model diagnosis: SIA generates a histogram of attributes
that highlights the semantic vulnerability of the ML model (i.e., attributes
that make the model fail). (2) Stronger attacks: SIA generates adversarial
examples with visually interpretable attributes that lead to higher attack
success rates than baseline methods. The adversarial training on SIA improves
the transferable robustness across different gradient-based attacks. (3)
Robustness to imbalanced datasets: we use SIA to augment the underrepresented
classes, which outperforms strong augmentation and re-balancing baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Initial version submitted to NeurIPS 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Controllable Inversion of Black-Box Face-Recognition Models via
  Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13006v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13006v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manuel Kansy, Anton Raël, Graziana Mignone, Jacek Naruniec, Christopher Schroers, Markus Gross, Romann M. Weber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Face recognition models embed a face image into a low-dimensional identity
vector containing abstract encodings of identity-specific facial features that
allow individuals to be distinguished from one another. We tackle the
challenging task of inverting the latent space of pre-trained face recognition
models without full model access (i.e. black-box setting). A variety of methods
have been proposed in literature for this task, but they have serious
shortcomings such as a lack of realistic outputs, long inference times, and
strong requirements for the data set and accessibility of the face recognition
model. Through an analysis of the black-box inversion problem, we show that the
conditional diffusion model loss naturally emerges and that we can effectively
sample from the inverse distribution even without an identity-specific loss.
Our method, named identity denoising diffusion probabilistic model (ID3PM),
leverages the stochastic nature of the denoising diffusion process to produce
high-quality, identity-preserving face images with various backgrounds,
lighting, poses, and expressions. We demonstrate state-of-the-art performance
in terms of identity preservation and diversity both qualitatively and
quantitatively. Our method is the first black-box face recognition model
inversion method that offers intuitive control over the generation process and
does not suffer from any of the common shortcomings from competing methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages. Preprint. Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adversarially Contrastive Estimation of Conditional Neural Processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13004v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13004v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zesheng Ye, Jing Du, Lina Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conditional Neural Processes~(CNPs) formulate distributions over functions
and generate function observations with exact conditional likelihoods. CNPs,
however, have limited expressivity for high-dimensional observations, since
their predictive distribution is factorized into a product of unconstrained
(typically) Gaussian outputs. Previously, this could be handled using latent
variables or autoregressive likelihood, but at the expense of intractable
training and quadratically increased complexity. Instead, we propose
calibrating CNPs with an adversarial training scheme besides regular maximum
likelihood estimates. Specifically, we train an energy-based model (EBM) with
noise contrastive estimation, which enforces EBM to identify true observations
from the generations of CNP. In this way, CNP must generate predictions closer
to the ground-truth to fool EBM, instead of merely optimizing with respect to
the fixed-form likelihood. From generative function reconstruction to
downstream regression and classification tasks, we demonstrate that our method
fits mainstream CNP members, showing effectiveness when unconstrained Gaussian
likelihood is defined, requiring minimal computation overhead while preserving
foundation properties of CNPs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking the Reliability of Post-training Quantization: a Particular
  Focus on Worst-case Performance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13003v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13003v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhihang Yuan, Jiawei Liu, Jiaxiang Wu, Dawei Yang, Qiang Wu, Guangyu Sun, Wenyu Liu, Xinggang Wang, Bingzhe Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Post-training quantization (PTQ) is a popular method for compressing deep
neural networks (DNNs) without modifying their original architecture or
training procedures. Despite its effectiveness and convenience, the reliability
of PTQ methods in the presence of some extrem cases such as distribution shift
and data noise remains largely unexplored. This paper first investigates this
problem on various commonly-used PTQ methods. We aim to answer several research
questions related to the influence of calibration set distribution variations,
calibration paradigm selection, and data augmentation or sampling strategies on
PTQ reliability. A systematic evaluation process is conducted across a wide
range of tasks and commonly-used PTQ paradigms. The results show that most
existing PTQ methods are not reliable enough in term of the worst-case group
performance, highlighting the need for more robust methods. Our findings
provide insights for developing PTQ methods that can effectively handle
distribution shift scenarios and enable the deployment of quantized DNNs in
real-world applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Planning Goals for Exploration <span class="chip">ICLR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13002v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13002v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edward S. Hu, Richard Chang, Oleh Rybkin, Dinesh Jayaraman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dropped into an unknown environment, what should an agent do to quickly learn
about the environment and how to accomplish diverse tasks within it? We address
this question within the goal-conditioned reinforcement learning paradigm, by
identifying how the agent should set its goals at training time to maximize
exploration. We propose "Planning Exploratory Goals" (PEG), a method that sets
goals for each training episode to directly optimize an intrinsic exploration
reward. PEG first chooses goal commands such that the agent's goal-conditioned
policy, at its current level of training, will end up in states with high
exploration potential. It then launches an exploration policy starting at those
promising states. To enable this direct optimization, PEG learns world models
and adapts sampling-based planning algorithms to "plan goal commands". In
challenging simulated robotics environments including a multi-legged ant robot
in a maze, and a robot arm on a cluttered tabletop, PEG exploration enables
more efficient and effective training of goal-conditioned policies relative to
baselines and ablations. Our ant successfully navigates a long maze, and the
robot arm successfully builds a stack of three blocks upon command. Website:
https://penn-pal-lab.github.io/peg/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera Ready version for ICLR2023 Spotlight</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automated Federated Learning in Mobile Edge Networks -- Fast Adaptation
  and Convergence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12999v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12999v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaoqun You, Kun Guo, Gang Feng, Peng Yang, Tony Q. S. Quek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) can be used in mobile edge networks to train machine
learning models in a distributed manner. Recently, FL has been interpreted
within a Model-Agnostic Meta-Learning (MAML) framework, which brings FL
significant advantages in fast adaptation and convergence over heterogeneous
datasets. However, existing research simply combines MAML and FL without
explicitly addressing how much benefit MAML brings to FL and how to maximize
such benefit over mobile edge networks. In this paper, we quantify the benefit
from two aspects: optimizing FL hyperparameters (i.e., sampled data size and
the number of communication rounds) and resource allocation (i.e., transmit
power) in mobile edge networks. Specifically, we formulate the MAML-based FL
design as an overall learning time minimization problem, under the constraints
of model accuracy and energy consumption. Facilitated by the convergence
analysis of MAML-based FL, we decompose the formulated problem and then solve
it using analytical solutions and the coordinate descent method. With the
obtained FL hyperparameters and resource allocation, we design a MAML-based FL
algorithm, called Automated Federated Learning (AutoFL), that is able to
conduct fast adaptation and convergence. Extensive experimental results verify
that AutoFL outperforms other benchmark algorithms regarding the learning time
and convergence performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> of Historical Learning: Learning Models with Learning History 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12992v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12992v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Li, Ge Wu, Lingfeng Yang, Wenhai Wang, Renjie Song, Jian Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  New knowledge originates from the old. The various types of elements,
deposited in the training history, are a large amount of wealth for improving
learning deep models. In this survey, we comprehensively review and summarize
the topic--``Historical Learning: Learning Models with Learning History'',
which learns better neural models with the help of their learning history
during its optimization, from three detailed aspects: Historical Type (what),
Functional Part (where) and Storage Form (how). To our best knowledge, it is
the first survey that systematically studies the methodologies which make use
of various historical statistics when training deep neural networks. The
discussions with related topics like recurrent/memory networks, ensemble
learning, and reinforcement learning are demonstrated. We also expose future
challenges of this topic and encourage the community to pay attention to the
think of historical learning principles when designing algorithms. The paper
list related to historical learning is available at
\url{https://github.com/Martinser/Awesome-Historical-Learning.}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Xiang Li and Ge Wu have equal contributions</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fault Prognosis of Turbofan Engines: Eventual Failure Prediction and
  Remaining Useful Life Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12982v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12982v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joseph Cohen, Xun Huan, Jun Ni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the era of industrial big data, prognostics and health management is
essential to improve the prediction of future failures to minimize inventory,
maintenance, and human costs. Used for the 2021 PHM Data Challenge, the new
Commercial Modular Aero-Propulsion System Simulation dataset from NASA is an
open-source benchmark containing simulated turbofan engine units flown under
realistic flight conditions. Deep learning approaches implemented previously
for this application attempt to predict the remaining useful life of the engine
units, but have not utilized labeled failure mode information, impeding
practical usage and explainability. To address these limitations, a new
prognostics approach is formulated with a customized loss function to
simultaneously predict the current health state, the eventual failing
component(s), and the remaining useful life. The proposed method incorporates
principal component analysis to orthogonalize statistical time-domain features,
which are inputs into supervised regressors such as random forests, extreme
random forests, XGBoost, and artificial neural networks. The highest performing
algorithm, ANN-Flux, achieves AUROC and AUPR scores exceeding 0.95 for each
classification. In addition, ANN-Flux reduces the remaining useful life RMSE by
38% for the same test split of the dataset compared to past work, with
significantly less computational cost.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint with 10 pages, 5 figures. Submitted to International Journal
  of Prognostics and Health Management (IJPHM)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Connected Superlevel Set in (Deep) Reinforcement Learning and its
  Application to Minimax Theorems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12981v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12981v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sihan Zeng, Thinh T. Doan, Justin Romberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The aim of this paper is to improve the understanding of the optimization
landscape for policy optimization problems in reinforcement learning.
Specifically, we show that the superlevel set of the objective function with
respect to the policy parameter is always a connected set both in the tabular
setting and under policies represented by a class of neural networks. In
addition, we show that the optimization objective as a function of the policy
parameter and reward satisfies a stronger "equiconnectedness" property. To our
best knowledge, these are novel and previously unknown discoveries.
  We present an application of the connectedness of these superlevel sets to
the derivation of minimax theorems for robust reinforcement learning. We show
that any minimax optimization program which is convex on one side and is
equiconnected on the other side observes the minimax equality (i.e. has a Nash
equilibrium). We find that this exact structure is exhibited by an interesting
robust reinforcement learning problem under an adversarial reward attack, and
the validity of its minimax equality immediately follows. This is the first
time such a result is established in the literature.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Continuous Indeterminate Probability Neural Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12964v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12964v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a general model called CIPNN - Continuous Indeterminate
Probability Neural Network, and this model is based on IPNN, which is used for
discrete latent random variables. Currently, posterior of continuous latent
variables is regarded as intractable, with the new theory proposed by IPNN this
problem can be solved. Our contributions are Four-fold. First, we derive the
analytical solution of the posterior calculation of continuous latent random
variables and propose a general classification model (CIPNN). Second, we
propose a general auto-encoder called CIPAE - Continuous Indeterminate
Probability Auto-Encoder, the decoder part is not a neural network and uses a
fully probabilistic inference model for the first time. Third, we propose a new
method to visualize the latent random variables, we use one of N dimensional
latent variables as a decoder to reconstruct the input image, which can work
even for classification tasks, in this way, we can see what each latent
variable has learned. Fourth, IPNN has shown great classification capability,
CIPNN has pushed this classification capability to infinity. Theoretical
advantages are reflected in experimental results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Forecast-Aware Model Driven LSTM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12963v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12963v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sophia Hamer, Jennifer Sleeman, Ivanka Stajner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Poor air quality can have a significant impact on human health. The National
Oceanic and Atmospheric Administration (NOAA) air quality forecasting guidance
is challenged by the increasing presence of extreme air quality events due to
extreme weather events such as wild fires and heatwaves. These extreme air
quality events further affect human health. Traditional methods used to correct
model bias make assumptions about linearity and the underlying distribution.
Extreme air quality events tend to occur without a strong signal leading up to
the event and this behavior tends to cause existing methods to either under or
over compensate for the bias. Deep learning holds promise for air quality
forecasting in the presence of extreme air quality events due to its ability to
generalize and learn nonlinear problems. However, in the presence of these
anomalous air quality events, standard deep network approaches that use a
single network for generalizing to future forecasts, may not always provide the
best performance even with a full feature-set including geography and
meteorology. In this work we describe a method that combines unsupervised
learning and a forecast-aware bi-directional LSTM network to perform bias
correction for operational air quality forecasting using AirNow station data
for ozone and PM2.5 in the continental US. Using an unsupervised clustering
method trained on station geographical features such as latitude and longitude,
urbanization, and elevation, the learned clusters direct training by
partitioning the training data for the LSTM networks. LSTMs are forecast-aware
and implemented using a unique way to perform learning forward and backwards in
time across forecasting days. When comparing the RMSE of the forecast model to
the RMSE of the bias corrected model, the bias corrected model shows
significant improvement (27\% lower RMSE for ozone) over the base forecast.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comprehensive Analysis of AI Biases in DeepFake Detection With
  Massively Annotated Databases 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.05845v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.05845v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ying Xu, Philipp Terhörst, Kiran Raja, Marius Pedersen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, image and video manipulations with Deepfake have become a
severe concern for security and society. Many detection models and datasets
have been proposed to detect Deepfake data reliably. However, there is an
increased concern that these models and training databases might be biased and,
thus, cause Deepfake detectors to fail. In this work, we investigate the bias
issue caused by public Deepfake datasets by (a) providing large-scale
demographic and non-demographic attribute annotations of 47 different
attributes for five popular Deepfake datasets and (b) comprehensively analysing
AI-bias of three state-of-the-art Deepfake detection backbone models on these
datasets. The investigation analyses the influence of a large variety of
distinctive attributes (from over 65M labels) on the detection performance,
including demographic (age, gender, ethnicity) and non-demographic (hair, skin,
accessories, etc.) information. The results indicate that investigated
databases lack diversity and, more importantly, show that the utilised Deepfake
detection backbone models are strongly biased towards many investigated
attributes. The Deepfake detection backbone methods, which are trained with
biased datasets, might output incorrect detection results, thereby leading to
generalisability, fairness, and security issues. We hope that the findings of
this study and the annotation databases will help to evaluate and mitigate bias
in future Deepfake detection techniques. The annotation datasets are publicly
available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A* Search Without Expansions: Learning Heuristic Functions with Deep
  Q-Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2102.04518v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2102.04518v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Forest Agostinelli, Alexander Shmakov, Stephen McAleer, Roy Fox, Pierre Baldi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficiently solving problems with large action spaces using A* search has
been of importance to the artificial intelligence community for decades. This
is because the computation and memory requirements of A* search grow linearly
with the size of the action space. This burden becomes even more apparent when
A* search uses a heuristic function learned by computationally expensive
function approximators, such as deep neural networks. To address this problem,
we introduce Q* search, a search algorithm that uses deep Q-networks to guide
search in order to take advantage of the fact that the sum of the transition
costs and heuristic values of the children of a node can be computed with a
single forward pass through a deep Q-network without explicitly generating
those children. This significantly reduces computation time and requires only
one node to be generated per iteration. We use Q* search to solve the Rubik's
cube when formulated with a large action space that includes 1872 meta-actions
and find that this 157-fold increase in the size of the action space incurs
less than a 4-fold increase in computation time and less than a 3-fold increase
in number of nodes generated when performing Q* search. Furthermore, Q* search
is up to 129 times faster and generates up to 1288 times fewer nodes than A*
search. Finally, although obtaining admissible heuristic functions from deep
neural networks is an ongoing area of research, we prove that Q* search is
guaranteed to find a shortest path given a heuristic function that neither
overestimates the cost of a shortest path nor underestimates the transition
cost.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Added theoretical results to show that Q* search is an admissible
  search algorithm. Added comparisons to deferred heuristic evaluation. Added
  experiments with Lights Out and the 35-Pancake puzzle</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sliced Optimal Partial Transport 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08049v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08049v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yikun Bai, Bernard Schmitzer, Mathew Thorpe, Soheil Kolouri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optimal transport (OT) has become exceedingly popular in machine learning,
data science, and computer vision. The core assumption in the OT problem is the
equal total amount of mass in source and target measures, which limits its
application. Optimal Partial Transport (OPT) is a recently proposed solution to
this limitation. Similar to the OT problem, the computation of OPT relies on
solving a linear programming problem (often in high dimensions), which can
become computationally prohibitive. In this paper, we propose an efficient
algorithm for calculating the OPT problem between two non-negative measures in
one dimension. Next, following the idea of sliced OT distances, we utilize
slicing to define the sliced OPT distance. Finally, we demonstrate the
computational and accuracy benefits of the sliced OPT-based method in various
numerical experiments. In particular, we show an application of our proposed
Sliced-OPT in noisy point cloud registration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Physics-Embedded Neural Networks: Graph Neural PDE Solvers with Mixed
  Boundary Conditions <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.11912v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.11912v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Masanobu Horie, Naoto Mitsume
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph neural network (GNN) is a promising approach to learning and predicting
physical phenomena described in boundary value problems, such as partial
differential equations (PDEs) with boundary conditions. However, existing
models inadequately treat boundary conditions essential for the reliable
prediction of such problems. In addition, because of the locally connected
nature of GNNs, it is difficult to accurately predict the state after a long
time, where interaction between vertices tends to be global. We present our
approach termed physics-embedded neural networks that considers boundary
conditions and predicts the state after a long time using an implicit method.
It is built based on an E(n)-equivariant GNN, resulting in high generalization
performance on various shapes. We demonstrate that our model learns flow
phenomena in complex shapes and outperforms a well-optimized classical solver
and a state-of-the-art machine learning model in speed-accuracy trade-off.
Therefore, our model can be a useful standard for realizing reliable, fast, and
accurate GNN-based PDE solvers. The code is available at
https://github.com/yellowshippo/penn-neurips2022.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating the Robustness of Deep Reinforcement Learning for Autonomous
  Policies in a Multi-agent Urban Driving Environment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.11947v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.11947v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aizaz Sharif, Dusica Marijan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep reinforcement learning is actively used for training autonomous car
policies in a simulated driving environment. Due to the large availability of
various reinforcement learning algorithms and the lack of their systematic
comparison across different driving scenarios, we are unsure of which ones are
more effective for training autonomous car software in single-agent as well as
multi-agent driving environments. A benchmarking framework for the comparison
of deep reinforcement learning in a vision-based autonomous driving will open
up the possibilities for training better autonomous car driving policies. To
address these challenges, we provide an open and reusable benchmarking
framework for systematic evaluation and comparative analysis of deep
reinforcement learning algorithms for autonomous driving in a single- and
multi-agent environment. Using the framework, we perform a comparative study of
discrete and continuous action space deep reinforcement learning algorithms. We
also propose a comprehensive multi-objective reward function designed for the
evaluation of deep reinforcement learning-based autonomous driving agents. We
run the experiments in a vision-only high-fidelity urban driving simulated
environments. The results indicate that only some of the deep reinforcement
learning algorithms perform consistently better across single and multi-agent
scenarios when trained in various multi-agent-only environment settings. For
example, A3C- and TD3-based autonomous cars perform comparatively better in
terms of more robust actions and minimal driving errors in both single and
multi-agent scenarios. We conclude that different deep reinforcement learning
algorithms exhibit different driving and testing performance in different
scenarios, which underlines the need for their systematic comparative analysis.
The benchmarking framework proposed in this paper facilitates such a
comparison.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Subgrid-scale Models with Neural Ordinary Differential
  Equations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.09967v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.09967v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shinhoo Kang, Emil M. Constantinescu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new approach to learning the subgrid-scale model when simulating
partial differential equations (PDEs) solved by the method of lines and their
representation in chaotic ordinary differential equations, based on neural
ordinary differential equations (NODEs). Solving systems with fine temporal and
spatial grid scales is an ongoing computational challenge, and closure models
are generally difficult to tune. Machine learning approaches have increased the
accuracy and efficiency of computational fluid dynamics solvers. In this
approach neural networks are used to learn the coarse- to fine-grid map, which
can be viewed as subgrid-scale parameterization. We propose a strategy that
uses the NODE and partial knowledge to learn the source dynamics at a
continuous level. Our method inherits the advantages of NODEs and can be used
to parameterize subgrid scales, approximate coupling operators, and improve the
efficiency of low-order solvers. Numerical results with the two-scale Lorenz 96
ODE, the convection-diffusion PDE, and the viscous Burgers' PDE are used to
illustrate this approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 20 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Student Engagement Detection Using Emotion Analysis, Eye Tracking and
  Head Movement with Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1909.12913v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1909.12913v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prabin Sharma, Shubham Joshi, Subash Gautam, Sneha Maharjan, Salik Ram Khanal, Manuel Cabral Reis, João Barroso, Vítor Manuel de Jesus Filipe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increase of distance learning, in general, and e-learning, in
particular, having a system capable of determining the engagement of students
is of primordial importance, and one of the biggest challenges, both for
teachers, researchers and policy makers. Here, we present a system to detect
the engagement level of the students. It uses only information provided by the
typical built-in web-camera present in a laptop computer, and was designed to
work in real time. We combine information about the movements of the eyes and
head, and facial emotions to produce a concentration index with three classes
of engagement: "very engaged", "nominally engaged" and "not engaged at all".
The system was tested in a typical e-learning scenario, and the results show
that it correctly identifies each period of time where students were "very
engaged", "nominally engaged" and "not engaged at all". Additionally, the
results also show that the students with best scores also have higher
concentration indexes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 9 Figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Monte Carlo Evaluation with Offline Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.13734v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.13734v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuze Liu, Shangtong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Monte Carlo (MC) methods are the most widely used methods to estimate the
performance of a policy. Given an interested policy, MC methods give estimates
by repeatedly running this policy to collect samples and taking the average of
the outcomes. Samples collected during this process are called online samples.
To get an accurate estimate, MC methods consume massive online samples. When
online samples are expensive, e.g., online recommendations and inventory
management, we want to reduce the number of online samples while achieving the
same estimate accuracy. To this end, we use off-policy MC methods that evaluate
the interested policy by running a different policy called behavior policy. We
design a tailored behavior policy such that the variance of the off-policy MC
estimator is provably smaller than the ordinary MC estimator. Importantly, this
tailored behavior policy can be efficiently learned from existing offline data,
i,e., previously logged data, which are much cheaper than online samples. With
reduced variance, our off-policy MC method requires fewer online samples to
evaluate the performance of a policy compared with the ordinary MC method.
Moreover, our off-policy MC estimator is always unbiased.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Normalizing Flows for Interventional Density Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.06203v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.06203v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valentyn Melnychuk, Dennis Frauen, Stefan Feuerriegel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing machine learning methods for causal inference usually estimate
quantities expressed via the mean of potential outcomes (e.g., average
treatment effect). However, such quantities do not capture the full information
about the distribution of potential outcomes. In this work, we estimate the
density of potential outcomes after interventions from observational data. For
this, we propose a novel, fully-parametric deep learning method called
Interventional Normalizing Flows. Specifically, we combine two normalizing
flows, namely (i) a nuisance flow for estimating nuisance parameters and (ii) a
target flow for a parametric estimation of the density of potential outcomes.
We further develop a tractable optimization objective based on a one-step bias
correction for an efficient and doubly robust estimation of the target flow
parameters. As a result our Interventional Normalizing Flows offer a properly
normalized density estimator. Across various experiments, we demonstrate that
our Interventional Normalizing Flows are expressive and highly effective, and
scale well with both sample size and high-dimensional confounding. To the best
of our knowledge, our Interventional Normalizing Flows are the first proper
fully-parametric, deep learning method for density estimation of potential
outcomes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sample-Efficient Multi-Objective Learning via Generalized Policy
  Improvement Prioritization <span class="chip">AAMAS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07784v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07784v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucas N. Alegre, Ana L. C. Bazzan, Diederik M. Roijers, Ann Nowé, Bruno C. da Silva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-objective reinforcement learning (MORL) algorithms tackle sequential
decision problems where agents may have different preferences over (possibly
conflicting) reward functions. Such algorithms often learn a set of policies
(each optimized for a particular agent preference) that can later be used to
solve problems with novel preferences. We introduce a novel algorithm that uses
Generalized Policy Improvement (GPI) to define principled, formally-derived
prioritization schemes that improve sample-efficient learning. They implement
active-learning strategies by which the agent can (i) identify the most
promising preferences/objectives to train on at each moment, to more rapidly
solve a given MORL problem; and (ii) identify which previous experiences are
most relevant when learning a policy for a particular agent preference, via a
novel Dyna-style MORL method. We prove our algorithm is guaranteed to always
converge to an optimal solution in a finite number of steps, or an
$\epsilon$-optimal solution (for a bounded $\epsilon$) if the agent is limited
and can only identify possibly sub-optimal policies. We also prove that our
method monotonically improves the quality of its partial solutions while
learning. Finally, we introduce a bound that characterizes the maximum utility
loss (with respect to the optimal solution) incurred by the partial solutions
computed by our method throughout learning. We empirically show that our method
outperforms state-of-the-art MORL algorithms in challenging multi-objective
tasks, both with discrete and continuous state and action spaces.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAMAS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Coupled Design of Exploiting Record Similarity for Practical Vertical
  Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.06312v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.06312v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaomin Wu, Qinbin Li, Bingsheng He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning is a learning paradigm to enable collaborative learning
across different parties without revealing raw data. Notably, vertical
federated learning (VFL), where parties share the same set of samples but only
hold partial features, has a wide range of real-world applications. However,
most existing studies in VFL disregard the "record linkage" process. They
design algorithms either assuming the data from different parties can be
exactly linked or simply linking each record with its most similar neighboring
record. These approaches may fail to capture the key features from other less
similar records. Moreover, such improper linkage cannot be corrected by
training since existing approaches provide no feedback on linkage during
training. In this paper, we design a novel coupled training paradigm, FedSim,
that integrates one-to-many linkage into the training process. Besides enabling
VFL in many real-world applications with fuzzy identifiers, FedSim also
achieves better performance in traditional VFL tasks. Moreover, we
theoretically analyze the additional privacy risk incurred by sharing
similarities. Our experiments on eight datasets with various similarity metrics
show that FedSim outperforms other state-of-the-art baselines. The codes of
FedSim are available at https://github.com/Xtra-Computing/FedSim.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A robust estimator of mutual information for deep learning
  interpretability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.00024v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.00024v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Davide Piras, Hiranya V. Peiris, Andrew Pontzen, Luisa Lucie-Smith, Ningyuan Guo, Brian Nord
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We develop the use of mutual information (MI), a well-established metric in
information theory, to interpret the inner workings of deep learning models. To
accurately estimate MI from a finite number of samples, we present GMM-MI
(pronounced $``$Jimmie$"$), an algorithm based on Gaussian mixture models that
can be applied to both discrete and continuous settings. GMM-MI is
computationally efficient, robust to the choice of hyperparameters and provides
the uncertainty on the MI estimate due to the finite sample size. We
extensively validate GMM-MI on toy data for which the ground truth MI is known,
comparing its performance against established mutual information estimators. We
then demonstrate the use of our MI estimator in the context of representation
learning, working with synthetic data and physical datasets describing highly
non-linear processes. We train deep learning models to encode high-dimensional
data within a meaningful compressed (latent) representation, and use GMM-MI to
quantify both the level of disentanglement between the latent variables, and
their association with relevant physical quantities, thus unlocking the
interpretability of the latent representation. We make GMM-MI publicly
available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 8 figures. Minor changes to match version accepted for
  publication in Machine Learning: Science and Technology. GMM-MI available at
  https://github.com/dpiras/GMM-MI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Low Dimensional State Spaces with Overparameterized Recurrent
  Neural Nets <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.14064v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.14064v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edo Cohen-Karlik, Itamar Menuhin-Gruman, Raja Giryes, Nadav Cohen, Amir Globerson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Overparameterization in deep learning typically refers to settings where a
trained neural network (NN) has representational capacity to fit the training
data in many ways, some of which generalize well, while others do not. In the
case of Recurrent Neural Networks (RNNs), there exists an additional layer of
overparameterization, in the sense that a model may exhibit many solutions that
generalize well for sequence lengths seen in training, some of which
extrapolate to longer sequences, while others do not. Numerous works have
studied the tendency of Gradient Descent (GD) to fit overparameterized NNs with
solutions that generalize well. On the other hand, its tendency to fit
overparameterized RNNs with solutions that extrapolate has been discovered only
recently and is far less understood. In this paper, we analyze the
extrapolation properties of GD when applied to overparameterized linear RNNs.
In contrast to recent arguments suggesting an implicit bias towards short-term
memory, we provide theoretical evidence for learning low-dimensional state
spaces, which can also model long-term memory. Our result relies on a dynamical
characterization which shows that GD (with small step size and near-zero
initialization) strives to maintain a certain form of balancedness, as well as
on tools developed in the context of the moment problem from statistics
(recovery of a probability distribution from its moments). Experiments
corroborate our theory, demonstrating extrapolation via learning
low-dimensional state spaces with both linear and non-linear RNNs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2023, 9 pages, 2 figures plus supplementary</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Predicting the performance of hybrid ventilation in buildings using a
  multivariate attention-based biLSTM Encoder-Decoder neural network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.04126v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.04126v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gaurav Chaudhary, Hicham Johra, Laurent Georges, Bjørn Austbø
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hybrid ventilation is an energy-efficient solution to provide fresh air for
most climates, given that it has a reliable control system. To operate such
systems optimally, a high-fidelity control-oriented modesl is required. It
should enable near-real time forecast of the indoor air temperature based on
operational conditions such as window opening and HVAC operating schedules.
However, physics-based control-oriented models (i.e., white-box models) are
labour-intensive and computationally expensive. Alternatively, black-box models
based on artificial neural networks can be trained to be good estimators for
building dynamics. This paper investigates the capabilities of a deep neural
network (DNN), which is a multivariate multi-head attention-based long
short-term memory (LSTM) encoder-decoder neural network, to predict indoor air
temperature when windows are opened or closed. Training and test data are
generated from a detailed multi-zone office building model (EnergyPlus).
Pseudo-random signals are used for the indoor air temperature setpoints and
window opening instances. The results indicate that the DNN is able to
accurately predict the indoor air temperature of five zones whenever windows
are opened or closed. The prediction error plateaus after the 24th step ahead
prediction (6 hr ahead prediction).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging the Potential of Novel Data in Power Line Communication of
  Electricity Grids 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.12693v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.12693v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christoph Balada, Max Bondorf, Sheraz Ahmed, Andreas Dengela, Markus Zdrallek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electricity grids have become an essential part of daily life, even if they
are often not noticed in everyday life. We usually only become particularly
aware of this dependence by the time the electricity grid is no longer
available. However, significant changes, such as the transition to renewable
energy (photovoltaic, wind turbines, etc.) and an increasing number of energy
consumers with complex load profiles (electric vehicles, home battery systems,
etc.), pose new challenges for the electricity grid. To address these
challenges, we propose two first-of-its-kind datasets based on measurements in
a broadband powerline communications (PLC) infrastructure. Both datasets FiN-1
and FiN-2, were collected during real practical use in a part of the German
low-voltage grid that supplies around 4.4 million people and show more than 13
billion datapoints collected by more than 5100 sensors. In addition, we present
different use cases in asset management, grid state visualization, forecasting,
predictive maintenance, and novelty detection to highlight the benefits of
these types of data. For these applications, we particularly highlight the use
of novel machine learning architectures to extract rich information from
real-world data that cannot be captured using traditional approaches. By
publishing the first large-scale real-world dataset, we aim to shed light on
the previously largely unrecognized potential of PLC data and emphasize
machine-learning-based research in low-voltage distribution networks by
presenting a variety of different use cases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Almost Sure Convergence of Dropout Algorithms for Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2002.02247v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2002.02247v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Albert Senen-Cerda, Jaron Sanders
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the convergence and convergence rate of stochastic training
algorithms for Neural Networks (NNs) that have been inspired by Dropout (Hinton
et al., 2012). With the goal of avoiding overfitting during training of NNs,
dropout algorithms consist in practice of multiplying the weight matrices of a
NN componentwise by independently drawn random matrices with $\{0, 1 \}$-valued
entries during each iteration of Stochastic Gradient Descent (SGD). This paper
presents a probability theoretical proof that for fully-connected NNs with
differentiable, polynomially bounded activation functions, if we project the
weights onto a compact set when using a dropout algorithm, then the weights of
the NN converge to a unique stationary point of a projected system of Ordinary
Differential Equations (ODEs). After this general convergence guarantee, we go
on to investigate the convergence rate of dropout. Firstly, we obtain generic
sample complexity bounds for finding $\epsilon$-stationary points of smooth
nonconvex functions using SGD with dropout that explicitly depend on the
dropout probability. Secondly, we obtain an upper bound on the rate of
convergence of Gradient Descent (GD) on the limiting ODEs of dropout algorithms
for NNs with the shape of arborescences of arbitrary depth and with linear
activation functions. The latter bound shows that for an algorithm such as
Dropout or Dropconnect (Wan et al., 2013), the convergence rate can be impaired
exponentially by the depth of the arborescence. In contrast, we experimentally
observe no such dependence for wide NNs with just a few dropout layers. We also
provide a heuristic argument for this observation. Our results suggest that
there is a change of scale of the effect of the dropout probability in the
convergence rate that depends on the relative size of the width of the NN
compared to its depth.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>52 pages, 3 figures. Added results pertaining to the convergence rate
  of Dropout SGD to $\epsilon$-stationary points and numerical experiments.
  Updated the introduction, conclusion and appendix. Changed format to
  one-column text</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Symmetries, flat minima, and the conserved quantities of gradient flow <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.17216v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.17216v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Zhao, Iordan Ganev, Robin Walters, Rose Yu, Nima Dehmamy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Empirical studies of the loss landscape of deep networks have revealed that
many local minima are connected through low-loss valleys. Yet, little is known
about the theoretical origin of such valleys. We present a general framework
for finding continuous symmetries in the parameter space, which carve out
low-loss valleys. Our framework uses equivariances of the activation functions
and can be applied to different layer architectures. To generalize this
framework to nonlinear neural networks, we introduce a novel set of nonlinear,
data-dependent symmetries. These symmetries can transform a trained model such
that it performs similarly on new samples, which allows ensemble building that
improves robustness under certain adversarial attacks. We then show that
conserved quantities associated with linear symmetries can be used to define
coordinates along low-loss valleys. The conserved quantities help reveal that
using common initialization methods, gradient flow only explores a small part
of the global minimum. By relating conserved quantities to convergence rate and
sharpness of the minimum, we provide insights on how initialization impacts
convergence and generalizability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Low-Rank Simplicity Bias in Deep Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2103.10427v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2103.10427v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minyoung Huh, Hossein Mobahi, Richard Zhang, Brian Cheung, Pulkit Agrawal, Phillip Isola
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern deep neural networks are highly over-parameterized compared to the
data on which they are trained, yet they often generalize remarkably well. A
flurry of recent work has asked: why do deep networks not overfit to their
training data? In this work, we make a series of empirical observations that
investigate and extend the hypothesis that deeper networks are inductively
biased to find solutions with lower effective rank embeddings. We conjecture
that this bias exists because the volume of functions that maps to low
effective rank embedding increases with depth. We show empirically that our
claim holds true on finite width linear and non-linear models on practical
learning paradigms and show that on natural data, these are often the solutions
that generalize well. We then show that the simplicity bias exists at both
initialization and after training and is resilient to hyper-parameters and
learning methods. We further demonstrate how linear over-parameterization of
deep non-linear models can be used to induce low-rank bias, improving
generalization performance on CIFAR and ImageNet without changing the modeling
capacity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GP-net: Flexible Viewpoint Grasp Proposal 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.10404v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.10404v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anna Konrad, John McDonald, Rudi Villing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the Grasp Proposal Network (GP-net), a Convolutional Neural
Network model which can generate 6-DOF grasps from flexible viewpoints, e.g. as
experienced by mobile manipulators. To train GP-net, we synthetically generate
a dataset containing depth-images and ground-truth grasp information. In
real-world experiments we use the EGAD! grasping benchmark to evaluate GP-net
against two commonly used algorithms, the Volumetric Grasping Network (VGN) and
the Grasp Pose Detection package (GPD), on a PAL TIAGo mobile manipulator. In
contrast to the state-of-the-art methods in robotic grasping, GP-net can be
used for grasping objects from flexible, unknown viewpoints without the need to
define the workspace and achieves a grasp success of 51.8% compared to 51.1%
for VGN and 33.6% for GPD. We provide a ROS package along with our code and
pre-trained models at https://aucoroboticsmu.github.io/GP-net/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Utilising the CLT Structure in Stochastic Gradient based Sampling :
  Improved Analysis and Faster Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.03792v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.03792v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aniket Das, Dheeraj Nagaraj, Anant Raj
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider stochastic approximations of sampling algorithms, such as
Stochastic Gradient Langevin Dynamics (SGLD) and the Random Batch Method (RBM)
for Interacting Particle Dynamcs (IPD). We observe that the noise introduced by
the stochastic approximation is nearly Gaussian due to the Central Limit
Theorem (CLT) while the driving Brownian motion is exactly Gaussian. We harness
this structure to absorb the stochastic approximation error inside the
diffusion process, and obtain improved convergence guarantees for these
algorithms. For SGLD, we prove the first stable convergence rate in KL
divergence without requiring uniform warm start, assuming the target density
satisfies a Log-Sobolev Inequality. Our result implies superior first-order
oracle complexity compared to prior works, under significantly milder
assumptions. We also prove the first guarantees for SGLD under even weaker
conditions such as H\"{o}lder smoothness and Poincare Inequality, thus bridging
the gap between the state-of-the-art guarantees for LMC and SGLD. Our analysis
motivates a new algorithm called covariance correction, which corrects for the
additional noise introduced by the stochastic approximation by rescaling the
strength of the diffusion. Finally, we apply our techniques to analyze RBM, and
significantly improve upon the guarantees in prior works (such as removing
exponential dependence on horizon), under minimal assumptions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Version 3 reduces the computational complexity of Covariance
  Correction and relaxes the almost-sure noise growth assumption for smooth
  SGLD</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Langevin Diffusion Variational Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.07743v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.07743v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tomas Geffner, Justin Domke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many methods that build powerful variational distributions based on
unadjusted Langevin transitions exist. Most of these were developed using a
wide range of different approaches and techniques. Unfortunately, the lack of a
unified analysis and derivation makes developing new methods and reasoning
about existing ones a challenging task. We address this giving a single
analysis that unifies and generalizes these existing techniques. The main idea
is to augment the target and variational by numerically simulating the
underdamped Langevin diffusion process and its time reversal. The benefits of
this approach are twofold: it provides a unified formulation for many existing
methods, and it simplifies the development of new ones. In fact, using our
formulation we propose a new method that combines the strengths of previously
existing algorithms; it uses underdamped Langevin transitions and powerful
augmentations parameterized by a score network. Our empirical evaluation shows
that our proposed method consistently outperforms relevant baselines in a wide
range of tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Omnigrok: Grokking Beyond Algorithmic Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.01117v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.01117v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziming Liu, Eric J. Michaud, Max Tegmark
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Grokking, the unusual phenomenon for algorithmic datasets where
generalization happens long after overfitting the training data, has remained
elusive. We aim to understand grokking by analyzing the loss landscapes of
neural networks, identifying the mismatch between training and test losses as
the cause for grokking. We refer to this as the "LU mechanism" because training
and test losses (against model weight norm) typically resemble "L" and "U",
respectively. This simple mechanism can nicely explain many aspects of
grokking: data size dependence, weight decay dependence, the emergence of
representations, etc. Guided by the intuitive picture, we are able to induce
grokking on tasks involving images, language and molecules. In the reverse
direction, we are able to eliminate grokking for algorithmic datasets. We
attribute the dramatic nature of grokking for algorithmic datasets to
representation learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Interpreting learning in biological neural networks as zero-order
  optimization method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.11777v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.11777v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Johannes Schmidt-Hieber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, significant progress has been made regarding the statistical
understanding of artificial neural networks (ANNs). ANNs are motivated by the
functioning of the brain, but differ in several crucial aspects. In particular,
the locality in the updating rule of the connection parameters in biological
neural networks (BNNs) makes it biologically implausible that the learning of
the brain is based on gradient descent. In this work, we look at the brain as a
statistical method for supervised learning. The main contribution is to relate
the local updating rule of the connection parameters in BNNs to a zero-order
optimization method. It is shown that the expected values of the iterates
implement a modification of gradient descent.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi PILOT: Learned Feasible Multiple Acquisition Trajectories for
  Dynamic MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07150v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07150v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tamir Shor, Tomer Weiss, Dor Noti, Alex Bronstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dynamic Magnetic Resonance Imaging (MRI) is known to be a powerful and
reliable technique for the dynamic imaging of internal organs and tissues,
making it a leading diagnostic tool. A major difficulty in using MRI in this
setting is the relatively long acquisition time (and, hence, increased cost)
required for imaging in high spatio-temporal resolution, leading to the
appearance of related motion artifacts and decrease in resolution. Compressed
Sensing (CS) techniques have become a common tool to reduce MRI acquisition
time by subsampling images in the k-space according to some acquisition
trajectory. Several studies have particularly focused on applying deep learning
techniques to learn these acquisition trajectories in order to attain better
image reconstruction, rather than using some predefined set of trajectories. To
the best of our knowledge, learning acquisition trajectories has been only
explored in the context of static MRI. In this study, we consider acquisition
trajectory learning in the dynamic imaging setting. We design an end-to-end
pipeline for the joint optimization of multiple per-frame acquisition
trajectories along with a reconstruction neural network, and demonstrate
improved image reconstruction quality in shorter acquisition times. The code
for reproducing all experiments is accessible at
https://github.com/tamirshor7/MultiPILOT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted For MIDL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Kernel Methods for Unobserved Confounding: Negative Controls, Proxies,
  and Instruments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2012.10315v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2012.10315v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rahul Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Negative control is a strategy for learning the causal relationship between
treatment and outcome in the presence of unmeasured confounding. The treatment
effect can nonetheless be identified if two auxiliary variables are available:
a negative control treatment (which has no effect on the actual outcome), and a
negative control outcome (which is not affected by the actual treatment). These
auxiliary variables can also be viewed as proxies for a traditional set of
control variables, and they bear resemblance to instrumental variables. I
propose a family of algorithms based on kernel ridge regression for learning
nonparametric treatment effects with negative controls. Examples include dose
response curves, dose response curves with distribution shift, and
heterogeneous treatment effects. Data may be discrete or continuous, and low,
high, or infinite dimensional. I prove uniform consistency and provide finite
sample rates of convergence. I estimate the dose response curve of cigarette
smoking on infant birth weight adjusting for unobserved confounding due to
household income, using a data set of singleton births in the state of
Pennsylvania between 1989 and 1991.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Real-Time Evaluation in Online Continual Learning: A New Hope <span class="chip">CVPR'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.01047v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.01047v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yasir Ghunaim, Adel Bibi, Kumail Alhamoud, Motasem Alfarra, Hasan Abed Al Kader Hammoud, Ameya Prabhu, Philip H. S. Torr, Bernard Ghanem
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current evaluations of Continual Learning (CL) methods typically assume that
there is no constraint on training time and computation. This is an unrealistic
assumption for any real-world setting, which motivates us to propose: a
practical real-time evaluation of continual learning, in which the stream does
not wait for the model to complete training before revealing the next data for
predictions. To do this, we evaluate current CL methods with respect to their
computational costs. We conduct extensive experiments on CLOC, a large-scale
dataset containing 39 million time-stamped images with geolocation labels. We
show that a simple baseline outperforms state-of-the-art CL methods under this
evaluation, questioning the applicability of existing methods in realistic
settings. In addition, we explore various CL components commonly used in the
literature, including memory sampling strategies and regularization approaches.
We find that all considered methods fail to be competitive against our simple
baseline. This surprisingly suggests that the majority of existing CL
literature is tailored to a specific class of streams that is not practical. We
hope that the evaluation we provide will be the first step towards a paradigm
shift to consider the computational cost in the development of online continual
learning methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR'23 as Highlight (Top 2.5%)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ gDDIM: Generalized denoising diffusion implicit models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.05564v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.05564v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qinsheng Zhang, Molei Tao, Yongxin Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Our goal is to extend the denoising diffusion implicit model (DDIM) to
general diffusion models~(DMs) besides isotropic diffusions. Instead of
constructing a non-Markov noising process as in the original DDIM, we examine
the mechanism of DDIM from a numerical perspective. We discover that the DDIM
can be obtained by using some specific approximations of the score when solving
the corresponding stochastic differential equation. We present an
interpretation of the accelerating effects of DDIM that also explains the
advantages of a deterministic sampling scheme over the stochastic one for fast
sampling. Building on this insight, we extend DDIM to general DMs, coined
generalized DDIM (gDDIM), with a small but delicate modification in
parameterizing the score network. We validate gDDIM in two non-isotropic DMs:
Blurring diffusion model (BDM) and Critically-damped Langevin diffusion model
(CLD). We observe more than 20 times acceleration in BDM. In the CLD, a
diffusion model by augmenting the diffusion process with velocity, our
algorithm achieves an FID score of 2.26, on CIFAR10, with only 50 number of
score function evaluations~(NFEs) and an FID score of 2.86 with only 27 NFEs.
Code is available at https://github.com/qsh-zh/gDDIM
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 9 figures, implementation https://github.com/qsh-zh/gDDIM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diffusion Models in Vision: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.04747v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.04747v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, Mubarak Shah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Denoising diffusion models represent a recent emerging topic in computer
vision, demonstrating remarkable results in the area of generative modeling. A
diffusion model is a deep generative model that is based on two stages, a
forward diffusion stage and a reverse diffusion stage. In the forward diffusion
stage, the input data is gradually perturbed over several steps by adding
Gaussian noise. In the reverse stage, a model is tasked at recovering the
original input data by learning to gradually reverse the diffusion process,
step by step. Diffusion models are widely appreciated for the quality and
diversity of the generated samples, despite their known computational burdens,
i.e. low speeds due to the high number of steps involved during sampling. In
this survey, we provide a comprehensive review of articles on denoising
diffusion models applied in vision, comprising both theoretical and practical
contributions in the field. First, we identify and present three generic
diffusion modeling frameworks, which are based on denoising diffusion
probabilistic models, noise conditioned score networks, and stochastic
differential equations. We further discuss the relations between diffusion
models and other deep generative models, including variational auto-encoders,
generative adversarial networks, energy-based models, autoregressive models and
normalizing flows. Then, we introduce a multi-perspective categorization of
diffusion models applied in computer vision. Finally, we illustrate the current
limitations of diffusion models and envision some interesting directions for
future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in IEEE Transactions on Pattern Analysis and Machine
  Intelligence. 25 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unlearnable Clusters: Towards Label-agnostic Unlearnable Examples <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.01217v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.01217v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaming Zhang, Xingjun Ma, Qi Yi, Jitao Sang, Yu-Gang Jiang, Yaowei Wang, Changsheng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is a growing interest in developing unlearnable examples (UEs) against
visual privacy leaks on the Internet. UEs are training samples added with
invisible but unlearnable noise, which have been found can prevent unauthorized
training of machine learning models. UEs typically are generated via a bilevel
optimization framework with a surrogate model to remove (minimize) errors from
the original samples, and then applied to protect the data against unknown
target models. However, existing UE generation methods all rely on an ideal
assumption called label-consistency, where the hackers and protectors are
assumed to hold the same label for a given sample. In this work, we propose and
promote a more practical label-agnostic setting, where the hackers may exploit
the protected data quite differently from the protectors. E.g., a m-class
unlearnable dataset held by the protector may be exploited by the hacker as a
n-class dataset. Existing UE generation methods are rendered ineffective in
this challenging setting. To tackle this challenge, we present a novel
technique called Unlearnable Clusters (UCs) to generate label-agnostic
unlearnable examples with cluster-wise perturbations. Furthermore, we propose
to leverage VisionandLanguage Pre-trained Models (VLPMs) like CLIP as the
surrogate model to improve the transferability of the crafted UCs to diverse
domains. We empirically verify the effectiveness of our proposed approach under
a variety of settings with different datasets, target models, and even
commercial platforms Microsoft Azure and Baidu PaddlePaddle. Code is available
at \url{https://github.com/jiamingzhang94/Unlearnable-Clusters}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Switchable Representation Learning Framework with Self-compatibility <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.08289v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.08289v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengsen Wu, Yan Bai, Yihang Lou, Xiongkun Linghu, Jianzhong He, Ling-Yu Duan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world visual search systems involve deployments on multiple platforms
with different computing and storage resources. Deploying a unified model that
suits the minimal-constrain platforms leads to limited accuracy. It is expected
to deploy models with different capacities adapting to the resource
constraints, which requires features extracted by these models to be aligned in
the metric space. The method to achieve feature alignments is called
``compatible learning''. Existing research mainly focuses on the one-to-one
compatible paradigm, which is limited in learning compatibility among multiple
models. We propose a Switchable representation learning Framework with
Self-Compatibility (SFSC). SFSC generates a series of compatible sub-models
with different capacities through one training process. The optimization of
sub-models faces gradients conflict, and we mitigate this problem from the
perspective of the magnitude and direction. We adjust the priorities of
sub-models dynamically through uncertainty estimation to co-optimize sub-models
properly. Besides, the gradients with conflicting directions are projected to
avoid mutual interference. SFSC achieves state-of-the-art performance on the
evaluated datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Global Optimality in Cooperative MARL with the Transformation
  And Distillation Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.11143v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.11143v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianing Ye, Chenghao Li, Jianhao Wang, Chongjie Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decentralized execution is one core demand in cooperative multi-agent
reinforcement learning (MARL). Recently, most popular MARL algorithms have
adopted decentralized policies to enable decentralized execution and use
gradient descent as their optimizer. However, there is hardly any theoretical
analysis of these algorithms taking the optimization method into consideration,
and we find that various popular MARL algorithms with decentralized policies
are suboptimal in toy tasks when gradient descent is chosen as their
optimization method. In this paper, we theoretically analyze two common classes
of algorithms with decentralized policies -- multi-agent policy gradient
methods and value-decomposition methods to prove their suboptimality when
gradient descent is used. In addition, we propose the Transformation And
Distillation (TAD) framework, which reformulates a multi-agent MDP as a special
single-agent MDP with a sequential structure and enables decentralized
execution by distilling the learned policy on the derived ``single-agent" MDP.
This approach uses a two-stage learning paradigm to address the optimization
problem in cooperative MARL, maintaining its performance guarantee.
Empirically, we implement TAD-PPO based on PPO, which can theoretically perform
optimal policy learning in the finite multi-agent MDPs and shows significant
outperformance on a large set of cooperative multi-agent tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CroSel: Cross Selection of Confident Pseudo Labels for Partial-Label
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10365v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10365v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiyu Tian, Hongxin Wei, Yiqun Wang, Lei Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Partial-label learning (PLL) is an important weakly supervised learning
problem, which allows each training example to have a candidate label set
instead of a single ground-truth label. Identification-based methods have been
widely explored to tackle label ambiguity issues in PLL, which regard the true
label as a latent variable to be identified. However, identifying the true
labels accurately and completely remains challenging, causing noise in pseudo
labels during model training. In this paper, we propose a new method called
CroSel, which leverages historical prediction information from models to
identify true labels for most training examples. First, we introduce a cross
selection strategy, which enables two deep models to select true labels of
partially labeled data for each other. Besides, we propose a novel consistent
regularization term called co-mix to avoid sample waste and tiny noise caused
by false selection. In this way, CroSel can pick out the true labels of most
examples with high precision. Extensive experiments demonstrate the superiority
of CroSel, which consistently outperforms previous state-of-the-art methods on
benchmark datasets. Additionally, our method achieves over 90\% accuracy and
quantity for selecting true labels on CIFAR-type datasets under various
settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Random Inverse Problems Over Graphs: Decentralized Online Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11789v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11789v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Li, Xiwei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We establish a framework of random inverse problems with real-time
observations over graphs, and present a decentralized online learning algorithm
based on online data streams, which unifies the distributed parameter
estimation in Hilbert space and the least mean square problem in reproducing
kernel Hilbert space (RKHS-LMS). We transform the algorithm convergence into
the asymptotic stability of randomly time-varying difference equations in
Hilbert space with L2-bounded martingale difference terms and develop the L2
-asymptotic stability theory. It is shown that if the network graph is
connected and the sequence of forward operators satisfies the
infinitedimensional spatio-temporal persistence of excitation condition, then
the estimates of all nodes are mean square and almost surely strongly
consistent. By equivalently transferring the distributed learning problem in
RKHS to the random inverse problem over graphs, we propose a decentralized
online learning algorithm in RKHS based on non-stationary and non-independent
online data streams, and prove that the algorithm is mean square and almost
surely strongly consistent if the operators induced by the random input data
satisfy the infinite-dimensional spatio-temporal persistence of excitation
condition.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spatially Selective Deep Non-linear Filters for Speaker Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.02420v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.02420v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kristina Tesch, Timo Gerkmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In a scenario with multiple persons talking simultaneously, the spatial
characteristics of the signals are the most distinct feature for extracting the
target signal. In this work, we develop a deep joint spatial-spectral
non-linear filter that can be steered in an arbitrary target direction. For
this we propose a simple and effective conditioning mechanism, which sets the
initial state of the filter's recurrent layers based on the target direction.
We show that this scheme is more effective than the baseline approach and
increases the flexibility of the filter at no performance cost. The resulting
spatially selective non-linear filters can also be used for speech separation
of an arbitrary number of speakers and enable very accurate multi-speaker
localization as we demonstrate in this paper.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>\copyright 2023 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hypothesis Testing for Unknown Dynamical Systems and System Anomaly
  Detection via Autoencoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.12358v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.12358v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haowei He, Jingzhao Zhang, Yanan Wang, Benben Jiang, Shaobo Huang, Chen Wang, Yang Zhang, Xuebing Han, Dongxu Guo, Guannan He, Minggao Ouyang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the hypothesis testing problem for unknown dynamical systems. More
specifically, we observe sequential input and output data from a dynamical
system with unknown parameters, and we aim to determine whether the collected
data is from a null distribution. Such a problem can have many applications.
Here we formulate anomaly detection as hypothesis testing where the anomaly is
defined through the alternative hypothesis. Consequently, hypothesis testing
algorithms can detect faults in real-world systems such as robots, weather,
energy systems, and stock markets. Although recent works achieved
state-of-the-art performances in these tasks with deep learning models, we show
that a careful analysis using hypothesis testing and graphical models can not
only justify the effectiveness of autoencoder models, but also lead to a novel
neural network design, termed DyAD (DYnamical system Anomaly Detection), with
improved performances. We then show that DyAD achieves state-of-the-art
performance on several existing datasets and a new dataset on battery anomaly
detection in electric vehicles.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lower Bound on the Bayesian Risk via Information Measures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12497v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12497v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amedeo Roberto Esposito, Adrien Vandenbroucque, Michael Gastpar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper focuses on parameter estimation and introduces a new method for
lower bounding the Bayesian risk. The method allows for the use of virtually
\emph{any} information measure, including R\'enyi's $\alpha$,
$\varphi$-Divergences, and Sibson's $\alpha$-Mutual Information. The approach
considers divergences as functionals of measures and exploits the duality
between spaces of measures and spaces of functions. In particular, we show that
one can lower bound the risk with any information measure by upper bounding its
dual via Markov's inequality. We are thus able to provide estimator-independent
impossibility results thanks to the Data-Processing Inequalities that
divergences satisfy. The results are then applied to settings of interest
involving both discrete and continuous parameters, including the
``Hide-and-Seek'' problem, and compared to the state-of-the-art techniques. An
important observation is that the behaviour of the lower bound in the number of
samples is influenced by the choice of the information measure. We leverage
this by introducing a new divergence inspired by the ``Hockey-Stick''
Divergence, which is demonstrated empirically to provide the largest
lower-bound across all considered settings. If the observations are subject to
privatisation, stronger impossibility results can be obtained via Strong
Data-Processing Inequalities. The paper also discusses some generalisations and
alternative directions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improved Regret Bounds for Online Kernel Selection under Bandit Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05018v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05018v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junfan Li, Shizhong Liao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we improve the regret bound for online kernel selection under
bandit feedback. Previous algorithm enjoys a $O((\Vert
f\Vert^2_{\mathcal{H}_i}+1)K^{\frac{1}{3}}T^{\frac{2}{3}})$ expected bound for
Lipschitz loss functions. We prove two types of regret bounds improving the
previous bound. For smooth loss functions, we propose an algorithm with a
$O(U^{\frac{2}{3}}K^{-\frac{1}{3}}(\sum^K_{i=1}L_T(f^\ast_i))^{\frac{2}{3}})$
expected bound where $L_T(f^\ast_i)$ is the cumulative losses of optimal
hypothesis in $\mathbb{H}_{i}=\{f\in\mathcal{H}_i:\Vert
f\Vert_{\mathcal{H}_i}\leq U\}$. The data-dependent bound keeps the previous
worst-case bound and is smaller if most of candidate kernels match well with
the data. For Lipschitz loss functions, we propose an algorithm with a
$O(U\sqrt{KT}\ln^{\frac{2}{3}}{T})$ expected bound asymptotically improving the
previous bound. We apply the two algorithms to online kernel selection with
time constraint and prove new regret bounds matching or improving the previous
$O(\sqrt{T\ln{K}} +\Vert
f\Vert^2_{\mathcal{H}_i}\max\{\sqrt{T},\frac{T}{\sqrt{\mathcal{R}}}\})$
expected bound where $\mathcal{R}$ is the time budget. Finally, we empirically
verify our algorithms on online regression and classification tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Policy Gradient Converges to the Globally Optimal Policy for Nearly
  Linear-Quadratic Regulators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08431v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08431v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinbin Han, Meisam Razaviyayn, Renyuan Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nonlinear control systems with partial information to the decision maker are
prevalent in a variety of applications. As a step toward studying such
nonlinear systems, this work explores reinforcement learning methods for
finding the optimal policy in the nearly linear-quadratic regulator systems. In
particular, we consider a dynamic system that combines linear and nonlinear
components, and is governed by a policy with the same structure. Assuming that
the nonlinear component comprises kernels with small Lipschitz coefficients, we
characterize the optimization landscape of the cost function. Although the cost
function is nonconvex in general, we establish the local strong convexity and
smoothness in the vicinity of the global optimizer. Additionally, we propose an
initialization mechanism to leverage these properties. Building on the
developments, we design a policy gradient algorithm that is guaranteed to
converge to the globally optimal policy with a linear rate.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distributed Random Reshuffling over Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.15287v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.15287v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kun Huang, Xiao Li, Andre Milzarek, Shi Pu, Junwen Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we consider distributed optimization problems where $n$
agents, each possessing a local cost function, collaboratively minimize the
average of the local cost functions over a connected network. To solve the
problem, we propose a distributed random reshuffling (D-RR) algorithm that
invokes the random reshuffling (RR) update in each agent. We show that D-RR
inherits favorable characteristics of RR for both smooth strongly convex and
smooth nonconvex objective functions. In particular, for smooth strongly convex
objective functions, D-RR achieves $\mathcal{O}(1/T^2)$ rate of convergence
(where $T$ counts epoch number) in terms of the squared distance between the
iterate and the global minimizer. When the objective function is assumed to be
smooth nonconvex, we show that D-RR drives the squared norm of gradient to $0$
at a rate of $\mathcal{O}(1/T^{2/3})$. These convergence results match those of
centralized RR (up to constant factors) and outperform the distributed
stochastic gradient descent (DSGD) algorithm if we run a relatively large
number of epochs. Finally, we conduct a set of numerical experiments to
illustrate the efficiency of the proposed D-RR method on both strongly convex
and nonconvex distributed optimization problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ POCS-based Clustering Algorithm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.08888v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.08888v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Le-Anh Tran, Henock M. Deberneh, Truong-Dong Do, Thanh-Dat Nguyen, My-Ha Le, Dong-Chul Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A novel clustering technique based on the projection onto convex set (POCS)
method, called POCS-based clustering algorithm, is proposed in this paper. The
proposed POCS-based clustering algorithm exploits a parallel projection method
of POCS to find appropriate cluster prototypes in the feature space. The
algorithm considers each data point as a convex set and projects the cluster
prototypes parallelly to the member data points. The projections are convexly
combined to minimize the objective function for data clustering purpose. The
performance of the proposed POCS-based clustering algorithm is verified through
experiments on various synthetic datasets. The experimental results show that
the proposed POCS-based clustering algorithm is competitive and efficient in
terms of clustering error and execution speed when compared with other
conventional clustering methods including Fuzzy C-Means (FCM) and K-means
clustering algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 4 figures, IWIS 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Delay-Aware Hierarchical Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12414v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12414v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Frank Po-Chen Lin, Seyyedali Hosseinalipour, Christopher Brinton, Nicolò Michelusi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning has gained popularity as a means of training models
distributed across the wireless edge. The paper introduces delay-aware
federated learning (DFL) to improve the efficiency of distributed machine
learning (ML) model training by addressing communication delays between edge
and cloud. DFL employs multiple stochastic gradient descent iterations on
device datasets during each global aggregation interval and intermittently
aggregates model parameters through edge servers in local subnetworks. The
cloud server synchronizes the local models with the global deployed model
computed via a local-global combiner at global synchronization. The convergence
behavior of DFL is theoretically investigated under a generalized data
heterogeneity metric. A set of conditions is obtained to achieve the sub-linear
convergence rate of O(1/k). Based on these findings, an adaptive control
algorithm is developed for DFL, implementing policies to mitigate energy
consumption and edge-to-cloud communication latency while aiming for a
sublinear convergence rate. Numerical evaluations show DFL's superior
performance in terms of faster global model convergence, reduced resource
consumption, and robustness against communication delays compared to existing
FL algorithms. In summary, this proposed method offers improved efficiency and
satisfactory results when dealing with both convex and non-convex loss
functions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A condensed version of this paper was presented at IEEE Globecom 2020</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Convex and Nonconvex Sublinear Regression with Application to
  Data-driven Learning of Reach Sets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.01919v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.01919v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shadi Haddad, Abhishek Halder
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider estimating a compact set from finite data by approximating the
support function of that set via sublinear regression. Support functions
uniquely characterize a compact set up to closure of convexification, and are
sublinear (convex as well as positive homogeneous of degree one). Conversely,
any sublinear function is the support function of a compact set. We leverage
this property to transcribe the task of learning a compact set to that of
learning its support function. We propose two algorithms to perform the
sublinear regression, one via convex and another via nonconvex programming. The
convex programming approach involves solving a quadratic program (QP). The
nonconvex programming approach involves training a input sublinear neural
network. We illustrate the proposed methods via numerical examples on learning
the reach sets of controlled dynamics subject to set-valued input uncertainties
from trajectory data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Containing a spread through sequential learning: to exploit or to
  explore? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.00141v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.00141v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingran Chen, Hesam Nikpey, Jungyeol Kim, Saswati Sarkar, Shirin Saeedi-Bidokhti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The spread of an undesirable contact process, such as an infectious disease
(e.g. COVID-19), is contained through testing and isolation of infected nodes.
The temporal and spatial evolution of the process (along with containment
through isolation) render such detection as fundamentally different from active
search detection strategies. In this work, through an active learning approach,
we design testing and isolation strategies to contain the spread and minimize
the cumulative infections under a given test budget. We prove that the
objective can be optimized, with performance guarantees, by greedily selecting
the nodes to test. We further design reward-based methodologies that
effectively minimize an upper bound on the cumulative infections and are
computationally more tractable in large networks. These policies, however, need
knowledge about the nodes' infection probabilities which are dynamically
changing and have to be learned by sequential testing. We develop a
message-passing framework for this purpose and, building on that, show novel
tradeoffs between exploitation of knowledge through reward-based heuristics and
exploration of the unknown through a carefully designed probabilistic testing.
The tradeoffs are fundamentally distinct from the classical counterparts under
active search or multi-armed bandit problems (MABs). We provably show the
necessity of exploration in a stylized network and show through simulations
that exploration can outperform exploitation in various synthetic and real-data
networks depending on the parameters of the network and the spread.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Realization of Causal Representation Learning to Adjust Confounding Bias
  in Latent Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.08573v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.08573v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jia Li, Xiang Li, Xiaowei Jia, Michael Steinbach, Vipin Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Causal DAGs(Directed Acyclic Graphs) are usually considered in a 2D plane.
Edges indicate causal effects' directions and imply their corresponding
time-passings. Due to the natural restriction of statistical models, effect
estimation is usually approximated by averaging the individuals' correlations,
i.e., observational changes over a specific time. However, in the context of
Machine Learning on large-scale questions with complex DAGs, such slight biases
can snowball to distort global models - More importantly, it has practically
impeded the development of AI, for instance, the weak generalizability of
causal models. In this paper, we redefine causal DAG as \emph{do-DAG}, in which
variables' values are no longer time-stamp-dependent, and timelines can be seen
as axes. By geometric explanation of multi-dimensional do-DAG, we identify the
\emph{Causal Representation Bias} and its necessary factors, differentiated
from common confounding biases. Accordingly, a DL(Deep Learning)-based
framework will be proposed as the general solution, along with a realization
method and experiments to verify its feasibility.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Importance and Applicability of <span class="highlight-title">Pre-Train</span>ing for Federated
  Learning <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.11488v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.11488v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hong-You Chen, Cheng-Hao Tu, Ziwei Li, Han-Wei Shen, Wei-Lun Chao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-training is prevalent in nowadays deep learning to improve the learned
model's performance. However, in the literature on federated learning (FL),
neural networks are mostly initialized with random weights. These attract our
interest in conducting a systematic study to explore pre-training for FL.
Across multiple visual recognition benchmarks, we found that pre-training can
not only improve FL, but also close its accuracy gap to the counterpart
centralized learning, especially in the challenging cases of non-IID clients'
data. To make our findings applicable to situations where pre-trained models
are not directly available, we explore pre-training with synthetic data or even
with clients' data in a decentralized manner, and found that they can already
improve FL notably. Interestingly, many of the techniques we explore are
complementary to each other to further boost the performance, and we view this
as a critical result toward scaling up deep FL for real-world applications. We
conclude our paper with an attempt to understand the effect of pre-training on
FL. We found that pre-training enables the learned global models under
different clients' data conditions to converge to the same loss basin, and
makes global aggregation in FL more stable. Nevertheless, pre-training seems to
not alleviate local model drifting, a fundamental problem in FL under non-IID
data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HAC-Net: A Hybrid Attention-Based Convolutional Neural Network for
  Highly Accurate Protein-Ligand Binding Affinity Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.12440v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.12440v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gregory W. Kyro, Rafael I. Brent, Victor S. Batista
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Applying deep learning concepts from image detection and graph theory has
greatly advanced protein-ligand binding affinity prediction, a challenge with
enormous ramifications for both drug discovery and protein engineering. We
build upon these advances by designing a novel deep learning architecture
consisting of a 3-dimensional convolutional neural network utilizing
channel-wise attention and two graph convolutional networks utilizing
attention-based aggregation of node features. HAC-Net (Hybrid Attention-Based
Convolutional Neural Network) obtains state-of-the-art results on the PDBbind
v.2016 core set, the most widely recognized benchmark in the field. We
extensively assess the generalizability of our model using multiple train-test
splits, each of which maximizes differences between either protein structures,
protein sequences, or ligand extended-connectivity fingerprints of complexes in
the training and test sets. Furthermore, we perform 10-fold cross-validation
with a similarity cutoff between SMILES strings of ligands in the training and
test sets, and also evaluate the performance of HAC-Net on lower-quality data.
We envision that this model can be extended to a broad range of supervised
learning problems related to structure-based biomolecular property prediction.
All of our software is available as open source at
https://github.com/gregory-kyro/HAC-Net/, and the HACNet Python package is
available through PyPI.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Convergence of No-Regret Learning Dynamics in Time-Varying Games 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.11241v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.11241v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ioannis Anagnostides, Ioannis Panageas, Gabriele Farina, Tuomas Sandholm
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most of the literature on learning in games has focused on the restrictive
setting where the underlying repeated game does not change over time. Much less
is known about the convergence of no-regret learning algorithms in dynamic
multiagent settings. In this paper, we characterize the convergence of
optimistic gradient descent (OGD) in time-varying games. Our framework yields
sharp convergence bounds for the equilibrium gap of OGD in zero-sum games
parameterized on natural variation measures of the sequence of games, subsuming
known results for static games. Furthermore, we establish improved second-order
variation bounds under strong convexity-concavity, as long as each game is
repeated multiple times. Our results also apply to time-varying general-sum
multi-player games via a bilinear formulation of correlated equilibria, which
has novel implications for meta-learning and for obtaining refined
variation-dependent regret bounds, addressing questions left open in prior
papers. Finally, we leverage our framework to also provide new insights on
dynamic regret guarantees in static games.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>V2 clarifies connections with prior work</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">7</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Egocentric Audio-Visual Object Localization <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13471v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13471v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Huang, Yapeng Tian, Anurag Kumar, Chenliang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans naturally perceive surrounding scenes by unifying sound and sight in a
first-person view. Likewise, machines are advanced to approach human
intelligence by learning with multisensory inputs from an egocentric
perspective. In this paper, we explore the challenging egocentric audio-visual
object localization task and observe that 1) egomotion commonly exists in
first-person recordings, even within a short duration; 2) The out-of-view sound
components can be created while wearers shift their attention. To address the
first problem, we propose a geometry-aware temporal aggregation module to
handle the egomotion explicitly. The effect of egomotion is mitigated by
estimating the temporal geometry transformation and exploiting it to update
visual representations. Moreover, we propose a cascaded feature enhancement
module to tackle the second issue. It improves cross-modal localization
robustness by disentangling visually-indicated audio representation. During
training, we take advantage of the naturally available audio-visual temporal
synchronization as the ``free'' self-supervision to avoid costly labeling. We
also annotate and create the Epic Sounding Object dataset for evaluation
purposes. Extensive experiments show that our method achieves state-of-the-art
localization performance in egocentric videos and can be generalized to diverse
audio-visual scenes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DDT: A Diffusion-Driven <span class="highlight-title">Transformer</span>-based Framework for Human Mesh
  Recovery from a Video 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13397v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13397v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ce Zheng, Guo-Jun Qi, Chen Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human mesh recovery (HMR) provides rich human body information for various
real-world applications such as gaming, human-computer interaction, and virtual
reality. Compared to single image-based methods, video-based methods can
utilize temporal information to further improve performance by incorporating
human body motion priors. However, many-to-many approaches such as VIBE suffer
from motion smoothness and temporal inconsistency. While many-to-one approaches
such as TCMR and MPS-Net rely on the future frames, which is non-causal and
time inefficient during inference. To address these challenges, a novel
Diffusion-Driven Transformer-based framework (DDT) for video-based HMR is
presented. DDT is designed to decode specific motion patterns from the input
sequence, enhancing motion smoothness and temporal consistency. As a
many-to-many approach, the decoder of our DDT outputs the human mesh of all the
frames, making DDT more viable for real-world applications where time
efficiency is crucial and a causal model is desired. Extensive experiments are
conducted on the widely used datasets (Human3.6M, MPI-INF-3DHP, and 3DPW),
which demonstrated the effectiveness and efficiency of our DDT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Plug-and-Play Regulators for Image-Text Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13371v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13371v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haiwen Diao, Ying Zhang, Wei Liu, Xiang Ruan, Huchuan Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Exploiting fine-grained correspondence and visual-semantic alignments has
shown great potential in image-text matching. Generally, recent approaches
first employ a cross-modal attention unit to capture latent region-word
interactions, and then integrate all the alignments to obtain the final
similarity. However, most of them adopt one-time forward association or
aggregation strategies with complex architectures or additional information,
while ignoring the regulation ability of network feedback. In this paper, we
develop two simple but quite effective regulators which efficiently encode the
message output to automatically contextualize and aggregate cross-modal
representations. Specifically, we propose (i) a Recurrent Correspondence
Regulator (RCR) which facilitates the cross-modal attention unit progressively
with adaptive attention factors to capture more flexible correspondence, and
(ii) a Recurrent Aggregation Regulator (RAR) which adjusts the aggregation
weights repeatedly to increasingly emphasize important alignments and dilute
unimportant ones. Besides, it is interesting that RCR and RAR are
plug-and-play: both of them can be incorporated into many frameworks based on
cross-modal interaction to obtain significant benefits, and their cooperation
achieves further improvements. Extensive experiments on MSCOCO and Flickr30K
datasets validate that they can bring an impressive and consistent R@1 gain on
multiple models, confirming the general effectiveness and generalization
ability of the proposed methods. Code and pre-trained models are available at:
https://github.com/Paranioar/RCAR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 9 figures, Accepted by TIP2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ POTTER: Pooling Attention <span class="highlight-title">Transformer</span> for Efficient Human Mesh Recovery <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13357v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13357v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ce Zheng, Xianpeng Liu, Guo-Jun Qi, Chen Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer architectures have achieved SOTA performance on the human mesh
recovery (HMR) from monocular images. However, the performance gain has come at
the cost of substantial memory and computational overhead. A lightweight and
efficient model to reconstruct accurate human mesh is needed for real-world
applications. In this paper, we propose a pure transformer architecture named
POoling aTtention TransformER (POTTER) for the HMR task from single images.
Observing that the conventional attention module is memory and computationally
expensive, we propose an efficient pooling attention module, which
significantly reduces the memory and computational cost without sacrificing
performance. Furthermore, we design a new transformer architecture by
integrating a High-Resolution (HR) stream for the HMR task. The high-resolution
local and global features from the HR stream can be utilized for recovering
more accurate human mesh. Our POTTER outperforms the SOTA method METRO by only
requiring 7% of total parameters and 14% of the Multiply-Accumulate Operations
on the Human3.6M (PA-MPJPE metric) and 3DPW (all three metrics) datasets. The
project webpage is https://zczcwh.github.io/potter_page.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Audio Diffusion Model for Speech Synthesis: A <span class="highlight-title">Survey</span> on Text To Speech
  and Speech Enhancement in Generative AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13336v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13336v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenshuang Zhang, Chaoning Zhang, Sheng Zheng, Mengchun Zhang, Maryam Qamar, Sung-Ho Bae, In So Kweon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative AI has demonstrated impressive performance in various fields,
among which speech synthesis is an interesting direction. With the diffusion
model as the most popular generative model, numerous works have attempted two
active tasks: text to speech and speech enhancement. This work conducts a
survey on audio diffusion model, which is complementary to existing surveys
that either lack the recent progress of diffusion-based speech synthesis or
highlight an overall picture of applying diffusion model in multiple fields.
Specifically, this work first briefly introduces the background of audio and
diffusion model. As for the text-to-speech task, we divide the methods into
three categories based on the stage where diffusion model is adopted: acoustic
model, vocoder and end-to-end framework. Moreover, we categorize various speech
enhancement tasks by either certain signals are removed or added into the input
speech. Comparisons of experimental results and discussions are also covered in
this survey.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Implementation of communication media around a mixed reality experience
  with HoloLens headset, as part of a digitalization of a nutrition workshop 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13079v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13079v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Owen Kevin Appadoo, Hugo Rositi, Sylvie Valarier, Marie-Claire Ombret, Émilie Gadéa, Christine Barret-Grimault, Christophe Lohou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The release of Microsoft's HoloLens headset addresses new types of issues
that would have been difficult to design without such a hardware. This
semi-transparent visor headset allows the user who wears it to view the
projection of 3D virtual objects placed in its real environment. The user can
also interact with these 3D objects, which can interact with each other. The
framework of this new technology is called mixed reality. We had the
opportunity to numerically transform a conventional human nutrition workshop
for patients waiting for bariatric surgery by developing a software called
HOLO_NUTRI using the HoloLens headset. Unlike our experience of user and
conventional programmer specialized in the development of interactive 3D
graphics applications, we realized that such a mixed reality experience
required specific programming concepts quite different from those of
conventional software or those of virtual reality applications, but above all
required a thorough reflection about communication for users. In this article,
we propose to explain our design of communication (graphic supports, tutorials
of use of material, explanatory videos), a step which was crucial for the good
progress of our project. The software was used by thirty patients from Le
Puy-en-Velay Hospital during 10 sessions of one hour and a half during which
patients had to take in hand the headset and software HOLO_NUTRI. We also
proposed a series of questions to patients to have an assessment of both the
adequacy and the importance of this communication approach for such experience.
As the mixed reality technology is very recent but the number of applications
based on it significantly increases, the reflection on the implementation of
the elements of communication described in this article (videos, exercise of
learning for the use of the headset, communication leaflet, etc.) can help
developers of such applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning a Practical SDR-to-HDRTV Up-conversion using New <span class="highlight-title">Dataset</span> and
  Degradation Models <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13031v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13031v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng Guo, Leidong Fan, Ziyu Xue, and Xiuhua Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In media industry, the demand of SDR-to-HDRTV up-conversion arises when users
possess HDR-WCG (high dynamic range-wide color gamut) TVs while most
off-the-shelf footage is still in SDR (standard dynamic range). The research
community has started tackling this low-level vision task by learning-based
approaches. When applied to real SDR, yet, current methods tend to produce dim
and desaturated result, making nearly no improvement on viewing experience.
Different from other network-oriented methods, we attribute such deficiency to
training set (HDR-SDR pair). Consequently, we propose new HDRTV dataset (dubbed
HDRTV4K) and new HDR-to-SDR degradation models. Then, it's used to train a
luminance-segmented network (LSN) consisting of a global mapping trunk, and two
Transformer branches on bright and dark luminance range. We also update
assessment criteria by tailored metrics and subjective experiment. Finally,
ablation studies are conducted to prove the effectiveness. Our work is
available at: https://github.com/AndreGuo/HDRTVDM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2023</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-03-22T00:00:00Z">2023-03-22</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">42</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Open-source Frame Semantic Parsing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12788v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12788v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Chanin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While the state-of-the-art for frame semantic parsing has progressed
dramatically in recent years, it is still difficult for end-users to apply
state-of-the-art models in practice. To address this, we present Frame Semantic
Transformer, an open-source Python library which achieves near state-of-the-art
performance on FrameNet 1.7, while focusing on ease-of-use. We use a T5 model
fine-tuned on Propbank and FrameNet exemplars as a base, and improve
performance by using FrameNet lexical units to provide hints to T5 at inference
time. We enhance robustness to real-world data by using textual data
augmentations during training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpretable Bangla Sarcasm Detection using <span class="highlight-title">BERT</span> and Explainable AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12772v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12772v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ramisa Anan, Tasnim Sakib Apon, Zeba Tahsin Hossain, Elizabeth Antora Modhu, Sudipta Mondal, MD. Golam Rabiul Alam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A positive phrase or a sentence with an underlying negative motive is usually
defined as sarcasm that is widely used in today's social media platforms such
as Facebook, Twitter, Reddit, etc. In recent times active users in social media
platforms are increasing dramatically which raises the need for an automated
NLP-based system that can be utilized in various tasks such as determining
market demand, sentiment analysis, threat detection, etc. However, since
sarcasm usually implies the opposite meaning and its detection is frequently a
challenging issue, data meaning extraction through an NLP-based model becomes
more complicated. As a result, there has been a lot of study on sarcasm
detection in English over the past several years, and there's been a noticeable
improvement and yet sarcasm detection in the Bangla language's state remains
the same. In this article, we present a BERT-based system that can achieve
99.60\% while the utilized traditional machine learning algorithms are only
capable of achieving 89.93\%. Additionally, we have employed Local
Interpretable Model-Agnostic Explanations that introduce explainability to our
system. Moreover, we have utilized a newly collected bangla sarcasm dataset,
BanglaSarc that was constructed specifically for the evaluation of this study.
This dataset consists of fresh records of sarcastic and non-sarcastic comments,
the majority of which are acquired from Facebook and YouTube comment sections.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can we trust the evaluation on Chat<span class="highlight-title">GPT</span>? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12767v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12767v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rachith Aiyappa, Jisun An, Haewoon Kwak, Yong-Yeol Ahn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ChatGPT, the first large language model (LLM) with mass adoption, has
demonstrated remarkable performance in numerous natural language tasks. Despite
its evident usefulness, evaluating ChatGPT's performance in diverse problem
domains remains challenging due to the closed nature of the model and its
continuous updates via Reinforcement Learning from Human Feedback (RLHF). We
highlight the issue of data contamination in ChatGPT evaluations, with a case
study of the task of stance detection. We discuss the challenge of preventing
data contamination and ensuring fair model evaluation in the age of closed and
continuously trained models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sparks of Artificial General Intelligence: Early experiments with <span class="highlight-title">GPT</span>-4 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12712v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12712v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, Yi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence (AI) researchers have been developing and refining
large language models (LLMs) that exhibit remarkable capabilities across a
variety of domains and tasks, challenging our understanding of learning and
cognition. The latest model developed by OpenAI, GPT-4, was trained using an
unprecedented scale of compute and data. In this paper, we report on our
investigation of an early version of GPT-4, when it was still in active
development by OpenAI. We contend that (this early version of) GPT-4 is part of
a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that
exhibit more general intelligence than previous AI models. We discuss the
rising capabilities and implications of these models. We demonstrate that,
beyond its mastery of language, GPT-4 can solve novel and difficult tasks that
span mathematics, coding, vision, medicine, law, psychology and more, without
needing any special prompting. Moreover, in all of these tasks, GPT-4's
performance is strikingly close to human-level performance, and often vastly
surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's
capabilities, we believe that it could reasonably be viewed as an early (yet
still incomplete) version of an artificial general intelligence (AGI) system.
In our exploration of GPT-4, we put special emphasis on discovering its
limitations, and we discuss the challenges ahead for advancing towards deeper
and more comprehensive versions of AGI, including the possible need for
pursuing a new paradigm that moves beyond next-word prediction. We conclude
with reflections on societal influences of the recent technological leap and
future research directions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Integrating Image Features with Convolutional Sequence-to-sequence
  Network for Multilingual Visual Question Answering <span class="chip">SP2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12671v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12671v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Triet Minh Thai, Son T. Luu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Question Answering (VQA) is a task that requires computers to give
correct answers for the input questions based on the images. This task can be
solved by humans with ease but is a challenge for computers. The
VLSP2022-EVJVQA shared task carries the Visual Question Answering task in the
multilingual domain on a newly released dataset: UIT-EVJVQA, in which the
questions and answers are written in three different languages: English,
Vietnamese and Japanese. We approached the challenge as a sequence-to-sequence
learning task, in which we integrated hints from pre-trained state-of-the-art
VQA models and image features with Convolutional Sequence-to-Sequence network
to generate the desired answers. Our results obtained up to 0.3442 by F1 score
on the public test set, 0.4210 on the private test set, and placed 3rd in the
competition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>VLSP2022-EVJVQA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating the Role of Target Arguments in Rumour Stance Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12665v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12665v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Li, Carolina Scarton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Considering a conversation thread, stance classification aims to identify the
opinion (e.g. agree or disagree) of replies towards a given target. The target
of the stance is expected to be an essential component in this task, being one
of the main factors that make it different from sentiment analysis. However, a
recent study shows that a target-oblivious model outperforms target-aware
models, suggesting that targets are not useful when predicting stance. This
paper re-examines this phenomenon for rumour stance classification (RSC) on
social media, where a target is a rumour story implied by the source tweet in
the conversation. We propose adversarial attacks in the test data, aiming to
assess the models robustness and evaluate the role of the data in the models
performance. Results show that state-of-the-art models, including approaches
that use the entire conversation thread, overly relying on superficial signals.
Our hypothesis is that the naturally high occurrence of target-independent
direct replies in RSC (e.g. "this is fake" or just "fake") results in the
impressive performance of target-oblivious models, highlighting the risk of
target instances being treated as noise during training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AfroDigits: A Community-Driven Spoken Digit <span class="highlight-title">Dataset</span> for African
  Languages <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12582v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12582v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chris Chinenye Emezue, Sanchit Gandhi, Lewis Tunstall, Abubakar Abid, Joshua Meyer, Quentin Lhoest, Pete Allen, Patrick Von Platen, Douwe Kiela, Yacine Jernite, Julien Chaumond, Merve Noyan, Omar Sanseviero
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advancement of speech technologies has been remarkable, yet its
integration with African languages remains limited due to the scarcity of
African speech corpora. To address this issue, we present AfroDigits, a
minimalist, community-driven dataset of spoken digits for African languages,
currently covering 38 African languages. As a demonstration of the practical
applications of AfroDigits, we conduct audio digit classification experiments
on six African languages [Igbo (ibo), Yoruba (yor), Rundi (run), Oshiwambo
(kua), Shona (sna), and Oromo (gax)] using the Wav2Vec2.0-Large and XLS-R
models. Our experiments reveal a useful insight on the effect of mixing African
speech corpora during finetuning. AfroDigits is the first published audio digit
dataset for African languages and we believe it will, among other things, pave
the way for Afro-centric speech applications such as the recognition of
telephone numbers, and street numbers. We release the dataset and platform
publicly at https://huggingface.co/datasets/chrisjay/crowd-speech-africa and
https://huggingface.co/spaces/chrisjay/afro-speech respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the AfricaNLP Workshop at ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RepoCoder: Repository-Level Code Completion Through Iterative Retrieval
  and Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12570v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12570v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fengji Zhang, Bei Chen, Yue Zhang, Jin Liu, Daoguang Zan, Yi Mao, Jian-Guang Lou, Weizhu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of repository-level code completion is to continue writing the
unfinished code based on a broader context of the repository. While for
automated code completion tools, it is difficult to utilize the useful
information scattered in different files. We propose RepoCoder, a simple,
generic, and effective framework to address the challenge. It streamlines the
repository-level code completion process by incorporating a similarity-based
retriever and a pre-trained code language model, which allows for the effective
utilization of repository-level information for code completion and grants the
ability to generate code at various levels of granularity. Furthermore,
RepoCoder utilizes a novel iterative retrieval-generation paradigm that bridges
the gap between retrieval context and the intended completion target. We also
propose a new benchmark RepoEval, which consists of the latest and high-quality
real-world repositories covering line, API invocation, and function body
completion scenarios. We test the performance of RepoCoder by using various
combinations of code retrievers and generators. Experimental results indicate
that RepoCoder significantly improves the zero-shot code completion baseline by
over 10% in all settings and consistently outperforms the vanilla
retrieval-augmented code completion approach. Furthermore, we validate the
effectiveness of RepoCoder through comprehensive analysis, providing valuable
insights for future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MEGA: Multilingual Evaluation of Generative AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12528v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12528v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kabir Ahuja, Rishav Hada, Millicent Ochieng, Prachi Jain, Harshita Diddee, Samuel Maina, Tanuja Ganu, Sameer Segal, Maxamed Axmed, Kalika Bali, Sunayana Sitaram
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative AI models have impressive performance on many Natural Language
Processing tasks such as language understanding, reasoning and language
generation. One of the most important questions that is being asked by the AI
community today is about the capabilities and limits of these models, and it is
clear that evaluating generative AI is very challenging. Most studies on
generative Large Language Models (LLMs) are restricted to English and it is
unclear how capable these models are at understanding and generating other
languages. We present the first comprehensive benchmarking of generative LLMs -
MEGA, which evaluates models on standard NLP benchmarks, covering 8 diverse
tasks and 33 typologically diverse languages. We also compare the performance
of generative LLMs to State of the Art (SOTA) non-autoregressive models on
these tasks to determine how well generative models perform compared to the
previous generation of LLMs. We present a thorough analysis of the performance
of models across languages and discuss some of the reasons why generative LLMs
are currently not optimal for all languages. We create a framework for
evaluating generative LLMs in the multilingual setting and provide directions
for future progress in the field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GrapeQA: GRaph Augmentation and Pruning to Enhance Question-Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12320v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12320v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dhaval Taunk, Lakshya Khanna, Pavan Kandru, Vasudeva Varma, Charu Sharma, Makarand Tapaswi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Commonsense question-answering (QA) methods combine the power of pre-trained
Language Models (LM) with the reasoning provided by Knowledge Graphs (KG). A
typical approach collects nodes relevant to the QA pair from a KG to form a
Working Graph (WG) followed by reasoning using Graph Neural Networks(GNNs).
This faces two major challenges: (i) it is difficult to capture all the
information from the QA in the WG, and (ii) the WG contains some irrelevant
nodes from the KG. To address these, we propose GrapeQA with two simple
improvements on the WG: (i) Prominent Entities for Graph Augmentation
identifies relevant text chunks from the QA pair and augments the WG with
corresponding latent representations from the LM, and (ii) Context-Aware Node
Pruning removes nodes that are less relevant to the QA pair. We evaluate our
results on OpenBookQA, CommonsenseQA and MedQA-USMLE and see that GrapeQA shows
consistent improvements over its LM + KG predecessor (QA-GNN in particular) and
large improvements on OpenBookQA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Self-supervised</span> Meta-<span class="highlight-title">Prompt</span> Learning with Meta-Gradient Regularization
  for Few-shot Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12314v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12314v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaihang Pan, Juncheng Li, Hongye Song, Jun Lin, Xiaozhong Liu, Siliang Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt tuning is a parameter-efficient method, which learns soft prompts and
conditions frozen language models to perform specific downstream tasks. Though
effective, prompt tuning under few-shot settings on the one hand heavily relies
on a good initialization of soft prompts. On the other hand, it can easily
result in overfitting. Existing works leverage pre-training or supervised
meta-learning to initialize soft prompts but they cannot data-efficiently
generalize to unseen downstream tasks. To address the above problems, this
paper proposes a novel Self-sUpervised meta-Prompt learning framework with
meta-gradient Regularization for few-shot generalization (SUPMER). We first
design a set of self-supervised anchor meta-training tasks with different task
formats and further enrich the task distribution with curriculum-based task
augmentation. Then a novel meta-gradient regularization method is integrated
into meta-prompt learning. It meta-learns to transform the raw gradients during
few-shot learning into a domain-generalizable direction, thus alleviating the
problem of overfitting. Extensive experiments show that SUPMER achieves better
performance for different few-shot downstream tasks, and also exhibits a
stronger domain generalization ability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ XWikiGen: Cross-lingual Summarization for Encyclopedic Text Generation
  in Low Resource Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12308v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12308v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dhaval Taunk, Shivprasad Sagare, Anupam Patil, Shivansh Subramanian, Manish Gupta, Vasudeva Varma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lack of encyclopedic text contributors, especially on Wikipedia, makes
automated text generation for \emph{low resource (LR) languages} a critical
problem. Existing work on Wikipedia text generation has focused on
\emph{English only} where English reference articles are summarized to generate
English Wikipedia pages. But, for low-resource languages, the scarcity of
reference articles makes monolingual summarization ineffective in solving this
problem. Hence, in this work, we propose \task{}, which is the task of
cross-lingual multi-document summarization of text from multiple reference
articles, written in various languages, to generate Wikipedia-style text.
Accordingly, we contribute a benchmark dataset, \data{}, spanning $\sim$69K
Wikipedia articles covering five domains and eight languages. We harness this
dataset to train a two-stage system where the input is a set of citations and a
section title and the output is a section-specific LR summary. The proposed
system is based on a novel idea of neural unsupervised extractive summarization
to coarsely identify salient information followed by a neural abstractive model
to generate the section-specific text. Extensive experiments show that
multi-domain training is better than the multi-lingual setup on average.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Turkish Speech Recognition via Hybrid CTC/Attention
  Architecture and Multi-feature Fusion Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12300v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12300v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyu Ren, Nurmement Yolwas, Huiru Wang, Wushour Slamu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, End-to-End speech recognition technology based on deep
learning has developed rapidly. Due to the lack of Turkish speech data, the
performance of Turkish speech recognition system is poor. Firstly, this paper
studies a series of speech recognition tuning technologies. The results show
that the performance of the model is the best when the data enhancement
technology combining speed perturbation with noise addition is adopted and the
beam search width is set to 16. Secondly, to maximize the use of effective
feature information and improve the accuracy of feature extraction, this paper
proposes a new feature extractor LSPC. LSPC and LiGRU network are combined to
form a shared encoder structure, and model compression is realized. The results
show that the performance of LSPC is better than MSPC and VGGnet when only
using Fbank features, and the WER is improved by 1.01% and 2.53% respectively.
Finally, based on the above two points, a new multi-feature fusion network is
proposed as the main structure of the encoder. The results show that the WER of
the proposed feature fusion network based on LSPC is improved by 0.82% and
1.94% again compared with the single feature (Fbank feature and Spectrogram
feature) extraction using LSPC. Our model achieves performance comparable to
that of advanced End-to-End models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating <span class="highlight-title">Transformer</span> Models and Human Behaviors on Chinese Character
  Naming <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12294v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12294v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaomeng Ma, Lingyu Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural network models have been proposed to explain the grapheme-phoneme
mapping process in humans for many alphabet languages. These models not only
successfully learned the correspondence of the letter strings and their
pronunciation, but also captured human behavior in nonce word naming tasks. How
would the neural models perform for a non-alphabet language (e.g., Chinese)
unknown character task? How well would the model capture human behavior? In
this study, we evaluate a set of transformer models and compare their
performances with human behaviors on an unknown Chinese character naming task.
We found that the models and humans behaved very similarly, that they had
similar accuracy distribution for each character, and had a substantial overlap
in answers. In addition, the models' answers are highly correlated with humans'
answers. These results suggested that the transformer models can well capture
human's character naming behavior.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by TACL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Are LLMs the Master of All Trades? : Exploring Domain-Agnostic Reasoning
  Skills of LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12810v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12810v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shrivats Agrawal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The potential of large language models (LLMs) to reason like humans has been
a highly contested topic in Machine Learning communities. However, the
reasoning abilities of humans are multifaceted and can be seen in various
forms, including analogical, spatial and moral reasoning, among others. This
fact raises the question whether LLMs can perform equally well across all these
different domains. This research work aims to investigate the performance of
LLMs on different reasoning tasks by conducting experiments that directly use
or draw inspirations from existing datasets on analogical and spatial
reasoning. Additionally, to evaluate the ability of LLMs to reason like human,
their performance is evaluted on more open-ended, natural language questions.
My findings indicate that LLMs excel at analogical and moral reasoning, yet
struggle to perform as proficiently on spatial reasoning tasks. I believe these
experiments are crucial for informing the future development of LLMs,
particularly in contexts that require diverse reasoning proficiencies. By
shedding light on the reasoning abilities of LLMs, this study aims to push
forward our understanding of how they can better emulate the cognitive
abilities of humans.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analyzing the Generalizability of Deep Contextualized Language
  Representations For Text Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12936v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12936v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Berfu Buyukoz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study evaluates the robustness of two state-of-the-art deep contextual
language representations, ELMo and DistilBERT, on supervised learning of binary
protest news classification and sentiment analysis of product reviews. A
"cross-context" setting is enabled using test sets that are distinct from the
training data. Specifically, in the news classification task, the models are
developed on local news from India and tested on the local news from China. In
the sentiment analysis task, the models are trained on movie reviews and tested
on customer reviews. This comparison is aimed at exploring the limits of the
representative power of today's Natural Language Processing systems on the path
to the systems that are generalizable to real-life scenarios. The models are
fine-tuned and fed into a Feed-Forward Neural Network and a Bidirectional Long
Short Term Memory network. Multinomial Naive Bayes and Linear Support Vector
Machine are used as traditional baselines. The results show that, in binary
text classification, DistilBERT is significantly better than ELMo on
generalizing to the cross-context setting. ELMo is observed to be significantly
more robust to the cross-context test data than both baselines. On the other
hand, the baselines performed comparably well to ELMo when the training and
test data are subsets of the same corpus (no cross-context). DistilBERT is also
found to be 30% smaller and 83% faster than ELMo. The results suggest that
DistilBERT can transfer generic semantic knowledge to other domains better than
ELMo. DistilBERT is also favorable in incorporating into real-life systems for
it requires a smaller computational training budget. When generalization is not
the utmost preference and test domain is similar to the training domain, the
traditional ML algorithms can still be considered as more economic alternatives
to deep language representations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Understanding the Generalization of Medical Text-to-SQL Models
  and <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12898v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12898v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Richard Tarbell, Kim-Kwang Raymond Choo, Glenn Dietrich, Anthony Rios
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electronic medical records (EMRs) are stored in relational databases. It can
be challenging to access the required information if the user is unfamiliar
with the database schema or general database fundamentals. Hence, researchers
have explored text-to-SQL generation methods that provide healthcare
professionals direct access to EMR data without needing a database expert.
However, currently available datasets have been essentially "solved" with
state-of-the-art models achieving accuracy greater than or near 90%. In this
paper, we show that there is still a long way to go before solving text-to-SQL
generation in the medical domain. To show this, we create new splits of the
existing medical text-to-SQL dataset MIMICSQL that better measure the
generalizability of the resulting models. We evaluate state-of-the-art language
models on our new split showing substantial drops in performance with accuracy
dropping from up to 92% to 28%, thus showing substantial room for improvement.
Moreover, we introduce a novel data augmentation approach to improve the
generalizability of the language models. Overall, this paper is the first step
towards developing more robust text-to-SQL models in the medical
domain.\footnote{The dataset and code will be released upon acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Small-Scale Switch <span class="highlight-title">Transformer</span> and NLP-based Model for Clinical
  Narratives Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12892v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12892v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thanh-Dung Le, Philippe Jouvet, Rita Noumeir
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, Transformer-based models such as the Switch Transformer have
achieved remarkable results in natural language processing tasks. However,
these models are often too complex and require extensive pre-training, which
limits their effectiveness for small clinical text classification tasks with
limited data. In this study, we propose a simplified Switch Transformer
framework and train it from scratch on a small French clinical text
classification dataset at CHU Sainte-Justine hospital. Our results demonstrate
that the simplified small-scale Transformer models outperform pre-trained
BERT-based models, including DistillBERT, CamemBERT, FlauBERT, and FrALBERT.
Additionally, using a mixture of expert mechanisms from the Switch Transformer
helps capture diverse patterns; hence, the proposed approach achieves better
results than a conventional Transformer with the self-attention mechanism.
Finally, our proposed framework achieves an accuracy of 87\%, precision at
87\%, and recall at 85\%, compared to the third-best pre-trained BERT-based
model, FlauBERT, which achieved an accuracy of 84\%, precision at 84\%, and
recall at 84\%. However, Switch Transformers have limitations, including a
generalization gap and sharp minima. We compare it with a multi-layer
perceptron neural network for small French clinical narratives classification
and show that the latter outperforms all other models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE Journal of Biomedical and Health Informatics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ JaCoText: A <span class="highlight-title">Pretrain</span>ed Model for Java Code-Text Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12869v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12869v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jessica López Espejel, Mahaman Sanoussi Yahaya Alassan, Walid Dahhane, El Hassane Ettifouri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pretrained transformer-based models have shown high performance in natural
language generation task. However, a new wave of interest has surged: automatic
programming language generation. This task consists of translating natural
language instructions to a programming code. Despite the fact that well-known
pretrained models on language generation have achieved good performance in
learning programming languages, effort is still needed in automatic code
generation. In this paper, we introduce JaCoText, a model based on Transformers
neural network. It aims to generate java source code from natural language
text. JaCoText leverages advantages of both natural language and code
generation models. More specifically, we study some findings from the state of
the art and use them to (1) initialize our model from powerful pretrained
models, (2) explore additional pretraining on our java dataset, (3) carry out
experiments combining the unimodal and bimodal data in the training, and (4)
scale the input and output length during the fine-tuning of the model.
Conducted experiments on CONCODE dataset show that JaCoText achieves new
state-of-the-art results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>International Conference on Code Generation and Implementation
  Volume: 17</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Salient Span Masking for Temporal Understanding <span class="chip">EACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12860v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12860v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeremy R. Cole, Aditi Chaudhary, Bhuwan Dhingra, Partha Talukdar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Salient Span Masking (SSM) has shown itself to be an effective strategy to
improve closed-book question answering performance. SSM extends general masked
language model pretraining by creating additional unsupervised training
sentences that mask a single entity or date span, thus oversampling factual
information. Despite the success of this paradigm, the span types and sampling
strategies are relatively arbitrary and not widely studied for other tasks.
Thus, we investigate SSM from the perspective of temporal tasks, where learning
a good representation of various temporal expressions is important. To that
end, we introduce Temporal Span Masking (TSM) intermediate training. First, we
find that SSM alone improves the downstream performance on three temporal tasks
by an avg. +5.8 points. Further, we are able to achieve additional improvements
(avg. +0.29 points) by adding the TSM task. These comprise the new best
reported results on the targeted tasks. Our analysis suggests that the
effectiveness of SSM stems from the sentences chosen in the training data
rather than the mask choice: sentences with entities frequently also contain
temporal expressions. Nonetheless, the additional targeted spans of TSM can
still improve performance, especially in a zero-shot context.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages 1 figure, to appear in EACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ W2KPE: Keyphrase Extraction with Word-Word Relation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13463v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13463v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wen Cheng, Shichen Dong, Wei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes our submission to ICASSP 2023 MUG Challenge Track 4,
Keyphrase Extraction, which aims to extract keyphrases most relevant to the
conference theme from conference materials. We model the challenge as a
single-class Named Entity Recognition task and developed techniques for better
performance on the challenge: For the data preprocessing, we encode the split
keyphrases after word segmentation. In addition, we increase the amount of
input information that the model can accept at one time by fusing multiple
preprocessed sentences into one segment. We replace the loss function with the
multi-class focal loss to address the sparseness of keyphrases. Besides, we
score each appearance of keyphrases and add an extra output layer to fit the
score to rank keyphrases. Exhaustive evaluations are performed to find the best
combination of the word segmentation tool, the pre-trained embedding model, and
the corresponding hyperparameters. With these proposals, we scored 45.04 on the
final test set.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Extracting Physical Rehabilitation Exercise Information from Clinical
  Notes: a Comparison of Rule-Based and Machine Learning Natural Language
  Processing Techniques 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13466v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13466v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stephen W. Shaffran, Fengyi Gao, Parker E. Denny, Bayan M. Aldhahwani, Allyn Bove, Shyam Visweswaran, Yanshan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Physical rehabilitation plays a crucial role in the recovery process of
post-stroke patients. By personalizing therapies for patients leveraging
predictive modeling and electronic health records (EHRs), healthcare providers
can make the rehabilitation process more efficient. Before predictive modeling
can provide decision support for the assignment of treatment plans, automated
methods are necessary to extract physical rehabilitation exercise information
from unstructured EHRs. We introduce a rule-based natural language processing
algorithm to annotate therapeutic procedures for stroke patients and compare it
to several small machine learning models. We find that our algorithm
outperforms these models in extracting half of the concepts where sufficient
data is available, and individual exercise descriptions can be assigned binary
labels with an f-score of no less than 0.75 per concept. More research needs to
be done before these algorithms can be deployed on unlabeled documents, but
current progress gives promise to the potential of precision rehabilitation
research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep RL with Hierarchical Action Exploration for Dialogue Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13465v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13465v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Itsugun Cho, Ryota Takahashi, Yusaku Yanase, Hiroaki Saito
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conventionally, since the natural language action space is astronomical,
approximate dynamic programming applied to dialogue generation involves policy
improvement with action sampling. However, such a practice is inefficient for
reinforcement learning (RL) because the eligible (high action value) responses
are very sparse, and the greedy policy sustained by the random sampling is
flabby. This paper shows that the performance of dialogue policy positively
correlated with sampling size by theoretical and experimental. We introduce a
novel dual-granularity Q-function to alleviate this limitation by exploring the
most promising response category to intervene in the sampling. It extracts the
actions following the grained hierarchy, which can achieve the optimum with
fewer policy iterations. Our approach learns in the way of offline RL from
multiple reward functions designed to recognize human emotional details.
Empirical studies demonstrate that our algorithm outperforms the baseline
methods. Further verification presents that ours can generate responses with
higher expected rewards and controllability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Wide to Deep: Dimension Lifting Network for Parameter-efficient
  Knowledge Graph Embedding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12816v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12816v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Borui Cai, Yong Xiang, Longxiang Gao, Di Wu, He Zhang, Jiong Jin, Tom Luan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge graph embedding (KGE) that maps entities and relations into vector
representations is essential for downstream tasks. Conventional KGE methods
require relatively high-dimensional entity representations to preserve the
structural information of knowledge graph, but lead to oversized model
parameters. Recent methods reduce model parameters by adopting low-dimensional
entity representations, while developing techniques (e.g., knowledge
distillation) to compensate for the reduced dimension. However, such operations
produce degraded model accuracy and limited reduction of model parameters.
Specifically, we view the concatenation of all entity representations as an
embedding layer, and then conventional KGE methods that adopt high-dimensional
entity representations equal to enlarging the width of the embedding layer to
gain expressiveness. To achieve parameter efficiency without sacrificing
accuracy, we instead increase the depth and propose a deeper embedding network
for entity representations, i.e., a narrow embedding layer and a multi-layer
dimension lifting network (LiftNet). Experiments on three public datasets show
that the proposed method (implemented based on TransE and DistMult) with
4-dimensional entity representations achieves more accurate link prediction
results than counterpart parameter-efficient KGE methods and strong KGE
baselines, including TransE and DistMult with 512-dimensional entity
representations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EmotionIC: Emotional Inertia and Contagion-driven Dependency Modelling
  for Emotion Recognition in Conversation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11117v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11117v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingjian Liu, Jiang Li, Xiaoping Wang, Zhigang Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emotion Recognition in Conversation (ERC) has attracted growing attention in
recent years as a result of the advancement and implementation of
human-computer interface technologies. However, previous approaches to modeling
global and local context dependencies lost the diversity of dependency
information and do not take the context dependency into account at the
classification level. In this paper, we propose a novel approach to dependency
modeling driven by Emotional Inertia and Contagion (EmotionIC) for
conversational emotion recognition at the feature extraction and classification
levels. At the feature extraction level, our designed Identity Masked
Multi-head Attention (IM-MHA) captures the identity-based long-distant context
in the dialogue to contain the diverse influence of different participants and
construct the global emotional atmosphere, while the devised Dialogue-based
Gate Recurrent Unit (DialogGRU) that aggregates the emotional tendencies of
dyadic dialogue is applied to refine the contextual features with inter- and
intra-speaker dependencies. At the classification level, by introducing skip
connections in Conditional Random Field (CRF), we elaborate the Skip-chain CRF
(SkipCRF) to capture the high-order dependencies within and between speakers,
and to emulate the emotional flow of distant participants. Experimental results
show that our method can significantly outperform the state-of-the-art models
on four benchmark datasets. The ablation studies confirm that our modules can
effectively model emotional inertia and contagion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Models Can Be Used to Estimate the Ideologies of
  Politicians in a Zero-Shot Learning Setting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12057v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12057v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patrick Y. Wu, Joshua A. Tucker, Jonathan Nagler, Solomon Messing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The mass aggregation of knowledge embedded in large language models (LLMs)
holds the promise of new solutions to problems of observability and measurement
in the social sciences. We examine the utility of one such model for a
particularly difficult measurement task: measuring the latent ideology of
lawmakers, which allows us to better understand functions that are core to
democracy, such as how politics shape policy and how political actors represent
their constituents. We scale the senators of the 116th United States Congress
along the liberal-conservative spectrum by prompting ChatGPT to select the more
liberal (or conservative) senator in pairwise comparisons. We show that the LLM
produced stable answers across repeated iterations, did not hallucinate, and
was not simply regurgitating information from a single source. This new scale
strongly correlates with pre-existing liberal-conservative scales such as
NOMINATE, but also differs in several important ways, such as correctly placing
senators who vote against their party for far-left or far-right ideological
reasons on the extreme ends. The scale also highly correlates with ideological
measures based on campaign giving and political activists' perceptions of these
senators. In addition to the potential for better-automated data collection and
information retrieval, our results suggest LLMs are likely to open new avenues
for measuring latent constructs like ideology that rely on aggregating large
quantities of data from public sources.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 4 figures; V2: fixed graphical error on Figure 2</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Visual Spatial Reasoning <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.00363v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.00363v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangyu Liu, Guy Emerson, Nigel Collier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spatial relations are a basic part of human cognition. However, they are
expressed in natural language in a variety of ways, and previous work has
suggested that current vision-and-language models (VLMs) struggle to capture
relational information. In this paper, we present Visual Spatial Reasoning
(VSR), a dataset containing more than 10k natural text-image pairs with 66
types of spatial relations in English (such as: under, in front of, and
facing). While using a seemingly simple annotation format, we show how the
dataset includes challenging linguistic phenomena, such as varying reference
frames. We demonstrate a large gap between human and model performance: the
human ceiling is above 95%, while state-of-the-art models only achieve around
70%. We observe that VLMs' by-relation performances have little correlation
with the number of training examples and the tested models are in general
incapable of recognising relations concerning the orientations of objects.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>TACL camera-ready version; code and data available at
  https://github.com/cambridgeltl/visual-spatial-reasoning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ COVID-19 event extraction from Twitter via extractive question answering
  with continuous <span class="highlight-title">prompt</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10659v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10659v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhang Jiang, Ramakanth Kavuluru
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As COVID-19 ravages the world, social media analytics could augment
traditional surveys in assessing how the pandemic evolves and capturing
consumer chatter that could help healthcare agencies in addressing it. This
typically involves mining disclosure events that mention testing positive for
the disease or discussions surrounding perceptions and beliefs in preventative
or treatment options. The 2020 shared task on COVID-19 event extraction
(conducted as part of the W-NUT workshop during the EMNLP conference)
introduced a new Twitter dataset for benchmarking event extraction from
COVID-19 tweets. In this paper, we cast the problem of event extraction as
extractive question answering using recent advances in continuous prompting in
language models. On the shared task test dataset, our approach leads to over 5%
absolute micro-averaged F1-score improvement over prior best results, across
all COVID-19 event slots. Our ablation study shows that continuous prompts have
a major impact on the eventual performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to appear in MEDINFO 2023. Code:
  https://github.com/bionlproc/twitter-covid-QA-extraction</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EVA-02: A Visual Representation for Neon Genesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11331v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11331v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Fang, Quan Sun, Xinggang Wang, Tiejun Huang, Xinlong Wang, Yue Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We launch EVA-02, a next-generation Transformer-based visual representation
pre-trained to reconstruct strong and robust language-aligned vision features
via masked image modeling. With an updated plain Transformer architecture as
well as extensive pre-training from an open & accessible giant CLIP vision
encoder, EVA-02 demonstrates superior performance compared to prior
state-of-the-art approaches across various representative vision tasks, while
utilizing significantly fewer parameters and compute budgets. Notably, using
exclusively publicly accessible training data, EVA-02 with only 304M parameters
achieves a phenomenal 90.0 fine-tuning top-1 accuracy on ImageNet-1K val set.
Additionally, our EVA-02-CLIP can reach up to 80.4 zero-shot top-1 on
ImageNet-1K, outperforming the previous largest & best open-sourced CLIP with
only ~1/6 parameters and ~1/6 image-text training data. We offer four EVA-02
variants in various model sizes, ranging from 6M to 304M parameters, all with
impressive performance. To facilitate open access and open research, we release
the complete suite of EVA-02 to the community at
https://github.com/baaivision/EVA/tree/master/EVA-02.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v2: Fix some known issues & typos. v1: To Asuka. Code & Models:
  https://github.com/baaivision/EVA/tree/master/EVA-02</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Chat<span class="highlight-title">GPT</span> Participates in a Computer Science Exam 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.09461v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.09461v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastian Bordt, Ulrike von Luxburg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We asked ChatGPT to participate in an undergraduate computer science exam on
''Algorithms and Data Structures''. The program was evaluated on the entire
exam as posed to the students. We hand-copied its answers onto an exam sheet,
which was subsequently graded in a blind setup alongside those of 200
participating students. We find that ChatGPT narrowly passed the exam,
obtaining 20.5 out of 40 points. This impressive performance indicates that
ChatGPT can indeed succeed in challenging tasks like university exams. At the
same time, the questions in our exam are structurally similar to those of other
exams, solved homework problems, and teaching materials that can be found
online and might have been part of ChatGPT's training data. Therefore, it would
be inadequate to conclude from this experiment that ChatGPT has any
understanding of computer science. We also assess the improvements brought by
GPT-4. We find that GPT-4 would have obtained about 17\% more exam points than
GPT-3.5, reaching the performance of the average student. The transcripts of
our conversations with ChatGPT are available at
\url{https://github.com/tml-tuebingen/chatgpt-algorithm-exam}, and the entire
graded exam is in the appendix of this paper.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UPRISE: Universal <span class="highlight-title">Prompt</span> Retrieval for Improving Zero-Shot Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08518v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08518v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daixuan Cheng, Shaohan Huang, Junyu Bi, Yuefeng Zhan, Jianfeng Liu, Yujing Wang, Hao Sun, Furu Wei, Denvy Deng, Qi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are popular for their impressive abilities, but
the need for model-specific fine-tuning or task-specific prompt engineering can
hinder their generalization. We propose UPRISE (Universal Prompt Retrieval for
Improving zero-Shot Evaluation), which tunes a lightweight and versatile
retriever that automatically retrieves prompts for a given zero-shot task
input. Specifically, we demonstrate universality in a cross-task and
cross-model scenario: the retriever is tuned on a diverse set of tasks, but
tested on unseen task types; we use a small frozen LLM, GPT-Neo-2.7B, for
tuning the retriever, but test the retriever on different LLMs of much larger
scales, such as BLOOM-7.1B, OPT-66B and GPT3-175B. Additionally, we show that
UPRISE mitigates the hallucination problem in our experiments with ChatGPT,
suggesting its potential to improve even the strongest LLMs. Our model and code
are available at https://github.com/microsoft/LMOps.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Update link to our model and code</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Causal Structure of Semantic Ambiguities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.06807v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.06807v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daphne Wang, Mehrnoosh Sadrzadeh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ambiguity is a natural language phenomenon occurring at different levels of
syntax, semantics, and pragmatics. It is widely studied; in Psycholinguistics,
for instance, we have a variety of competing studies for the human
disambiguation processes. These studies are empirical and based on eyetracking
measurements. Here we take first steps towards formalizing these processes for
semantic ambiguities where we identified the presence of two features: (1)
joint plausibility degrees of different possible interpretations, (2) causal
structures according to which certain words play a more substantial role in the
processes. The novel sheaf-theoretic model of definite causality developed by
Gogioso and Pinzani in QPL 2021 offers tools to model and reason about these
features. We applied this theory to a dataset of ambiguous phrases extracted
from Psycholinguistics literature and their human plausibility judgements
collected by us using the Amazon Mechanical Turk engine. We measured the causal
fractions of different disambiguation orders within the phrases and discovered
two prominent orders: from subject to verb in the subject-verb and from object
to verb in the verb object phrases. We also found evidence for delay in the
disambiguation of polysemous vs homonymous verbs, again compatible with
Psycholinguistic findings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative Bias for Robust Visual Question Answering <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.00690v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.00690v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jae Won Cho, Dong-jin Kim, Hyeonggon Ryu, In So Kweon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of Visual Question Answering (VQA) is known to be plagued by the
issue of VQA models exploiting biases within the dataset to make its final
prediction. Various previous ensemble based debiasing methods have been
proposed where an additional model is purposefully trained to be biased in
order to train a robust target model. However, these methods compute the bias
for a model simply from the label statistics of the training data or from
single modal branches. In this work, in order to better learn the bias a target
VQA model suffers from, we propose a generative method to train the bias model
directly from the target model, called GenB. In particular, GenB employs a
generative network to learn the bias in the target model through a combination
of the adversarial objective and knowledge distillation. We then debias our
target model with GenB as a bias model, and show through extensive experiments
the effects of our method on various VQA bias datasets including VQA-CP2,
VQA-CP1, GQA-OOD, and VQA-CE, and show state-of-the-art results with the LXMERT
architecture on VQA-CP2.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tell Me What Happened: Unifying Text-guided Video Completion via
  Multimodal Masked Video Generation <span class="chip">CVPR'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.12824v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.12824v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tsu-Jui Fu, Licheng Yu, Ning Zhang, Cheng-Yang Fu, Jong-Chyi Su, William Yang Wang, Sean Bell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating a video given the first several static frames is challenging as it
anticipates reasonable future frames with temporal coherence. Besides video
prediction, the ability to rewind from the last frame or infilling between the
head and tail is also crucial, but they have rarely been explored for video
completion. Since there could be different outcomes from the hints of just a
few frames, a system that can follow natural language to perform video
completion may significantly improve controllability. Inspired by this, we
introduce a novel task, text-guided video completion (TVC), which requests the
model to generate a video from partial frames guided by an instruction. We then
propose Multimodal Masked Video Generation (MMVG) to address this TVC task.
During training, MMVG discretizes the video frames into visual tokens and masks
most of them to perform video completion from any time point. At inference
time, a single MMVG model can address all 3 cases of TVC, including video
prediction, rewind, and infilling, by applying corresponding masking
conditions. We evaluate MMVG in various video scenarios, including egocentric,
animation, and gaming. Extensive experimental results indicate that MMVG is
effective in generating high-quality visual appearances with text guidance for
TVC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR'23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Character, Word, or Both? Revisiting the Segmentation Granularity for
  Chinese <span class="highlight-title">Pre-train</span>ed Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10893v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10893v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinnian Liang, Zefan Zhou, Hui Huang, Shuangzhi Wu, Tong Xiao, Muyun Yang, Zhoujun Li, Chao Bian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pretrained language models (PLMs) have shown marvelous improvements across
various NLP tasks. Most Chinese PLMs simply treat an input text as a sequence
of characters, and completely ignore word information. Although Whole Word
Masking can alleviate this, the semantics in words is still not well
represented. In this paper, we revisit the segmentation granularity of Chinese
PLMs. We propose a mixed-granularity Chinese BERT (MigBERT) by considering both
characters and words. To achieve this, we design objective functions for
learning both character and word-level representations. We conduct extensive
experiments on various Chinese NLP tasks to evaluate existing PLMs as well as
the proposed MigBERT. Experimental results show that MigBERT achieves new SOTA
performance on all these tasks. Further analysis demonstrates that words are
semantically richer than characters. More interestingly, we show that MigBERT
also works with Japanese. Our code and model have been released
here~\footnote{https://github.com/xnliang98/MigBERT}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VoP: Text-Video Co-operative <span class="highlight-title">Prompt</span> Tuning for Cross-Modal Retrieval <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.12764v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.12764v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siteng Huang, Biao Gong, Yulin Pan, Jianwen Jiang, Yiliang Lv, Yuyuan Li, Donglin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many recent studies leverage the pre-trained CLIP for text-video cross-modal
retrieval by tuning the backbone with additional heavy modules, which not only
brings huge computational burdens with much more parameters, but also leads to
the knowledge forgetting from upstream models. In this work, we propose the
VoP: Text-Video Co-operative Prompt Tuning for efficient tuning on the
text-video retrieval task. The proposed VoP is an end-to-end framework with
both video & text prompts introducing, which can be regarded as a powerful
baseline with only 0.1% trainable parameters. Further, based on the
spatio-temporal characteristics of videos, we develop three novel video prompt
mechanisms to improve the performance with different scales of trainable
parameters. The basic idea of the VoP enhancement is to model the frame
position, frame context, and layer function with specific trainable prompts,
respectively. Extensive experiments show that compared to full fine-tuning, the
enhanced VoP achieves a 1.4% average R@1 gain across five text-video retrieval
benchmarks with 6x less parameter overhead. The code will be available at
https://github.com/bighuang624/VoP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Focusing on Potential Named Entities During Active Label Acquisition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.03837v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.03837v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Osman Berk Sapci, Oznur Tastan, Reyyan Yeniterzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Named entity recognition (NER) aims to identify mentions of named entities in
an unstructured text and classify them into predefined named entity classes.
While deep learning-based pre-trained language models help to achieve good
predictive performances in NER, many domain-specific NER applications still
call for a substantial amount of labeled data. Active learning (AL), a general
framework for the label acquisition problem, has been used for NER tasks to
minimize the annotation cost without sacrificing model performance. However,
the heavily imbalanced class distribution of tokens introduces challenges in
designing effective AL querying methods for NER. We propose several AL sentence
query evaluation functions that pay more attention to potential positive
tokens, and evaluate these proposed functions with both sentence-based and
token-based cost evaluation strategies. We also propose a better data-driven
normalization approach to penalize sentences that are too long or too short.
Our experiments on three datasets from different domains reveal that the
proposed approach reduces the number of annotated tokens while achieving better
or comparable prediction performance with conventional methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Summarize the Past to Predict the Future: Natural Language Descriptions
  of Context Boost Multimodal Object Interaction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09209v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09209v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Razvan-George Pasca, Alexey Gavryushin, Yen-Ling Kuo, Luc Van Gool, Otmar Hilliges, Xi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study object interaction anticipation in egocentric videos. This task
requires an understanding of the spatiotemporal context formed by past actions
on objects, coined action context. We propose TransFusion, a multimodal
transformer-based architecture. It exploits the representational power of
language by summarising the action context. TransFusion leverages pre-trained
image captioning and vision-language models to extract the action context from
past video frames. This action context together with the next video frame is
processed by the multimodal fusion module to forecast the next object
interaction. Our model enables more efficient end-to-end learning. The large
pre-trained language models add common sense and a generalisation capability.
Experiments on Ego4D and EPIC-KITCHENS-100 show the effectiveness of our
multimodal fusion model. They also highlight the benefits of using
language-based context summaries in a task where vision seems to suffice. Our
method outperforms state-of-the-art approaches by 40.4% in relative terms in
overall mAP on the Ego4D test set. We validate the effectiveness of TransFusion
via experiments on EPIC-KITCHENS-100. Video and code are available at:
https://eth-ait.github.io/transfusion-proj/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Prompt</span>DA: Label-guided Data Augmentation for <span class="highlight-title">Prompt</span>-based Few-shot
  Learners <span class="chip">EACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.09229v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.09229v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Canyu Chen, Kai Shu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in large pre-trained language models (PLMs) lead to
impressive gains in natural language understanding (NLU) tasks with
task-specific fine-tuning. However, directly fine-tuning PLMs heavily relies on
sufficient labeled training instances, which are usually hard to obtain.
Prompt-based tuning on PLMs has shown to be powerful for various downstream
few-shot tasks. Existing works studying prompt-based tuning for few-shot NLU
tasks mainly focus on deriving proper label words with a verbalizer or
generating prompt templates to elicit semantics from PLMs. In addition,
conventional data augmentation strategies such as synonym substitution, though
widely adopted in low-resource scenarios, only bring marginal improvements for
prompt-based few-shot learning. Thus, an important research question arises:
how to design effective data augmentation methods for prompt-based few-shot
tuning? To this end, considering the label semantics are essential in
prompt-based tuning, we propose a novel label-guided data augmentation
framework PromptDA, which exploits the enriched label semantic information for
data augmentation. Extensive experiment results on few-shot text classification
tasks demonstrate the superior performance of the proposed framework by
effectively leveraging label semantics and data augmentation for natural
language understanding. Our code is available at
https://github.com/canyuchen/PromptDA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Proceedings of EACL 2023 main conference. Code is
  available at https://github.com/canyuchen/PromptDA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ cTBL: Augmenting Large Language Models for Conversational Tables 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12024v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12024v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anirudh S Sundar, Larry Heck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An open challenge in multimodal conversational AI requires augmenting large
language models with information from textual and non-textual sources for
multi-turn dialogue. To address this problem, this paper introduces
Conversational Tables (cTBL), a three-step encoder-decoder approach to retrieve
tabular information and generate dialogue responses grounded on the retrieved
information. cTBL uses Transformer encoder embeddings for Dense Table Retrieval
and obtains up to 5% relative improvement in Top-1 and Top-3 accuracy over
sparse retrieval on the HyrbiDialogue dataset. Additionally, cTBL performs
tabular knowledge retrieval using both encoder and decoder models, resulting in
up to 46% relative improvement in ROUGE scores and better human evaluation for
response generation on HyrbiDialogue.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Automatic Severity Assessment of Dysarthric speech by using
  <span class="highlight-title">Self-supervised</span> Model with Multi-task Learning <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.15387v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.15387v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eun Jung Yeo, Kwanghee Choi, Sunhee Kim, Minhwa Chung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic assessment of dysarthric speech is essential for sustained
treatments and rehabilitation. However, obtaining atypical speech is
challenging, often leading to data scarcity issues. To tackle the problem, we
propose a novel automatic severity assessment method for dysarthric speech,
using the self-supervised model in conjunction with multi-task learning.
Wav2vec 2.0 XLS-R is jointly trained for two different tasks: severity
classification and auxiliary automatic speech recognition (ASR). For the
baseline experiments, we employ hand-crafted acoustic features and machine
learning classifiers such as SVM, MLP, and XGBoost. Explored on the Korean
dysarthric speech QoLT database, our model outperforms the traditional baseline
methods, with a relative percentage increase of 1.25% for F1-score. In
addition, the proposed model surpasses the model trained without ASR head,
achieving 10.61% relative percentage improvements. Furthermore, we present how
multi-task learning affects the severity classification performance by
analyzing the latent representations and regularization effect.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-lingual Evaluation of Code Generation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.14868v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.14868v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ben Athiwaratkun, Sanjay Krishna Gouda, Zijian Wang, Xiaopeng Li, Yuchen Tian, Ming Tan, Wasi Uddin Ahmad, Shiqi Wang, Qing Sun, Mingyue Shang, Sujan Kumar Gonugondla, Hantian Ding, Varun Kumar, Nathan Fulton, Arash Farahani, Siddhartha Jain, Robert Giaquinto, Haifeng Qian, Murali Krishna Ramanathan, Ramesh Nallapati, Baishakhi Ray, Parminder Bhatia, Sudipta Sengupta, Dan Roth, Bing Xiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present new benchmarks on evaluation code generation models: MBXP and
Multilingual HumanEval, and MathQA-X. These datasets cover over 10 programming
languages and are generated using a scalable conversion framework that
transpiles prompts and test cases from the original Python datasets into the
corresponding data in the target language. Using these benchmarks, we are able
to assess the performance of code generation models in a multi-lingual fashion,
and discovered generalization ability of language models on out-of-domain
languages, advantages of multi-lingual models over mono-lingual, the ability of
few-shot prompting to teach the model new languages, and zero-shot translation
abilities even on mono-lingual settings. Furthermore, we use our code
generation model to perform large-scale bootstrapping to obtain synthetic
canonical solutions in several languages, which can be used for other
code-related evaluations such as code insertion, robustness, or summarization
tasks. Overall, our benchmarks represents a significant step towards a deeper
understanding of language models' code generation abilities. We publicly
release our code and datasets at https://github.com/amazon-research/mxeval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and data release: https://github.com/amazon-research/mxeval</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">137</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CiCo: Domain-Aware Sign Language Retrieval via Cross-Lingual Contrastive
  Learning <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12793v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12793v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiting Cheng, Fangyun Wei, Jianmin Bao, Dong Chen, Wenqiang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work focuses on sign language retrieval-a recently proposed task for
sign language understanding. Sign language retrieval consists of two sub-tasks:
text-to-sign-video (T2V) retrieval and sign-video-to-text (V2T) retrieval.
Different from traditional video-text retrieval, sign language videos, not only
contain visual signals but also carry abundant semantic meanings by themselves
due to the fact that sign languages are also natural languages. Considering
this character, we formulate sign language retrieval as a cross-lingual
retrieval problem as well as a video-text retrieval task. Concretely, we take
into account the linguistic properties of both sign languages and natural
languages, and simultaneously identify the fine-grained cross-lingual (i.e.,
sign-to-word) mappings while contrasting the texts and the sign videos in a
joint embedding space. This process is termed as cross-lingual contrastive
learning. Another challenge is raised by the data scarcity issue-sign language
datasets are orders of magnitude smaller in scale than that of speech
recognition. We alleviate this issue by adopting a domain-agnostic sign encoder
pre-trained on large-scale sign videos into the target domain via
pseudo-labeling. Our framework, termed as domain-aware sign language retrieval
via Cross-lingual Contrastive learning or CiCo for short, outperforms the
pioneering method by large margins on various datasets, e.g., +22.4 T2V and
+28.0 V2T R@1 improvements on How2Sign dataset, and +13.7 T2V and +17.1 V2T R@1
improvements on PHOENIX-2014T dataset. Code and models are available at:
https://github.com/FangyunWei/SLRT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2023. Code and models are available at:
  https://github.com/FangyunWei/SLRT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SHERF: Generalizable Human NeRF from a Single Image 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12791v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12791v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shoukang Hu, Fangzhou Hong, Liang Pan, Haiyi Mei, Lei Yang, Ziwei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing Human NeRF methods for reconstructing 3D humans typically rely on
multiple 2D images from multi-view cameras or monocular videos captured from
fixed camera views. However, in real-world scenarios, human images are often
captured from random camera angles, presenting challenges for high-quality 3D
human reconstruction. In this paper, we propose SHERF, the first generalizable
Human NeRF model for recovering animatable 3D humans from a single input image.
SHERF extracts and encodes 3D human representations in canonical space,
enabling rendering and animation from free views and poses. To achieve
high-fidelity novel view and pose synthesis, the encoded 3D human
representations should capture both global appearance and local fine-grained
textures. To this end, we propose a bank of 3D-aware hierarchical features,
including global, point-level, and pixel-aligned features, to facilitate
informative encoding. Global features enhance the information extracted from
the single input image and complement the information missing from the partial
2D observation. Point-level features provide strong clues of 3D human
structure, while pixel-aligned features preserve more fine-grained details. To
effectively integrate the 3D-aware hierarchical feature bank, we design a
feature fusion transformer. Extensive experiments on THuman, RenderPeople,
ZJU_MoCap, and HuMMan datasets demonstrate that SHERF achieves state-of-the-art
performance, with better generalizability for novel view and pose synthesis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project webpage: https://skhu101.github.io/SHERF/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diffuse-Denoise-Count: Accurate Crowd-Counting with Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12790v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12790v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yasiru Ranasinghe, Nithin Gopalakrishnan Nair, Wele Gedara Chaminda Bandara, Vishal M. Patel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Crowd counting is a key aspect of crowd analysis and has been typically
accomplished by estimating a crowd-density map and summing over the density
values. However, this approach suffers from background noise accumulation and
loss of density due to the use of broad Gaussian kernels to create the ground
truth density maps. This issue can be overcome by narrowing the Gaussian
kernel. However, existing approaches perform poorly when trained with such
ground truth density maps. To overcome this limitation, we propose using
conditional diffusion models to predict density maps, as diffusion models are
known to model complex distributions well and show high fidelity to training
data during crowd-density map generation. Furthermore, as the intermediate time
steps of the diffusion process are noisy, we incorporate a regression branch
for direct crowd estimation only during training to improve the feature
learning. In addition, owing to the stochastic nature of the diffusion model,
we introduce producing multiple density maps to improve the counting
performance contrary to the existing crowd counting pipelines. Further, we also
differ from the density summation and introduce contour detection followed by
summation as the counting operation, which is more immune to background noise.
We conduct extensive experiments on public datasets to validate the
effectiveness of our method. Specifically, our novel crowd-counting pipeline
improves the error of crowd-counting by up to $6\%$ on JHU-CROWD++ and up to
$7\%$ on UCF-QNRF.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The project is available at
  https://github.com/dylran/DiffuseDenoiseCount</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Instruct-NeRF2NeRF: Editing 3D Scenes with Instructions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12789v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12789v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayaan Haque, Matthew Tancik, Alexei A. Efros, Aleksander Holynski, Angjoo Kanazawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a method for editing NeRF scenes with text-instructions. Given a
NeRF of a scene and the collection of images used to reconstruct it, our method
uses an image-conditioned diffusion model (InstructPix2Pix) to iteratively edit
the input images while optimizing the underlying scene, resulting in an
optimized 3D scene that respects the edit instruction. We demonstrate that our
proposed method is able to edit large-scale, real-world scenes, and is able to
accomplish more realistic, targeted edits than prior work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://instruct-nerf2nerf.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EPro-PnP: Generalized End-to-End Probabilistic Perspective-n-Points for
  Monocular Object Pose Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12787v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12787v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hansheng Chen, Wei Tian, Pichao Wang, Fan Wang, Lu Xiong, Hao Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Locating 3D objects from a single RGB image via Perspective-n-Point (PnP) is
a long-standing problem in computer vision. Driven by end-to-end deep learning,
recent studies suggest interpreting PnP as a differentiable layer, allowing for
partial learning of 2D-3D point correspondences by backpropagating the
gradients of pose loss. Yet, learning the entire correspondences from scratch
is highly challenging, particularly for ambiguous pose solutions, where the
globally optimal pose is theoretically non-differentiable w.r.t. the points. In
this paper, we propose the EPro-PnP, a probabilistic PnP layer for general
end-to-end pose estimation, which outputs a distribution of pose with
differentiable probability density on the SE(3) manifold. The 2D-3D coordinates
and corresponding weights are treated as intermediate variables learned by
minimizing the KL divergence between the predicted and target pose
distribution. The underlying principle generalizes previous approaches, and
resembles the attention mechanism. EPro-PnP can enhance existing correspondence
networks, closing the gap between PnP-based method and the task-specific
leaders on the LineMOD 6DoF pose estimation benchmark. Furthermore, EPro-PnP
helps to explore new possibilities of network design, as we demonstrate a novel
deformable correspondence network with the state-of-the-art pose accuracy on
the nuScenes 3D object detection benchmark. Our code is available at
https://github.com/tjiiv-cprg/EPro-PnP-v2.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code available at https://github.com/tjiiv-cprg/EPro-PnP-v2. arXiv
  admin note: substantial text overlap with arXiv:2203.13254</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FeatureNeRF: Learning Generalizable NeRFs by Distilling Foundation
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12786v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12786v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianglong Ye, Naiyan Wang, Xiaolong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works on generalizable NeRFs have shown promising results on novel
view synthesis from single or few images. However, such models have rarely been
applied on other downstream tasks beyond synthesis such as semantic
understanding and parsing. In this paper, we propose a novel framework named
FeatureNeRF to learn generalizable NeRFs by distilling pre-trained vision
foundation models (e.g., DINO, Latent Diffusion). FeatureNeRF leverages 2D
pre-trained foundation models to 3D space via neural rendering, and then
extract deep features for 3D query points from NeRF MLPs. Consequently, it
allows to map 2D images to continuous 3D semantic feature volumes, which can be
used for various downstream tasks. We evaluate FeatureNeRF on tasks of 2D/3D
semantic keypoint transfer and 2D/3D object part segmentation. Our extensive
experiments demonstrate the effectiveness of FeatureNeRF as a generalizable 3D
semantic feature extractor. Our project page is available at
https://jianglongye.com/featurenerf/ .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://jianglongye.com/featurenerf/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tube-Link: A Flexible Cross Tube Baseline for Universal Video
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12782v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12782v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangtai Li, Haobo Yuan, Wenwei Zhang, Guangliang Cheng, Jiangmiao Pang, Chen Change Loy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of video segmentation is to accurately segment and track every pixel
in diverse scenarios. In this paper, we present Tube-Link, a versatile
framework that addresses multiple core tasks of video segmentation with a
unified architecture. Our framework is a near-online approach that takes a
short subclip as input and outputs the corresponding spatial-temporal tube
masks. To enhance the modeling of cross-tube relationships, we propose an
effective way to perform tube-level linking via attention along the queries. In
addition, we introduce temporal contrastive learning to instance-wise
discriminative features for tube-level association. Our approach offers
flexibility and efficiency for both short and long video inputs, as the length
of each subclip can be varied according to the needs of datasets or scenarios.
Tube-Link outperforms existing specialized architectures by a significant
margin on five video segmentation datasets. Specifically, it achieves almost
13% relative improvements on VIPSeg and 4% improvements on KITTI-STEP over the
strong baseline Video K-Net. When using a ResNet50 backbone on Youtube-VIS-2019
and 2021, Tube-Link boosts IDOL by 3% and 4%, respectively. Code will be
available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://github.com/lxtGH/Tube-Link</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LFM-3D: Learnable Feature Matching Across Wide Baselines Using 3D
  Signals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12779v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12779v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arjun Karpur, Guilherme Perrotta, Ricardo Martin-Brualla, Howard Zhou, Andre Araujo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Finding localized correspondences across different images of the same object
is crucial to understand its geometry. In recent years, this problem has seen
remarkable progress with the advent of deep learning based local image features
and learnable matchers. Still, learnable matchers often underperform when there
exists only small regions of co-visibility between image pairs (i.e. wide
camera baselines). To address this problem, we leverage recent progress in
coarse single-view geometry estimation methods. We propose LFM-3D, a Learnable
Feature Matching framework that uses models based on graph neural networks, and
enhances their capabilities by integrating noisy, estimated 3D signals to boost
correspondence estimation. When integrating 3D signals into the matcher model,
we show that a suitable positional encoding is critical to effectively make use
of the low-dimensional 3D information. We experiment with two different 3D
signals - normalized object coordinates and monocular depth estimates - and
evaluate our method on large-scale (synthetic and real) datasets containing
object-centric image pairs across wide baselines. We observe strong feature
matching improvements compared to 2D-only methods, with up to +6% total recall
and +28% precision at fixed recall. We additionally demonstrate that the
resulting improved correspondences lead to much higher relative posing accuracy
for in-the-wild image pairs, with a more than 8% boost compared to the 2D-only
approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dense Distinct Query for End-to-End Object Detection <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12776v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12776v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shilong Zhang, Wang xinjiang, Jiaqi Wang, Jiangmiao Pang, Chengqi Lyu, Wenwei Zhang, Ping Luo, Kai Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One-to-one label assignment in object detection has successfully obviated the
need for non-maximum suppression (NMS) as postprocessing and makes the pipeline
end-to-end. However, it triggers a new dilemma as the widely used sparse
queries cannot guarantee a high recall, while dense queries inevitably bring
more similar queries and encounter optimization difficulties. As both sparse
and dense queries are problematic, then what are the expected queries in
end-to-end object detection? This paper shows that the solution should be Dense
Distinct Queries (DDQ). Concretely, we first lay dense queries like traditional
detectors and then select distinct ones for one-to-one assignments. DDQ blends
the advantages of traditional and recent end-to-end detectors and significantly
improves the performance of various detectors including FCN, R-CNN, and DETRs.
Most impressively, DDQ-DETR achieves 52.1 AP on MS-COCO dataset within 12
epochs using a ResNet-50 backbone, outperforming all existing detectors in the
same setting. DDQ also shares the benefit of end-to-end detectors in crowded
scenes and achieves 93.8 AP on CrowdHuman. We hope DDQ can inspire researchers
to consider the complementarity between traditional methods and end-to-end
detectors. The source code can be found at
\url{https://github.com/jshilong/DDQ}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR2023. Code has been released at
  https://github.com/jshilong/DDQ</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spherical <span class="highlight-title">Transformer</span> for LiDAR-based 3D Recognition <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12766v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12766v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Lai, Yukang Chen, Fanbin Lu, Jianhui Liu, Jiaya Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LiDAR-based 3D point cloud recognition has benefited various applications.
Without specially considering the LiDAR point distribution, most current
methods suffer from information disconnection and limited receptive field,
especially for the sparse distant points. In this work, we study the
varying-sparsity distribution of LiDAR points and present SphereFormer to
directly aggregate information from dense close points to the sparse distant
ones. We design radial window self-attention that partitions the space into
multiple non-overlapping narrow and long windows. It overcomes the
disconnection issue and enlarges the receptive field smoothly and dramatically,
which significantly boosts the performance of sparse distant points. Moreover,
to fit the narrow and long windows, we propose exponential splitting to yield
fine-grained position encoding and dynamic feature selection to increase model
representation ability. Notably, our method ranks 1st on both nuScenes and
SemanticKITTI semantic segmentation benchmarks with 81.9% and 74.8% mIoU,
respectively. Also, we achieve the 3rd place on nuScenes object detection
benchmark with 72.8% NDS and 68.5% mAP. Code is available at
https://github.com/dvlab-research/SphereFormer.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2023. Code is available at
  https://github.com/dvlab-research/SphereFormer.git</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncertainty Aware Active Learning for Reconfiguration of <span class="highlight-title">Pre-train</span>ed
  Deep Object-Detection Networks for New Target Domains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12760v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12760v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaming Na, Varuna De-Silva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object detection is one of the most important and fundamental aspects of
computer vision tasks, which has been broadly utilized in pose estimation,
object tracking and instance segmentation models. To obtain training data for
object detection model efficiently, many datasets opt to obtain their
unannotated data in video format and the annotator needs to draw a bounding box
around each object in the images. Annotating every frame from a video is costly
and inefficient since many frames contain very similar information for the
model to learn from. How to select the most informative frames from a video to
annotate has become a highly practical task to solve but attracted little
attention in research. In this paper, we proposed a novel active learning
algorithm for object detection models to tackle this problem. In the proposed
active learning algorithm, both classification and localization informativeness
of unlabelled data are measured and aggregated. Utilizing the temporal
information from video frames, two novel localization informativeness
measurements are proposed. Furthermore, a weight curve is proposed to avoid
querying adjacent frames. Proposed active learning algorithm with multiple
configurations was evaluated on the MuPoTS dataset and FootballPD dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MaskCon: Masked Contrastive Learning for Coarse-Labelled <span class="highlight-title">Dataset</span> <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12756v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12756v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Feng, Ioannis Patras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning has achieved great success in recent years with the aid of
advanced neural network structures and large-scale human-annotated datasets.
However, it is often costly and difficult to accurately and efficiently
annotate large-scale datasets, especially for some specialized domains where
fine-grained labels are required. In this setting, coarse labels are much
easier to acquire as they do not require expert knowledge. In this work, we
propose a contrastive learning method, called $\textbf{Mask}$ed
$\textbf{Con}$trastive learning~($\textbf{MaskCon}$) to address the
under-explored problem setting, where we learn with a coarse-labelled dataset
in order to address a finer labelling problem. More specifically, within the
contrastive learning framework, for each sample our method generates
soft-labels with the aid of coarse labels against other samples and another
augmented view of the sample in question. By contrast to self-supervised
contrastive learning where only the sample's augmentations are considered hard
positives, and in supervised contrastive learning where only samples with the
same coarse labels are considered hard positives, we propose soft labels based
on sample distances, that are masked by the coarse labels. This allows us to
utilize both inter-sample relations and coarse labels. We demonstrate that our
method can obtain as special cases many existing state-of-the-art works and
that it provides tighter bounds on the generalization error. Experimentally,
our method achieves significant improvement over the current state-of-the-art
in various datasets, including CIFAR10, CIFAR100, ImageNet-1K, Standford Online
Products and Stanford Cars196 datasets. Code and annotations are available at
https://github.com/MrChenFeng/MaskCon_CVPR2023.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023 camera-ready version. Codes are available at
  https://github.com/MrChenFeng/MaskCon_CVPR2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dense Network Expansion for Class Incremental Learning <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12696v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12696v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyuan Hu, Yunsheng Li, Jiancheng Lyu, Dashan Gao, Nuno Vasconcelos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The problem of class incremental learning (CIL) is considered.
State-of-the-art approaches use a dynamic architecture based on network
expansion (NE), in which a task expert is added per task. While effective from
a computational standpoint, these methods lead to models that grow quickly with
the number of tasks. A new NE method, dense network expansion (DNE), is
proposed to achieve a better trade-off between accuracy and model complexity.
This is accomplished by the introduction of dense connections between the
intermediate layers of the task expert networks, that enable the transfer of
knowledge from old to new tasks via feature sharing and reusing. This sharing
is implemented with a cross-task attention mechanism, based on a new task
attention block (TAB), that fuses information across tasks. Unlike traditional
attention mechanisms, TAB operates at the level of the feature mixing and is
decoupled with spatial attentions. This is shown more effective than a joint
spatial-and-task attention for CIL. The proposed DNE approach can strictly
maintain the feature space of old classes while growing the network and feature
scale at a much slower rate than previous methods. In result, it outperforms
the previous SOTA methods by a margin of 4\% in terms of accuracy, with similar
or even smaller model scale.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pix2Video: Video Editing using Image Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12688v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12688v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Duygu Ceylan, Chun-Hao Paul Huang, Niloy J. Mitra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image diffusion models, trained on massive image collections, have emerged as
the most versatile image generator model in terms of quality and diversity.
They support inverting real images and conditional (e.g., text) generation,
making them attractive for high-quality image editing applications. We
investigate how to use such pre-trained image models for text-guided video
editing. The critical challenge is to achieve the target edits while still
preserving the content of the source video. Our method works in two simple
steps: first, we use a pre-trained structure-guided (e.g., depth) image
diffusion model to perform text-guided edits on an anchor frame; then, in the
key step, we progressively propagate the changes to the future frames via
self-attention feature injection to adapt the core denoising step of the
diffusion model. We then consolidate the changes by adjusting the latent code
for the frame before continuing the process. Our approach is training-free and
generalizes to a wide range of edits. We demonstrate the effectiveness of the
approach by extensive experimentation and compare it against four different
prior and parallel efforts (on ArXiv). We demonstrate that realistic
text-guided video edits are possible, without any compute-intensive
preprocessing or video-specific finetuning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uni-Fusion: Universal Continuous Mapping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12678v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12678v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yijun Yuan, Andreas Nuechter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Uni-Fusion, an universal continuous mapping framework for
surfaces, surface properties (color, infrared, etc.) and more (latent features
in CLIP embedding space, etc.). We propose the first Universal Implicit
Encoding model that supports encoding of both geometry and various types of
properties (RGB, infrared, feature and etc.) without the need for any training.
Based on that, our framework divides the point cloud into regular grid voxels
and produces a latent feature in each voxel to form a Latent Implicit Map (LIM)
for geometries and arbitrary properties. Then, by fusing a Local LIM of new
frame to Global LIM, an incremental reconstruction is approached. Encoded with
corresponding types of data, our Latent Implicit Map is capable to generate
continuous surfaces, surface properties fields, surface feature fields and any
other possible options. To demonstrate the capabilities of our model, we
implement three applications: (1) incremental reconstruction for surfaces and
color (2) 2D-to-3D fabricated properties transfers (3) open-vocabulary scene
understanding by producing a text CLIP feature field on surfaces. We evaluate
Uni-Fusion by comparing in corresponding applications, from which, Uni-Fusion
shows high flexibility to various of application while performing best or
competitive. The project page of Uni-Fusion is available at
https://jarrome.github.io/Uni-Fusion/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://jarrome.github.io/Uni-Fusion/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VecFontSDF: Learning to Reconstruct and Synthesize High-quality Vector
  Fonts via Signed Distance Functions <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12675v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12675v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeqing Xia, Bojun Xiong, Zhouhui Lian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Font design is of vital importance in the digital content design and modern
printing industry. Developing algorithms capable of automatically synthesizing
vector fonts can significantly facilitate the font design process. However,
existing methods mainly concentrate on raster image generation, and only a few
approaches can directly synthesize vector fonts. This paper proposes an
end-to-end trainable method, VecFontSDF, to reconstruct and synthesize
high-quality vector fonts using signed distance functions (SDFs). Specifically,
based on the proposed SDF-based implicit shape representation, VecFontSDF
learns to model each glyph as shape primitives enclosed by several parabolic
curves, which can be precisely converted to quadratic B\'ezier curves that are
widely used in vector font products. In this manner, most image generation
methods can be easily extended to synthesize vector fonts. Qualitative and
quantitative experiments conducted on a publicly-available dataset demonstrate
that our method obtains high-quality results on several tasks, including vector
font reconstruction, interpolation, and few-shot vector font synthesis,
markedly outperforming the state of the art.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Integrating Image Features with Convolutional Sequence-to-sequence
  Network for Multilingual Visual Question Answering <span class="chip">SP2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12671v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12671v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Triet Minh Thai, Son T. Luu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Question Answering (VQA) is a task that requires computers to give
correct answers for the input questions based on the images. This task can be
solved by humans with ease but is a challenge for computers. The
VLSP2022-EVJVQA shared task carries the Visual Question Answering task in the
multilingual domain on a newly released dataset: UIT-EVJVQA, in which the
questions and answers are written in three different languages: English,
Vietnamese and Japanese. We approached the challenge as a sequence-to-sequence
learning task, in which we integrated hints from pre-trained state-of-the-art
VQA models and image features with Convolutional Sequence-to-Sequence network
to generate the desired answers. Our results obtained up to 0.3442 by F1 score
on the public test set, 0.4210 on the private test set, and placed 3rd in the
competition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>VLSP2022-EVJVQA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Correlational Image Modeling for <span class="highlight-title">Self-Supervised</span> Visual <span class="highlight-title">Pre-Train</span>ing <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12670v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12670v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Li, Jiahao Xie, Chen Change Loy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Correlational Image Modeling (CIM), a novel and surprisingly
effective approach to self-supervised visual pre-training. Our CIM performs a
simple pretext task: we randomly crop image regions (exemplars) from an input
image (context) and predict correlation maps between the exemplars and the
context. Three key designs enable correlational image modeling as a nontrivial
and meaningful self-supervisory task. First, to generate useful
exemplar-context pairs, we consider cropping image regions with various scales,
shapes, rotations, and transformations. Second, we employ a bootstrap learning
framework that involves online and target encoders. During pre-training, the
former takes exemplars as inputs while the latter converts the context. Third,
we model the output correlation maps via a simple cross-attention block, within
which the context serves as queries and the exemplars offer values and keys. We
show that CIM performs on par or better than the current state of the art on
self-supervised and transfer benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Extended Study of Human-like Behavior under Adversarial Training <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12669v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12669v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Gavrikov, Janis Keuper, Margret Keuper
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural networks have a number of shortcomings. Amongst the severest ones is
the sensitivity to distribution shifts which allows models to be easily fooled
into wrong predictions by small perturbations to inputs that are often
imperceivable to humans and do not have to carry semantic meaning. Adversarial
training poses a partial solution to address this issue by training models on
worst-case perturbations. Yet, recent work has also pointed out that the
reasoning in neural networks is different from humans. Humans identify objects
by shape, while neural nets mainly employ texture cues. Exemplarily, a model
trained on photographs will likely fail to generalize to datasets containing
sketches. Interestingly, it was also shown that adversarial training seems to
favorably increase the shift toward shape bias. In this work, we revisit this
observation and provide an extensive analysis of this effect on various
architectures, the common $\ell_2$- and $\ell_\infty$-training, and
Transformer-based models. Further, we provide a possible explanation for this
phenomenon from a frequency perspective.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, accepted at the CVPR 2023 Workshop "The 3rd Workshop of
  Adversarial Machine Learning on Computer Vision: Art of Robustness"</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reliable and Efficient Evaluation of Adversarial Robustness for Deep
  Hashing-Based Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12658v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12658v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xunguang Wang, Jiawang Bai, Xinyue Xu, Xiaomeng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep hashing has been extensively applied to massive image retrieval due to
its efficiency and effectiveness. Recently, several adversarial attacks have
been presented to reveal the vulnerability of deep hashing models against
adversarial examples. However, existing attack methods suffer from degraded
performance or inefficiency because they underutilize the semantic relations
between original samples or spend a lot of time learning these relations with a
deep neural network. In this paper, we propose a novel Pharos-guided Attack,
dubbed PgA, to evaluate the adversarial robustness of deep hashing networks
reliably and efficiently. Specifically, we design pharos code to represent the
semantics of the benign image, which preserves the similarity to semantically
relevant samples and dissimilarity to irrelevant ones. It is proven that we can
quickly calculate the pharos code via a simple math formula. Accordingly, PgA
can directly conduct a reliable and efficient attack on deep hashing-based
retrieval by maximizing the similarity between the hash code of the adversarial
example and the pharos code. Extensive experiments on the benchmark datasets
verify that the proposed algorithm outperforms the prior state-of-the-arts in
both attack strength and speed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2204.10779</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MI-SegNet: Mutual Information-Based US Segmentation for Unseen Domain
  Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12649v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12649v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Bi, Zhongliang Jiang, Ricarda Clarenbach, Reza Ghotbi, Angelos Karlas, Nassir Navab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generalization capabilities of learning-based medical image segmentation
across domains are currently limited by the performance degradation caused by
the domain shift, particularly for ultrasound (US) imaging. The quality of US
images heavily relies on carefully tuned acoustic parameters, which vary across
sonographers, machines, and settings. To improve the generalizability on US
images across domains, we propose MI-SegNet, a novel mutual information (MI)
based framework to explicitly disentangle the anatomical and domain feature
representations; therefore, robust domain-independent segmentation can be
expected. Two encoders are employed to extract the relevant features for the
disentanglement. The segmentation only uses the anatomical feature map for its
prediction. In order to force the encoders to learn meaningful feature
representations a cross-reconstruction method is used during training.
Transformations, specific to either domain or anatomy are applied to guide the
encoders in their respective feature extraction task. Additionally, any MI
present in both feature maps is punished to further promote separate feature
spaces. We validate the generalizability of the proposed domain-independent
segmentation approach on several datasets with varying parameters and machines.
Furthermore, we demonstrate the effectiveness of the proposed MI-SegNet serving
as a pre-trained model by comparing it with state-of-the-art networks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Feature-Conditioned Cascaded Video Diffusion Models for Precise
  Echocardiogram Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12644v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12644v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hadrien Reynaud, Mengyun Qiao, Mischa Dombrowski, Thomas Day, Reza Razavi, Alberto Gomez, Paul Leeson, Bernhard Kainz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image synthesis is expected to provide value for the translation of machine
learning methods into clinical practice. Fundamental problems like model
robustness, domain transfer, causal modelling, and operator training become
approachable through synthetic data. Especially, heavily operator-dependant
modalities like Ultrasound imaging require robust frameworks for image and
video generation. So far, video generation has only been possible by providing
input data that is as rich as the output data, e.g., image sequence plus
conditioning in, video out. However, clinical documentation is usually scarce
and only single images are reported and stored, thus retrospective
patient-specific analysis or the generation of rich training data becomes
impossible with current approaches. In this paper, we extend elucidated
diffusion models for video modelling to generate plausible video sequences from
single images and arbitrary conditioning with clinical parameters. We explore
this idea within the context of echocardiograms by looking into the variation
of the Left Ventricle Ejection Fraction, the most essential clinical metric
gained from these examinations. We use the publicly available EchoNet-Dynamic
dataset for all our experiments. Our image to sequence approach achieves an R2
score of 93%, which is 38 points higher than recently proposed sequence to
sequence generation methods. A public demo is available here: bit.ly/3HTskPF.
Code and models will be available at:
https://github.com/HReynaud/EchoDiffusion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reveal to Revise: An Explainable AI Life Cycle for Iterative Bias
  Correction of Deep Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12641v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12641v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Frederik Pahde, Maximilian Dreyer, Wojciech Samek, Sebastian Lapuschkin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art machine learning models often learn spurious correlations
embedded in the training data. This poses risks when deploying these models for
high-stake decision-making, such as in medical applications like skin cancer
detection. To tackle this problem, we propose Reveal to Revise (R2R), a
framework entailing the entire eXplainable Artificial Intelligence (XAI) life
cycle, enabling practitioners to iteratively identify, mitigate, and
(re-)evaluate spurious model behavior with a minimal amount of human
interaction. In the first step (1), R2R reveals model weaknesses by finding
outliers in attributions or through inspection of latent concepts learned by
the model. Secondly (2), the responsible artifacts are detected and spatially
localized in the input data, which is then leveraged to (3) revise the model
behavior. Concretely, we apply the methods of RRR, CDEP and ClArC for model
correction, and (4) (re-)evaluate the model's performance and remaining
sensitivity towards the artifact. Using two medical benchmark datasets for
Melanoma detection and bone age estimation, we apply our R2R framework to VGG,
ResNet and EfficientNet architectures and thereby reveal and correct real
dataset-intrinsic artifacts, as well as synthetic variants in a controlled
setting. Completing the XAI life cycle, we demonstrate multiple R2R iterations
to mitigate different biases. Code is available on
https://github.com/maxdreyer/Reveal2Revise.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OcTr: Octree-based <span class="highlight-title">Transformer</span> for 3D Object Detection <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12621v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12621v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Zhou, Yanan Zhang, Jiaxin Chen, Di Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A key challenge for LiDAR-based 3D object detection is to capture sufficient
features from large scale 3D scenes especially for distant or/and occluded
objects. Albeit recent efforts made by Transformers with the long sequence
modeling capability, they fail to properly balance the accuracy and efficiency,
suffering from inadequate receptive fields or coarse-grained holistic
correlations. In this paper, we propose an Octree-based Transformer, named
OcTr, to address this issue. It first constructs a dynamic octree on the
hierarchical feature pyramid through conducting self-attention on the top level
and then recursively propagates to the level below restricted by the octants,
which captures rich global context in a coarse-to-fine manner while maintaining
the computational complexity under control. Furthermore, for enhanced
foreground perception, we propose a hybrid positional embedding, composed of
the semantic-aware positional embedding and attention mask, to fully exploit
semantic and geometry clues. Extensive experiments are conducted on the Waymo
Open Dataset and KITTI Dataset, and OcTr reaches newly state-of-the-art
results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Perceptual Quality Assessment Exploration for AIGC Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12618v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12618v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zicheng Zhang, Chunyi Li, Wei Sun, Xiaohong Liu, Xiongkuo Min, Guangtao Zhai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  \underline{AI} \underline{G}enerated \underline{C}ontent (\textbf{AIGC}) has
gained widespread attention with the increasing efficiency of deep learning in
content creation. AIGC, created with the assistance of artificial intelligence
technology, includes various forms of content, among which the AI-generated
images (AGIs) have brought significant impact to society and have been applied
to various fields such as entertainment, education, social media, etc. However,
due to hardware limitations and technical proficiency, the quality of AIGC
images (AGIs) varies, necessitating refinement and filtering before practical
use. Consequently, there is an urgent need for developing objective models to
assess the quality of AGIs. Unfortunately, no research has been carried out to
investigate the perceptual quality assessment for AGIs specifically. Therefore,
in this paper, we first discuss the major evaluation aspects such as technical
issues, AI artifacts, unnaturalness, discrepancy, and aesthetics for AGI
quality assessment. Then we present the first perceptual AGI quality assessment
database, AGIQA-1K, which consists of 1,080 AGIs generated from diffusion
models. A well-organized subjective experiment is followed to collect the
quality labels of the AGIs. Finally, we conduct a benchmark experiment to
evaluate the performance of current image quality assessment (IQA) models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-view Feature Extraction based on Triple Contrastive Heads 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12615v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12615v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongjie Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-view feature extraction is an efficient approach for alleviating the
issue of dimensionality in highdimensional multi-view data. Contrastive
learning (CL), which is a popular self-supervised learning method, has recently
attracted considerable attention. In this study, we propose a novel multi-view
feature extraction method based on triple contrastive heads, which combines the
sample-, recovery- , and feature-level contrastive losses to extract the
sufficient yet minimal subspace discriminative information in compliance with
information bottleneck principle. In MFETCH, we construct the feature-level
contrastive loss, which removes the redundent information in the consistency
information to achieve the minimality of the subspace discriminative
information. Moreover, the recovery-level contrastive loss is also constructed
in MFETCH, which captures the view-specific discriminative information to
achieve the sufficiency of the subspace discriminative information.The
numerical experiments demonstrate that the proposed method offers a strong
advantage for multi-view feature extraction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2302.03932</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RaBit: Parametric Modeling of 3D Biped Cartoon Characters with a
  Topological-consistent <span class="highlight-title">Dataset</span> <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12564v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12564v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongjin Luo, Shengcai Cai, Jinguo Dong, Ruibo Ming, Liangdong Qiu, Xiaohang Zhan, Xiaoguang Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Assisting people in efficiently producing visually plausible 3D characters
has always been a fundamental research topic in computer vision and computer
graphics. Recent learning-based approaches have achieved unprecedented accuracy
and efficiency in the area of 3D real human digitization. However, none of the
prior works focus on modeling 3D biped cartoon characters, which are also in
great demand in gaming and filming. In this paper, we introduce 3DBiCar, the
first large-scale dataset of 3D biped cartoon characters, and RaBit, the
corresponding parametric model. Our dataset contains 1,500 topologically
consistent high-quality 3D textured models which are manually crafted by
professional artists. Built upon the data, RaBit is thus designed with a
SMPL-like linear blend shape model and a StyleGAN-based neural UV-texture
generator, simultaneously expressing the shape, pose, and texture. To
demonstrate the practicality of 3DBiCar and RaBit, various applications are
conducted, including single-view reconstruction, sketch-based modeling, and 3D
cartoon animation. For the single-view reconstruction setting, we find a
straightforward global mapping from input images to the output UV-based texture
maps tends to lose detailed appearances of some local parts (e.g., nose, ears).
Thus, a part-sensitive texture reasoner is adopted to make all important local
areas perceived. Experiments further demonstrate the effectiveness of our
method both qualitatively and quantitatively. 3DBiCar and RaBit are available
at gaplab.cuhk.edu.cn/projects/RaBit.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023, Project page: https://gaplab.cuhk.edu.cn/projects/RaBit/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Q-HyViT: Post-Training Quantization for Hybrid Vision <span class="highlight-title">Transformer</span> with
  Bridge Block Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12557v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12557v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jemin Lee, Yongin Kwon, Jeman Park, Misun Yu, Hwanjun Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, vision transformers (ViT) have replaced convolutional neural
network models in numerous tasks, including classification, detection, and
segmentation. However, the high computational requirements of ViTs hinder their
widespread implementation. To address this issue, researchers have proposed
efficient hybrid transformer architectures that combine convolutional and
transformer layers and optimize attention computation for linear complexity.
Additionally, post-training quantization has been proposed as a means of
mitigating computational demands. Combining quantization techniques and
efficient hybrid transformer structures is crucial to maximize the acceleration
of vision transformers on mobile devices. However, no prior investigation has
applied quantization to efficient hybrid transformers. In this paper, at first,
we discover that the straightforward manner to apply the existing PTQ methods
for ViT to efficient hybrid transformers results in a drastic accuracy drop due
to the following challenges: (i) highly dynamic ranges, (ii) zero-point
overflow, (iii) diverse normalization, and (iv) limited model parameters (<5M).
To overcome these challenges, we propose a new post-training quantization
method, which is the first to quantize efficient hybrid vision transformers
(MobileViTv1 and MobileViTv2) with a significant margin (an average improvement
of 7.75%) compared to existing PTQ methods (EasyQuant, FQ-ViT, and PTQ4ViT). We
plan to release our code at https://github.com/Q-HyViT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deployment of Image Analysis Algorithms under Prevalence Shifts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12540v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12540v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patrick Godau, Piotr Kalinowski, Evangelia Christodoulou, Annika Reinke, Minu Tizabi, Luciana Ferrer, Paul Jäger, Lena Maier-Hein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain gaps are among the most relevant roadblocks in the clinical
translation of machine learning (ML)-based solutions for medical image
analysis. While current research focuses on new training paradigms and network
architectures, little attention is given to the specific effect of prevalence
shifts on an algorithm deployed in practice. Such discrepancies between class
frequencies in the data used for a method's development/validation and that in
its deployment environment(s) are of great importance, for example in the
context of artificial intelligence (AI) democratization, as disease prevalences
may vary widely across time and location. Our contribution is twofold. First,
we empirically demonstrate the potentially severe consequences of missing
prevalence handling by analyzing (i) the extent of miscalibration, (ii) the
deviation of the decision threshold from the optimum, and (iii) the ability of
validation metrics to reflect neural network performance on the deployment
population as a function of the discrepancy between development and deployment
prevalence. Second, we propose a workflow for prevalence-aware image
classification that uses estimated deployment prevalences to adjust a trained
classifier to a new environment, without requiring additional annotated
deployment data. Comprehensive experiments based on a diverse set of 30 medical
classification tasks showcase the benefit of the proposed workflow in
generating better classifier decisions and more reliable performance estimates
compared to current practice.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pixel-wise Agricultural Image Time Series Classification: Comparisons
  and a Deformable Prototype-based Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12533v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12533v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elliot Vincent, Jean Ponce, Mathieu Aubry
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Improvements in Earth observation by satellites allow for imagery of ever
higher temporal and spatial resolution. Leveraging this data for agricultural
monitoring is key for addressing environmental and economic challenges. Current
methods for crop segmentation using temporal data either rely on annotated data
or are heavily engineered to compensate the lack of supervision. In this paper,
we present and compare datasets and methods for both supervised and
unsupervised pixel-wise segmentation of satellite image time series (SITS). We
also introduce an approach to add invariance to spectral deformations and
temporal shifts to classical prototype-based methods such as K-means and
Nearest Centroid Classifier (NCC). We show this simple and highly interpretable
method leads to meaningful results in both the supervised and unsupervised
settings and significantly improves the state of the art for unsupervised
classification of agricultural time series on four recent SITS datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sibling-Attack: Rethinking Transferable Adversarial Attacks against Face
  Recognition <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12512v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12512v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zexin Li, Bangjie Yin, Taiping Yao, Juefeng Guo, Shouhong Ding, Simin Chen, Cong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A hard challenge in developing practical face recognition (FR) attacks is due
to the black-box nature of the target FR model, i.e., inaccessible gradient and
parameter information to attackers. While recent research took an important
step towards attacking black-box FR models through leveraging transferability,
their performance is still limited, especially against online commercial FR
systems that can be pessimistic (e.g., a less than 50% ASR--attack success rate
on average). Motivated by this, we present Sibling-Attack, a new FR attack
technique for the first time explores a novel multi-task perspective (i.e.,
leveraging extra information from multi-correlated tasks to boost attacking
transferability). Intuitively, Sibling-Attack selects a set of tasks correlated
with FR and picks the Attribute Recognition (AR) task as the task used in
Sibling-Attack based on theoretical and quantitative analysis. Sibling-Attack
then develops an optimization framework that fuses adversarial gradient
information through (1) constraining the cross-task features to be under the
same space, (2) a joint-task meta optimization framework that enhances the
gradient compatibility among tasks, and (3) a cross-task gradient stabilization
method which mitigates the oscillation effect during attacking. Extensive
experiments demonstrate that Sibling-Attack outperforms state-of-the-art FR
attack techniques by a non-trivial margin, boosting ASR by 12.61% and 55.77% on
average on state-of-the-art pre-trained FR models and two well-known, widely
used commercial FR systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 fivures, accepted by CVPR 2023 as a poster paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cross-Modal Implicit Relation Reasoning and Aligning for Text-to-Image
  Person Retrieval <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12501v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12501v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ding Jiang, Mang Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image person retrieval aims to identify the target person based on a
given textual description query. The primary challenge is to learn the mapping
of visual and textual modalities into a common latent space. Prior works have
attempted to address this challenge by leveraging separately pre-trained
unimodal models to extract visual and textual features. However, these
approaches lack the necessary underlying alignment capabilities required to
match multimodal data effectively. Besides, these works use prior information
to explore explicit part alignments, which may lead to the distortion of
intra-modality information. To alleviate these issues, we present IRRA: a
cross-modal Implicit Relation Reasoning and Aligning framework that learns
relations between local visual-textual tokens and enhances global image-text
matching without requiring additional prior supervision. Specifically, we first
design an Implicit Relation Reasoning module in a masked language modeling
paradigm. This achieves cross-modal interaction by integrating the visual cues
into the textual tokens with a cross-modal multimodal interaction encoder.
Secondly, to globally align the visual and textual embeddings, Similarity
Distribution Matching is proposed to minimize the KL divergence between
image-text similarity distributions and the normalized label matching
distributions. The proposed method achieves new state-of-the-art results on all
three public datasets, with a notable margin of about 3%-9% for Rank-1 accuracy
compared to prior methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2023. Codes are available at this
  https://github.com/anosorae/IRRA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Domain-Specific <span class="highlight-title">Pre-Train</span>ing for Effective Semantic Perception in
  Agricultural Robotics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12499v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12499v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gianmarco Roggiolani, Federico Magistri, Tiziano Guadagnino, Jan Weyler, Giorgio Grisetti, Cyrill Stachniss, Jens Behley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Agricultural robots have the prospect to enable more efficient and
sustainable agricultural production of food, feed, and fiber. Perception of
crops and weeds is a central component of agricultural robots that aim to
monitor fields and assess the plants as well as their growth stage in an
automatic manner. Semantic perception mostly relies on deep learning using
supervised approaches, which require time and qualified workers to label fairly
large amounts of data. In this paper, we look into the problem of reducing the
amount of labels without compromising the final segmentation performance. For
robots operating in the field, pre-training networks in a supervised way is
already a popular method to reduce the number of required labeled images. We
investigate the possibility of pre-training in a self-supervised fashion using
data from the target domain. To better exploit this data, we propose a set of
domain-specific augmentation strategies. We evaluate our pre-training on
semantic segmentation and leaf instance segmentation, two important tasks in
our domain. The experimental results suggest that pre-training with
domain-specific data paired with our data augmentation strategy leads to
superior performance compared to commonly used pre-trainings. Furthermore, the
pre-trained networks obtain similar performance to the fully supervised with
less labeled data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Label-Efficient Deep Learning in Medical Image Analysis: Challenges and
  Future Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12484v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12484v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng Jin, Zhengrui Guo, Yi Lin, Luyang Luo, Hao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning has seen rapid growth in recent years and achieved
state-of-the-art performance in a wide range of applications. However, training
models typically requires expensive and time-consuming collection of large
quantities of labeled data. This is particularly true within the scope of
medical imaging analysis (MIA), where data are limited and labels are expensive
to be acquired. Thus, label-efficient deep learning methods are developed to
make comprehensive use of the labeled data as well as the abundance of
unlabeled and weak-labeled data. In this survey, we extensively investigated
over 300 recent papers to provide a comprehensive overview of recent progress
on label-efficient learning strategies in MIA. We first present the background
of label-efficient learning and categorize the approaches into different
schemes. Next, we examine the current state-of-the-art methods in detail
through each scheme. Specifically, we provide an in-depth investigation,
covering not only canonical semi-supervised, self-supervised, and
multi-instance learning schemes, but also recently emerged active and
annotation-efficient learning strategies. Moreover, as a comprehensive
contribution to the field, this survey not only elucidates the commonalities
and unique features of the surveyed methods but also presents a detailed
analysis of the current challenges in the field and suggests potential avenues
for future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MEDIMP: Medical Images and <span class="highlight-title">Prompt</span>s for renal transplant representation
  learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12445v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12445v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leo Milecki, Vicky Kalogeiton, Sylvain Bodard, Dany Anglicheau, Jean-Michel Correas, Marc-Olivier Timsit, Maria Vakalopoulou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Renal transplantation emerges as the most effective solution for end-stage
renal disease. Occurring from complex causes, a substantial risk of transplant
chronic dysfunction persists and may lead to graft loss. Medical imaging plays
a substantial role in renal transplant monitoring in clinical practice.
However, graft supervision is multi-disciplinary, notably joining nephrology,
urology, and radiology, while identifying robust biomarkers from such
high-dimensional and complex data for prognosis is challenging. In this work,
taking inspiration from the recent success of Large Language Models (LLMs), we
propose MEDIMP -- Medical Images and Prompts -- a model to learn meaningful
multi-modal representations of renal transplant Dynamic Contrast-Enhanced
Magnetic Resonance Imaging (DCE MRI) by incorporating structural
clinicobiological data after translating them into text prompts. MEDIMP is
based on contrastive learning from joint text-image paired embeddings to
perform this challenging task. Moreover, we propose a framework that generates
medical prompts using automatic textual data augmentations from LLMs. Our goal
is to learn meaningful manifolds of renal transplant DCE MRI, interesting for
the prognosis of the transplant or patient status (2, 3, and 4 years after the
transplant), fully exploiting the available multi-modal data in the most
efficient way. Extensive experiments and comparisons with other renal
transplant representation learning methods with limited data prove the
effectiveness of MEDIMP in a relevant clinical setting, giving new directions
toward medical prompts. Our code is available at
https://github.com/leomlck/MEDIMP.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised Domain Adaptation for Training Event-Based Networks Using
  Contrastive Learning and Uncorrelated Conditioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12424v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12424v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dayuan Jian, Mohammad Rostami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event-based cameras offer reliable measurements for preforming computer
vision tasks in high-dynamic range environments and during fast motion
maneuvers. However, adopting deep learning in event-based vision faces the
challenge of annotated data scarcity due to recency of event cameras.
Transferring the knowledge that can be obtained from conventional camera
annotated data offers a practical solution to this challenge. We develop an
unsupervised domain adaptation algorithm for training a deep network for
event-based data image classification using contrastive learning and
uncorrelated conditioning of data. Our solution outperforms the existing
algorithms for this purpose.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Text with Knowledge Graph Augmented <span class="highlight-title">Transformer</span> for Video Captioning <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12423v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12423v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Gu, Guang Chen, Yufei Wang, Libo Zhang, Tiejian Luo, Longyin Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video captioning aims to describe the content of videos using natural
language. Although significant progress has been made, there is still much room
to improve the performance for real-world applications, mainly due to the
long-tail words challenge. In this paper, we propose a text with knowledge
graph augmented transformer (TextKG) for video captioning. Notably, TextKG is a
two-stream transformer, formed by the external stream and internal stream. The
external stream is designed to absorb additional knowledge, which models the
interactions between the additional knowledge, e.g., pre-built knowledge graph,
and the built-in information of videos, e.g., the salient object regions,
speech transcripts, and video captions, to mitigate the long-tail words
challenge. Meanwhile, the internal stream is designed to exploit the
multi-modality information in videos (e.g., the appearance of video frames,
speech transcripts, and video captions) to ensure the quality of caption
results. In addition, the cross attention mechanism is also used in between the
two streams for sharing information. In this way, the two streams can help each
other for more accurate results. Extensive experiments conducted on four
challenging video captioning datasets, i.e., YouCookII, ActivityNet Captions,
MSRVTT, and MSVD, demonstrate that the proposed method performs favorably
against the state-of-the-art methods. Specifically, the proposed TextKG method
outperforms the best published results by improving 18.7% absolute CIDEr scores
on the YouCookII dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Region-wise matching for image inpainting based on adaptive weighted
  low-rank decomposition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12421v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12421v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shenghai Liao, Xuya Liu, Ruyi Han, Shujun Fu, Yuanfeng Zhou, Yuliang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Digital image inpainting is an interpolation problem, inferring the content
in the missing (unknown) region to agree with the known region data such that
the interpolated result fulfills some prior knowledge. Low-rank and nonlocal
self-similarity are two important priors for image inpainting. Based on the
nonlocal self-similarity assumption, an image is divided into overlapped square
target patches (submatrices) and the similar patches of any target patch are
reshaped as vectors and stacked into a patch matrix. Such a patch matrix
usually enjoys a property of low rank or approximately low rank, and its
missing entries are recoveried by low-rank matrix approximation (LRMA)
algorithms. Traditionally, $n$ nearest neighbor similar patches are searched
within a local window centered at a target patch. However, for an image with
missing lines, the generated patch matrix is prone to having entirely-missing
rows such that the downstream low-rank model fails to reconstruct it well. To
address this problem, we propose a region-wise matching (RwM) algorithm by
dividing the neighborhood of a target patch into multiple subregions and then
search the most similar one within each subregion. A non-convex weighted
low-rank decomposition (NC-WLRD) model for LRMA is also proposed to reconstruct
all degraded patch matrices grouped by the proposed RwM algorithm. We solve the
proposed NC-WLRD model by the alternating direction method of multipliers
(ADMM) and analyze the convergence in detail. Numerous experiments on line
inpainting (entire-row/column missing) demonstrate the superiority of our
method over other competitive inpainting algorithms. Unlike other
low-rank-based matrix completion methods and inpainting algorithms, the
proposed model NC-WLRD is also effective for removing random-valued impulse
noise and structural noise (stripes).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>region-wise matching algorithm, image inpainting, 20 pages, 18
  figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BiCro: Noisy Correspondence Rectification for Multi-modality Data via
  Bi-directional Cross-modal Similarity Consistency <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12419v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12419v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuo Yang, Zhaopan Xu, Kai Wang, Yang You, Hongxun Yao, Tongliang Liu, Min Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As one of the most fundamental techniques in multimodal learning, cross-modal
matching aims to project various sensory modalities into a shared feature
space. To achieve this, massive and correctly aligned data pairs are required
for model training. However, unlike unimodal datasets, multimodal datasets are
extremely harder to collect and annotate precisely. As an alternative, the
co-occurred data pairs (e.g., image-text pairs) collected from the Internet
have been widely exploited in the area. Unfortunately, the cheaply collected
dataset unavoidably contains many mismatched data pairs, which have been proven
to be harmful to the model's performance. To address this, we propose a general
framework called BiCro (Bidirectional Cross-modal similarity consistency),
which can be easily integrated into existing cross-modal matching models and
improve their robustness against noisy data. Specifically, BiCro aims to
estimate soft labels for noisy data pairs to reflect their true correspondence
degree. The basic idea of BiCro is motivated by that -- taking image-text
matching as an example -- similar images should have similar textual
descriptions and vice versa. Then the consistency of these two similarities can
be recast as the estimated soft labels to train the matching model. The
experiments on three popular cross-modal matching datasets demonstrate that our
method significantly improves the noise-robustness of various matching models,
and surpass the state-of-the-art by a clear margin.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CLIP^2: Contrastive Language-Image-Point <span class="highlight-title">Pretrain</span>ing from Real-World
  Point Cloud Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12417v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12417v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihan Zeng, Chenhan Jiang, Jiageng Mao, Jianhua Han, Chaoqiang Ye, Qingqiu Huang, Dit-Yan Yeung, Zhen Yang, Xiaodan Liang, Hang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive Language-Image Pre-training, benefiting from large-scale
unlabeled text-image pairs, has demonstrated great performance in open-world
vision understanding tasks. However, due to the limited Text-3D data pairs,
adapting the success of 2D Vision-Language Models (VLM) to the 3D space remains
an open problem. Existing works that leverage VLM for 3D understanding
generally resort to constructing intermediate 2D representations for the 3D
data, but at the cost of losing 3D geometry information. To take a step toward
open-world 3D vision understanding, we propose Contrastive Language-Image-Point
Cloud Pretraining (CLIP^2) to directly learn the transferable 3D point cloud
representation in realistic scenarios with a novel proxy alignment mechanism.
Specifically, we exploit naturally-existed correspondences in 2D and 3D
scenarios, and build well-aligned and instance-based text-image-point proxies
from those complex scenarios. On top of that, we propose a cross-modal
contrastive objective to learn semantic and instance-level aligned point cloud
representation. Experimental results on both indoor and outdoor scenarios show
that our learned 3D representation has great transfer ability in downstream
tasks, including zero-shot and few-shot 3D recognition, which boosts the
state-of-the-art methods by large margins. Furthermore, we provide analyses of
the capability of different representations in real scenarios and present the
optional ensemble scheme.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Balanced Spherical Grid for Egocentric View Synthesis <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12408v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12408v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changwoon Choi, Sang Min Kim, Young Min Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present EgoNeRF, a practical solution to reconstruct large-scale
real-world environments for VR assets. Given a few seconds of casually captured
360 video, EgoNeRF can efficiently build neural radiance fields which enable
high-quality rendering from novel viewpoints. Motivated by the recent
acceleration of NeRF using feature grids, we adopt spherical coordinate instead
of conventional Cartesian coordinate. Cartesian feature grid is inefficient to
represent large-scale unbounded scenes because it has a spatially uniform
resolution, regardless of distance from viewers. The spherical parameterization
better aligns with the rays of egocentric images, and yet enables factorization
for performance enhancement. However, the na\"ive spherical grid suffers from
irregularities at two poles, and also cannot represent unbounded scenes. To
avoid singularities near poles, we combine two balanced grids, which results in
a quasi-uniform angular grid. We also partition the radial grid exponentially
and place an environment map at infinity to represent unbounded scenes.
Furthermore, with our resampling technique for grid-based methods, we can
increase the number of valid samples to train NeRF volume. We extensively
evaluate our method in our newly introduced synthetic and real-world egocentric
360 video datasets, and it consistently achieves state-of-the-art performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UMC: A Unified Bandwidth-efficient and Multi-resolution based
  Collaborative Perception Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12400v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12400v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianhang Wang, Guang Chen, Kai Chen, Zhengfa Liu, Bo Zhang, Alois Knoll, Changjun Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-agent collaborative perception (MCP) has recently attracted much
attention. It includes three key processes: communication for sharing,
collaboration for integration, and reconstruction for different downstream
tasks. Existing methods pursue designing the collaboration process alone,
ignoring their intrinsic interactions and resulting in suboptimal performance.
In contrast, we aim to propose a Unified Collaborative perception framework
named UMC, optimizing the communication, collaboration, and reconstruction
processes with the Multi-resolution technique. The communication introduces a
novel trainable multi-resolution and selective-region (MRSR) mechanism,
achieving higher quality and lower bandwidth. Then, a graph-based collaboration
is proposed, conducting on each resolution to adapt the MRSR. Finally, the
reconstruction integrates the multi-resolution collaborative features for
downstream tasks. Since the general metric can not reflect the performance
enhancement brought by MCP systematically, we introduce a brand-new evaluation
metric that evaluates the MCP from different perspectives. To verify our
algorithm, we conducted experiments on the V2X-Sim and OPV2V datasets. Our
quantitative and qualitative experiments prove that the proposed UMC greatly
outperforms the state-of-the-art collaborative perception approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multiscale Attention via Wavelet Neural Operators for Vision
  <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12398v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12398v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anahita Nekoozadeh, Mohammad Reza Ahmadzadeh, Zahra Mardani, Morteza Mardani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers have achieved widespread success in computer vision. At their
heart, there is a Self-Attention (SA) mechanism, an inductive bias that
associates each token in the input with every other token through a weighted
basis. The standard SA mechanism has quadratic complexity with the sequence
length, which impedes its utility to long sequences appearing in high
resolution vision. Recently, inspired by operator learning for PDEs, Adaptive
Fourier Neural Operators (AFNO) were introduced for high resolution attention
based on global convolution that is efficiently implemented via FFT. However,
the AFNO global filtering cannot well represent small and moderate scale
structures that commonly appear in natural images. To leverage the
coarse-to-fine scale structures we introduce a Multiscale Wavelet Attention
(MWA) by leveraging wavelet neural operators which incurs linear complexity in
the sequence size. We replace the attention in ViT with MWA and our experiments
with CIFAR and ImageNet classification demonstrate significant improvement over
alternative Fourier-based attentions such as AFNO and Global Filter Network
(GFN).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rigidity-Aware Detection for 6D Object Pose Estimation <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12396v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12396v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Hai, Rui Song, Jiaojiao Li, Mathieu Salzmann, Yinlin Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most recent 6D object pose estimation methods first use object detection to
obtain 2D bounding boxes before actually regressing the pose. However, the
general object detection methods they use are ill-suited to handle cluttered
scenes, thus producing poor initialization to the subsequent pose network. To
address this, we propose a rigidity-aware detection method exploiting the fact
that, in 6D pose estimation, the target objects are rigid. This lets us
introduce an approach to sampling positive object regions from the entire
visible object area during training, instead of naively drawing samples from
the bounding box center where the object might be occluded. As such, every
visible object part can contribute to the final bounding box prediction,
yielding better detection robustness. Key to the success of our approach is a
visibility map, which we propose to build using a minimum barrier distance
between every pixel in the bounding box and the box boundary. Our results on
seven challenging 6D pose estimation datasets evidence that our method
outperforms general detection frameworks by a large margin. Furthermore,
combined with a pose regression network, we obtain state-of-the-art pose
estimation results on the challenging BOP benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Road Extraction with Satellite Images and Partial Road Maps 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12394v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12394v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianxiong Xu, Cheng Long, Liang Yu, Chen Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Road extraction is a process of automatically generating road maps mainly
from satellite images. Existing models all target to generate roads from the
scratch despite that a large quantity of road maps, though incomplete, are
publicly available (e.g. those from OpenStreetMap) and can help with road
extraction. In this paper, we propose to conduct road extraction based on
satellite images and partial road maps, which is new. We then propose a
two-branch Partial to Complete Network (P2CNet) for the task, which has two
prominent components: Gated Self-Attention Module (GSAM) and Missing Part (MP)
loss. GSAM leverages a channel-wise self-attention module and a gate module to
capture long-range semantics, filter out useless information, and better fuse
the features from two branches. MP loss is derived from the partial road maps,
trying to give more attention to the road pixels that do not exist in partial
road maps. Extensive experiments are conducted to demonstrate the effectiveness
of our model, e.g. P2CNet achieves state-of-the-art performance with the IoU
scores of 70.71% and 75.52%, respectively, on the SpaceNet and OSM datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by IEEE Transactions on Geoscience and
  Remote Sensing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RegFormer: An Efficient Projection-Aware <span class="highlight-title">Transformer</span> Network for
  Large-Scale Point Cloud Registration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12384v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12384v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiuming Liu, Guangming Wang, Zhe Liu, Chaokang Jiang, Marc Pollefeys, Hesheng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although point cloud registration has achieved remarkable advances in
object-level and indoor scenes, large-scale registration methods are rarely
explored. Challenges mainly arise from the huge point number, complex
distribution, and outliers of outdoor LiDAR scans. In addition, most existing
registration works generally adopt a two-stage paradigm: They first find
correspondences by extracting discriminative local features, and then leverage
estimators (eg. RANSAC) to filter outliers, which are highly dependent on
well-designed descriptors and post-processing choices. To address these
problems, we propose an end-to-end transformer network (RegFormer) for
large-scale point cloud alignment without any further post-processing.
Specifically, a projection-aware hierarchical transformer is proposed to
capture long-range dependencies and filter outliers by extracting point
features globally. Our transformer has linear complexity, which guarantees high
efficiency even for large-scale scenes. Furthermore, to effectively reduce
mismatches, a bijective association transformer is designed for regressing the
initial transformation. Extensive experiments on KITTI and NuScenes datasets
demonstrate that our RegFormer achieves state-of-the-art performance in terms
of both accuracy and efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VMCML: Video and Music Matching via Cross-Modality Lifting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12379v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12379v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi-Shan Lee, Wei-Cheng Tseng, Fu-En Wang, Min Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a content-based system for matching video and background music.
The system aims to address the challenges in music recommendation for new users
or new music give short-form videos. To this end, we propose a cross-modal
framework VMCML that finds a shared embedding space between video and music
representations. To ensure the embedding space can be effectively shared by
both representations, we leverage CosFace loss based on margin-based cosine
similarity loss. Furthermore, we establish a large-scale dataset called MSVD,
in which we provide 390 individual music and the corresponding matched 150,000
videos. We conduct extensive experiments on Youtube-8M and our MSVD datasets.
Our quantitative and qualitative results demonstrate the effectiveness of our
proposed framework and achieve state-of-the-art video and music matching
performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ $P^{3}O$: Transferring Visual Representations for Reinforcement Learning
  via <span class="highlight-title">Prompt</span>ing <span class="chip">ICME</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12371v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12371v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guoliang You, Xiaomeng Chu, Yifan Duan, Jie Peng, Jianmin Ji, Yu Zhang, Yanyong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is important for deep reinforcement learning (DRL) algorithms to transfer
their learned policies to new environments that have different visual inputs.
In this paper, we introduce Prompt based Proximal Policy Optimization
($P^{3}O$), a three-stage DRL algorithm that transfers visual representations
from a target to a source environment by applying prompting. The process of
$P^{3}O$ consists of three stages: pre-training, prompting, and predicting. In
particular, we specify a prompt-transformer for representation conversion and
propose a two-step training process to train the prompt-transformer for the
target environment, while the rest of the DRL pipeline remains unchanged. We
implement $P^{3}O$ and evaluate it on the OpenAI CarRacing video game. The
experimental results show that $P^{3}O$ outperforms the state-of-the-art visual
transferring schemes. In particular, $P^{3}O$ allows the learned policies to
perform well in environments with different visual inputs, which is much more
effective than retraining the policies in these environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted to be presented at the upcoming IEEE
  International Conference on Multimedia & Expo (ICME) in 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Weakly Supervised Video Representation Learning with Unaligned Text for
  Sequential Videos <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12370v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12370v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sixun Dong, Huazhang Hu, Dongze Lian, Weixin Luo, Yicheng Qian, Shenghua Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential video understanding, as an emerging video understanding task, has
driven lots of researchers' attention because of its goal-oriented nature. This
paper studies weakly supervised sequential video understanding where the
accurate time-stamp level text-video alignment is not provided. We solve this
task by borrowing ideas from CLIP. Specifically, we use a transformer to
aggregate frame-level features for video representation and use a pre-trained
text encoder to encode the texts corresponding to each action and the whole
video, respectively. To model the correspondence between text and video, we
propose a multiple granularity loss, where the video-paragraph contrastive loss
enforces matching between the whole video and the complete script, and a
fine-grained frame-sentence contrastive loss enforces the matching between each
action and its description. As the frame-sentence correspondence is not
available, we propose to use the fact that video actions happen sequentially in
the temporal domain to generate pseudo frame-sentence correspondence and
supervise the network training with the pseudo labels. Extensive experiments on
video sequence verification and text-to-video matching show that our method
outperforms baselines by a large margin, which validates the effectiveness of
our proposed approach. Code is available at https://github.com/svip-lab/WeakSVR
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023. Code: https://github.com/svip-lab/WeakSVR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unbiased Multiple Instance Learning for Weakly Supervised Video Anomaly
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12369v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12369v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hui Lv, Zhongqi Yue, Qianru Sun, Bin Luo, Zhen Cui, Hanwang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Weakly Supervised Video Anomaly Detection (WSVAD) is challenging because the
binary anomaly label is only given on the video level, but the output requires
snippet-level predictions. So, Multiple Instance Learning (MIL) is prevailing
in WSVAD. However, MIL is notoriously known to suffer from many false alarms
because the snippet-level detector is easily biased towards the abnormal
snippets with simple context, confused by the normality with the same bias, and
missing the anomaly with a different pattern. To this end, we propose a new MIL
framework: Unbiased MIL (UMIL), to learn unbiased anomaly features that improve
WSVAD. At each MIL training iteration, we use the current detector to divide
the samples into two groups with different context biases: the most confident
abnormal/normal snippets and the rest ambiguous ones. Then, by seeking the
invariant features across the two sample groups, we can remove the variant
context biases. Extensive experiments on benchmarks UCF-Crime and TAD
demonstrate the effectiveness of our UMIL. Our code is provided at
https://github.com/ktr-hubrt/UMIL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages,10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MAIR: Multi-view Attention Inverse Rendering with 3D Spatially-Varying
  Lighting Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12368v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12368v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        JunYong Choi, SeokYeong Lee, Haesol Park, Seung-Won Jung, Ig-Jae Kim, Junghyun Cho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a scene-level inverse rendering framework that uses multi-view
images to decompose the scene into geometry, a SVBRDF, and 3D spatially-varying
lighting. Because multi-view images provide a variety of information about the
scene, multi-view images in object-level inverse rendering have been taken for
granted. However, owing to the absence of multi-view HDR synthetic dataset,
scene-level inverse rendering has mainly been studied using single-view image.
We were able to successfully perform scene-level inverse rendering using
multi-view images by expanding OpenRooms dataset and designing efficient
pipelines to handle multi-view images, and splitting spatially-varying
lighting. Our experiments show that the proposed method not only achieves
better performance than single-view-based methods, but also achieves robust
performance on unseen real-world scene. Also, our sophisticated 3D
spatially-varying lighting volume allows for photorealistic object insertion in
any 3D location.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatically Predict Material Properties with Microscopic Image Example
  Polymer Compatibility 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12360v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12360v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhilong Liang, Zhenzhi Tan, Ruixin Hong, Wanli Ouyang, Jinying Yuan, Changshui Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many material properties are manifested in the morphological appearance and
characterized with microscopic image, such as scanning electron microscopy
(SEM). Polymer compatibility is a key physical quantity of polymer material and
commonly and intuitively judged by SEM images. However, human observation and
judgement for the images is time-consuming, labor-intensive and hard to be
quantified. Computer image recognition with machine learning method can make up
the defects of artificial judging, giving accurate and quantitative judgement.
We achieve automatic compatibility recognition utilizing convolution neural
network and transfer learning method, and the model obtains up to 94% accuracy.
We also put forward a quantitative criterion for polymer compatibility with
this model. The proposed method can be widely applied to the quantitative
characterization of the microstructure and properties of various materials.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NUWA-XL: Diffusion over Diffusion for eXtremely Long Video Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12346v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12346v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengming Yin, Chenfei Wu, Huan Yang, Jianfeng Wang, Xiaodong Wang, Minheng Ni, Zhengyuan Yang, Linjie Li, Shuguang Liu, Fan Yang, Jianlong Fu, Gong Ming, Lijuan Wang, Zicheng Liu, Houqiang Li, Nan Duan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose NUWA-XL, a novel Diffusion over Diffusion
architecture for eXtremely Long video generation. Most current work generates
long videos segment by segment sequentially, which normally leads to the gap
between training on short videos and inferring long videos, and the sequential
generation is inefficient. Instead, our approach adopts a ``coarse-to-fine''
process, in which the video can be generated in parallel at the same
granularity. A global diffusion model is applied to generate the keyframes
across the entire time range, and then local diffusion models recursively fill
in the content between nearby frames. This simple yet effective strategy allows
us to directly train on long videos (3376 frames) to reduce the
training-inference gap, and makes it possible to generate all segments in
parallel. To evaluate our model, we build FlintstonesHD dataset, a new
benchmark for long video generation. Experiments show that our model not only
generates high-quality long videos with both global and local coherence, but
also decreases the average inference time from 7.55min to 26s (by 94.26\%) at
the same hardware setting when generating 1024 frames. The homepage link is
\url{https://msra-nuwa.azurewebsites.net/}
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LD-ZNet: A Latent Diffusion Approach for Text-Based Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12343v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12343v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Koutilya Pnvr, Bharat Singh, Pallabi Ghosh, Behjat Siddiquie, David Jacobs
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a technique for segmenting real and AI-generated images using
latent diffusion models (LDMs) trained on internet-scale datasets. First, we
show that the latent space of LDMs (z-space) is a better input representation
compared to other feature representations like RGB images or CLIP encodings for
text-based image segmentation. By training the segmentation models on the
latent z-space, which creates a compressed representation across several
domains like different forms of art, cartoons, illustrations, and photographs,
we are also able to bridge the domain gap between real and AI-generated images.
We show that the internal features of LDMs contain rich semantic information
and present a technique in the form of LD-ZNet to further boost the performance
of text-based segmentation. Overall, we show up to 6% improvement over standard
baselines for text-to-image segmentation on natural images. For AI-generated
imagery, we show close to 20% improvement compared to state-of-the-art
techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Supplementary material is included in the paper following the
  references section</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ One-Step Detection Paradigm for Hyperspectral Anomaly Detection via
  Spectral Deviation Relationship Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12342v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12342v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingtao Li, Xinyu Wang, Shaoyu Wang, Hengwei Zhao, Liangpei Zhang, Yanfei Zhong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hyperspectral anomaly detection (HAD) involves identifying the targets that
deviate spectrally from their surroundings, without prior knowledge. Recently,
deep learning based methods have become the mainstream HAD methods, due to
their powerful spatial-spectral feature extraction ability. However, the
current deep detection models are optimized to complete a proxy task (two-step
paradigm), such as background reconstruction or generation, rather than
achieving anomaly detection directly. This leads to suboptimal results and poor
transferability, which means that the deep model is trained and tested on the
same image. In this paper, an unsupervised transferred direct detection (TDD)
model is proposed, which is optimized directly for the anomaly detection task
(one-step paradigm) and has transferability. Specially, the TDD model is
optimized to identify the spectral deviation relationship according to the
anomaly definition. Compared to learning the specific background distribution
as most models do, the spectral deviation relationship is universal for
different images and guarantees the model transferability. To train the TDD
model in an unsupervised manner, an anomaly sample simulation strategy is
proposed to generate numerous pairs of anomaly samples. Furthermore, a global
self-attention module and a local self-attention module are designed to help
the model focus on the "spectrally deviating" relationship. The TDD model was
validated on four public HAD datasets. The results show that the proposed TDD
model can successfully overcome the limitation of traditional model training
and testing on a single image, and the model has a powerful detection ability
and excellent transferability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Music-Driven Group Choreography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12337v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12337v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nhat Le, Thang Pham, Tuong Do, Erman Tjiputra, Quang D. Tran, Anh Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Music-driven choreography is a challenging problem with a wide variety of
industrial applications. Recently, many methods have been proposed to
synthesize dance motions from music for a single dancer. However, generating
dance motion for a group remains an open problem. In this paper, we present
$\rm AIOZ-GDANCE$, a new large-scale dataset for music-driven group dance
generation. Unlike existing datasets that only support single dance, our new
dataset contains group dance videos, hence supporting the study of group
choreography. We propose a semi-autonomous labeling method with humans in the
loop to obtain the 3D ground truth for our dataset. The proposed dataset
consists of $16.7$ hours of paired music and 3D motion from in-the-wild videos,
covering $7$ dance styles and $16$ music genres. We show that naively applying
single dance generation technique to creating group dance motion may lead to
unsatisfactory results, such as inconsistent movements and collisions between
dancers. Based on our new dataset, we propose a new method that takes an input
music sequence and a set of 3D positions of dancers to efficiently produce
multiple group-coherent choreographies. We propose new evaluation metrics for
measuring group dance quality and perform intensive experiments to demonstrate
the effectiveness of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted in cvpr 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Weakly-Supervised Temporal Action Localization by Inferring
  Snippet-Feature Affinity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12332v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12332v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wulian Yun, Mengshi Qi, Chuanming Wang, Huadong Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Weakly-supervised temporal action localization aims to locate action regions
and identify action categories in untrimmed videos, only taking video-level
labels as the supervised information. Pseudo label generation is a promising
strategy to solve the challenging problem, but most existing methods are
limited to employing snippet-wise classification results to guide the
generation, and they ignore that the natural temporal structure of the video
can also provide rich information to assist such a generation process. In this
paper, we propose a novel weakly-supervised temporal action localization method
by inferring snippet-feature affinity. First, we design an affinity inference
module that exploits the affinity relationship between temporal neighbor
snippets to generate initial coarse pseudo labels. Then, we introduce an
information interaction module that refines the coarse labels by enhancing the
discriminative nature of snippet-features through exploring intra- and
inter-video relationships. Finally, the high-fidelity pseudo labels generated
from the information interaction module are used to supervise the training of
the action localization network. Extensive experiments on two publicly
available datasets, i.e., THUMOS14 and ActivityNet v1.3, demonstrate our
proposed method achieves significant improvements compared to the
state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Make Encoder Great Again in 3D GAN Inversion through Geometry and
  Occlusion-Aware Encoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12326v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12326v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyang Yuan, Yiming Zhu, Yu Li, Hongyu Liu, Chun Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D GAN inversion aims to achieve high reconstruction fidelity and reasonable
3D geometry simultaneously from a single image input. However, existing 3D GAN
inversion methods rely on time-consuming optimization for each individual case.
In this work, we introduce a novel encoder-based inversion framework based on
EG3D, one of the most widely-used 3D GAN models. We leverage the inherent
properties of EG3D's latent space to design a discriminator and a background
depth regularization. This enables us to train a geometry-aware encoder capable
of converting the input image into corresponding latent code. Additionally, we
explore the feature space of EG3D and develop an adaptive refinement stage that
improves the representation ability of features in EG3D to enhance the recovery
of fine-grained textural details. Finally, we propose an occlusion-aware fusion
operation to prevent distortion in unobserved regions. Our method achieves
impressive results comparable to optimization-based methods while operating up
to 500 times faster. Our framework is well-suited for applications such as
semantic editing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://eg3d-goae.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Re-thinking Federated Active Learning based on Inter-class Diversity <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12317v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12317v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        SangMook Kim, Sangmin Bae, Hwanjun Song, Se-Young Yun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although federated learning has made awe-inspiring advances, most studies
have assumed that the client's data are fully labeled. However, in a real-world
scenario, every client may have a significant amount of unlabeled instances.
Among the various approaches to utilizing unlabeled data, a federated active
learning framework has emerged as a promising solution. In the decentralized
setting, there are two types of available query selector models, namely
'global' and 'local-only' models, but little literature discusses their
performance dominance and its causes. In this work, we first demonstrate that
the superiority of two selector models depends on the global and local
inter-class diversity. Furthermore, we observe that the global and local-only
models are the keys to resolving the imbalance of each side. Based on our
findings, we propose LoGo, a FAL sampling strategy robust to varying local
heterogeneity levels and global imbalance ratio, that integrates both models by
two steps of active selection scheme. LoGo consistently outperforms six active
learning strategies in the total number of 38 experimental settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distribution Aligned Diffusion and Prototype-guided network for
  Unsupervised Domain Adaptive Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12313v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12313v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haipeng Zhou, Lei Zhu, Yuyin Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Diffusion Probabilistic Model (DPM) has emerged as a highly effective
generative model in the field of computer vision. Its intermediate latent
vectors offer rich semantic information, making it an attractive option for
various downstream tasks such as segmentation and detection. In order to
explore its potential further, we have taken a step forward and considered a
more complex scenario in the medical image domain, specifically, under an
unsupervised adaptation condition. To this end, we propose a Diffusion-based
and Prototype-guided network (DP-Net) for unsupervised domain adaptive
segmentation. Concretely, our DP-Net consists of two stages: 1) Distribution
Aligned Diffusion (DADiff), which involves training a domain discriminator to
minimize the difference between the intermediate features generated by the DPM,
thereby aligning the inter-domain distribution; and 2) Prototype-guided
Consistency Learning (PCL), which utilizes feature centroids as prototypes and
applies a prototype-guided loss to ensure that the segmentor learns consistent
content from both source and target domains. Our approach is evaluated on
fundus datasets through a series of experiments, which demonstrate that the
performance of the proposed method is reliable and outperforms state-of-the-art
methods. Our work presents a promising direction for using DPM in complex
medical image scenarios, opening up new possibilities for further research in
medical imaging.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Curvature-Balanced Feature Manifold Learning for Long-Tailed
  Classification <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12307v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12307v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanbiao Ma, Licheng Jiao, Fang Liu, Shuyuan Yang, Xu Liu, Lingling Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To address the challenges of long-tailed classification, researchers have
proposed several approaches to reduce model bias, most of which assume that
classes with few samples are weak classes. However, recent studies have shown
that tail classes are not always hard to learn, and model bias has been
observed on sample-balanced datasets, suggesting the existence of other factors
that affect model bias. In this work, we systematically propose a series of
geometric measurements for perceptual manifolds in deep neural networks, and
then explore the effect of the geometric characteristics of perceptual
manifolds on classification difficulty and how learning shapes the geometric
characteristics of perceptual manifolds. An unanticipated finding is that the
correlation between the class accuracy and the separation degree of perceptual
manifolds gradually decreases during training, while the negative correlation
with the curvature gradually increases, implying that curvature imbalance leads
to model bias. Therefore, we propose curvature regularization to facilitate the
model to learn curvature-balanced and flatter perceptual manifolds. Evaluations
on multiple long-tailed and non-long-tailed datasets show the excellent
performance and exciting generality of our approach, especially in achieving
significant performance improvements based on current state-of-the-art
techniques. Our work opens up a geometric analysis perspective on model bias
and reminds researchers to pay attention to model bias on non-long-tailed and
even sample-balanced datasets. The code and model will be made public.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20pages, Accepted by CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SiamTHN: Siamese Target Highlight Network for Visual Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12304v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12304v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahao Bao, Kaiqiang Chen, Xian Sun, Liangjin Zhao, Wenhui Diao, Menglong Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Siamese network based trackers develop rapidly in the field of visual object
tracking in recent years. The majority of siamese network based trackers now in
use treat each channel in the feature maps generated by the backbone network
equally, making the similarity response map sensitive to background influence
and hence challenging to focus on the target region. Additionally, there are no
structural links between the classification and regression branches in these
trackers, and the two branches are optimized separately during training.
Therefore, there is a misalignment between the classification and regression
branches, which results in less accurate tracking results. In this paper, a
Target Highlight Module is proposed to help the generated similarity response
maps to be more focused on the target region. To reduce the misalignment and
produce more precise tracking results, we propose a corrective loss to train
the model. The two branches of the model are jointly tuned with the use of
corrective loss to produce more reliable prediction results. Experiments on 5
challenging benchmark datasets reveal that the method outperforms current
models in terms of performance, and runs at 38 fps, proving its effectiveness
and efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NLOS-NeuS: Non-line-of-sight Neural Implicit Surface 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12280v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12280v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuki Fujimura, Takahiro Kushida, Takuya Funatomi, Yasuhiro Mukaigawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Non-line-of-sight (NLOS) imaging is conducted to infer invisible scenes from
indirect light on visible objects. The neural transient field (NeTF) was
proposed for representing scenes as neural radiance fields in NLOS scenes. We
propose NLOS neural implicit surface (NLOS-NeuS), which extends the NeTF to
neural implicit surfaces with a signed distance function (SDF) for
reconstructing three-dimensional surfaces in NLOS scenes. We introduce two
constraints as loss functions for correctly learning an SDF to avoid non-zero
level-set surfaces. We also introduce a lower bound constraint of an SDF based
on the geometry of the first-returning photons. The experimental results
indicate that these constraints are essential for learning a correct SDF in
NLOS scenes. Compared with previous methods with discretized representation,
NLOS-NeuS with the neural continuous representation enables us to reconstruct
smooth surfaces while preserving fine details in NLOS scenes. To the best of
our knowledge, this is the first study on neural implicit surfaces with volume
rendering in NLOS scenes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Hierarchical Hybrid Learning Framework for Multi-agent Trajectory
  Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12274v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12274v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujun Jiao, Mingze Miao, Zhishuai Yin, Chunyuan Lei, Xu Zhu, Linzhen Nie, Bo Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate and robust trajectory prediction of neighboring agents is critical
for autonomous vehicles traversing in complex scenes. Most methods proposed in
recent years are deep learning-based due to their strength in encoding complex
interactions. However, unplausible predictions are often generated since they
rely heavily on past observations and cannot effectively capture the transient
and contingency interactions from sparse samples. In this paper, we propose a
hierarchical hybrid framework of deep learning (DL) and reinforcement learning
(RL) for multi-agent trajectory prediction, to cope with the challenge of
predicting motions shaped by multi-scale interactions. In the DL stage, the
traffic scene is divided into multiple intermediate-scale heterogenous graphs
based on which Transformer-style GNNs are adopted to encode heterogenous
interactions at intermediate and global levels. In the RL stage, we divide the
traffic scene into local sub-scenes utilizing the key future points predicted
in the DL stage. To emulate the motion planning procedure so as to produce
trajectory predictions, a Transformer-based Proximal Policy Optimization (PPO)
incorporated with a vehicle kinematics model is devised to plan motions under
the dominant influence of microscopic interactions. A multi-objective reward is
designed to balance between agent-centric accuracy and scene-wise
compatibility. Experimental results show that our proposal matches the
state-of-the-arts on the Argoverse forecasting benchmark. It's also revealed by
the visualized results that the hierarchical learning framework captures the
multi-scale interactions and improves the feasibility and compliance of the
predicted trajectories.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EBSR: Enhanced Binary Neural Network for Image Super-Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12270v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12270v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renjie Wei, Shuwen Zhang, Zechun Liu, Meng Li, Yuchen Fan, Runsheng Wang, Ru Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While the performance of deep convolutional neural networks for image
super-resolution (SR) has improved significantly, the rapid increase of memory
and computation requirements hinders their deployment on resource-constrained
devices. Quantized networks, especially binary neural networks (BNN) for SR
have been proposed to significantly improve the model inference efficiency but
suffer from large performance degradation. We observe the activation
distribution of SR networks demonstrates very large pixel-to-pixel,
channel-to-channel, and image-to-image variation, which is important for high
performance SR but gets lost during binarization. To address the problem, we
propose two effective methods, including the spatial re-scaling as well as
channel-wise shifting and re-scaling, which augments binary convolutions by
retaining more spatial and channel-wise information. Our proposed models,
dubbed EBSR, demonstrate superior performance over prior art methods both
quantitatively and qualitatively across different datasets and different model
sizes. Specifically, for x4 SR on Set5 and Urban100, EBSRlight improves the
PSNR by 0.31 dB and 0.28 dB compared to SRResNet-E2FIF, respectively, while
EBSR outperforms EDSR-E2FIF by 0.29 dB and 0.32 dB PSNR, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AUTO: Adaptive Outlier Optimization for Online Test-Time OOD Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12267v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12267v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Puning Yang, Jian Liang, Jie Cao, Ran He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Out-of-distribution (OOD) detection is a crucial aspect of deploying machine
learning models in open-world applications. Empirical evidence suggests that
training with auxiliary outliers substantially improves OOD detection. However,
such outliers typically exhibit a distribution gap compared to the test OOD
data and do not cover all possible test OOD scenarios. Additionally,
incorporating these outliers introduces additional training burdens. In this
paper, we introduce a novel paradigm called test-time OOD detection, which
utilizes unlabeled online data directly at test time to improve OOD detection
performance. While this paradigm is efficient, it also presents challenges such
as catastrophic forgetting. To address these challenges, we propose adaptive
outlier optimization (AUTO), which consists of an in-out-aware filter, an ID
memory bank, and a semantically-consistent objective. AUTO adaptively mines
pseudo-ID and pseudo-OOD samples from test data, utilizing them to optimize
networks in real time during inference. Extensive results on CIFAR-10,
CIFAR-100, and ImageNet benchmarks demonstrate that AUTO significantly enhances
OOD detection performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Encoding Binary Concepts in the Latent Space of Generative Models for
  Enhancing Data Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12255v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12255v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zizhao Hu, Mohammad Rostami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Binary concepts are empirically used by humans to generalize efficiently. And
they are based on Bernoulli distribution which is the building block of
information. These concepts span both low-level and high-level features such as
"large vs small" and "a neuron is active or inactive". Binary concepts are
ubiquitous features and can be used to transfer knowledge to improve model
generalization. We propose a novel binarized regularization to facilitate
learning of binary concepts to improve the quality of data generation in
autoencoders. We introduce a binarizing hyperparameter $r$ in data generation
process to disentangle the latent space symmetrically. We demonstrate that this
method can be applied easily to existing variational autoencoder (VAE) variants
to encourage symmetric disentanglement, improve reconstruction quality, and
prevent posterior collapse without computation overhead. We also demonstrate
that this method can boost existing models to learn more transferable
representations and generate more representative samples for the input
distribution which can alleviate catastrophic forgetting using generative
replay under continual learning settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ State-of-the-art optical-based physical adversarial attacks for deep
  learning computer vision systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12249v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12249v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junbin Fang, You Jiang, Canjian Jiang, Zoe L. Jiang, Siu-Ming Yiu, Chuanyi Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial attacks can mislead deep learning models to make false
predictions by implanting small perturbations to the original input that are
imperceptible to the human eye, which poses a huge security threat to the
computer vision systems based on deep learning. Physical adversarial attacks,
which is more realistic, as the perturbation is introduced to the input before
it is being captured and converted to a binary image inside the vision system,
when compared to digital adversarial attacks. In this paper, we focus on
physical adversarial attacks and further classify them into invasive and
non-invasive. Optical-based physical adversarial attack techniques (e.g. using
light irradiation) belong to the non-invasive category. As the perturbations
can be easily ignored by humans as the perturbations are very similar to the
effects generated by a natural environment in the real world. They are highly
invisibility and executable and can pose a significant or even lethal threats
to real systems. This paper focuses on optical-based physical adversarial
attack techniques for computer vision systems, with emphasis on the
introduction and discussion of optical-based physical adversarial attack
techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the Benefits of Visual <span class="highlight-title">Prompt</span>ing in Differential Privacy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12247v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12247v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhe Li, Yu-Lin Tsai, Xuebin Ren, Chia-Mu Yu, Pin-Yu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Prompting (VP) is an emerging and powerful technique that allows
sample-efficient adaptation to downstream tasks by engineering a well-trained
frozen source model. In this work, we explore the benefits of VP in
constructing compelling neural network classifiers with differential privacy
(DP). We explore and integrate VP into canonical DP training methods and
demonstrate its simplicity and efficiency. In particular, we discover that VP
in tandem with PATE, a state-of-the-art DP training method that leverages the
knowledge transfer from an ensemble of teachers, achieves the state-of-the-art
privacy-utility trade-off with minimum expenditure of privacy budget. Moreover,
we conduct additional experiments on cross-domain image classification with a
sufficient domain gap to further unveil the advantage of VP in DP. Lastly, we
also conduct extensive ablation studies to validate the effectiveness and
contribution of VP under DP consideration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Object Pose Estimation with Statistical Guarantees: Conformal Keypoint
  Detection and Geometric Uncertainty Propagation <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12246v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12246v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heng Yang, Marco Pavone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The two-stage object pose estimation paradigm first detects semantic
keypoints on the image and then estimates the 6D pose by minimizing
reprojection errors. Despite performing well on standard benchmarks, existing
techniques offer no provable guarantees on the quality and uncertainty of the
estimation. In this paper, we inject two fundamental changes, namely conformal
keypoint detection and geometric uncertainty propagation, into the two-stage
paradigm and propose the first pose estimator that endows an estimation with
provable and computable worst-case error bounds. On one hand, conformal
keypoint detection applies the statistical machinery of inductive conformal
prediction to convert heuristic keypoint detections into circular or elliptical
prediction sets that cover the groundtruth keypoints with a user-specified
marginal probability (e.g., 90%). Geometric uncertainty propagation, on the
other, propagates the geometric constraints on the keypoints to the 6D object
pose, leading to a Pose UnceRtainty SEt (PURSE) that guarantees coverage of the
groundtruth pose with the same probability. The PURSE, however, is a nonconvex
set that does not directly lead to estimated poses and uncertainties.
Therefore, we develop RANdom SAmple averaGing (RANSAG) to compute an average
pose and apply semidefinite relaxation to upper bound the worst-case errors
between the average pose and the groundtruth. On the LineMOD Occlusion dataset
we demonstrate: (i) the PURSE covers the groundtruth with valid probabilities;
(ii) the worst-case error bounds provide correct uncertainty quantification;
and (iii) the average pose achieves better or similar accuracy as
representative methods based on sparse keypoints.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2023 as a highlight paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Preventing Dimensional Collapse of Incomplete Multi-View Clustering via
  Direct Contrastive Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12241v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12241v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaiwu Zhang, Shiqiang Du, Baokai Liu, Shengxia Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Incomplete multi-view clustering (IMVC) is an unsupervised approach, among
which IMVC via contrastive learning has received attention due to its excellent
performance. The previous methods have the following problems: 1) Over-reliance
on additional projection heads when solving the dimensional collapse problem in
which latent features are only valid in lower-dimensional subspaces during
clustering. However, many parameters in the projection heads are unnecessary.
2) The recovered view contain inconsistent private information and useless
private information will mislead the learning of common semantics due to
consistent learning and reconstruction learning on the same feature. To address
the above issues, we propose a novel incomplete multi-view contrastive
clustering framework. This framework directly optimizes the latent feature
subspace, utilizes the learned feature vectors and their sub-vectors for
reconstruction learning and consistency learning, thereby effectively avoiding
dimensional collapse without relying on projection heads. Since reconstruction
loss and contrastive loss are performed on different features, the adverse
effect of useless private information is reduced. For the incomplete data, the
missing information is recovered by the cross-view prediction mechanism and the
inconsistent information from different views is discarded by the minimum
conditional entropy to further avoid the influence of private information.
Extensive experimental results of the method on 5 public datasets show that the
method achieves state-of-the-art clustering results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EfficientTrain: Exploring Generalized Curriculum Learning for Training
  Visual Backbones 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.09703v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.09703v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yulin Wang, Yang Yue, Rui Lu, Tianjiao Liu, Zhao Zhong, Shiji Song, Gao Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The superior performance of modern deep networks usually comes with a costly
training procedure. This paper presents a new curriculum learning approach for
the efficient training of visual backbones (e.g., vision Transformers). Our
work is inspired by the inherent learning dynamics of deep networks: we
experimentally show that at an earlier training stage, the model mainly learns
to recognize some 'easier-to-learn' discriminative patterns within each
example, e.g., the lower-frequency components of images and the original
information before data augmentation. Driven by this phenomenon, we propose a
curriculum where the model always leverages all the training data at each
epoch, while the curriculum starts with only exposing the 'easier-to-learn'
patterns of each example, and introduces gradually more difficult patterns. To
implement this idea, we 1) introduce a cropping operation in the Fourier
spectrum of the inputs, which enables the model to learn from only the
lower-frequency components efficiently, 2) demonstrate that exposing the
features of original images amounts to adopting weaker data augmentation, and
3) integrate 1) and 2) and design a curriculum learning schedule with a
greedy-search algorithm. The resulting approach, EfficientTrain, is simple,
general, yet surprisingly effective. In the absence of hyper-parameter tuning,
it reduces the training wall-time of a wide variety of popular models (e.g.,
ResNet, ConvNeXt, DeiT, PVT, Swin, and CSWin) by >1.5x on ImageNet-1K/22K
without sacrificing the accuracy. It is also effective for self-supervised
learning (e.g., MAE). Code is available at
https://github.com/LeapLabTHU/EfficientTrain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multiple Appropriate Facial Reaction Generation in Dyadic Interaction
  Settings: What, Why and How? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.06514v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.06514v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyang Song, Micol Spitale, Yiming Luo, Batuhan Bal, Hatice Gunes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  According to the Stimulus Organism Response (SOR) theory, all human
behavioral reactions are stimulated by context, where people will process the
received stimulus and produce an appropriate reaction. This implies that in a
specific context for a given input stimulus, a person can react differently
according to their internal state and other contextual factors. Analogously, in
dyadic interactions, humans communicate using verbal and nonverbal cues, where
a broad spectrum of listeners' non-verbal reactions might be appropriate for
responding to a specific speaker behaviour. There already exists a body of work
that investigated the problem of automatically generating an appropriate
reaction for a given input. However, none attempted to automatically generate
multiple appropriate reactions in the context of dyadic interactions and
evaluate the appropriateness of those reactions using objective measures. This
paper starts by defining the facial Multiple Appropriate Reaction Generation
(fMARG) task for the first time in the literature and proposes a new set of
objective evaluation metrics to evaluate the appropriateness of the generated
reactions. The paper subsequently introduces a framework to predict, generate,
and evaluate multiple appropriate facial reactions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SemDeDup: Data-efficient learning at web-scale through semantic
  deduplication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.09540v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.09540v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amro Abbas, Kushal Tirumala, Dániel Simig, Surya Ganguli, Ari S. Morcos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Progress in machine learning has been driven in large part by massive
increases in data. However, large web-scale datasets such as LAION are largely
uncurated beyond searches for exact duplicates, potentially leaving much
redundancy. Here, we introduce SemDeDup, a method which leverages embeddings
from pre-trained models to identify and remove semantic duplicates: data pairs
which are semantically similar, but not exactly identical. Removing semantic
duplicates preserves performance and speeds up learning. Analyzing a subset of
LAION, we show that SemDeDup can remove 50% of the data with minimal
performance loss, effectively halving training time. Moreover, performance
increases out of distribution. Also, analyzing language models trained on C4, a
partially curated dataset, we show that SemDeDup improves over prior approaches
while providing efficiency gains. SemDeDup provides an example of how simple
ways of leveraging quality embeddings can be used to make models learn faster
with less data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DeepAstroUDA: Semi-Supervised Universal Domain Adaptation for
  Cross-<span class="highlight-title">Survey</span> Galaxy Morphology Classification and Anomaly Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.02005v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.02005v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        A. Ćiprijanović, A. Lewis, K. Pedro, S. Madireddy, B. Nord, G. N. Perdue, S. M. Wild
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence methods show great promise in increasing the quality
and speed of work with large astronomical datasets, but the high complexity of
these methods leads to the extraction of dataset-specific, non-robust features.
Therefore, such methods do not generalize well across multiple datasets. We
present a universal domain adaptation method, \textit{DeepAstroUDA}, as an
approach to overcome this challenge. This algorithm performs semi-supervised
domain adaptation and can be applied to datasets with different data
distributions and class overlaps. Non-overlapping classes can be present in any
of the two datasets (the labeled source domain, or the unlabeled target
domain), and the method can even be used in the presence of unknown classes. We
apply our method to three examples of galaxy morphology classification tasks of
different complexities ($3$-class and $10$-class problems), with anomaly
detection: 1) datasets created after different numbers of observing years from
a single survey (LSST mock data of $1$ and $10$ years of observations); 2) data
from different surveys (SDSS and DECaLS); and 3) data from observing fields
with different depths within one survey (wide field and Stripe 82 deep field of
SDSS). For the first time, we demonstrate the successful use of domain
adaptation between very discrepant observational datasets.
\textit{DeepAstroUDA} is capable of bridging the gap between two astronomical
surveys, increasing classification accuracy in both domains (up to $40\%$ on
the unlabeled data), and making model performance consistent across datasets.
Furthermore, our method also performs well as an anomaly detection algorithm
and successfully clusters unknown class samples even in the unlabeled target
dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in Machine Learning Science and Technology (MLST); 24 pages,
  14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Instance Distillation for Object Detection in Autonomous
  Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.11097v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.11097v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qizhen Lan, Qing Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, knowledge distillation (KD) has been widely used to derive
efficient models. Through imitating a large teacher model, a lightweight
student model can achieve comparable performance with more efficiency. However,
most existing knowledge distillation methods are focused on classification
tasks. Only a limited number of studies have applied knowledge distillation to
object detection, especially in time-sensitive autonomous driving scenarios. In
this paper, we propose Adaptive Instance Distillation (AID) to selectively
impart teacher's knowledge to the student to improve the performance of
knowledge distillation. Unlike previous KD methods that treat all instances
equally, our AID can attentively adjust the distillation weights of instances
based on the teacher model's prediction loss. We verified the effectiveness of
our AID method through experiments on the KITTI and the COCO traffic datasets.
The results show that our method improves the performance of state-of-the-art
attention-guided and non-local distillation methods and achieves better
distillation results on both single-stage and two-stage detectors. Compared to
the baseline, our AID led to an average of 2.7% and 2.1% mAP increases for
single-stage and two-stage detectors, respectively. Furthermore, our AID is
also shown to be useful for self-distillation to improve the teacher model's
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Style Transfer for 2D Talking Head Animation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.09799v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.09799v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Trong-Thang Pham, Nhat Le, Tuong Do, Hung Nguyen, Erman Tjiputra, Quang D. Tran, Anh Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-driven talking head animation is a challenging research topic with many
real-world applications. Recent works have focused on creating photo-realistic
2D animation, while learning different talking or singing styles remains an
open problem. In this paper, we present a new method to generate talking head
animation with learnable style references. Given a set of style reference
frames, our framework can reconstruct 2D talking head animation based on a
single input image and an audio stream. Our method first produces facial
landmarks motion from the audio stream and constructs the intermediate style
patterns from the style reference images. We then feed both outputs into a
style-aware image generator to generate the photo-realistic and fidelity 2D
animation. In practice, our framework can extract the style information of a
specific character and transfer it to any new static image for talking head
animation. The intensive experimental results show that our method achieves
better results than recent state-of-the-art approaches qualitatively and
quantitatively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Visual Spatial Reasoning <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.00363v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.00363v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangyu Liu, Guy Emerson, Nigel Collier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spatial relations are a basic part of human cognition. However, they are
expressed in natural language in a variety of ways, and previous work has
suggested that current vision-and-language models (VLMs) struggle to capture
relational information. In this paper, we present Visual Spatial Reasoning
(VSR), a dataset containing more than 10k natural text-image pairs with 66
types of spatial relations in English (such as: under, in front of, and
facing). While using a seemingly simple annotation format, we show how the
dataset includes challenging linguistic phenomena, such as varying reference
frames. We demonstrate a large gap between human and model performance: the
human ceiling is above 95%, while state-of-the-art models only achieve around
70%. We observe that VLMs' by-relation performances have little correlation
with the number of training examples and the tested models are in general
incapable of recognising relations concerning the orientations of objects.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>TACL camera-ready version; code and data available at
  https://github.com/cambridgeltl/visual-spatial-reasoning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RoBIC: A benchmark suite for assessing classifiers robustness <span class="chip">ICIP 2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2102.05368v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2102.05368v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thibault Maho, Benoît Bonnet, Teddy Furon, Erwan Le Merrer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many defenses have emerged with the development of adversarial attacks.
Models must be objectively evaluated accordingly. This paper systematically
tackles this concern by proposing a new parameter-free benchmark we coin RoBIC.
RoBIC fairly evaluates the robustness of image classifiers using a new
half-distortion measure. It gauges the robustness of the network against white
and black box attacks, independently of its accuracy. RoBIC is faster than the
other available benchmarks. We present the significant differences in the
robustness of 16 recent models as assessed by RoBIC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, accepted to ICIP 2021</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CgAT: Center-Guided Adversarial Training for Deep Hashing-Based
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.10779v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.10779v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xunguang Wang, Yiqun Lin, Xiaomeng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep hashing has been extensively utilized in massive image retrieval because
of its efficiency and effectiveness. However, deep hashing models are
vulnerable to adversarial examples, making it essential to develop adversarial
defense methods for image retrieval. Existing solutions achieved limited
defense performance because of using weak adversarial samples for training and
lacking discriminative optimization objectives to learn robust features. In
this paper, we present a min-max based Center-guided Adversarial Training,
namely CgAT, to improve the robustness of deep hashing networks through worst
adversarial examples. Specifically, we first formulate the center code as a
semantically-discriminative representative of the input image content, which
preserves the semantic similarity with positive samples and dissimilarity with
negative examples. We prove that a mathematical formula can calculate the
center code immediately. After obtaining the center codes in each optimization
iteration of the deep hashing network, they are adopted to guide the
adversarial training process. On the one hand, CgAT generates the worst
adversarial examples as augmented data by maximizing the Hamming distance
between the hash codes of the adversarial examples and the center codes. On the
other hand, CgAT learns to mitigate the effects of adversarial samples by
minimizing the Hamming distance to the center codes. Extensive experiments on
the benchmark datasets demonstrate the effectiveness of our adversarial
training algorithm in defending against adversarial attacks for deep
hashing-based retrieval. Compared with the current state-of-the-art defense
method, we significantly improve the defense performance by an average of
18.61\%, 12.35\%, and 11.56\% on FLICKR-25K, NUS-WIDE, and MS-COCO,
respectively. The code is available at https://github.com/xunguangwang/CgAT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DrapeNet: Garment Generation and <span class="highlight-title">Self-Supervised</span> Draping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.11277v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.11277v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca De Luigi, Ren Li, Benoît Guillard, Mathieu Salzmann, Pascal Fua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent approaches to drape garments quickly over arbitrary human bodies
leverage self-supervision to eliminate the need for large training sets.
However, they are designed to train one network per clothing item, which
severely limits their generalization abilities. In our work, we rely on
self-supervision to train a single network to drape multiple garments. This is
achieved by predicting a 3D deformation field conditioned on the latent codes
of a generative network, which models garments as unsigned distance fields. Our
pipeline can generate and drape previously unseen garments of any topology,
whose shape can be edited by manipulating their latent codes. Being fully
differentiable, our formulation makes it possible to recover accurate 3D models
of garments from partial observations -- images or 3D scans -- via gradient
descent. Our code is publicly available at
https://github.com/liren2515/DrapeNet .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EVA-02: A Visual Representation for Neon Genesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11331v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11331v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Fang, Quan Sun, Xinggang Wang, Tiejun Huang, Xinlong Wang, Yue Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We launch EVA-02, a next-generation Transformer-based visual representation
pre-trained to reconstruct strong and robust language-aligned vision features
via masked image modeling. With an updated plain Transformer architecture as
well as extensive pre-training from an open & accessible giant CLIP vision
encoder, EVA-02 demonstrates superior performance compared to prior
state-of-the-art approaches across various representative vision tasks, while
utilizing significantly fewer parameters and compute budgets. Notably, using
exclusively publicly accessible training data, EVA-02 with only 304M parameters
achieves a phenomenal 90.0 fine-tuning top-1 accuracy on ImageNet-1K val set.
Additionally, our EVA-02-CLIP can reach up to 80.4 zero-shot top-1 on
ImageNet-1K, outperforming the previous largest & best open-sourced CLIP with
only ~1/6 parameters and ~1/6 image-text training data. We offer four EVA-02
variants in various model sizes, ranging from 6M to 304M parameters, all with
impressive performance. To facilitate open access and open research, we release
the complete suite of EVA-02 to the community at
https://github.com/baaivision/EVA/tree/master/EVA-02.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v2: Fix some known issues & typos. v1: To Asuka. Code & Models:
  https://github.com/baaivision/EVA/tree/master/EVA-02</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unbiased Supervised Contrastive Learning <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.05568v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.05568v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carlo Alberto Barbano, Benoit Dufumier, Enzo Tartaglione, Marco Grangetto, Pietro Gori
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many datasets are biased, namely they contain easy-to-learn features that are
highly correlated with the target class only in the dataset but not in the true
underlying distribution of the data. For this reason, learning unbiased models
from biased data has become a very relevant research topic in the last years.
In this work, we tackle the problem of learning representations that are robust
to biases. We first present a margin-based theoretical framework that allows us
to clarify why recent contrastive losses (InfoNCE, SupCon, etc.) can fail when
dealing with biased data. Based on that, we derive a novel formulation of the
supervised contrastive loss (epsilon-SupInfoNCE), providing more accurate
control of the minimal distance between positive and negative samples.
Furthermore, thanks to our theoretical framework, we also propose FairKL, a new
debiasing regularization loss, that works well even with extremely biased data.
We validate the proposed losses on standard vision datasets including CIFAR10,
CIFAR100, and ImageNet, and we assess the debiasing capability of FairKL with
epsilon-SupInfoNCE, reaching state-of-the-art performance on a number of biased
datasets, including real instances of biases in the wild.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TBP-Former: Learning Temporal Bird's-Eye-View Pyramid for Joint
  Perception and Prediction in Vision-Centric Autonomous Driving <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.09998v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.09998v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaoheng Fang, Zi Wang, Yiqi Zhong, Junhao Ge, Siheng Chen, Yanfeng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-centric joint perception and prediction (PnP) has become an emerging
trend in autonomous driving research. It predicts the future states of the
traffic participants in the surrounding environment from raw RGB images.
However, it is still a critical challenge to synchronize features obtained at
multiple camera views and timestamps due to inevitable geometric distortions
and further exploit those spatial-temporal features. To address this issue, we
propose a temporal bird's-eye-view pyramid transformer (TBP-Former) for
vision-centric PnP, which includes two novel designs. First, a
pose-synchronized BEV encoder is proposed to map raw image inputs with any
camera pose at any time to a shared and synchronized BEV space for better
spatial-temporal synchronization. Second, a spatial-temporal pyramid
transformer is introduced to comprehensively extract multi-scale BEV features
and predict future BEV states with the support of spatial-temporal priors.
Extensive experiments on nuScenes dataset show that our proposed framework
overall outperforms all state-of-the-art vision-based prediction methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic Relevance Learning for Few-Shot Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2108.02235v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2108.02235v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weijie Liu, Chong Wang, Haohe Li, Shenghao Yu, Jiafei Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Expensive bounding-box annotations have limited the development of object
detection task. Thus, it is necessary to focus on more challenging task of
few-shot object detection. It requires the detector to recognize objects of
novel classes with only a few training samples. Nowadays, many existing popular
methods adopting training way similar to meta-learning have achieved promising
performance, such as Meta R-CNN series. However, support data is only used as
the class attention to guide the detecting of query images each time. Their
relevance to each other remains unexploited. Moreover, a lot of recent works
treat the support data and query images as independent branch without
considering the relationship between them. To address this issue, we propose a
dynamic relevance learning model, which utilizes the relationship between all
support images and Region of Interest (RoI) on the query images to construct a
dynamic graph convolutional network (GCN). By adjusting the prediction
distribution of the base detector using the output of this GCN, the proposed
model serves as a hard auxiliary classification task, which guides the detector
to improve the class representation implicitly. Comprehensive experiments have
been conducted on Pascal VOC and MS-COCO dataset. The proposed model achieves
the best overall performance, which shows its effectiveness of learning more
generalized features. Our code is available at
https://github.com/liuweijie19980216/DRL-for-FSOD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 8 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AeDet: Azimuth-invariant Multi-view 3D Object Detection <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.12501v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.12501v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengjian Feng, Zequn Jie, Yujie Zhong, Xiangxiang Chu, Lin Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent LSS-based multi-view 3D object detection has made tremendous progress,
by processing the features in Brid-Eye-View (BEV) via the convolutional
detector. However, the typical convolution ignores the radial symmetry of the
BEV features and increases the difficulty of the detector optimization. To
preserve the inherent property of the BEV features and ease the optimization,
we propose an azimuth-equivariant convolution (AeConv) and an
azimuth-equivariant anchor. The sampling grid of AeConv is always in the radial
direction, thus it can learn azimuth-invariant BEV features. The proposed
anchor enables the detection head to learn predicting azimuth-irrelevant
targets. In addition, we introduce a camera-decoupled virtual depth to unify
the depth prediction for the images with different camera intrinsic parameters.
The resultant detector is dubbed Azimuth-equivariant Detector (AeDet).
Extensive experiments are conducted on nuScenes, and AeDet achieves a 62.0%
NDS, surpassing the recent multi-view 3D object detectors such as PETRv2 and
BEVDepth by a large margin. Project page: https://fcjian.github.io/aedet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancement of Novel View Synthesis Using Omnidirectional Image
  Completion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09957v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09957v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takayuki Hara, Tatsuya Harada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we present a method for synthesizing novel views from a single
360-degree RGB-D image based on the neural radiance field (NeRF) . Prior
studies relied on the neighborhood interpolation capability of multi-layer
perceptrons to complete missing regions caused by occlusion and zooming, which
leads to artifacts. In the method proposed in this study, the input image is
reprojected to 360-degree RGB images at other camera positions, the missing
regions of the reprojected images are completed by a 2D image generative model,
and the completed images are utilized to train the NeRF. Because multiple
completed images contain inconsistencies in 3D, we introduce a method to learn
the NeRF model using a subset of completed images that cover the target scene
with less overlap of completed regions. The selection of such a subset of
images can be attributed to the maximum weight independent set problem, which
is solved through simulated annealing. Experiments demonstrated that the
proposed method can synthesize plausible novel views while preserving the
features of the scene for both artificial and real-world data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 20 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Diffusion Training via Min-SNR Weighting Strategy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.09556v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.09556v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tiankai Hang, Shuyang Gu, Chen Li, Jianmin Bao, Dong Chen, Han Hu, Xin Geng, Baining Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Denoising diffusion models have been a mainstream approach for image
generation, however, training these models often suffers from slow convergence.
In this paper, we discovered that the slow convergence is partly due to
conflicting optimization directions between timesteps. To address this issue,
we treat the diffusion training as a multi-task learning problem, and introduce
a simple yet effective approach referred to as Min-SNR-$\gamma$. This method
adapts loss weights of timesteps based on clamped signal-to-noise ratios, which
effectively balances the conflicts among timesteps. Our results demonstrate a
significant improvement in converging speed, 3.4$\times$ faster than previous
weighting strategies. It is also more effective, achieving a new record FID
score of 2.06 on the ImageNet $256\times256$ benchmark using smaller
architectures than that employed in previous state-of-the-art. The code is
available at https://github.com/TiankaiHang/Min-SNR-Diffusion-Training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LargeKernel3D: Scaling up Kernels in 3D Sparse CNNs <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.10555v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.10555v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yukang Chen, Jianhui Liu, Xiangyu Zhang, Xiaojuan Qi, Jiaya Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advance in 2D CNNs has revealed that large kernels are important.
However, when directly applying large convolutional kernels in 3D CNNs, severe
difficulties are met, where those successful module designs in 2D become
surprisingly ineffective on 3D networks, including the popular depth-wise
convolution. To address this vital challenge, we instead propose the
spatial-wise partition convolution and its large-kernel module. As a result, it
avoids the optimization and efficiency issues of naive 3D large kernels. Our
large-kernel 3D CNN network, LargeKernel3D, yields notable improvement in 3D
tasks of semantic segmentation and object detection. It achieves 73.9% mIoU on
the ScanNetv2 semantic segmentation and 72.8% NDS nuScenes object detection
benchmarks, ranking 1st on the nuScenes LIDAR leaderboard. The performance
further boosts to 74.2% NDS with a simple multi-modal fusion. In addition,
LargeKernel3D can be scaled to 17x17x17 kernel size on Waymo 3D object
detection. For the first time, we show that large kernels are feasible and
essential for 3D visual tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In CVPR 2023. Code is at
  https://github.com/dvlab-research/LargeKernel3D</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scanning Only Once: An End-to-end Framework for Fast Temporal Grounding
  in Long Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08345v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08345v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yulin Pan, Xiangteng He, Biao Gong, Yiliang Lv, Yujun Shen, Yuxin Peng, Deli Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video temporal grounding aims to pinpoint a video segment that matches the
query description. Despite the recent advance in short-form videos
(\textit{e.g.}, in minutes), temporal grounding in long videos (\textit{e.g.},
in hours) is still at its early stage. To address this challenge, a common
practice is to employ a sliding window, yet can be inefficient and inflexible
due to the limited number of frames within the window. In this work, we propose
an end-to-end framework for fast temporal grounding, which is able to model an
hours-long video with \textbf{one-time} network execution. Our pipeline is
formulated in a coarse-to-fine manner, where we first extract context knowledge
from non-overlapped video clips (\textit{i.e.}, anchors), and then supplement
the anchors that highly response to the query with detailed content knowledge.
Besides the remarkably high pipeline efficiency, another advantage of our
approach is the capability of capturing long-range temporal correlation, thanks
to modeling the entire video as a whole, and hence facilitates more accurate
grounding. Experimental results suggest that, on the long-form video datasets
MAD and Ego4d, our method significantly outperforms state-of-the-arts, and
achieves \textbf{14.6$\times$} / \textbf{102.8$\times$} higher efficiency
respectively. Project can be found at
\url{https://github.com/afcedf/SOONet.git}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Linguistic Query-Guided Mask Generation for Referring Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06429v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06429v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhichao Wei, Xiaohao Chen, Mingqiang Chen, Siyu Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Referring image segmentation aims to segment the image region of interest
according to the given language expression, which is a typical multi-modal
task. Existing methods either adopt the pixel classification-based or the
learnable query-based framework for mask generation, both of which are
insufficient to deal with various text-image pairs with a fix number of
parametric prototypes. In this work, we propose an end-to-end framework built
on transformer to perform Linguistic query-Guided mask generation, dubbed
LGFormer. It views the linguistic features as query to generate a specialized
prototype for arbitrary input image-text pair, thus generating more consistent
segmentation results. Moreover, we design several cross-modal interaction
modules (\eg, vision-language bidirectional attention module, VLBA) in both
encoder and decoder to achieve better cross-modal alignment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VideoFusion: Decomposed Diffusion Models for High-Quality Video
  Generation <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08320v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08320v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang, Liang Wang, Yujun Shen, Deli Zhao, Jingren Zhou, Tieniu Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A diffusion probabilistic model (DPM), which constructs a forward diffusion
process by gradually adding noise to data points and learns the reverse
denoising process to generate new samples, has been shown to handle complex
data distribution. Despite its recent success in image synthesis, applying DPMs
to video generation is still challenging due to high-dimensional data spaces.
Previous methods usually adopt a standard diffusion process, where frames in
the same video clip are destroyed with independent noises, ignoring the content
redundancy and temporal correlation. This work presents a decomposed diffusion
process via resolving the per-frame noise into a base noise that is shared
among all frames and a residual noise that varies along the time axis. The
denoising pipeline employs two jointly-learned networks to match the noise
decomposition accordingly. Experiments on various datasets confirm that our
approach, termed as VideoFusion, surpasses both GAN-based and diffusion-based
alternatives in high-quality video generation. We further show that our
decomposed formulation can benefit from pre-trained image diffusion models and
well-support text-conditioned video creation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CycDA: Unsupervised Cycle Domain Adaptation from Image to Video <span class="chip">ECCV2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.16244v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.16244v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Lin, Anna Kukleva, Kunyang Sun, Horst Possegger, Hilde Kuehne, Horst Bischof
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although action recognition has achieved impressive results over recent
years, both collection and annotation of video training data are still
time-consuming and cost intensive. Therefore, image-to-video adaptation has
been proposed to exploit labeling-free web image source for adapting on
unlabeled target videos. This poses two major challenges: (1) spatial domain
shift between web images and video frames; (2) modality gap between image and
video data. To address these challenges, we propose Cycle Domain Adaptation
(CycDA), a cycle-based approach for unsupervised image-to-video domain
adaptation by leveraging the joint spatial information in images and videos on
the one hand and, on the other hand, training an independent spatio-temporal
model to bridge the modality gap. We alternate between the spatial and
spatio-temporal learning with knowledge transfer between the two in each cycle.
We evaluate our approach on benchmark datasets for image-to-video as well as
for mixed-source domain adaptation achieving state-of-the-art results and
demonstrating the benefits of our cyclic adaptation. Code is available at
\url{https://github.com/wlin-at/CycDA}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ECCV2022. Supplementary included</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Semantic Brain Decoding: from fMRI to conceptually similar image
  reconstruction of visual stimuli 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06726v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06726v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Ferrante, Tommaso Boccato, Nicola Toschi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Brain decoding is a field of computational neuroscience that uses measurable
brain activity to infer mental states or internal representations of perceptual
inputs. Therefore, we propose a novel approach to brain decoding that also
relies on semantic and contextual similarity. We employ an fMRI dataset of
natural image vision and create a deep learning decoding pipeline inspired by
the existence of both bottom-up and top-down processes in human vision. We
train a linear brain-to-feature model to map fMRI activity features to visual
stimuli features, assuming that the brain projects visual information onto a
space that is homeomorphic to the latent space represented by the last
convolutional layer of a pretrained convolutional neural network, which
typically collects a variety of semantic features that summarize and highlight
similarities and differences between concepts. These features are then
categorized in the latent space using a nearest-neighbor strategy, and the
results are used to condition a generative latent diffusion model to create
novel images. From fMRI data only, we produce reconstructions of visual stimuli
that match the original content very well on a semantic level, surpassing the
state of the art in previous literature. We evaluate our work and obtain good
results using a quantitative semantic metric (the Wu-Palmer similarity metric
over the WordNet lexicon, which had an average value of 0.57) and perform a
human evaluation experiment that resulted in correct evaluation, according to
the multiplicity of human criteria in evaluating image similarity, in over 80%
of the test set.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MedNeXt: <span class="highlight-title">Transformer</span>-driven Scaling of ConvNets for Medical Image
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.09975v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.09975v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saikat Roy, Gregor Koehler, Constantin Ulrich, Michael Baumgartner, Jens Petersen, Fabian Isensee, Paul F. Jaeger, Klaus Maier-Hein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There has been exploding interest in embracing Transformer-based
architectures for medical image segmentation. However, the lack of large-scale
annotated medical datasets make achieving performances equivalent to those in
natural images challenging. Convolutional networks, in contrast, have higher
inductive biases and consequently, are easily trainable to high performance.
Recently, the ConvNeXt architecture attempted to modernize the standard ConvNet
by mirroring Transformer blocks. In this work, we improve upon this to design a
modernized and scalable convolutional architecture customized to challenges of
data-scarce medical settings. We introduce MedNeXt, a Transformer-inspired
large kernel segmentation network which introduces - 1) A fully ConvNeXt 3D
Encoder-Decoder Network for medical image segmentation, 2) Residual ConvNeXt up
and downsampling blocks to preserve semantic richness across scales, 3) A novel
technique to iteratively increase kernel sizes by upsampling small kernel
networks, to prevent performance saturation on limited medical data, 4)
Compound scaling at multiple levels (depth, width, kernel size) of MedNeXt.
This leads to state-of-the-art performance on 4 tasks on CT and MRI modalities
and varying dataset sizes, representing a modernized deep architecture for
medical image segmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ μSplit: efficient image decomposition for microscopy data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.12872v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.12872v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         Ashesh, Alexander Krull, Moises Di Sante, Francesco Silvio Pasqualini, Florian Jug
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present uSplit, a dedicated approach for trained image decomposition in
the context of fluorescence microscopy images. We find that best results using
regular deep architectures are achieved when large image patches are used
during training, making memory consumption the limiting factor to further
improving performance. We therefore introduce lateral contextualization (LC), a
memory efficient way to train powerful networks and show that LC leads to
consistent and significant improvements on the task at hand. We integrate LC
with U-Nets, Hierarchical AEs, and Hierarchical VAEs, for which we formulate a
modified ELBO loss. Additionally, LC enables training deeper hierarchical
models than otherwise possible and, interestingly, helps to reduce tiling
artefacts that are inherently impossible to avoid when using tiled VAE
predictions. We apply uSplit to five decomposition tasks, one on a synthetic
dataset, four others derived from real microscopy data. LC achieves SOTA
results (average improvements to the best baseline of 2.36 dB PSNR), while
simultaneously requiring considerably less GPU memory.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 7 figures, 9 pages supplement, 8 supplementary figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NeRFLiX: High-Quality Neural View Synthesis by Learning a
  Degradation-Driven Inter-viewpoint MiXer <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06919v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06919v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kun Zhou, Wenbo Li, Yi Wang, Tao Hu, Nianjuan Jiang, Xiaoguang Han, Jiangbo Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural radiance fields (NeRF) show great success in novel view synthesis.
However, in real-world scenes, recovering high-quality details from the source
images is still challenging for the existing NeRF-based approaches, due to the
potential imperfect calibration information and scene representation
inaccuracy. Even with high-quality training frames, the synthetic novel views
produced by NeRF models still suffer from notable rendering artifacts, such as
noise, blur, etc. Towards to improve the synthesis quality of NeRF-based
approaches, we propose NeRFLiX, a general NeRF-agnostic restorer paradigm by
learning a degradation-driven inter-viewpoint mixer. Specially, we design a
NeRF-style degradation modeling approach and construct large-scale training
data, enabling the possibility of effectively removing NeRF-native rendering
artifacts for existing deep neural networks. Moreover, beyond the degradation
removal, we propose an inter-viewpoint aggregation framework that is able to
fuse highly related high-quality training images, pushing the performance of
cutting-edge NeRF models to entirely new levels and producing highly
photo-realistic synthetic views.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2023; Project Page: see
  https://redrock303.github.io/nerflix/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Side Adapter Network for Open-Vocabulary Semantic Segmentation <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.12242v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.12242v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengde Xu, Zheng Zhang, Fangyun Wei, Han Hu, Xiang Bai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a new framework for open-vocabulary semantic segmentation
with the pre-trained vision-language model, named Side Adapter Network (SAN).
Our approach models the semantic segmentation task as a region recognition
problem. A side network is attached to a frozen CLIP model with two branches:
one for predicting mask proposals, and the other for predicting attention bias
which is applied in the CLIP model to recognize the class of masks. This
decoupled design has the benefit CLIP in recognizing the class of mask
proposals. Since the attached side network can reuse CLIP features, it can be
very light. In addition, the entire network can be trained end-to-end, allowing
the side network to be adapted to the frozen CLIP model, which makes the
predicted mask proposals CLIP-aware. Our approach is fast, accurate, and only
adds a few additional trainable parameters. We evaluate our approach on
multiple semantic segmentation benchmarks. Our method significantly outperforms
other counterparts, with up to 18 times fewer trainable parameters and 19 times
faster inference speed. We hope our approach will serve as a solid baseline and
help ease future research in open-vocabulary semantic segmentation. The code
will be available at https://github.com/MendelXu/SAN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR2023 Highlight</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MARLIN: Masked Autoencoder for facial video Representation LearnINg <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.06627v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.06627v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhixi Cai, Shreya Ghosh, Kalin Stefanov, Abhinav Dhall, Jianfei Cai, Hamid Rezatofighi, Reza Haffari, Munawar Hayat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a self-supervised approach to learn universal facial
representations from videos, that can transfer across a variety of facial
analysis tasks such as Facial Attribute Recognition (FAR), Facial Expression
Recognition (FER), DeepFake Detection (DFD), and Lip Synchronization (LS). Our
proposed framework, named MARLIN, is a facial video masked autoencoder, that
learns highly robust and generic facial embeddings from abundantly available
non-annotated web crawled facial videos. As a challenging auxiliary task,
MARLIN reconstructs the spatio-temporal details of the face from the densely
masked facial regions which mainly include eyes, nose, mouth, lips, and skin to
capture local and global aspects that in turn help in encoding generic and
transferable features. Through a variety of experiments on diverse downstream
tasks, we demonstrate MARLIN to be an excellent facial video encoder as well as
feature extractor, that performs consistently well across a variety of
downstream tasks including FAR (1.13% gain over supervised benchmark), FER
(2.64% gain over unsupervised benchmark), DFD (1.86% gain over unsupervised
benchmark), LS (29.36% gain for Frechet Inception Distance), and even in low
data regime. Our code and models are available at
https://github.com/ControlNet/MARLIN .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Diversify for Single Domain Generalization <span class="chip">ICCV 2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2108.11726v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2108.11726v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijian Wang, Yadan Luo, Ruihong Qiu, Zi Huang, Mahsa Baktashmotlagh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain generalization (DG) aims to generalize a model trained on multiple
source (i.e., training) domains to a distributionally different target (i.e.,
test) domain. In contrast to the conventional DG that strictly requires the
availability of multiple source domains, this paper considers a more realistic
yet challenging scenario, namely Single Domain Generalization (Single-DG),
where only one source domain is available for training. In this scenario, the
limited diversity may jeopardize the model generalization on unseen target
domains. To tackle this problem, we propose a style-complement module to
enhance the generalization power of the model by synthesizing images from
diverse distributions that are complementary to the source ones. More
specifically, we adopt a tractable upper bound of mutual information (MI)
between the generated and source samples and perform a two-step optimization
iteratively: (1) by minimizing the MI upper bound approximation for each sample
pair, the generated images are forced to be diversified from the source
samples; (2) subsequently, we maximize the MI between the samples from the same
semantic category, which assists the network to learn discriminative features
from diverse-styled images. Extensive experiments on three benchmark datasets
demonstrate the superiority of our approach, which surpasses the
state-of-the-art single-DG methods by up to 25.14%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV 2021</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Active Learning for Deep Neural Networks on Edge Devices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.10836v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.10836v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuya Senzaki, Christian Hamelain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When dealing with deep neural network (DNN) applications on edge devices,
continuously updating the model is important. Although updating a model with
real incoming data is ideal, using all of them is not always feasible due to
limits, such as labeling and communication costs. Thus, it is necessary to
filter and select the data to use for training (i.e., active learning) on the
device. In this paper, we formalize a practical active learning problem for
DNNs on edge devices and propose a general task-agnostic framework to tackle
this problem, which reduces it to a stream submodular maximization. This
framework is light enough to be run with low computational resources, yet
provides solutions whose quality is theoretically guaranteed thanks to the
submodular property. Through this framework, we can configure data selection
criteria flexibly, including using methods proposed in previous active learning
studies. We evaluate our approach on both classification and object detection
tasks in a practical setting to simulate a real-life scenario. The results of
our study show that the proposed framework outperforms all other methods in
both tasks, while running at a practical speed on real devices.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hand Avatar: Free-Pose Hand Animation and Rendering from Monocular Video 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.12782v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.12782v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingyu Chen, Baoyuan Wang, Heung-Yeung Shum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present HandAvatar, a novel representation for hand animation and
rendering, which can generate smoothly compositional geometry and
self-occlusion-aware texture. Specifically, we first develop a MANO-HD model as
a high-resolution mesh topology to fit personalized hand shapes. Sequentially,
we decompose hand geometry into per-bone rigid parts, and then re-compose
paired geometry encodings to derive an across-part consistent occupancy field.
As for texture modeling, we propose a self-occlusion-aware shading field
(SelF). In SelF, drivable anchors are paved on the MANO-HD surface to record
albedo information under a wide variety of hand poses. Moreover, directed soft
occupancy is designed to describe the ray-to-surface relation, which is
leveraged to generate an illumination field for the disentanglement of
pose-independent albedo and pose-dependent illumination. Trained from monocular
video data, our HandAvatar can perform free-pose hand animation and rendering
while at the same time achieving superior appearance fidelity. We also
demonstrate that HandAvatar provides a route for hand appearance editing.
Project website: https://seanchenxy.github.io/HandAvatarWeb.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Shape-Guided Diffusion with Inside-Outside Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.00210v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.00210v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dong Huk Park, Grace Luo, Clayton Toste, Samaneh Azadi, Xihui Liu, Maka Karalashvili, Anna Rohrbach, Trevor Darrell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When manipulating an object, existing text-to-image diffusion models often
ignore the shape of the object and generate content that is incorrectly scaled,
cut off, or replaced with background content. We propose a training-free
method, Shape-Guided Diffusion, that modifies pretrained diffusion models to be
sensitive to shape input specified by a user or automatically inferred from
text. We use a novel Inside-Outside Attention mechanism during the inversion
and generation process to apply this shape constraint to the cross- and
self-attention maps. Our mechanism designates which spatial region is the
object (inside) vs. background (outside) then associates edits specified by
text prompts to the correct region. We demonstrate the efficacy of our method
on the shape-guided editing task, where the model must replace an object
according to a text prompt and object mask. We curate a new ShapePrompts
benchmark derived from MS-COCO and achieve SOTA results in shape faithfulness
without a degradation in text alignment or image realism according to both
automatic metrics and annotator ratings. Our data and code will be made
available at https://shape-guided-diffusion.github.io.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Legs as Manipulator: Pushing Quadrupedal Agility Beyond Locomotion <span class="chip">ICRA 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11330v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11330v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuxin Cheng, Ashish Kumar, Deepak Pathak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Locomotion has seen dramatic progress for walking or running across
challenging terrains. However, robotic quadrupeds are still far behind their
biological counterparts, such as dogs, which display a variety of agile skills
and can use the legs beyond locomotion to perform several basic manipulation
tasks like interacting with objects and climbing. In this paper, we take a step
towards bridging this gap by training quadruped robots not only to walk but
also to use the front legs to climb walls, press buttons, and perform object
interaction in the real world. To handle this challenging optimization, we
decouple the skill learning broadly into locomotion, which involves anything
that involves movement whether via walking or climbing a wall, and
manipulation, which involves using one leg to interact while balancing on the
other three legs. These skills are trained in simulation using curriculum and
transferred to the real world using our proposed sim2real variant that builds
upon recent locomotion success. Finally, we combine these skills into a robust
long-term plan by learning a behavior tree that encodes a high-level task
hierarchy from one clean expert demonstration. We evaluate our method in both
simulation and real-world showing successful executions of both short as well
as long-range tasks and how robustness helps confront external perturbations.
Videos at https://robot-skills.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICRA 2023. Videos at https://robot-skills.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A numerical approximation method for the Fisher-Rao distance between
  multivariate normal distributions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.08175v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.08175v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Frank Nielsen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a simple method to approximate Rao's distance between multivariate
normal distributions based on discretizing curves joining normal distributions
and approximating Rao's distances between successive nearby normal
distributions on the curves by the square root of Jeffreys divergence, the
symmetrized Kullback-Leibler divergence. We consider experimentally the linear
interpolation curves in the ordinary, natural and expectation parameterizations
of the normal distributions, and compare these curves with a curve derived from
the Calvo and Oller's isometric embedding of the Fisher-Rao $d$-variate normal
manifold into the cone of $(d+1)\times (d+1)$ symmetric positive-definite
matrices [Journal of multivariate analysis 35.2 (1990): 223-242]. We report on
our experiments and assess the quality of our approximation technique by
comparing the numerical approximations with both lower and upper bounds.
Finally, we present several information-geometric properties of the Calvo and
Oller's isometric embedding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>42 pages, 17 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 360BEV: Panoramic Semantic Mapping for Indoor Bird's-Eye View 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11910v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11910v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhifeng Teng, Jiaming Zhang, Kailun Yang, Kunyu Peng, Hao Shi, Simon Reiß, Ke Cao, Rainer Stiefelhagen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Seeing only a tiny part of the whole is not knowing the full circumstance.
Bird's-eye-view (BEV) perception, a process of obtaining allocentric maps from
egocentric views, is restricted when using a narrow Field of View (FoV) alone.
In this work, mapping from 360{\deg} panoramas to BEV semantics, the 360BEV
task, is established for the first time to achieve holistic representations of
indoor scenes in a top-down view. Instead of relying on narrow-FoV image
sequences, a panoramic image with depth information is sufficient to generate a
holistic BEV semantic map. To benchmark 360BEV, we present two indoor datasets,
360BEV-Matterport and 360BEV-Stanford, both of which include egocentric
panoramic images and semantic segmentation labels, as well as allocentric
semantic maps. Besides delving deep into different mapping paradigms, we
propose a dedicated solution for panoramic semantic mapping, namely 360Mapper.
Through extensive experiments, our methods achieve 44.32% and 45.78% in mIoU on
both datasets respectively, surpassing previous counterparts with gains of
+7.60% and +9.70% in mIoU. Code and datasets will be available at:
https://jamycheung.github.io/360BEV.html.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and datasets will be available at:
  https://jamycheung.github.io/360BEV.html</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Joint Liver and Hepatic Lesion Segmentation in MRI using a Hybrid CNN
  with <span class="highlight-title">Transformer</span> Layers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.10981v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.10981v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georg Hille, Shubham Agrawal, Pavan Tummala, Christian Wybranski, Maciej Pech, Alexey Surov, Sylvia Saalfeld
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning-based segmentation of the liver and hepatic lesions therein
steadily gains relevance in clinical practice due to the increasing incidence
of liver cancer each year. Whereas various network variants with overall
promising results in the field of medical image segmentation have been
successfully developed over the last years, almost all of them struggle with
the challenge of accurately segmenting hepatic lesions in magnetic resonance
imaging (MRI). This led to the idea of combining elements of convolutional and
transformer-based architectures to overcome the existing limitations. This work
presents a hybrid network called SWTR-Unet, consisting of a pretrained ResNet,
transformer blocks as well as a common Unet-style decoder path. This network
was primarily applied to single-modality non-contrast-enhanced liver MRI and
additionally to the publicly available computed tomography (CT) data of the
liver tumor segmentation (LiTS) challenge to verify the applicability on other
modalities. For a broader evaluation, multiple state-of-the-art networks were
implemented and applied, ensuring a direct comparability. Furthermore,
correlation analysis and an ablation study were carried out, to investigate
various influencing factors on the segmentation accuracy of the presented
method. With Dice scores of averaged 98+-2% for liver and 81+-28% lesion
segmentation on the MRI dataset and 97+-2% and 79+-25%, respectively on the CT
dataset, the proposed SWTR-Unet proved to be a precise approach for liver and
hepatic lesion segmentation with state-of-the-art results for MRI and competing
accuracy in CT imaging. The achieved segmentation accuracy was found to be on
par with manually performed expert segmentations as indicated by inter-observer
variabilities for liver lesion segmentation. In conclusion, the presented
method could save valuable time and resources in clinical practice.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative Bias for Robust Visual Question Answering <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.00690v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.00690v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jae Won Cho, Dong-jin Kim, Hyeonggon Ryu, In So Kweon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of Visual Question Answering (VQA) is known to be plagued by the
issue of VQA models exploiting biases within the dataset to make its final
prediction. Various previous ensemble based debiasing methods have been
proposed where an additional model is purposefully trained to be biased in
order to train a robust target model. However, these methods compute the bias
for a model simply from the label statistics of the training data or from
single modal branches. In this work, in order to better learn the bias a target
VQA model suffers from, we propose a generative method to train the bias model
directly from the target model, called GenB. In particular, GenB employs a
generative network to learn the bias in the target model through a combination
of the adversarial objective and knowledge distillation. We then debias our
target model with GenB as a bias model, and show through extensive experiments
the effects of our method on various VQA bias datasets including VQA-CP2,
VQA-CP1, GQA-OOD, and VQA-CE, and show state-of-the-art results with the LXMERT
architecture on VQA-CP2.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tell Me What Happened: Unifying Text-guided Video Completion via
  Multimodal Masked Video Generation <span class="chip">CVPR'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.12824v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.12824v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tsu-Jui Fu, Licheng Yu, Ning Zhang, Cheng-Yang Fu, Jong-Chyi Su, William Yang Wang, Sean Bell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating a video given the first several static frames is challenging as it
anticipates reasonable future frames with temporal coherence. Besides video
prediction, the ability to rewind from the last frame or infilling between the
head and tail is also crucial, but they have rarely been explored for video
completion. Since there could be different outcomes from the hints of just a
few frames, a system that can follow natural language to perform video
completion may significantly improve controllability. Inspired by this, we
introduce a novel task, text-guided video completion (TVC), which requests the
model to generate a video from partial frames guided by an instruction. We then
propose Multimodal Masked Video Generation (MMVG) to address this TVC task.
During training, MMVG discretizes the video frames into visual tokens and masks
most of them to perform video completion from any time point. At inference
time, a single MMVG model can address all 3 cases of TVC, including video
prediction, rewind, and infilling, by applying corresponding masking
conditions. We evaluate MMVG in various video scenarios, including egocentric,
animation, and gaming. Extensive experimental results indicate that MMVG is
effective in generating high-quality visual appearances with text guidance for
TVC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR'23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Self-supervised</span> Character-to-Character Distillation for Text Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.00288v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.00288v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tongkun Guan, Wei Shen, Xue Yang, Qi Feng, Zekun Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When handling complicated text images (e.g., irregular structures, low
resolution, heavy occlusion, and uneven illumination), existing supervised text
recognition methods are data-hungry. Although these methods employ large-scale
synthetic text images to reduce the dependence on annotated real images, the
domain gap still limits the recognition performance. Therefore, exploring the
robust text feature representations on unlabeled real images by self-supervised
learning is a good solution. However, existing self-supervised text recognition
methods conduct sequence-to-sequence representation learning by roughly
splitting the visual features along the horizontal axis, which limits the
flexibility of the augmentations, as large geometric-based augmentations may
lead to sequence-to-sequence feature inconsistency. Motivated by this, we
propose a novel self-supervised Character-to-Character Distillation method,
CCD, which enables versatile augmentations to facilitate general text
representation learning. Specifically, we delineate the character structures of
unlabeled real images by designing a self-supervised character segmentation
module. Following this, CCD easily enriches the diversity of local characters
while keeping their pairwise alignment under flexible augmentations, using the
transformation matrix between two augmented views from images. Experiments
demonstrate that CCD achieves state-of-the-art results, with average
performance gains of 1.38% in text recognition, 1.7% in text segmentation, 0.24
dB (PSNR) and 0.0321 (SSIM) in text super-resolution. Code will be released
soon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OpenGait: Revisiting Gait Recognition Toward Better Practicality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.06597v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.06597v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Fan, Junhao Liang, Chuanfu Shen, Saihui Hou, Yongzhen Huang, Shiqi Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gait recognition is one of the most critical long-distance identification
technologies and increasingly gains popularity in both research and industry
communities. Despite the significant progress made in indoor datasets, much
evidence shows that gait recognition techniques perform poorly in the wild.
More importantly, we also find that some conclusions drawn from indoor datasets
cannot be generalized to real applications. Therefore, the primary goal of this
paper is to present a comprehensive benchmark study for better practicality
rather than only a particular model for better performance. To this end, we
first develop a flexible and efficient gait recognition codebase named
OpenGait. Based on OpenGait, we deeply revisit the recent development of gait
recognition by re-conducting the ablative experiments. Encouragingly,we detect
some unperfect parts of certain prior woks, as well as new insights. Inspired
by these discoveries, we develop a structurally simple, empirically powerful,
and practically robust baseline model, GaitBase. Experimentally, we
comprehensively compare GaitBase with many current gait recognition methods on
multiple public datasets, and the results reflect that GaitBase achieves
significantly strong performance in most cases regardless of indoor or outdoor
situations. Code is available at https://github.com/ShiqiYu/OpenGait.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Causality-based Dual-Contrastive Learning Framework for Domain
  Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09120v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09120v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zining Chen, Weiqiu Wang, Zhicheng Zhao, Aidong Men
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain Generalization (DG) is essentially a sub-branch of out-of-distribution
generalization, which trains models from multiple source domains and
generalizes to unseen target domains. Recently, some domain generalization
algorithms have emerged, but most of them were designed with non-transferable
complex architecture. Additionally, contrastive learning has become a promising
solution for simplicity and efficiency in DG. However, existing contrastive
learning neglected domain shifts that caused severe model confusions. In this
paper, we propose a Dual-Contrastive Learning (DCL) module on feature and
prototype contrast. Moreover, we design a novel Causal Fusion Attention (CFA)
module to fuse diverse views of a single image to attain prototype.
Furthermore, we introduce a Similarity-based Hard-pair Mining (SHM) strategy to
leverage information on diversity shift. Extensive experiments show that our
method outperforms state-of-the-art algorithms on three DG datasets. The
proposed algorithm can also serve as a plug-and-play module without usage of
domain labels.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Inadequate proof of the effectiveness of the method</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Human Motion Representations: A Unified Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.06551v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.06551v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wentao Zhu, Xiaoxuan Ma, Zhaoyang Liu, Libin Liu, Wayne Wu, Yizhou Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a unified perspective on tackling various human-centric video
tasks by learning human motion representations from large-scale and
heterogeneous data resources. Specifically, we propose a pretraining stage in
which a motion encoder is trained to recover the underlying 3D motion from
noisy partial 2D observations. The motion representations acquired in this way
incorporate geometric, kinematic, and physical knowledge about human motion,
which can be easily transferred to multiple downstream tasks. We implement the
motion encoder with a Dual-stream Spatio-temporal Transformer (DSTformer)
neural network. It could capture long-range spatio-temporal relationships among
the skeletal joints comprehensively and adaptively, exemplified by the lowest
3D pose estimation error so far when trained from scratch. Furthermore, our
proposed framework achieves state-of-the-art performance on all three
downstream tasks by simply finetuning the pretrained motion encoder with a
simple regression head (1-2 layers), which demonstrates the versatility of the
learned motion representations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://motionbert.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fully <span class="highlight-title">Self-Supervised</span> Depth Estimation from Defocus Clue <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10752v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10752v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haozhe Si, Bin Zhao, Dong Wang, Yupeng Gao, Mulin Chen, Zhigang Wang, Xuelong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Depth-from-defocus (DFD), modeling the relationship between depth and defocus
pattern in images, has demonstrated promising performance in depth estimation.
Recently, several self-supervised works try to overcome the difficulties in
acquiring accurate depth ground-truth. However, they depend on the all-in-focus
(AIF) images, which cannot be captured in real-world scenarios. Such limitation
discourages the applications of DFD methods. To tackle this issue, we propose a
completely self-supervised framework that estimates depth purely from a sparse
focal stack. We show that our framework circumvents the needs for the depth and
AIF image ground-truth, and receives superior predictions, thus closing the gap
between the theoretical success of DFD works and their applications in the real
world. In particular, we propose (i) a more realistic setting for DFD tasks,
where no depth or AIF image ground-truth is available; (ii) a novel
self-supervision framework that provides reliable predictions of depth and AIF
image under the challenging setting. The proposed framework uses a neural model
to predict the depth and AIF image, and utilizes an optical model to validate
and refine the prediction. We verify our framework on three benchmark datasets
with rendered focal stacks and real focal stacks. Qualitative and quantitative
evaluations show that our method provides a strong baseline for self-supervised
DFD tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023 camera-ready version. The code is released at
  https://github.com/Ehzoahis/DEReD</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UniDAformer: Unified Domain Adaptive Panoptic Segmentation <span class="highlight-title">Transformer</span>
  via Hierarchical Mask Calibration <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.15083v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.15083v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyi Zhang, Jiaxing Huang, Xiaoqin Zhang, Shijian Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain adaptive panoptic segmentation aims to mitigate data annotation
challenge by leveraging off-the-shelf annotated data in one or multiple related
source domains. However, existing studies employ two separate networks for
instance segmentation and semantic segmentation which lead to excessive network
parameters as well as complicated and computationally intensive training and
inference processes. We design UniDAformer, a unified domain adaptive panoptic
segmentation transformer that is simple but can achieve domain adaptive
instance segmentation and semantic segmentation simultaneously within a single
network. UniDAformer introduces Hierarchical Mask Calibration (HMC) that
rectifies inaccurate predictions at the level of regions, superpixels and
pixels via online self-training on the fly. It has three unique features: 1) it
enables unified domain adaptive panoptic adaptation; 2) it mitigates false
predictions and improves domain adaptive panoptic segmentation effectively; 3)
it is end-to-end trainable with a much simpler training and inference pipeline.
Extensive experiments over multiple public benchmarks show that UniDAformer
achieves superior domain adaptive panoptic segmentation as compared with the
state-of-the-art.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ StyleRF: Zero-shot 3D Style Transfer of Neural Radiance Fields <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10598v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10598v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kunhao Liu, Fangneng Zhan, Yiwen Chen, Jiahui Zhang, Yingchen Yu, Abdulmotaleb El Saddik, Shijian Lu, Eric Xing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D style transfer aims to render stylized novel views of a 3D scene with
multi-view consistency. However, most existing work suffers from a three-way
dilemma over accurate geometry reconstruction, high-quality stylization, and
being generalizable to arbitrary new styles. We propose StyleRF (Style Radiance
Fields), an innovative 3D style transfer technique that resolves the three-way
dilemma by performing style transformation within the feature space of a
radiance field. StyleRF employs an explicit grid of high-level features to
represent 3D scenes, with which high-fidelity geometry can be reliably restored
via volume rendering. In addition, it transforms the grid features according to
the reference style which directly leads to high-quality zero-shot style
transfer. StyleRF consists of two innovative designs. The first is
sampling-invariant content transformation that makes the transformation
invariant to the holistic statistics of the sampled 3D points and accordingly
ensures multi-view consistency. The second is deferred style transformation of
2D feature maps which is equivalent to the transformation of 3D points but
greatly reduces memory footprint without degrading multi-view consistency.
Extensive experiments show that StyleRF achieves superior 3D stylization
quality with precise geometry reconstruction and it can generalize to various
new styles in a zero-shot manner.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2023. Project website:
  https://kunhao-liu.github.io/StyleRF/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PLA: Language-Driven Open-Vocabulary 3D Scene Understanding <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.16312v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.16312v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Runyu Ding, Jihan Yang, Chuhui Xue, Wenqing Zhang, Song Bai, Xiaojuan Qi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-vocabulary scene understanding aims to localize and recognize unseen
categories beyond the annotated label space. The recent breakthrough of 2D
open-vocabulary perception is largely driven by Internet-scale paired
image-text data with rich vocabulary concepts. However, this success cannot be
directly transferred to 3D scenarios due to the inaccessibility of large-scale
3D-text pairs. To this end, we propose to distill knowledge encoded in
pre-trained vision-language (VL) foundation models through captioning
multi-view images from 3D, which allows explicitly associating 3D and
semantic-rich captions. Further, to foster coarse-to-fine visual-semantic
representation learning from captions, we design hierarchical 3D-caption pairs,
leveraging geometric constraints between 3D scenes and multi-view images.
Finally, by employing contrastive learning, the model learns language-aware
embeddings that connect 3D and text for open-vocabulary tasks. Our method not
only remarkably outperforms baseline methods by 25.8% $\sim$ 44.7% hIoU and
14.5% $\sim$ 50.4% hAP$_{50}$ in open-vocabulary semantic and instance
segmentation, but also shows robust transferability on challenging zero-shot
domain transfer tasks. See the project website at
https://dingry.github.io/projects/PLA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DA-DETR: Domain Adaptive Detection <span class="highlight-title">Transformer</span> with Information Fusion <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2103.17084v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2103.17084v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyi Zhang, Jiaxing Huang, Zhipeng Luo, Gongjie Zhang, Xiaoqin Zhang, Shijian Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent detection transformer (DETR) simplifies the object detection
pipeline by removing hand-crafted designs and hyperparameters as employed in
conventional two-stage object detectors. However, how to leverage the simple
yet effective DETR architecture in domain adaptive object detection is largely
neglected. Inspired by the unique DETR attention mechanisms, we design DA-DETR,
a domain adaptive object detection transformer that introduces information
fusion for effective transfer from a labeled source domain to an unlabeled
target domain. DA-DETR introduces a novel CNN-Transformer Blender (CTBlender)
that fuses the CNN features and Transformer features ingeniously for effective
feature alignment and knowledge transfer across domains. Specifically,
CTBlender employs the Transformer features to modulate the CNN features across
multiple scales where the high-level semantic information and the low-level
spatial information are fused for accurate object identification and
localization. Extensive experiments show that DA-DETR achieves superior
detection performance consistently across multiple widely adopted domain
adaptation benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HRDFuse: Monocular 360°Depth Estimation by Collaboratively Learning
  Holistic-with-Regional Depth Distributions <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11616v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11616v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Ai, Zidong cao, Yan-pei Cao, Ying Shan, Lin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Depth estimation from a monocular 360{\deg} image is a burgeoning problem
owing to its holistic sensing of a scene. Recently, some methods, \eg,
OmniFusion, have applied the tangent projection (TP) to represent a
360{\deg}image and predicted depth values via patch-wise regressions, which are
merged to get a depth map with equirectangular projection (ERP) format.
However, these methods suffer from 1) non-trivial process of merging plenty of
patches; 2) capturing less holistic-with-regional contextual information by
directly regressing the depth value of each pixel. In this paper, we propose a
novel framework, \textbf{HRDFuse}, that subtly combines the potential of
convolutional neural networks (CNNs) and transformers by collaboratively
learning the \textit{holistic} contextual information from the ERP and the
\textit{regional} structural information from the TP. Firstly, we propose a
spatial feature alignment (\textbf{SFA}) module that learns feature
similarities between the TP and ERP to aggregate the TP features into a
complete ERP feature map in a pixel-wise manner. Secondly, we propose a
collaborative depth distribution classification (\textbf{CDDC}) module that
learns the \textbf{holistic-with-regional} histograms capturing the ERP and TP
depth distributions. As such, the final depth values can be predicted as a
linear combination of histogram bin centers. Lastly, we adaptively combine the
depth predictions from ERP and TP to obtain the final depth map. Extensive
experiments show that our method predicts\textbf{ more smooth and accurate
depth} results while achieving \textbf{favorably better} results than the SOTA
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at CVPR2023, 20 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PanopticPartFormer++: A Unified and Decoupled View for Panoptic Part
  Segmentation <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.00954v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.00954v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangtai Li, Shilin Xu, Yibo Yang, Haobo Yuan, Guangliang Cheng, Yunhai Tong, Zhouchen Lin, Ming-Hsuan Yang, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Panoptic Part Segmentation (PPS) unifies panoptic and part segmentation into
one task. Previous works utilize separate approaches to handle things, stuff,
and part predictions without shared computation and task association. We aim to
unify these tasks at the architectural level, designing the first end-to-end
unified framework, Panoptic-PartFormer. Moreover, we find the previous metric
PartPQ biases to PQ. To handle both issues, we first design a meta-architecture
that decouples part features and things/stuff features, respectively. We model
things, stuff, and parts as object queries and directly learn to optimize all
three forms of prediction as a unified mask prediction and classification
problem. We term our model as Panoptic-PartFormer. Second, we propose a new
metric Part-Whole Quality (PWQ), better to measure this task from pixel-region
and part-whole perspectives. It also decouples the errors for part segmentation
and panoptic segmentation. Third, inspired by Mask2Former, based on our
meta-architecture, we propose Panoptic-PartFormer++ and design a new part-whole
cross-attention scheme to boost part segmentation qualities further. We design
a new part-whole interaction method using masked cross attention. Finally,
extensive ablation studies and analysis demonstrate the effectiveness of both
Panoptic-PartFormer and Panoptic-PartFormer++. Compared with previous
Panoptic-PartFormer, our Panoptic-PartFormer++ achieves 2% PartPQ and 3% PWQ
improvements on the Cityscapes PPS dataset and 5% PartPQ on the Pascal Context
PPS dataset. On both datasets, Panoptic-PartFormer++ achieves new
state-of-the-art results. Our models can serve as a strong baseline and aid
future research in PPS. The source code and trained models will be available
at~\url{https://github.com/lxtGH/Panoptic-PartFormer}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extension of PanopticPartFormer (ECCV 2022). Code:
  https://github.com/lxtGH/Panoptic-PartFormer. Update Results</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SVDiff: Compact Parameter Space for Diffusion Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11305v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11305v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ligong Han, Yinxiao Li, Han Zhang, Peyman Milanfar, Dimitris Metaxas, Feng Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have achieved remarkable success in text-to-image
generation, enabling the creation of high-quality images from text prompts or
other modalities. However, existing methods for customizing these models are
limited by handling multiple personalized subjects and the risk of overfitting.
Moreover, their large number of parameters is inefficient for model storage. In
this paper, we propose a novel approach to address these limitations in
existing text-to-image diffusion models for personalization. Our method
involves fine-tuning the singular values of the weight matrices, leading to a
compact and efficient parameter space that reduces the risk of overfitting and
language-drifting. We also propose a Cut-Mix-Unmix data-augmentation technique
to enhance the quality of multi-subject image generation and a simple
text-based image editing framework. Our proposed SVDiff method has a
significantly smaller model size (1.7MB for StableDiffusion) compared to
existing methods (vanilla DreamBooth 3.66GB, Custom Diffusion 73MB), making it
more practical for real-world applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 21 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An advanced YOLOv3 method for small object detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.02809v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.02809v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baokai Liu, Fengjie He, Shiqiang Du, Jiacheng Li, Wenjie Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Small object detection has important application value in the fields of
autonomous driving and drone scene analysis. As one of the most advanced object
detection algorithms, YOLOv3 suffers some challenges when detecting small
objects, such as the problem of detection failure of small objects and occluded
objects. To solve these problems, an improved YOLOv3 algorithm for small object
detection is proposed. In the proposed method, the dilated convolutions mish
(DCM) module is introduced into the backbone network of YOLOv3 to improve the
feature expression ability by fusing the feature maps of different receptive
fields. In the neck network of YOLOv3, the convolutional block attention module
(CBAM) and multi-level fusion module are introduced to select the important
information for small object detection in the shallow network, suppress the
uncritical information, and use the fusion module to fuse the feature maps of
different scales, so as to improve the detection accuracy of the algorithm. In
addition, the Soft-NMS and Complete-IoU (CloU) strategies are applied to
candidate frame screening, which improves the accuracy of the algorithm for the
detection of occluded objects. The ablation experiment of the MS COCO2017
object detection task proves the effectiveness of several modules introduced in
this paper for small object detection. The experimental results on the MS
COCO2017, VOC2007, and VOC2012 datasets show that the Average Precision (AP) of
this method is 16.5%, 8.71%, and 9.68% higher than that of YOLOv3,
respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Referring Image Matting <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.05149v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.05149v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jizhizi Li, Jing Zhang, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Different from conventional image matting, which either requires user-defined
scribbles/trimap to extract a specific foreground object or directly extracts
all the foreground objects in the image indiscriminately, we introduce a new
task named Referring Image Matting (RIM) in this paper, which aims to extract
the meticulous alpha matte of the specific object that best matches the given
natural language description, thus enabling a more natural and simpler
instruction for image matting. First, we establish a large-scale challenging
dataset RefMatte by designing a comprehensive image composition and expression
generation engine to automatically produce high-quality images along with
diverse text attributes based on public datasets. RefMatte consists of 230
object categories, 47,500 images, 118,749 expression-region entities, and
474,996 expressions. Additionally, we construct a real-world test set with 100
high-resolution natural images and manually annotate complex phrases to
evaluate the out-of-domain generalization abilities of RIM methods.
Furthermore, we present a novel baseline method CLIPMat for RIM, including a
context-embedded prompt, a text-driven semantic pop-up, and a multi-level
details extractor. Extensive experiments on RefMatte in both keyword and
expression settings validate the superiority of CLIPMat over representative
methods. We hope this work could provide novel insights into image matting and
encourage more follow-up studies. The dataset, code and models are available at
https://github.com/JizhiziLi/RIM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR2023. The dataset, code and models are available at
  https://github.com/JizhiziLi/RIM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ExtremeNeRF: Few-shot Neural Radiance Fields Under Unconstrained
  Illumination 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11728v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11728v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        SeokYeong Lee, JunYong Choi, Seungryong Kim, Ig-Jae Kim, Junghyun Cho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a new challenge that synthesizes a novel view in a
more practical environment, where the number of input multi-view images is
limited and illumination variations are significant. Despite recent success,
neural radiance fields (NeRF) require a massive amount of input multi-view
images taken under constrained illuminations. To address the problem, we
suggest ExtremeNeRF, which utilizes occlusion-aware multiview albedo
consistency, supported by geometric alignment and depth consistency. We extract
intrinsic image components that should be illumination-invariant across
different views, enabling direct appearance comparison between the input and
novel view under unconstrained illumination. We provide extensive experimental
results for an evaluation of the task, using the newly built NeRF Extreme
benchmark, which is the first in-the-wild novel view synthesis benchmark taken
under multiple viewing directions and varying illuminations. The project page
is at https://seokyeong94.github.io/ExtremeNeRF/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://seokyeong94.github.io/ExtremeNeRF/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PVT++: A Simple End-to-End Latency-Aware Visual Tracking Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.11629v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.11629v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Li, Ziyuan Huang, Junjie Ye, Yiming Li, Sebastian Scherer, Hang Zhao, Changhong Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual object tracking is essential to intelligent robots. Most existing
approaches have ignored the online latency that can cause severe performance
degradation during real-world processing. Especially for unmanned aerial
vehicles (UAVs), where robust tracking is more challenging and onboard
computation is limited, the latency issue can be fatal. In this work, we
present a simple framework for end-to-end latency-aware tracking, i.e.,
end-to-end predictive visual tracking (PVT++). Unlike existing solutions that
naively append Kalman Filters after trackers, PVT++ can be jointly optimized,
so that it takes not only motion information but can also leverage the rich
visual knowledge in most pre-trained tracker models for robust prediction.
Besides, to bridge the training-evaluation domain gap, we propose a relative
motion factor, empowering PVT++ to generalize to the challenging and complex
UAV tracking scenes. These careful designs have made the small-capacity
lightweight PVT++ a widely effective solution. Additionally, this work presents
an extended latency-aware evaluation benchmark for assessing an any-speed
tracker in the online setting. Empirical results on a robotic platform from the
aerial perspective show that PVT++ can achieve significant performance gain on
various trackers and exhibit higher accuracy than prior solutions, largely
mitigating the degradation brought by latency. Our code will be made public.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WDiscOOD: Out-of-Distribution Detection via Whitened Linear Discriminant
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07543v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07543v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiye Chen, Yunzhi Lin, Ruinian Xu, Patricio A. Vela
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks are susceptible to generating overconfident yet
erroneous predictions when presented with data beyond known concepts. This
challenge underscores the importance of detecting out-of-distribution (OOD)
samples in the open world. In this work, we propose a novel feature-space OOD
detection score that jointly reasons with both class-specific and
class-agnostic information. Specifically, our approach utilizes Whitened Linear
Discriminant Analysis to project features into two subspaces - the
discriminative and residual subspaces - in which the ID classes are maximally
separated and closely clustered, respectively. The OOD score is then determined
by combining the deviation from the input data to the ID distribution in both
subspaces. The efficacy of our method, named WDiscOOD, is verified on the
large-scale ImageNet-1k benchmark, with six OOD datasets that covers a variety
of distribution shifts. WDiscOOD demonstrates superior performance on deep
classifiers with diverse backbone architectures, including CNN and vision
transformer. Furthermore, we also show that our method can more effectively
detect novel concepts in representation space trained with contrastive
objectives, including supervised contrastive loss and multi-modality
contrastive loss.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VINet: Lightweight, Scalable, and Heterogeneous Cooperative Perception
  for 3D Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07060v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07060v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengwei Bai, Guoyuan Wu, Matthew J. Barth, Yongkang Liu, Emrah Akin Sisbot, Kentaro Oguchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Utilizing the latest advances in Artificial Intelligence (AI), the computer
vision community is now witnessing an unprecedented evolution in all kinds of
perception tasks, particularly in object detection. Based on multiple spatially
separated perception nodes, Cooperative Perception (CP) has emerged to
significantly advance the perception of automated driving. However, current
cooperative object detection methods mainly focus on ego-vehicle efficiency
without considering the practical issues of system-wide costs. In this paper,
we introduce VINet, a unified deep learning-based CP network for scalable,
lightweight, and heterogeneous cooperative 3D object detection. VINet is the
first CP method designed from the standpoint of large-scale system-level
implementation and can be divided into three main phases: 1) Global
Pre-Processing and Lightweight Feature Extraction which prepare the data into
global style and extract features for cooperation in a lightweight manner; 2)
Two-Stream Fusion which fuses the features from scalable and heterogeneous
perception nodes; and 3) Central Feature Backbone and 3D Detection Head which
further process the fused features and generate cooperative detection results.
An open-source data experimental platform is designed and developed for CP
dataset acquisition and model evaluation. The experimental analysis shows that
VINet can reduce 84% system-level computational cost and 94% system-level
communication cost while improving the 3D detection accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Causal Reasoning Meets Visual Representation Learning: A Prospective
  Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.12037v8">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.12037v8.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Liu, Yushen Wei, Hong Yan, Guanbin Li, Liang Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual representation learning is ubiquitous in various real-world
applications, including visual comprehension, video understanding, multi-modal
analysis, human-computer interaction, and urban computing. Due to the emergence
of huge amounts of multi-modal heterogeneous spatial/temporal/spatial-temporal
data in big data era, the lack of interpretability, robustness, and
out-of-distribution generalization are becoming the challenges of the existing
visual models. The majority of the existing methods tend to fit the original
data/variable distributions and ignore the essential causal relations behind
the multi-modal knowledge, which lacks unified guidance and analysis about why
modern visual representation learning methods easily collapse into data bias
and have limited generalization and cognitive abilities. Inspired by the strong
inference ability of human-level agents, recent years have therefore witnessed
great effort in developing causal reasoning paradigms to realize robust
representation and model learning with good cognitive ability. In this paper,
we conduct a comprehensive review of existing causal reasoning methods for
visual representation learning, covering fundamental theories, models, and
datasets. The limitations of current methods and datasets are also discussed.
Moreover, we propose some prospective challenges, opportunities, and future
research directions for benchmarking causal reasoning algorithms in visual
representation learning. This paper aims to provide a comprehensive overview of
this emerging field, attract attention, encourage discussions, bring to the
forefront the urgency of developing novel causal reasoning methods, publicly
available benchmarks, and consensus-building standards for reliable visual
representation learning and related real-world applications more efficiently.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 14 figures. This work has been accepted by Machine
  Intelligence Research. The arxiv version is kept updating by adding more
  novel methods, datasets and insights. The official video interpretation of
  this paper can be referred at https://youtu.be/2lfNaTkcTHI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VoP: Text-Video Co-operative <span class="highlight-title">Prompt</span> Tuning for Cross-Modal Retrieval <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.12764v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.12764v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siteng Huang, Biao Gong, Yulin Pan, Jianwen Jiang, Yiliang Lv, Yuyuan Li, Donglin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many recent studies leverage the pre-trained CLIP for text-video cross-modal
retrieval by tuning the backbone with additional heavy modules, which not only
brings huge computational burdens with much more parameters, but also leads to
the knowledge forgetting from upstream models. In this work, we propose the
VoP: Text-Video Co-operative Prompt Tuning for efficient tuning on the
text-video retrieval task. The proposed VoP is an end-to-end framework with
both video & text prompts introducing, which can be regarded as a powerful
baseline with only 0.1% trainable parameters. Further, based on the
spatio-temporal characteristics of videos, we develop three novel video prompt
mechanisms to improve the performance with different scales of trainable
parameters. The basic idea of the VoP enhancement is to model the frame
position, frame context, and layer function with specific trainable prompts,
respectively. Extensive experiments show that compared to full fine-tuning, the
enhanced VoP achieves a 1.4% average R@1 gain across five text-video retrieval
benchmarks with 6x less parameter overhead. The code will be available at
https://github.com/bighuang624/VoP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Text-Visual <span class="highlight-title">Prompt</span>ing for Efficient 2D Temporal Video Grounding <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04995v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04995v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yimeng Zhang, Xin Chen, Jinghan Jia, Sijia Liu, Ke Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we study the problem of temporal video grounding (TVG), which
aims to predict the starting/ending time points of moments described by a text
sentence within a long untrimmed video. Benefiting from fine-grained 3D visual
features, the TVG techniques have achieved remarkable progress in recent years.
However, the high complexity of 3D convolutional neural networks (CNNs) makes
extracting dense 3D visual features time-consuming, which calls for intensive
memory and computing resources. Towards efficient TVG, we propose a novel
text-visual prompting (TVP) framework, which incorporates optimized
perturbation patterns (that we call 'prompts') into both visual inputs and
textual features of a TVG model. In sharp contrast to 3D CNNs, we show that TVP
allows us to effectively co-train vision encoder and language encoder in a 2D
TVG model and improves the performance of crossmodal feature fusion using only
low-complexity sparse 2D visual features. Further, we propose a
Temporal-Distance IoU (TDIoU) loss for efficient learning of TVG. Experiments
on two benchmark datasets, Charades-STA and ActivityNet Captions datasets,
empirically show that the proposed TVP significantly boosts the performance of
2D TVG (e.g., 9.79% improvement on Charades-STA and 30.77% improvement on
ActivityNet Captions) and achieves 5x inference acceleration over TVG using 3D
visual features. Codes are available at Open.Intel.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Representation Uncertainty in <span class="highlight-title">Self-Supervised</span> Learning as Variational
  Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11437v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11437v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hiroki Nakamura, Masashi Okada, Tadahiro Taniguchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, a novel self-supervised learning (SSL) method is proposed,
which learns not only representations but also representations uncertainties by
considering SSL in terms of variational inference. SSL is a method of learning
representation without labels by maximizing the similarity between image
representations of different augmented views of the same image. Variational
autoencoder (VAE) is an unsupervised representation learning method that trains
a probabilistic generative model with variational inference. VAE and SSL can
learn representations without labels, but the relationship between VAE and SSL
has not been revealed. In this paper, the theoretical relationship between SSL
and variational inference is clarified. In addition, variational inference
SimSiam (VI-SimSiam) is proposed, which can predict the representation
uncertainty by interpreting SimSiam with variational inference and defining the
latent space distribution. The experiment qualitatively showed that VISimSiam
could learn uncertainty by comparing input images and predicted uncertainties.
We also revealed a relationship between estimated uncertainty and
classification accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 12 figures, work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RIAV-MVS: Recurrent-Indexing an Asymmetric Volume for Multi-View Stereo <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.14320v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.14320v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changjiang Cai, Pan Ji, Qingan Yan, Yi Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a learning-based method for multi-view depth estimation
from posed images. Our core idea is a "learning-to-optimize" paradigm that
iteratively indexes a plane-sweeping cost volume and regresses the depth map
via a convolutional Gated Recurrent Unit (GRU). Since the cost volume plays a
paramount role in encoding the multi-view geometry, we aim to improve its
construction both at pixel- and frame- levels. At the pixel level, we propose
to break the symmetry of the Siamese network (which is typically used in MVS to
extract image features) by introducing a transformer block to the reference
image (but not to the source images). Such an asymmetric volume allows the
network to extract global features from the reference image to predict its
depth map. Given potential inaccuracies in the poses between reference and
source images, we propose to incorporate a residual pose network to correct the
relative poses. This essentially rectifies the cost volume at the frame level.
We conduct extensive experiments on real-world MVS datasets and show that our
method achieves state-of-the-art performance in terms of both within-dataset
evaluation and cross-dataset generalization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fix the Noise: Disentangling Source Feature for Transfer Learning of
  StyleGAN <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.14079v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.14079v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongyeun Lee, Jae Young Lee, Doyeon Kim, Jaehyun Choi, Junmo Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transfer learning of StyleGAN has recently shown great potential to solve
diverse tasks, especially in domain translation. Previous methods utilized a
source model by swapping or freezing weights during transfer learning, however,
they have limitations on visual quality and controlling source features. In
other words, they require additional models that are computationally demanding
and have restricted control steps that prevent a smooth transition. In this
paper, we propose a new approach to overcome these limitations. Instead of
swapping or freezing, we introduce a simple feature matching loss to improve
generation quality. In addition, to control the degree of source features, we
train a target model with the proposed strategy, FixNoise, to preserve the
source features only in a disentangled subspace of a target feature space.
Owing to the disentangled feature space, our method can smoothly control the
degree of the source features in a single model. Extensive experiments
demonstrate that the proposed method can generate more consistent and realistic
images than previous works.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Full CVPR 2023 paper is available at arXiv:2303.11545. Best paper of
  CVPRW AICC 2022 (CVPR 2022 Workshop on AI for Content Creation). The code is
  available at https://github.com/LeeDongYeun/FixNoise</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SDFusion: Multimodal 3D Shape Completion, Reconstruction, and Generation <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.04493v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.04493v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yen-Chi Cheng, Hsin-Ying Lee, Sergey Tulyakov, Alexander Schwing, Liangyan Gui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we present a novel framework built to simplify 3D asset
generation for amateur users. To enable interactive generation, our method
supports a variety of input modalities that can be easily provided by a human,
including images, text, partially observed shapes and combinations of these,
further allowing to adjust the strength of each input. At the core of our
approach is an encoder-decoder, compressing 3D shapes into a compact latent
representation, upon which a diffusion model is learned. To enable a variety of
multi-modal inputs, we employ task-specific encoders with dropout followed by a
cross-attention mechanism. Due to its flexibility, our model naturally supports
a variety of tasks, outperforming prior works on shape completion, image-based
3D reconstruction, and text-to-3D. Most interestingly, our model can combine
all these tasks into one swiss-army-knife tool, enabling the user to perform
shape generation using incomplete shapes, images, and textual descriptions at
the same time, providing the relative weights for each input and facilitating
interactivity. Despite our approach being shape-only, we further show an
efficient method to texture the generated shape using large-scale text-to-image
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In CVPR 2023. Project page and code is available at:
  https://yccyenchicheng.github.io/SDFusion/. Fix some typos</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ StructVPR: Distill Structural Knowledge with Weighting Samples for
  Visual Place Recognition <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.00937v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.00937v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanqing Shen, Sanping Zhou, Jingwen Fu, Ruotong Wang, Shitao Chen, Nanning Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual place recognition (VPR) is usually considered as a specific image
retrieval problem. Limited by existing training frameworks, most deep
learning-based works cannot extract sufficiently stable global features from
RGB images and rely on a time-consuming re-ranking step to exploit spatial
structural information for better performance. In this paper, we propose
StructVPR, a novel training architecture for VPR, to enhance structural
knowledge in RGB global features and thus improve feature stability in a
constantly changing environment. Specifically, StructVPR uses segmentation
images as a more definitive source of structural knowledge input into a CNN
network and applies knowledge distillation to avoid online segmentation and
inference of seg-branch in testing. Considering that not all samples contain
high-quality and helpful knowledge, and some even hurt the performance of
distillation, we partition samples and weigh each sample's distillation loss to
enhance the expected knowledge precisely. Finally, StructVPR achieves
impressive performance on several benchmarks using only global retrieval and
even outperforms many two-stage approaches by a large margin. After adding
additional re-ranking, ours achieves state-of-the-art performance while
maintaining a low computational cost.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by CVPR2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Privacy-preserving Pedestrian Tracking using Distributed 3D LiDARs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.09915v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.09915v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Masakazu Ohno, Riki Ukyo, Tatsuya Amano, Hamada Rizk, Hirozumi Yamaguchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing demand for intelligent environments unleashes an extraordinary
cycle of privacy-aware applications that makes individuals' life more
comfortable and safe. Examples of these applications include pedestrian
tracking systems in large areas. Although the ubiquity of camera-based systems,
they are not a preferable solution due to the vulnerability of leaking the
privacy of pedestrians. In this paper, we introduce a novel privacy-preserving
system for pedestrian tracking in smart environments using multiple distributed
LiDARs of non-overlapping views. The system is designed to leverage LiDAR
devices to track pedestrians in partially covered areas due to practical
constraints, e.g., occlusion or cost. Therefore, the system uses the point
cloud captured by different LiDARs to extract discriminative features that are
used to train a metric learning model for pedestrian matching purposes. To
boost the system's robustness, we leverage a probabilistic approach to model
and adapt the dynamic mobility patterns of individuals and thus connect their
sub-trajectories. We deployed the system in a large-scale testbed with 70
colorless LiDARs and conducted three different experiments. The evaluation
result at the entrance hall confirms the system's ability to accurately track
the pedestrians with a 0.98 F-measure even with zero-covered areas. This result
highlights the promise of the proposed system as the next generation of
privacy-preserving tracking means in smart environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 21st International Conference on Pervasive Computing
  and Communications (PerCom 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhanced Training of Query-Based Object Detection via Selective Query
  Recollection <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07593v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07593v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangyi Chen, Han Zhang, Kai Hu, Yu-kai Huang, Chenchen Zhu, Marios Savvides
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates a phenomenon where query-based object detectors
mispredict at the last decoding stage while predicting correctly at an
intermediate stage. We review the training process and attribute the overlooked
phenomenon to two limitations: lack of training emphasis and cascading errors
from decoding sequence. We design and present Selective Query Recollection
(SQR), a simple and effective training strategy for query-based object
detectors. It cumulatively collects intermediate queries as decoding stages go
deeper and selectively forwards the queries to the downstream stages aside from
the sequential structure. Such-wise, SQR places training emphasis on later
stages and allows later stages to work with intermediate queries from earlier
stages directly. SQR can be easily plugged into various query-based object
detectors and significantly enhances their performance while leaving the
inference pipeline unchanged. As a result, we apply SQR on Adamixer, DAB-DETR,
and Deformable-DETR across various settings (backbone, number of queries,
schedule) and consistently brings 1.4-2.8 AP improvement.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR2023</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ End-to-End Personalized Next Location Recommendation via Contrastive
  User Preference Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12507v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12507v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Luo, Ye Liu, Fu-lai Chung, Yu Liu, Chang Wen Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predicting the next location is a highly valuable and common need in many
location-based services such as destination prediction and route planning. The
goal of next location recommendation is to predict the next point-of-interest a
user might go to based on the user's historical trajectory. Most existing
models learn mobility patterns merely from users' historical check-in sequences
while overlooking the significance of user preference modeling. In this work, a
novel Point-of-Interest Transformer (POIFormer) with contrastive user
preference modeling is developed for end-to-end next location recommendation.
This model consists of three major modules: history encoder, query generator,
and preference decoder. History encoder is designed to model mobility patterns
from historical check-in sequences, while query generator explicitly learns
user preferences to generate user-specific intention queries. Finally,
preference decoder combines the intention queries and historical information to
predict the user's next location. Extensive comparisons with representative
schemes and ablation studies on four real-world datasets demonstrate the
effectiveness and superiority of the proposed scheme under various settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DeepProphet2 -- A Deep Learning Gene Recommendation Engine 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.01918v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.01918v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniele Brambilla, Davide Maria Giacomini, Luca Muscarnera, Andrea Mazzoleni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  New powerful tools for tackling life science problems have been created by
recent advances in machine learning. The purpose of the paper is to discuss the
potential advantages of gene recommendation performed by artificial
intelligence (AI). Indeed, gene recommendation engines try to solve this
problem: if the user is interested in a set of genes, which other genes are
likely to be related to the starting set and should be investigated? This task
was solved with a custom deep learning recommendation engine, DeepProphet2
(DP2), which is freely available to researchers worldwide via
https://www.generecommender.com?utm_source=DeepProphet2_paper&utm_medium=pdf.
Hereafter, insights behind the algorithm and its practical applications are
illustrated.
  The gene recommendation problem can be addressed by mapping the genes to a
metric space where a distance can be defined to represent the real semantic
distance between them. To achieve this objective a transformer-based model has
been trained on a well-curated freely available paper corpus, PubMed. The paper
describes multiple optimization procedures that were employed to obtain the
best bias-variance trade-off, focusing on embedding size and network depth. In
this context, the model's ability to discover sets of genes implicated in
diseases and pathways was assessed through cross-validation. A simple
assumption guided the procedure: the network had no direct knowledge of
pathways and diseases but learned genes' similarities and the interactions
among them. Moreover, to further investigate the space where the neural network
represents genes, the dimensionality of the embedding was reduced, and the
results were projected onto a human-comprehensible space. In conclusion, a set
of use cases illustrates the algorithm's potential applications in a real word
setting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Focusing on Potential Named Entities During Active Label Acquisition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.03837v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.03837v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Osman Berk Sapci, Oznur Tastan, Reyyan Yeniterzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Named entity recognition (NER) aims to identify mentions of named entities in
an unstructured text and classify them into predefined named entity classes.
While deep learning-based pre-trained language models help to achieve good
predictive performances in NER, many domain-specific NER applications still
call for a substantial amount of labeled data. Active learning (AL), a general
framework for the label acquisition problem, has been used for NER tasks to
minimize the annotation cost without sacrificing model performance. However,
the heavily imbalanced class distribution of tokens introduces challenges in
designing effective AL querying methods for NER. We propose several AL sentence
query evaluation functions that pay more attention to potential positive
tokens, and evaluate these proposed functions with both sentence-based and
token-based cost evaluation strategies. We also propose a better data-driven
normalization approach to penalize sentences that are too long or too short.
Our experiments on three datasets from different domains reveal that the
proposed approach reduces the number of annotated tokens while achieving better
or comparable prediction performance with conventional methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">134</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Open-source Frame Semantic Parsing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12788v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12788v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Chanin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While the state-of-the-art for frame semantic parsing has progressed
dramatically in recent years, it is still difficult for end-users to apply
state-of-the-art models in practice. To address this, we present Frame Semantic
Transformer, an open-source Python library which achieves near state-of-the-art
performance on FrameNet 1.7, while focusing on ease-of-use. We use a T5 model
fine-tuned on Propbank and FrameNet exemplars as a base, and improve
performance by using FrameNet lexical units to provide hints to T5 at inference
time. We enhance robustness to real-world data by using textual data
augmentations during training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Matryoshka Policy Gradient for Entropy-Regularized RL: Convergence and
  Global Optimality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12785v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12785v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        François Ged, Maria Han Veiga
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A novel Policy Gradient (PG) algorithm, called Matryoshka Policy Gradient
(MPG), is introduced and studied, in the context of max-entropy reinforcement
learning, where an agent aims at maximising entropy bonuses additional to its
cumulative rewards. MPG differs from standard PG in that it trains a sequence
of policies to learn finite horizon tasks simultaneously, instead of a single
policy for the single standard objective. For softmax policies, we prove
convergence of MPG and global optimality of the limit by showing that the only
critical point of the MPG objective is the optimal policy; these results hold
true even in the case of continuous compact state space. MPG is intuitive,
theoretically sound and we furthermore show that the optimal policy of the
standard max-entropy objective can be approximated arbitrarily well by the
optimal policy of the MPG framework. Finally, we justify that MPG is well
suited when the policies are parametrized with neural networks and we provide
an simple criterion to verify the global optimality of the policy at
convergence. As a proof of concept, we evaluate numerically MPG on standard
test benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Conformal Prediction for Time Series with Modern Hopfield Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12783v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12783v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreas Auer, Martin Gauch, Daniel Klotz, Sepp Hochreiter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To quantify uncertainty, conformal prediction methods are gaining
continuously more interest and have already been successfully applied to
various domains. However, they are difficult to apply to time series as the
autocorrelative structure of time series violates basic assumptions required by
conformal prediction. We propose HopCPT, a novel conformal prediction approach
for time series that not only copes with temporal structures but leverages
them. We show that our approach is theoretically well justified for time series
where temporal dependencies are present. In experiments, we demonstrate that
our new approach outperforms state-of-the-art conformal prediction methods on
multiple real-world time series datasets from four different domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can we trust the evaluation on Chat<span class="highlight-title">GPT</span>? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12767v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12767v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rachith Aiyappa, Jisun An, Haewoon Kwak, Yong-Yeol Ahn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ChatGPT, the first large language model (LLM) with mass adoption, has
demonstrated remarkable performance in numerous natural language tasks. Despite
its evident usefulness, evaluating ChatGPT's performance in diverse problem
domains remains challenging due to the closed nature of the model and its
continuous updates via Reinforcement Learning from Human Feedback (RLHF). We
highlight the issue of data contamination in ChatGPT evaluations, with a case
study of the task of stance detection. We discuss the challenge of preventing
data contamination and ensuring fair model evaluation in the age of closed and
continuously trained models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LSTM-based Video Quality Prediction Accounting for Temporal Distortions
  in Videoconferencing Calls <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12761v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12761v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriel Mittag, Babak Naderi, Vishak Gopal, Ross Cutler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current state-of-the-art video quality models, such as VMAF, give excellent
prediction results by comparing the degraded video with its reference video.
However, they do not consider temporal distortions (e.g., frame freezes or
skips) that occur during videoconferencing calls. In this paper, we present a
data-driven approach for modeling such distortions automatically by training an
LSTM with subjective quality ratings labeled via crowdsourcing. The videos were
collected from live videoconferencing calls in 83 different network conditions.
We applied QR codes as markers on the source videos to create aligned
references and compute temporal features based on the alignment vectors. Using
these features together with VMAF core features, our proposed model achieves a
PCC of 0.99 on the validation set. Furthermore, our model outputs per-frame
quality that gives detailed insight into the cause of video quality
impairments. The VCM model and dataset are open-sourced at
https://github.com/microsoft/Video_Call_MOS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Strategy Synthesis in Markov Decision Processes Under Limited Sampling
  Access 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12718v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12718v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christel Baier, Clemens Dubslaff, Patrick Wienhöft, Stefan J. Kiebel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A central task in control theory, artificial intelligence, and formal methods
is to synthesize reward-maximizing strategies for agents that operate in
partially unknown environments. In environments modeled by gray-box Markov
decision processes (MDPs), the impact of the agents' actions are known in terms
of successor states but not the stochastics involved. In this paper, we devise
a strategy synthesis algorithm for gray-box MDPs via reinforcement learning
that utilizes interval MDPs as internal model. To compete with limited sampling
access in reinforcement learning, we incorporate two novel concepts into our
algorithm, focusing on rapid and successful learning rather than on stochastic
guarantees and optimality: lower confidence bound exploration reinforces
variants of already learned practical strategies and action scoping reduces the
learning action space to promising actions. We illustrate benefits of our
algorithms by means of a prototypical implementation applied on examples from
the AI and formal methods communities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at NASA Formal Methods (NFM) 2023. This is
  an extended version with the full appendix containing proofs, further
  pseudocode with explanations and additional experiment figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Causal Reasoning in the Presence of Latent Confounders via Neural ADMG
  Learning <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12703v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12703v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew Ashman, Chao Ma, Agrin Hilmkil, Joel Jennings, Cheng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Latent confounding has been a long-standing obstacle for causal reasoning
from observational data. One popular approach is to model the data using
acyclic directed mixed graphs (ADMGs), which describe ancestral relations
between variables using directed and bidirected edges. However, existing
methods using ADMGs are based on either linear functional assumptions or a
discrete search that is complicated to use and lacks computational tractability
for large datasets. In this work, we further extend the existing body of work
and develop a novel gradient-based approach to learning an ADMG with non-linear
functional relations from observational data. We first show that the presence
of latent confounding is identifiable under the assumptions of bow-free ADMGs
with non-linear additive noise models. With this insight, we propose a novel
neural causal model based on autoregressive flows for ADMG learning. This not
only enables us to determine complex causal structural relationships behind the
data in the presence of latent confounding, but also estimate their functional
relationships (hence treatment effects) simultaneously. We further validate our
approach via experiments on both synthetic and real-world datasets, and
demonstrate the competitive performance against relevant baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera ready version for ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Conformal Prediction by Reweighting Nonconformity Score 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12695v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12695v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Salim I. Amoukou, Nicolas J. B Brunel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite attractive theoretical guarantees and practical successes, Predictive
Interval (PI) given by Conformal Prediction (CP) may not reflect the
uncertainty of a given model. This limitation arises from CP methods using a
constant correction for all test points, disregarding their individual
uncertainties, to ensure coverage properties. To address this issue, we propose
using a Quantile Regression Forest (QRF) to learn the distribution of
nonconformity scores and utilizing the QRF's weights to assign more importance
to samples with residuals similar to the test point. This approach results in
PI lengths that are more aligned with the model's uncertainty. In addition, the
weights learnt by the QRF provide a partition of the features space, allowing
for more efficient computations and improved adaptiveness of the PI through
groupwise conformalization. Our approach enjoys an assumption-free finite
sample marginal and training-conditional coverage, and under suitable
assumptions, it also ensures conditional coverage. Our methods work for any
nonconformity score and are available as a Python package. We conduct
experiments on simulated and real-world data that demonstrate significant
improvements compared to existing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Extended Study of Human-like Behavior under Adversarial Training <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12669v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12669v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Gavrikov, Janis Keuper, Margret Keuper
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural networks have a number of shortcomings. Amongst the severest ones is
the sensitivity to distribution shifts which allows models to be easily fooled
into wrong predictions by small perturbations to inputs that are often
imperceivable to humans and do not have to carry semantic meaning. Adversarial
training poses a partial solution to address this issue by training models on
worst-case perturbations. Yet, recent work has also pointed out that the
reasoning in neural networks is different from humans. Humans identify objects
by shape, while neural nets mainly employ texture cues. Exemplarily, a model
trained on photographs will likely fail to generalize to datasets containing
sketches. Interestingly, it was also shown that adversarial training seems to
favorably increase the shift toward shape bias. In this work, we revisit this
observation and provide an extensive analysis of this effect on various
architectures, the common $\ell_2$- and $\ell_\infty$-training, and
Transformer-based models. Further, we provide a possible explanation for this
phenomenon from a frequency perspective.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, accepted at the CVPR 2023 Workshop "The 3rd Workshop of
  Adversarial Machine Learning on Computer Vision: Art of Robustness"</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Posthoc Interpretation via Quantization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12659v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12659v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cem Subakan, Francesco Paissan, Mirco Ravanelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce a new approach, called "Posthoc Interpretation
via Quantization (PIQ)", for interpreting decisions made by trained
classifiers. Our method utilizes vector quantization to transform the
representations of a classifier into a discrete, class-specific latent space.
The class-specific codebooks act as a bottleneck that forces the interpreter to
focus on the parts of the input data deemed relevant by the classifier for
making a prediction. We evaluated our method through quantitative and
qualitative studies and found that PIQ generates interpretations that are more
easily understood by participants to our user studies when compared to several
other interpretation methods in the literature.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>* Equal contribution</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reliable and Efficient Evaluation of Adversarial Robustness for Deep
  Hashing-Based Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12658v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12658v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xunguang Wang, Jiawang Bai, Xinyue Xu, Xiaomeng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep hashing has been extensively applied to massive image retrieval due to
its efficiency and effectiveness. Recently, several adversarial attacks have
been presented to reveal the vulnerability of deep hashing models against
adversarial examples. However, existing attack methods suffer from degraded
performance or inefficiency because they underutilize the semantic relations
between original samples or spend a lot of time learning these relations with a
deep neural network. In this paper, we propose a novel Pharos-guided Attack,
dubbed PgA, to evaluate the adversarial robustness of deep hashing networks
reliably and efficiently. Specifically, we design pharos code to represent the
semantics of the benign image, which preserves the similarity to semantically
relevant samples and dissimilarity to irrelevant ones. It is proven that we can
quickly calculate the pharos code via a simple math formula. Accordingly, PgA
can directly conduct a reliable and efficient attack on deep hashing-based
retrieval by maximizing the similarity between the hash code of the adversarial
example and the pharos code. Extensive experiments on the benchmark datasets
verify that the proposed algorithm outperforms the prior state-of-the-arts in
both attack strength and speed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2204.10779</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Traffic Volume Prediction using Memory-Based Recurrent Neural Networks:
  A comparative analysis of LSTM and GRU 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12643v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12643v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lokesh Chandra Das
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predicting traffic volume in real-time can improve both traffic flow and road
safety. A precise traffic volume forecast helps alert drivers to the flow of
traffic along their preferred routes, preventing potential deadlock situations.
Existing parametric models cannot reliably forecast traffic volume in dynamic
and complex traffic conditions. Therefore, in order to evaluate and forecast
the traffic volume for every given time step in a real-time manner, we develop
non-linear memory-based deep neural network models. Our extensive experiments
run on the Metro Interstate Traffic Volume dataset demonstrate the
effectiveness of the proposed models in predicting traffic volume in highly
dynamic and heterogeneous traffic environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Democratising AI: Multiple Meanings, Goals, and Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12642v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12642v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elizabeth Seger, Aviv Ovadya, Ben Garfinkel, Divya Siddarth, Allan Dafoe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Numerous parties are calling for the democratisation of AI, but the phrase is
used to refer to a variety of goals, the pursuit of which sometimes conflict.
This paper identifies four kinds of AI democratisation that are commonly
discussed: (1) the democratisation of AI use, (2) the democratisation of AI
development, (3) the democratisation of AI profits, and (4) the democratisation
of AI governance. Numerous goals and methods of achieving each form of
democratisation are discussed. The main takeaway from this paper is that AI
democratisation is a multifarious and sometimes conflicting concept that should
not be conflated with improving AI accessibility. If we want to move beyond
ambiguous commitments to democratising AI, to productive discussions of
concrete policies and trade-offs, then we need to recognise the principal role
of the democratisation of AI governance in navigating tradeoffs and risks
across decisions around use, development, and profits.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semi-supervised counterfactual explanations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12634v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12634v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shravan Kumar Sajja, Sumanta Mukherjee, Satyam Dwivedi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Counterfactual explanations for machine learning models are used to find
minimal interventions to the feature values such that the model changes the
prediction to a different output or a target output. A valid counterfactual
explanation should have likely feature values. Here, we address the challenge
of generating counterfactual explanations that lie in the same data
distribution as that of the training data and more importantly, they belong to
the target class distribution. This requirement has been addressed through the
incorporation of auto-encoder reconstruction loss in the counterfactual search
process. Connecting the output behavior of the classifier to the latent space
of the auto-encoder has further improved the speed of the counterfactual search
process and the interpretability of the resulting counterfactual explanations.
Continuing this line of research, we show further improvement in the
interpretability of counterfactual explanations when the auto-encoder is
trained in a semi-supervised fashion with class tagged input data. We
empirically evaluate our approach on several datasets and show considerable
improvement in-terms of several metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 3 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do Backdoors Assist Membership Inference Attacks? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12589v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12589v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yumeki Goto, Nami Ashizawa, Toshiki Shibahara, Naoto Yanai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When an adversary provides poison samples to a machine learning model,
privacy leakage, such as membership inference attacks that infer whether a
sample was included in the training of the model, becomes effective by moving
the sample to an outlier. However, the attacks can be detected because
inference accuracy deteriorates due to poison samples. In this paper, we
discuss a \textit{backdoor-assisted membership inference attack}, a novel
membership inference attack based on backdoors that return the adversary's
expected output for a triggered sample. We found three crucial insights through
experiments with an academic benchmark dataset. We first demonstrate that the
backdoor-assisted membership inference attack is unsuccessful. Second, when we
analyzed loss distributions to understand the reason for the unsuccessful
results, we found that backdoors cannot separate loss distributions of training
and non-training samples. In other words, backdoors cannot affect the
distribution of clean samples. Third, we also show that poison and triggered
samples activate neurons of different distributions. Specifically, backdoors
make any clean sample an inlier, contrary to poisoning samples. As a result, we
confirm that backdoors cannot assist membership inference.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neuro-Symbolic Reasoning Shortcuts: Mitigation Strategies and their
  Limitations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12578v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12578v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emanuele Marconato, Stefano Teso, Andrea Passerini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neuro-symbolic predictors learn a mapping from sub-symbolic inputs to
higher-level concepts and then carry out (probabilistic) logical inference on
this intermediate representation. This setup offers clear advantages in terms
of consistency to symbolic prior knowledge, and is often believed to provide
interpretability benefits in that - by virtue of complying with the knowledge -
the learned concepts can be better understood by human stakeholders. However,
it was recently shown that this setup is affected by reasoning shortcuts
whereby predictions attain high accuracy by leveraging concepts with unintended
semantics, yielding poor out-of-distribution performance and compromising
interpretability. In this short paper, we establish a formal link between
reasoning shortcuts and the optima of the loss function, and identify
situations in which reasoning shortcuts can arise. Based on this, we discuss
limitations of natural mitigation strategies such as reconstruction and concept
supervision.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Wasserstein Auto-encoded MDPs: Formal Verification of Efficiently
  Distilled RL Policies with Many-sided Guarantees <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12558v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12558v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florent Delgrange, Ann Nowé, Guillermo A. Pérez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although deep reinforcement learning (DRL) has many success stories, the
large-scale deployment of policies learned through these advanced techniques in
safety-critical scenarios is hindered by their lack of formal guarantees.
Variational Markov Decision Processes (VAE-MDPs) are discrete latent space
models that provide a reliable framework for distilling formally verifiable
controllers from any RL policy. While the related guarantees address relevant
practical aspects such as the satisfaction of performance and safety
properties, the VAE approach suffers from several learning flaws (posterior
collapse, slow learning speed, poor dynamics estimates), primarily due to the
absence of abstraction and representation guarantees to support latent
optimization. We introduce the Wasserstein auto-encoded MDP (WAE-MDP), a latent
space model that fixes those issues by minimizing a penalized form of the
optimal transport between the behaviors of the agent executing the original
policy and the distilled policy, for which the formal guarantees apply. Our
approach yields bisimulation guarantees while learning the distilled policy,
allowing concrete optimization of the abstraction and representation model
quality. Our experiments show that, besides distilling policies up to 10 times
faster, the latent model quality is indeed better in general. Moreover, we
present experiments from a simple time-to-failure verification algorithm on the
latent space. The fact that our approach enables such simple verification
techniques highlights its applicability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2023, 9 pages main text, 14 pages appendix (excluding
  references)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deployment of Image Analysis Algorithms under Prevalence Shifts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12540v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12540v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patrick Godau, Piotr Kalinowski, Evangelia Christodoulou, Annika Reinke, Minu Tizabi, Luciana Ferrer, Paul Jäger, Lena Maier-Hein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain gaps are among the most relevant roadblocks in the clinical
translation of machine learning (ML)-based solutions for medical image
analysis. While current research focuses on new training paradigms and network
architectures, little attention is given to the specific effect of prevalence
shifts on an algorithm deployed in practice. Such discrepancies between class
frequencies in the data used for a method's development/validation and that in
its deployment environment(s) are of great importance, for example in the
context of artificial intelligence (AI) democratization, as disease prevalences
may vary widely across time and location. Our contribution is twofold. First,
we empirically demonstrate the potentially severe consequences of missing
prevalence handling by analyzing (i) the extent of miscalibration, (ii) the
deviation of the decision threshold from the optimum, and (iii) the ability of
validation metrics to reflect neural network performance on the deployment
population as a function of the discrepancy between development and deployment
prevalence. Second, we propose a workflow for prevalence-aware image
classification that uses estimated deployment prevalences to adjust a trained
classifier to a new environment, without requiring additional annotated
deployment data. Comprehensive experiments based on a diverse set of 30 medical
classification tasks showcase the benefit of the proposed workflow in
generating better classifier decisions and more reliable performance estimates
compared to current practice.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Split-Et-Impera: A Framework for the Design of Distributed Deep Learning
  Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12524v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12524v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luigi Capogrosso, Federico Cunico, Michele Lora, Marco Cristani, Franco Fummi, Davide Quaglia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many recent pattern recognition applications rely on complex distributed
architectures in which sensing and computational nodes interact together
through a communication network. Deep neural networks (DNNs) play an important
role in this scenario, furnishing powerful decision mechanisms, at the price of
a high computational effort. Consequently, powerful state-of-the-art DNNs are
frequently split over various computational nodes, e.g., a first part stays on
an embedded device and the rest on a server. Deciding where to split a DNN is a
challenge in itself, making the design of deep learning applications even more
complicated. Therefore, we propose Split-Et-Impera, a novel and practical
framework that i) determines the set of the best-split points of a neural
network based on deep network interpretability principles without performing a
tedious try-and-test approach, ii) performs a communication-aware simulation
for the rapid evaluation of different neural network rearrangements, and iii)
suggests the best match between the quality of service requirements of the
application and the performance in terms of accuracy and latency time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26th International Symposium on Design and Diagnostics of Electronic
  Circuits and Systems (DDECS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Domain-Specific <span class="highlight-title">Pre-Train</span>ing for Effective Semantic Perception in
  Agricultural Robotics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12499v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12499v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gianmarco Roggiolani, Federico Magistri, Tiziano Guadagnino, Jan Weyler, Giorgio Grisetti, Cyrill Stachniss, Jens Behley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Agricultural robots have the prospect to enable more efficient and
sustainable agricultural production of food, feed, and fiber. Perception of
crops and weeds is a central component of agricultural robots that aim to
monitor fields and assess the plants as well as their growth stage in an
automatic manner. Semantic perception mostly relies on deep learning using
supervised approaches, which require time and qualified workers to label fairly
large amounts of data. In this paper, we look into the problem of reducing the
amount of labels without compromising the final segmentation performance. For
robots operating in the field, pre-training networks in a supervised way is
already a popular method to reduce the number of required labeled images. We
investigate the possibility of pre-training in a self-supervised fashion using
data from the target domain. To better exploit this data, we propose a set of
domain-specific augmentation strategies. We evaluate our pre-training on
semantic segmentation and leaf instance segmentation, two important tasks in
our domain. The experimental results suggest that pre-training with
domain-specific data paired with our data augmentation strategy leads to
superior performance compared to commonly used pre-trainings. Furthermore, the
pre-trained networks obtain similar performance to the fully supervised with
less labeled data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lower Bound on the Bayesian Risk via Information Measure 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12497v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12497v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amedeo Roberto Esposito, Adrien Vandenbroucque, Michael Gastpar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper focuses on parameter estimation and introduces a new method for
lower bounding the Bayesian risk. The method allows for the use of virtually
\emph{any} information measure, including R\'enyi's $\alpha$,
$\varphi$-Divergences, and Sibson's $\alpha$-Mutual Information. The approach
considers divergences as functionals of measures and exploits the duality
between spaces of measures and spaces of functions. In particular, we show that
one can lower bound the risk with any information measure by upper bounding its
dual via Markov's inequality. We are thus able to provide estimator-independent
impossibility results thanks to the Data-Processing Inequalities that
divergences satisfy. The results are then applied to settings of interest
involving both discrete and continuous parameters, including the
``Hide-and-Seek'' problem, and compared to the state-of-the-art techniques. An
important observation is that the behaviour of the lower bound in the number of
samples is influenced by the choice of the information measure. We leverage
this by introducing a new divergence inspired by the ``Hockey-Stick''
Divergence, which is demonstrated empirically to provide the largest
lower-bound across all considered settings. If the observations are subject to
privatisation, stronger impossibility results can be obtained via Strong
Data-Processing Inequalities. The paper also discusses some generalisations and
alternative directions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting DeepFool: generalization and improvement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12481v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12481v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alireza Abdollahpourrostam, Mahed Abroshan, Seyed-Mohsen Moosavi-Dezfooli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks have been known to be vulnerable to adversarial
examples, which are inputs that are modified slightly to fool the network into
making incorrect predictions. This has led to a significant amount of research
on evaluating the robustness of these networks against such perturbations. One
particularly important robustness metric is the robustness to minimal l2
adversarial perturbations. However, existing methods for evaluating this
robustness metric are either computationally expensive or not very accurate. In
this paper, we introduce a new family of adversarial attacks that strike a
balance between effectiveness and computational efficiency. Our proposed
attacks are generalizations of the well-known DeepFool (DF) attack, while they
remain simple to understand and implement. We demonstrate that our attacks
outperform existing methods in terms of both effectiveness and computational
efficiency. Our proposed attacks are also suitable for evaluating the
robustness of large models and can be used to perform adversarial training (AT)
to achieve state-of-the-art robustness to minimal l2 adversarial perturbations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ $\mathcal{C}^k$-continuous Spline Approximation with TensorFlow Gradient
  Descent Optimizers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12454v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12454v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefan Huber, Hannes Waclawek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work we present an "out-of-the-box" application of Machine Learning
(ML) optimizers for an industrial optimization problem. We introduce a
piecewise polynomial model (spline) for fitting of $\mathcal{C}^k$-continuos
functions, which can be deployed in a cam approximation setting. We then use
the gradient descent optimization context provided by the machine learning
framework TensorFlow to optimize the model parameters with respect to
approximation quality and $\mathcal{C}^k$-continuity and evaluate available
optimizers. Our experiments show that the problem solution is feasible using
TensorFlow gradient tapes and that AMSGrad and SGD show the best results among
available TensorFlow optimizers. Furthermore, we introduce a novel
regularization approach to improve SGD convergence. Although experiments show
that remaining discontinuities after optimization are small, we can eliminate
these errors using a presented algorithm which has impact only on affected
derivatives in the local spline segment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This preprint has not undergone peer review or any post-submission
  improvements or corrections. The Version of Record of this contribution is
  published in Computer Aided Systems Theory - EUROCAST 2022 and is available
  online at https://doi.org/10.1007/978-3-031-25312-6_68</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Human-Inspired Force Strategies for Robotic Assembly 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12440v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12440v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefan Scherzinger, Arne Roennau, Rüdiger Dillmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The programming of robotic assembly tasks is a key component in manufacturing
and automation. Force-sensitive assembly, however, often requires reactive
strategies to handle slight changes in positioning and unforeseen part jamming.
Learning such strategies from human performance is a promising approach, but
faces two common challenges: the handling of low part clearances which is
difficult to capture from demonstrations and learning intuitive strategies
offline without access to the real hardware. We address these two challenges by
learning probabilistic force strategies from data that are easily acquired
offline in a robot-less simulation from human demonstrations with a joystick.
We combine a Long Short Term Memory (LSTM) and a Mixture Density Network (MDN)
to model human-inspired behavior in such a way that the learned strategies
transfer easily onto real hardware. The experiments show a UR10e robot that
completes a plastic assembly with clearances of less than 100 micrometers whose
strategies were solely demonstrated in simulation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 8 figures. Submitted to the IEEE International Conference on
  Automation Science and Engineering (CASE) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised Domain Adaptation for Training Event-Based Networks Using
  Contrastive Learning and Uncorrelated Conditioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12424v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12424v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dayuan Jian, Mohammad Rostami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event-based cameras offer reliable measurements for preforming computer
vision tasks in high-dynamic range environments and during fast motion
maneuvers. However, adopting deep learning in event-based vision faces the
challenge of annotated data scarcity due to recency of event cameras.
Transferring the knowledge that can be obtained from conventional camera
annotated data offers a practical solution to this challenge. We develop an
unsupervised domain adaptation algorithm for training a deep network for
event-based data image classification using contrastive learning and
uncorrelated conditioning of data. Our solution outperforms the existing
algorithms for this purpose.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Delay-Aware Hierarchical Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12414v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12414v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Frank Po-Chen Lin, Seyyedali Hosseinalipour, Christopher Brinton, Nicolò Michelusi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning has gained popularity as a means of training models
distributed across the wireless edge. The paper introduces delay-aware
federated learning (DFL) to improve the efficiency of distributed machine
learning (ML) model training by addressing communication delays between edge
and cloud. DFL employs multiple stochastic gradient descent iterations on
device datasets during each global aggregation interval and intermittently
aggregates model parameters through edge servers in local subnetworks. The
cloud server synchronizes the local models with the global deployed model
computed via a local-global combiner at global synchronization. The convergence
behavior of DFL is theoretically investigated under a generalized data
heterogeneity metric. A set of conditions is obtained to achieve the sub-linear
convergence rate of O(1/k). Based on these findings, an adaptive control
algorithm is developed for DFL, implementing policies to mitigate energy
consumption and edge-to-cloud communication latency while aiming for a
sublinear convergence rate. Numerical evaluations show DFL's superior
performance in terms of faster global model convergence, reduced resource
consumption, and robustness against communication delays compared to existing
FL algorithms. In summary, this proposed method offers improved efficiency and
satisfactory results when dealing with both convex and non-convex loss
functions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A condensed version of this paper was presented at IEEE Globecom 2020</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EDGI: Equivariant Diffusion for Planning with Embodied Agents <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12410v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12410v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Johann Brehmer, Joey Bose, Pim de Haan, Taco Cohen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Embodied agents operate in a structured world, often solving tasks with
spatial, temporal, and permutation symmetries. Most algorithms for planning and
model-based reinforcement learning (MBRL) do not take this rich geometric
structure into account, leading to sample inefficiency and poor generalization.
We introduce the Equivariant Diffuser for Generating Interactions (EDGI), an
algorithm for MBRL and planning that is equivariant with respect to the product
of the spatial symmetry group $\mathrm{SE(3)}$, the discrete-time translation
group $\mathbb{Z}$, and the object permutation group $\mathrm{S}_n$. EDGI
follows the Diffuser framework (Janner et al. 2022) in treating both learning a
world model and planning in it as a conditional generative modeling problem,
training a diffusion model on an offline trajectory dataset. We introduce a new
$\mathrm{SE(3)} \times \mathbb{Z} \times \mathrm{S}_n$-equivariant diffusion
model that supports multiple representations. We integrate this model in a
planning loop, where conditioning and classifier-based guidance allow us to
softly break the symmetry for specific tasks as needed. On navigation and
object manipulation tasks, EDGI improves sample efficiency and generalization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Reincarnating RL workshop at ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multiscale Attention via Wavelet Neural Operators for Vision
  <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12398v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12398v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anahita Nekoozadeh, Mohammad Reza Ahmadzadeh, Zahra Mardani, Morteza Mardani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers have achieved widespread success in computer vision. At their
heart, there is a Self-Attention (SA) mechanism, an inductive bias that
associates each token in the input with every other token through a weighted
basis. The standard SA mechanism has quadratic complexity with the sequence
length, which impedes its utility to long sequences appearing in high
resolution vision. Recently, inspired by operator learning for PDEs, Adaptive
Fourier Neural Operators (AFNO) were introduced for high resolution attention
based on global convolution that is efficiently implemented via FFT. However,
the AFNO global filtering cannot well represent small and moderate scale
structures that commonly appear in natural images. To leverage the
coarse-to-fine scale structures we introduce a Multiscale Wavelet Attention
(MWA) by leveraging wavelet neural operators which incurs linear complexity in
the sequence size. We replace the attention in ViT with MWA and our experiments
with CIFAR and ImageNet classification demonstrate significant improvement over
alternative Fourier-based attentions such as AFNO and Global Filter Network
(GFN).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Disturbance Injection under Partial Automation: Robust Imitation
  Learning for Long-horizon Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12375v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12375v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hirotaka Tahara, Hikaru Sasaki, Hanbit Oh, Edgar Anarossi, Takamitsu Matsubara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Partial Automation (PA) with intelligent support systems has been introduced
in industrial machinery and advanced automobiles to reduce the burden of long
hours of human operation. Under PA, operators perform manual operations
(providing actions) and operations that switch to automatic/manual mode
(mode-switching). Since PA reduces the total duration of manual operation,
these two action and mode-switching operations can be replicated by imitation
learning with high sample efficiency. To this end, this paper proposes
Disturbance Injection under Partial Automation (DIPA) as a novel imitation
learning framework. In DIPA, mode and actions (in the manual mode) are assumed
to be observables in each state and are used to learn both action and
mode-switching policies. The above learning is robustified by injecting
disturbances into the operator's actions to optimize the disturbance's level
for minimizing the covariate shift under PA. We experimentally validated the
effectiveness of our method for long-horizon tasks in two simulations and a
real robot environment and confirmed that our method outperformed the previous
methods and reduced the demonstration burden.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, Accepted by Robotics and Automation Letters (RA-L) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AIIPot: Adaptive Intelligent-Interaction Honeypot for IoT Devices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12367v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12367v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Volviane Saphir Mfogo, Alain Zemkoho, Laurent Njilla, Marcellin Nkenlifack, Charles Kamhoua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of the Internet of Things (IoT) has raised concerns about
the security of connected devices. There is a need to develop suitable and
cost-efficient methods to identify vulnerabilities in IoT devices in order to
address them before attackers seize opportunities to compromise them. The
deception technique is a prominent approach to improving the security posture
of IoT systems. Honeypot is a popular deception technique that mimics
interaction in real fashion and encourages unauthorised users (attackers) to
launch attacks. Due to the large number and the heterogeneity of IoT devices,
manually crafting the low and high-interaction honeypots is not affordable.
This has forced researchers to seek innovative ways to build honeypots for IoT
devices. In this paper, we propose a honeypot for IoT devices that uses machine
learning techniques to learn and interact with attackers automatically. The
evaluation of the proposed model indicates that our system can improve the
session length with attackers and capture more attacks on the IoT network.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ExBEHRT: Extended <span class="highlight-title">Transformer</span> for Electronic Health Records to Predict
  Disease Subtypes & Progressions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12364v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12364v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maurice Rupp, Oriane Peter, Thirupathi Pattipaka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we introduce ExBEHRT, an extended version of BEHRT (BERT
applied to electronic health records), and apply different algorithms to
interpret its results. While BEHRT considers only diagnoses and patient age, we
extend the feature space to several multimodal records, namely demographics,
clinical characteristics, vital signs, smoking status, diagnoses, procedures,
medications, and laboratory tests, by applying a novel method to unify the
frequencies and temporal dimensions of the different features. We show that
additional features significantly improve model performance for various
downstream tasks in different diseases. To ensure robustness, we interpret
model predictions using an adaptation of expected gradients, which has not been
previously applied to transformers with EHR data and provides more granular
interpretations than previous approaches such as feature and token importances.
Furthermore, by clustering the model representations of oncology patients, we
show that the model has an implicit understanding of the disease and is able to
classify patients with the same cancer type into different risk groups. Given
the additional features and interpretability, ExBEHRT can help make informed
decisions about disease trajectories, diagnoses, and risk factors of various
diseases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distribution-restrained Softmax Loss for the Model Robustness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12363v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12363v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Wang, Chen Li, Jinzhe Jiang, Xin Zhang, Yaqian Zhao, Weifeng Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, the robustness of deep learning models has received widespread
attention, and various methods for improving model robustness have been
proposed, including adversarial training, model architecture modification,
design of loss functions, certified defenses, and so on. However, the principle
of the robustness to attacks is still not fully understood, also the related
research is still not sufficient. Here, we have identified a significant factor
that affects the robustness of models: the distribution characteristics of
softmax values for non-real label samples. We found that the results after an
attack are highly correlated with the distribution characteristics, and thus we
proposed a loss function to suppress the distribution diversity of softmax. A
large number of experiments have shown that our method can improve robustness
without significant time consumption.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Wasserstein Adversarial Examples on Univariant Time Series Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12357v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12357v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenjie Wang, Li Xiong, Jian Lou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial examples are crafted by adding indistinguishable perturbations to
normal examples in order to fool a well-trained deep learning model to
misclassify. In the context of computer vision, this notion of
indistinguishability is typically bounded by $L_{\infty}$ or other norms.
However, these norms are not appropriate for measuring indistinguishiability
for time series data. In this work, we propose adversarial examples in the
Wasserstein space for time series data for the first time and utilize
Wasserstein distance to bound the perturbation between normal examples and
adversarial examples. We introduce Wasserstein projected gradient descent
(WPGD), an adversarial attack method for perturbing univariant time series
data. We leverage the closed-form solution of Wasserstein distance in the 1D
space to calculate the projection step of WPGD efficiently with the gradient
descent method. We further propose a two-step projection so that the search of
adversarial examples in the Wasserstein space is guided and constrained by
Euclidean norms to yield more effective and imperceptible perturbations. We
empirically evaluate the proposed attack on several time series datasets in the
healthcare domain. Extensive results demonstrate that the Wasserstein attack is
powerful and can successfully attack most of the target classifiers with a high
attack success rate. To better study the nature of Wasserstein adversarial
example, we evaluate a strong defense mechanism named Wasserstein smoothing for
potential certified robustness defense. Although the defense can achieve some
accuracy gain, it still has limitations in many cases and leaves space for
developing a stronger certified robustness method to Wasserstein adversarial
examples on univariant time series data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Reinforcement Learning for Localizability-Enhanced Navigation in
  Dynamic Human Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12354v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12354v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Chen, Quecheng Qiu, Xiangyu Liu, Guangda Chen, Shunyi Yao, Jie Peng, Jianmin Ji, Yanyong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reliable localization is crucial for autonomous robots to navigate
efficiently and safely. Some navigation methods can plan paths with high
localizability (which describes the capability of acquiring reliable
localization). By following these paths, the robot can access the sensor
streams that facilitate more accurate location estimation results by the
localization algorithms. However, most of these methods require prior knowledge
and struggle to adapt to unseen scenarios or dynamic changes. To overcome these
limitations, we propose a novel approach for localizability-enhanced navigation
via deep reinforcement learning in dynamic human environments. Our proposed
planner automatically extracts geometric features from 2D laser data that are
helpful for localization. The planner learns to assign different importance to
the geometric features and encourages the robot to navigate through areas that
are helpful for laser localization. To facilitate the learning of the planner,
we suggest two techniques: (1) an augmented state representation that considers
the dynamic changes and the confidence of the localization results, which
provides more information and allows the robot to make better decisions, (2) a
reward metric that is capable to offer both sparse and dense feedback on
behaviors that affect localization accuracy. Our method exhibits significant
improvements in lost rate and arrival rate when tested in previously unseen
environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Training Multilayer Perceptrons by Sampling with Quantum Annealers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12352v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12352v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Frances Fengyi Yang, Michele Sasdelli, Tat-Jun Chin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A successful application of quantum annealing to machine learning is training
restricted Boltzmann machines (RBM). However, many neural networks for vision
applications are feedforward structures, such as multilayer perceptrons (MLP).
Backpropagation is currently the most effective technique to train MLPs for
supervised learning. This paper aims to be forward-looking by exploring the
training of MLPs using quantum annealers. We exploit an equivalence between
MLPs and energy-based models (EBM), which are a variation of RBMs with a
maximum conditional likelihood objective. This leads to a strategy to train
MLPs with quantum annealers as a sampling engine. We prove our setup for MLPs
with sigmoid activation functions and one hidden layer, and demonstrated
training of binary image classifiers on small subsets of the MNIST and
Fashion-MNIST datasets using the D-Wave quantum annealer. Although problem
sizes that are feasible on current annealers are limited, we obtained
comprehensive results on feasible instances that validate our ideas. Our work
establishes the potential of quantum computing for training MLPs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EasyDGL: Encode, Train and Interpret for Continuous-time Dynamic Graph
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12341v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12341v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Chen, Haoyu Geng, Nianzu Yang, Xiaokang Yang, Junchi Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dynamic graphs arise in various real-world applications, and it is often
welcomed to model the dynamics directly in continuous time domain for its
flexibility. This paper aims to design an easy-to-use pipeline (termed as
EasyDGL which is also due to its implementation by DGL toolkit) composed of
three key modules with both strong fitting ability and interpretability.
Specifically the proposed pipeline which involves encoding, training and
interpreting: i) a temporal point process (TPP) modulated attention
architecture to endow the continuous-time resolution with the coupled
spatiotemporal dynamics of the observed graph with edge-addition events; ii) a
principled loss composed of task-agnostic TPP posterior maximization based on
observed events on the graph, and a task-aware loss with a masking strategy
over dynamic graph, where the covered tasks include dynamic link prediction,
dynamic node classification and node traffic forecasting; iii) interpretation
of the model outputs (e.g., representations and predictions) with scalable
perturbation-based quantitative analysis in the graph Fourier domain, which
could more comprehensively reflect the behavior of the learned model. Extensive
experimental results on public benchmarks show the superior performance of our
EasyDGL for time-conditioned predictive tasks, and in particular demonstrate
that EasyDGL can effectively quantify the predictive power of frequency content
that a model learn from the evolving graph data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Re-thinking Federated Active Learning based on Inter-class Diversity <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12317v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12317v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        SangMook Kim, Sangmin Bae, Hwanjun Song, Se-Young Yun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although federated learning has made awe-inspiring advances, most studies
have assumed that the client's data are fully labeled. However, in a real-world
scenario, every client may have a significant amount of unlabeled instances.
Among the various approaches to utilizing unlabeled data, a federated active
learning framework has emerged as a promising solution. In the decentralized
setting, there are two types of available query selector models, namely
'global' and 'local-only' models, but little literature discusses their
performance dominance and its causes. In this work, we first demonstrate that
the superiority of two selector models depends on the global and local
inter-class diversity. Furthermore, we observe that the global and local-only
models are the keys to resolving the imbalance of each side. Based on our
findings, we propose LoGo, a FAL sampling strategy robust to varying local
heterogeneity levels and global imbalance ratio, that integrates both models by
two steps of active selection scheme. LoGo consistently outperforms six active
learning strategies in the total number of 38 experimental settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TsSHAP: Robust model agnostic feature-based explainability for time
  series forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12316v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12316v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vikas C. Raykar, Arindam Jati, Sumanta Mukherjee, Nupur Aggarwal, Kanthi Sarpatwar, Giridhar Ganapavarapu, Roman Vaculin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A trustworthy machine learning model should be accurate as well as
explainable. Understanding why a model makes a certain decision defines the
notion of explainability. While various flavors of explainability have been
well-studied in supervised learning paradigms like classification and
regression, literature on explainability for time series forecasting is
relatively scarce.
  In this paper, we propose a feature-based explainability algorithm, TsSHAP,
that can explain the forecast of any black-box forecasting model. The method is
agnostic of the forecasting model and can provide explanations for a forecast
in terms of interpretable features defined by the user a prior.
  The explanations are in terms of the SHAP values obtained by applying the
TreeSHAP algorithm on a surrogate model that learns a mapping between the
interpretable feature space and the forecast of the black-box model.
  Moreover, we formalize the notion of local, semi-local, and global
explanations in the context of time series forecasting, which can be useful in
several scenarios. We validate the efficacy and robustness of TsSHAP through
extensive experiments on multiple datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Self-supervised</span> Meta-<span class="highlight-title">Prompt</span> Learning with Meta-Gradient Regularization
  for Few-shot Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12314v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12314v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaihang Pan, Juncheng Li, Hongye Song, Jun Lin, Xiaozhong Liu, Siliang Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt tuning is a parameter-efficient method, which learns soft prompts and
conditions frozen language models to perform specific downstream tasks. Though
effective, prompt tuning under few-shot settings on the one hand heavily relies
on a good initialization of soft prompts. On the other hand, it can easily
result in overfitting. Existing works leverage pre-training or supervised
meta-learning to initialize soft prompts but they cannot data-efficiently
generalize to unseen downstream tasks. To address the above problems, this
paper proposes a novel Self-sUpervised meta-Prompt learning framework with
meta-gradient Regularization for few-shot generalization (SUPMER). We first
design a set of self-supervised anchor meta-training tasks with different task
formats and further enrich the task distribution with curriculum-based task
augmentation. Then a novel meta-gradient regularization method is integrated
into meta-prompt learning. It meta-learns to transform the raw gradients during
few-shot learning into a domain-generalizable direction, thus alleviating the
problem of overfitting. Extensive experiments show that SUPMER achieves better
performance for different few-shot downstream tasks, and also exhibits a
stronger domain generalization ability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Frozen Language Model Helps ECG Zero-Shot Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12311v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12311v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Li, Che Liu, Sibo Cheng, Rossella Arcucci, Shenda Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The electrocardiogram (ECG) is one of the most commonly used non-invasive,
convenient medical monitoring tools that assist in the clinical diagnosis of
heart diseases. Recently, deep learning (DL) techniques, particularly
self-supervised learning (SSL), have demonstrated great potential in the
classification of ECG. SSL pre-training has achieved competitive performance
with only a small amount of annotated data after fine-tuning. However, current
SSL methods rely on the availability of annotated data and are unable to
predict labels not existing in fine-tuning datasets. To address this challenge,
we propose Multimodal ECG-Text Self-supervised pre-training (METS), the first
work to utilize the auto-generated clinical reports to guide ECG SSL
pre-training. We use a trainable ECG encoder and a frozen language model to
embed paired ECG and automatically machine-generated clinical reports
separately. The SSL aims to maximize the similarity between paired ECG and
auto-generated report while minimize the similarity between ECG and other
reports. In downstream classification tasks, METS achieves around 10%
improvement in performance without using any annotated data via zero-shot
classification, compared to other supervised and SSL baselines that rely on
annotated data. Furthermore, METS achieves the highest recall and F1 scores on
the MIT-BIH dataset, despite MIT-BIH containing different classes of ECG
compared to the pre-trained dataset. The extensive experiments have
demonstrated the advantages of using ECG-Text multimodal self-supervised
learning in terms of generalizability, effectiveness, and efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Logical Expressiveness of Graph Neural Network for Knowledge Graph
  Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12306v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12306v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haiquan Qiu, Yongqi Zhang, Yong Li, Quanming Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) have been recently introduced to learn from
knowledge graph (KG) and achieved state-of-the-art performance in KG reasoning.
However, a theoretical certification for their good empirical performance is
still absent. Besides, while logic in KG is important for inductive and
interpretable inference, existing GNN-based methods are just designed to fit
data distributions with limited knowledge of their logical expressiveness. We
propose to fill the above gap in this paper. Specifically, we theoretically
analyze GNN from logical expressiveness and find out what kind of logical rules
can be captured from KG. Our results first show that GNN can capture logical
rules from graded modal logic, providing a new theoretical tool for analyzing
the expressiveness of GNN for KG reasoning; and a query labeling trick makes it
easier for GNN to capture logical rules, explaining why SOTA methods are mainly
based on labeling trick. Finally, insights from our theory motivate the
development of an entity labeling method for capturing difficult logical rules.
Experimental results are consistent with our theoretical results and verify the
effectiveness of our proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Anomaly Detection in Aeronautics Data with Quantum-compatible Discrete
  Deep Generative Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12302v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12302v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Templin, Milad Memarzadeh, Walter Vinci, P. Aaron Lott, Ata Akbari Asanjan, Anthony Alexiades Armenakas, Eleanor Rieffel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep generative learning cannot only be used for generating new data with
statistical characteristics derived from input data but also for anomaly
detection, by separating nominal and anomalous instances based on their
reconstruction quality. In this paper, we explore the performance of three
unsupervised deep generative models -- variational autoencoders (VAEs) with
Gaussian, Bernoulli, and Boltzmann priors -- in detecting anomalies in
flight-operations data of commercial flights consisting of multivariate time
series. We devised two VAE models with discrete latent variables (DVAEs), one
with a factorized Bernoulli prior and one with a restricted Boltzmann machine
(RBM) as prior, because of the demand for discrete-variable models in
machine-learning applications and because the integration of quantum devices
based on two-level quantum systems requires such models. The DVAE with RBM
prior, using a relatively simple -- and classically or quantum-mechanically
enhanceable -- sampling technique for the evolution of the RBM's negative
phase, performed better than the Bernoulli DVAE and on par with the Gaussian
model, which has a continuous latent space. Our studies demonstrate the
competitiveness of a discrete deep generative model with its Gaussian
counterpart on anomaly-detection tasks. Moreover, the DVAE model with RBM prior
can be easily integrated with quantum sampling by outsourcing its generative
process to measurements of quantum states obtained from a quantum annealer or
gate-model device.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 7 figures, 3 tables, appendix, supplementary material</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A General Algorithm for Solving Rank-one Matrix Sensing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12298v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12298v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lianke Qin, Zhao Song, Ruizhe Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Matrix sensing has many real-world applications in science and engineering,
such as system control, distance embedding, and computer vision. The goal of
matrix sensing is to recover a matrix $A_\star \in \mathbb{R}^{n \times n}$,
based on a sequence of measurements $(u_i,b_i) \in \mathbb{R}^{n} \times
\mathbb{R}$ such that $u_i^\top A_\star u_i = b_i$. Previous work [ZJD15]
focused on the scenario where matrix $A_{\star}$ has a small rank, e.g.
rank-$k$. Their analysis heavily relies on the RIP assumption, making it
unclear how to generalize to high-rank matrices. In this paper, we relax that
rank-$k$ assumption and solve a much more general matrix sensing problem. Given
an accuracy parameter $\delta \in (0,1)$, we can compute $A \in \mathbb{R}^{n
\times n}$ in $\widetilde{O}(m^{3/2} n^2 \delta^{-1} )$, such that $ |u_i^\top
A u_i - b_i| \leq \delta$ for all $i \in [m]$. We design an efficient algorithm
with provable convergence guarantees using stochastic gradient descent for this
problem.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Prototype Helps Federated Learning: Towards Faster Convergence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12296v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12296v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Qiao, Seong-Bae Park, Sun Moo Kang, Choong Seon Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) is a distributed machine learning technique in which
multiple clients cooperate to train a shared model without exchanging their raw
data. However, heterogeneity of data distribution among clients usually leads
to poor model inference. In this paper, a prototype-based federated learning
framework is proposed, which can achieve better inference performance with only
a few changes to the last global iteration of the typical federated learning
process. In the last iteration, the server aggregates the prototypes
transmitted from distributed clients and then sends them back to local clients
for their respective model inferences. Experiments on two baseline datasets
show that our proposal can achieve higher accuracy (at least 1%) and relatively
efficient communication than two popular baselines under different
heterogeneous settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>3 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fairness Improves Learning from Noisily Labeled Long-Tailed Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12291v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12291v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaheng Wei, Zhaowei Zhu, Gang Niu, Tongliang Liu, Sijia Liu, Masashi Sugiyama, Yang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Both long-tailed and noisily labeled data frequently appear in real-world
applications and impose significant challenges for learning. Most prior works
treat either problem in an isolated way and do not explicitly consider the
coupling effects of the two. Our empirical observation reveals that such
solutions fail to consistently improve the learning when the dataset is
long-tailed with label noise. Moreover, with the presence of label noise,
existing methods do not observe universal improvements across different
sub-populations; in other words, some sub-populations enjoyed the benefits of
improved accuracy at the cost of hurting others. Based on these observations,
we introduce the Fairness Regularizer (FR), inspired by regularizing the
performance gap between any two sub-populations. We show that the introduced
fairness regularizer improves the performances of sub-populations on the tail
and the overall learning performance. Extensive experiments demonstrate the
effectiveness of the proposed solution when complemented with certain existing
popular robust or class-balanced methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Road Configurations for Improved Autonomous Vehicle-Pedestrian
  Interactions using Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12289v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12289v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiming Ye, Yuxiang Feng, Jose Javier Escribano Macias, Marc Stettler, Panagiotis Angeloudis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The deployment of Autonomous Vehicles (AVs) poses considerable challenges and
unique opportunities for the design and management of future urban road
infrastructure. In light of this disruptive transformation, the Right-Of-Way
(ROW) composition of road space has the potential to be renewed. Design
approaches and intelligent control models have been proposed to address this
problem, but we lack an operational framework that can dynamically generate ROW
plans for AVs and pedestrians in response to real-time demand. Based on
microscopic traffic simulation, this study explores Reinforcement Learning (RL)
methods for evolving ROW compositions. We implement a centralised paradigm and
a distributive learning paradigm to separately perform the dynamic control on
several road network configurations. Experimental results indicate that the
algorithms have the potential to improve traffic flow efficiency and allocate
more space for pedestrians. Furthermore, the distributive learning algorithm
outperforms its centralised counterpart regarding computational cost (49.55\%),
benchmark rewards (25.35\%), best cumulative rewards (24.58\%), optimal actions
(13.49\%) and rate of convergence. This novel road management technique could
potentially contribute to the flow-adaptive and active mobility-friendly
streets in the AVs era.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 7 figures, Copyright \c{opyright} 2023, IEEE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hardness of Independent Learning and Sparse Equilibrium Computation in
  Markov Games 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12287v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12287v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dylan J. Foster, Noah Golowich, Sham M. Kakade
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of decentralized multi-agent reinforcement learning
in Markov games. A fundamental question is whether there exist algorithms that,
when adopted by all agents and run independently in a decentralized fashion,
lead to no-regret for each player, analogous to celebrated convergence results
in normal-form games. While recent work has shown that such algorithms exist
for restricted settings (notably, when regret is defined with respect to
deviations to Markovian policies), the question of whether independent
no-regret learning can be achieved in the standard Markov game framework was
open. We provide a decisive negative resolution this problem, both from a
computational and statistical perspective. We show that:
  - Under the widely-believed assumption that PPAD-hard problems cannot be
solved in polynomial time, there is no polynomial-time algorithm that attains
no-regret in general-sum Markov games when executed independently by all
players, even when the game is known to the algorithm designer and the number
of players is a small constant.
  - When the game is unknown, no algorithm, regardless of computational
efficiency, can achieve no-regret without observing a number of episodes that
is exponential in the number of players.
  Perhaps surprisingly, our lower bounds hold even for seemingly easier setting
in which all agents are controlled by a a centralized algorithm. They are
proven via lower bounds for a simpler problem we refer to as SparseCCE, in
which the goal is to compute a coarse correlated equilibrium that is sparse in
the sense that it can be represented as a mixture of a small number of product
policies. The crux of our approach is a novel application of aggregation
techniques from online learning, whereby we show that any algorithm for the
SparseCCE problem can be used to compute approximate Nash equilibria for
non-zero sum normal-form games.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>51 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reducing Air Pollution through Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12285v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12285v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dimitris Bertsimas, Leonard Boussioux, Cynthia Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a data-driven approach to mitigate the effects of air
pollution from industrial plants on nearby cities by linking operational
decisions with weather conditions. Our method combines predictive and
prescriptive machine learning models to forecast short-term wind speed and
direction and recommend operational decisions to reduce or pause the industrial
plant's production. We exhibit several trade-offs between reducing
environmental impact and maintaining production activities. The predictive
component of our framework employs various machine learning models, such as
gradient-boosted tree-based models and ensemble methods, for time series
forecasting. The prescriptive component utilizes interpretable optimal policy
trees to propose multiple trade-offs, such as reducing dangerous emissions by
33-47% and unnecessary costs by 40-63%. Our deployed models significantly
reduced forecasting errors, with a range of 38-52% for less than 12-hour lead
time and 14-46% for 12 to 48-hour lead time compared to official weather
forecasts. We have successfully implemented the predictive component at the OCP
Safi site, which is Morocco's largest chemical industrial plant, and are
currently in the process of deploying the prescriptive component. Our framework
enables sustainable industrial development by eliminating the
pollution-industrial activity trade-off through data-driven weather-based
operational decisions, significantly enhancing factory optimization and
sustainability. This modernizes factory planning and resource allocation while
maintaining environmental compliance. The predictive component has boosted
production efficiency, leading to cost savings and reduced environmental impact
by minimizing air pollution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to Manufacturing and Service Operations Management</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synthetic Health-related Longitudinal Data with Mixed-type Variables
  Generated using Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12281v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12281v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicholas I-Hsien Kuo, Louisa Jorm, Sebastiano Barbieri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel approach to simulating electronic health records
(EHRs) using diffusion probabilistic models (DPMs). Specifically, we
demonstrate the effectiveness of DPMs in synthesising longitudinal EHRs that
capture mixed-type variables, including numeric, binary, and categorical
variables. To our knowledge, this represents the first use of DPMs for this
purpose. We compared our DPM-simulated datasets to previous state-of-the-art
results based on generative adversarial networks (GANs) for two clinical
applications: acute hypotension and human immunodeficiency virus (ART for HIV).
Given the lack of similar previous studies in DPMs, a core component of our
work involves exploring the advantages and caveats of employing DPMs across a
wide range of aspects. In addition to assessing the realism of the synthetic
datasets, we also trained reinforcement learning (RL) agents on the synthetic
data to evaluate their utility for supporting the development of downstream
machine learning models. Finally, we estimated that our DPM-simulated datasets
are secure and posed a low patient exposure risk for public access.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stochastic Nonsmooth Convex Optimization with Heavy-Tailed Noises 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12277v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12277v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijian Liu, Zhengyuan Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, several studies consider the stochastic optimization problem but in
a heavy-tailed noise regime, i.e., the difference between the stochastic
gradient and the true gradient is assumed to have a finite $p$-th moment (say
being upper bounded by $\sigma^{p}$ for some $\sigma\geq0$) where $p\in(1,2]$,
which not only generalizes the traditional finite variance assumption ($p=2$)
but also has been observed in practice for several different tasks. Under this
challenging assumption, lots of new progress has been made for either convex or
nonconvex problems, however, most of which only consider smooth objectives. In
contrast, people have not fully explored and well understood this problem when
functions are nonsmooth. This paper aims to fill this crucial gap by providing
a comprehensive analysis of stochastic nonsmooth convex optimization with
heavy-tailed noises. We revisit a simple clipping-based algorithm, whereas,
which is only proved to converge in expectation but under the additional strong
convexity assumption. Under appropriate choices of parameters, for both convex
and strongly convex functions, we not only establish the first high-probability
rates but also give refined in-expectation bounds compared with existing works.
Remarkably, all of our results are optimal (or nearly optimal up to logarithmic
factors) with respect to the time horizon $T$ even when $T$ is unknown in
advance. Additionally, we show how to make the algorithm parameter-free with
respect to $\sigma$, in other words, the algorithm can still guarantee
convergence without any prior knowledge of $\sigma$.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AUTO: Adaptive Outlier Optimization for Online Test-Time OOD Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12267v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12267v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Puning Yang, Jian Liang, Jie Cao, Ran He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Out-of-distribution (OOD) detection is a crucial aspect of deploying machine
learning models in open-world applications. Empirical evidence suggests that
training with auxiliary outliers substantially improves OOD detection. However,
such outliers typically exhibit a distribution gap compared to the test OOD
data and do not cover all possible test OOD scenarios. Additionally,
incorporating these outliers introduces additional training burdens. In this
paper, we introduce a novel paradigm called test-time OOD detection, which
utilizes unlabeled online data directly at test time to improve OOD detection
performance. While this paradigm is efficient, it also presents challenges such
as catastrophic forgetting. To address these challenges, we propose adaptive
outlier optimization (AUTO), which consists of an in-out-aware filter, an ID
memory bank, and a semantically-consistent objective. AUTO adaptively mines
pseudo-ID and pseudo-OOD samples from test data, utilizing them to optimize
networks in real time during inference. Extensive results on CIFAR-10,
CIFAR-100, and ImageNet benchmarks demonstrate that AUTO significantly enhances
OOD detection performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Challenges and opportunities for machine learning in multiscale
  computational modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12261v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12261v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Phong C. H. Nguyen, Joseph B. Choi, H. S. Udaykumar, Stephen Baek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many mechanical engineering applications call for multiscale computational
modeling and simulation. However, solving for complex multiscale systems
remains computationally onerous due to the high dimensionality of the solution
space. Recently, machine learning (ML) has emerged as a promising solution that
can either serve as a surrogate for, accelerate or augment traditional
numerical methods. Pioneering work has demonstrated that ML provides solutions
to governing systems of equations with comparable accuracy to those obtained
using direct numerical methods, but with significantly faster computational
speed. These high-speed, high-fidelity estimations can facilitate the solving
of complex multiscale systems by providing a better initial solution to
traditional solvers. This paper provides a perspective on the opportunities and
challenges of using ML for complex multiscale modeling and simulation. We first
outline the current state-of-the-art ML approaches for simulating multiscale
systems and highlight some of the landmark developments. Next, we discuss
current challenges for ML in multiscale computational modeling, such as the
data and discretization dependence, interpretability, and data sharing and
collaborative platform development. Finally, we suggest several potential
research directions for the future.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Information-Based Sensor Placement for Data-Driven Estimation of
  Unsteady Flows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12260v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12260v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John Graff, Albert Medina, Francis Lagor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimation of unsteady flow fields around flight vehicles may improve flow
interactions and lead to enhanced vehicle performance. Although flow-field
representations can be very high-dimensional, their dynamics can have low-order
representations and may be estimated using a few, appropriately placed
measurements. This paper presents a sensor-selection framework for the intended
application of data-driven, flow-field estimation. This framework combines
data-driven modeling, steady-state Kalman Filter design, and a sparsification
technique for sequential selection of sensors. This paper also uses the sensor
selection framework to design sensor arrays that can perform well across a
variety of operating conditions. Flow estimation results on numerical data show
that the proposed framework produces arrays that are highly effective at
flow-field estimation for the flow behind and an airfoil at a high angle of
attack using embedded pressure sensors. Analysis of the flow fields reveals
that paths of impinging stagnation points along the airfoil's surface during a
shedding period of the flow are highly informative locations for placement of
pressure sensors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 9 figures, submitted to AIAA Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Encoding Binary Concepts in the Latent Space of Generative Models for
  Enhancing Data Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12255v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12255v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zizhao Hu, Mohammad Rostami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Binary concepts are empirically used by humans to generalize efficiently. And
they are based on Bernoulli distribution which is the building block of
information. These concepts span both low-level and high-level features such as
"large vs small" and "a neuron is active or inactive". Binary concepts are
ubiquitous features and can be used to transfer knowledge to improve model
generalization. We propose a novel binarized regularization to facilitate
learning of binary concepts to improve the quality of data generation in
autoencoders. We introduce a binarizing hyperparameter $r$ in data generation
process to disentangle the latent space symmetrically. We demonstrate that this
method can be applied easily to existing variational autoencoder (VAE) variants
to encourage symmetric disentanglement, improve reconstruction quality, and
prevent posterior collapse without computation overhead. We also demonstrate
that this method can boost existing models to learn more transferable
representations and generate more representative samples for the input
distribution which can alleviate catastrophic forgetting using generative
replay under continual learning settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the Benefits of Visual <span class="highlight-title">Prompt</span>ing in Differential Privacy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12247v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12247v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhe Li, Yu-Lin Tsai, Xuebin Ren, Chia-Mu Yu, Pin-Yu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Prompting (VP) is an emerging and powerful technique that allows
sample-efficient adaptation to downstream tasks by engineering a well-trained
frozen source model. In this work, we explore the benefits of VP in
constructing compelling neural network classifiers with differential privacy
(DP). We explore and integrate VP into canonical DP training methods and
demonstrate its simplicity and efficiency. In particular, we discover that VP
in tandem with PATE, a state-of-the-art DP training method that leverages the
knowledge transfer from an ensemble of teachers, achieves the state-of-the-art
privacy-utility trade-off with minimum expenditure of privacy budget. Moreover,
we conduct additional experiments on cross-domain image classification with a
sufficient domain gap to further unveil the advantage of VP in DP. Lastly, we
also conduct extensive ablation studies to validate the effectiveness and
contribution of VP under DP consideration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Error Analysis of Physics-Informed Neural Networks for Approximating
  Dynamic PDEs of Second Order in Time 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12245v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12245v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanxia Qian, Yongchao Zhang, Yunqing Huang, Suchuan Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the approximation of a class of dynamic partial differential
equations (PDE) of second order in time by the physics-informed neural network
(PINN) approach, and provide an error analysis of PINN for the wave equation,
the Sine-Gordon equation and the linear elastodynamic equation. Our analyses
show that, with feed-forward neural networks having two hidden layers and the
$\tanh$ activation function, the PINN approximation errors for the solution
field, its time derivative and its gradient field can be effectively bounded by
the training loss and the number of training data points (quadrature points).
Our analyses further suggest new forms for the training loss function, which
contain certain residuals that are crucial to the error estimate but would be
absent from the canonical PINN loss formulation. Adopting these new forms for
the loss function leads to a variant PINN algorithm. We present ample numerical
experiments with the new PINN algorithm for the wave equation, the Sine-Gordon
equation and the linear elastodynamic equation, which show that the method can
capture the solution well.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>46 pages, 14 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Shaky Foundations of Clinical Foundation Models: A <span class="highlight-title">Survey</span> of Large
  Language Models and Foundation Models for EMRs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12961v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12961v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Wornow, Yizhe Xu, Rahul Thapa, Birju Patel, Ethan Steinberg, Scott Fleming, Michael A. Pfeffer, Jason Fries, Nigam H. Shah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The successes of foundation models such as ChatGPT and AlphaFold have spurred
significant interest in building similar models for electronic medical records
(EMRs) to improve patient care and hospital operations. However, recent hype
has obscured critical gaps in our understanding of these models' capabilities.
We review over 80 foundation models trained on non-imaging EMR data (i.e.
clinical text and/or structured data) and create a taxonomy delineating their
architectures, training data, and potential use cases. We find that most models
are trained on small, narrowly-scoped clinical datasets (e.g. MIMIC-III) or
broad, public biomedical corpora (e.g. PubMed) and are evaluated on tasks that
do not provide meaningful insights on their usefulness to health systems. In
light of these findings, we propose an improved evaluation framework for
measuring the benefits of clinical foundation models that is more closely
grounded to metrics that matter in healthcare.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 4 figures, submitted to NPJ Digital Medicine</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Variantional autoencoder with decremental information bottleneck for
  disentanglement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12959v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12959v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiantao Wu, Shentong Mo, Muhammad Awais, Sara Atito, Xingshen Zhang, Lin Wang, Xiang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One major challenge of disentanglement learning with variational autoencoders
is the trade-off between disentanglement and reconstruction fidelity. Previous
incremental methods with only on latent space cannot optimize these two targets
simultaneously, so they expand the Information Bottleneck while training to
{optimize from disentanglement to reconstruction. However, a large bottleneck
will lose the constraint of disentanglement, causing the information diffusion
problem. To tackle this issue, we present a novel decremental variational
autoencoder with disentanglement-invariant transformations to optimize multiple
objectives in different layers, termed DeVAE, for balancing disentanglement and
reconstruction fidelity by decreasing the information bottleneck of diverse
latent spaces gradually. Benefiting from the multiple latent spaces, DeVAE
allows simultaneous optimization of multiple objectives to optimize
reconstruction while keeping the constraint of disentanglement, avoiding
information diffusion. DeVAE is also compatible with large models with
high-dimension latent space. Experimental results on dSprites and Shapes3D that
DeVAE achieves \fix{R2q6}{a good balance between disentanglement and
reconstruction.DeVAE shows high tolerant of hyperparameters and on
high-dimensional latent spaces.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reinforcement Learning with Exogenous States and Rewards 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12957v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12957v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        George Trimponias, Thomas G. Dietterich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Exogenous state variables and rewards can slow reinforcement learning by
injecting uncontrolled variation into the reward signal. This paper formalizes
exogenous state variables and rewards and shows that if the reward function
decomposes additively into endogenous and exogenous components, the MDP can be
decomposed into an exogenous Markov Reward Process (based on the exogenous
reward) and an endogenous Markov Decision Process (optimizing the endogenous
reward). Any optimal policy for the endogenous MDP is also an optimal policy
for the original MDP, but because the endogenous reward typically has reduced
variance, the endogenous MDP is easier to solve. We study settings where the
decomposition of the state space into exogenous and endogenous state spaces is
not given but must be discovered. The paper introduces and proves correctness
of algorithms for discovering the exogenous and endogenous subspaces of the
state space when they are mixed through linear combination. These algorithms
can be applied during reinforcement learning to discover the exogenous space,
remove the exogenous reward, and focus reinforcement learning on the endogenous
MDP. Experiments on a variety of challenging synthetic MDPs show that these
methods, applied online, discover large exogenous state spaces and produce
substantial speedups in reinforcement learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Greatly extends the initial work reported in 1806.01584</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TSI-GAN: Unsupervised Time Series Anomaly Detection using Convolutional
  Cycle-Consistent Generative Adversarial Networks <span class="chip">PAKDD 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12952v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12952v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shyam Sundar Saravanan, Tie Luo, Mao Van Ngo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anomaly detection is widely used in network intrusion detection, autonomous
driving, medical diagnosis, credit card frauds, etc. However, several key
challenges remain open, such as lack of ground truth labels, presence of
complex temporal patterns, and generalizing over different datasets. This paper
proposes TSI-GAN, an unsupervised anomaly detection model for time-series that
can learn complex temporal patterns automatically and generalize well, i.e., no
need for choosing dataset-specific parameters, making statistical assumptions
about underlying data, or changing model architectures. To achieve these goals,
we convert each input time-series into a sequence of 2D images using two
encoding techniques with the intent of capturing temporal patterns and various
types of deviance. Moreover, we design a reconstructive GAN that uses
convolutional layers in an encoder-decoder network and employs
cycle-consistency loss during training to ensure that inverse mappings are
accurate as well. In addition, we also instrument a Hodrick-Prescott filter in
post-processing to mitigate false positives. We evaluate TSI-GAN using 250
well-curated and harder-than-usual datasets and compare with 8 state-of-the-art
baseline methods. The results demonstrate the superiority of TSI-GAN to all the
baselines, offering an overall performance improvement of 13% and 31% over the
second-best performer MERLIN and the third-best performer LSTM-AE,
respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in the Proceedings of PAKDD 2023 (27th Pacific-Asia
  Conference on Knowledge Discovery and Data Mining)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Multi-time Hamilton-Jacobi PDEs for Certain Scientific
  Machine Learning Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12928v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12928v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paula Chen, Tingwei Meng, Zongren Zou, Jérôme Darbon, George Em Karniadakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hamilton-Jacobi partial differential equations (HJ PDEs) have deep
connections with a wide range of fields, including optimal control,
differential games, and imaging sciences. By considering the time variable to
be a higher dimensional quantity, HJ PDEs can be extended to the multi-time
case. In this paper, we establish a novel theoretical connection between
specific optimization problems arising in machine learning and the multi-time
Hopf formula, which corresponds to a representation of the solution to certain
multi-time HJ PDEs. Through this connection, we increase the interpretability
of the training process of certain machine learning applications by showing
that when we solve these learning problems, we also solve a multi-time HJ PDE
and, by extension, its corresponding optimal control problem. As a first
exploration of this connection, we develop the relation between the regularized
linear regression problem and the Linear Quadratic Regulator (LQR). We then
leverage our theoretical connection to adapt standard LQR solvers (namely,
those based on the Riccati ordinary differential equations) to design new
training approaches for machine learning. Finally, we provide some numerical
examples that demonstrate the versatility and possible computational advantages
of our Riccati-based approach in the context of continual learning,
post-training calibration, transfer learning, and sparse dynamics
identification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages in total, 23 pages for the main text, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting the Fragility of Influence Functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12922v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12922v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jacob R. Epifano, Ravi P. Ramachandran, Aaron J. Masino, Ghulam Rasool
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the last few years, many works have tried to explain the predictions of
deep learning models. Few methods, however, have been proposed to verify the
accuracy or faithfulness of these explanations. Recently, influence functions,
which is a method that approximates the effect that leave-one-out training has
on the loss function, has been shown to be fragile. The proposed reason for
their fragility remains unclear. Although previous work suggests the use of
regularization to increase robustness, this does not hold in all cases. In this
work, we seek to investigate the experiments performed in the prior work in an
effort to understand the underlying mechanisms of influence function fragility.
First, we verify influence functions using procedures from the literature under
conditions where the convexity assumptions of influence functions are met.
Then, we relax these assumptions and study the effects of non-convexity by
using deeper models and more complex datasets. Here, we analyze the key metrics
and procedures that are used to validate influence functions. Our results
indicate that the validation procedures may cause the observed fragility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 5 figures, accepted to Neural Networks</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stability is Stable: Connections between Replicability, Privacy, and
  Adaptive Generalization <span class="chip">STOC 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12921v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12921v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mark Bun, Marco Gaboardi, Max Hopkins, Russell Impagliazzo, Rex Lei, Toniann Pitassi, Jessica Sorrell, Satchit Sivakumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The notion of replicable algorithms was introduced in Impagliazzo et al.
[STOC '22] to describe randomized algorithms that are stable under the
resampling of their inputs. More precisely, a replicable algorithm gives the
same output with high probability when its randomness is fixed and it is run on
a new i.i.d. sample drawn from the same distribution. Using replicable
algorithms for data analysis can facilitate the verification of published
results by ensuring that the results of an analysis will be the same with high
probability, even when that analysis is performed on a new data set.
  In this work, we establish new connections and separations between
replicability and standard notions of algorithmic stability. In particular, we
give sample-efficient algorithmic reductions between perfect generalization,
approximate differential privacy, and replicability for a broad class of
statistical problems. Conversely, we show any such equivalence must break down
computationally: there exist statistical problems that are easy under
differential privacy, but that cannot be solved replicably without breaking
public-key cryptography. Furthermore, these results are tight: our reductions
are statistically optimal, and we show that any computational separation
between DP and replicability must imply the existence of one-way functions.
  Our statistical reductions give a new algorithmic framework for translating
between notions of stability, which we instantiate to answer several open
questions in replicability and privacy. This includes giving sample-efficient
replicable algorithms for various PAC learning, distribution estimation, and
distribution testing problems, algorithmic amplification of $\delta$ in
approximate DP, conversions from item-level to user-level privacy, and the
existence of private agnostic-to-realizable learning reductions under
structured distributions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>STOC 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-distillation for surgical action recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12915v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12915v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amine Yamlahi, Thuy Nuong Tran, Patrick Godau, Melanie Schellenberg, Dominik Michael, Finn-Henri Smidt, Jan-Hinrich Noelke, Tim Adler, Minu Dietlinde Tizabi, Chinedu Nwoye, Nicolas Padoy, Lena Maier-Hein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Surgical scene understanding is a key prerequisite for contextaware decision
support in the operating room. While deep learning-based approaches have
already reached or even surpassed human performance in various fields, the task
of surgical action recognition remains a major challenge. With this
contribution, we are the first to investigate the concept of self-distillation
as a means of addressing class imbalance and potential label ambiguity in
surgical video analysis. Our proposed method is a heterogeneous ensemble of
three models that use Swin Transfomers as backbone and the concepts of
self-distillation and multi-task learning as core design choices. According to
ablation studies performed with the CholecT45 challenge data via
cross-validation, the biggest performance boost is achieved by the usage of
soft labels obtained by self-distillation. External validation of our method on
an independent test set was achieved by providing a Docker container of our
inference model to the challenge organizers. According to their analysis, our
method outperforms all other solutions submitted to the latest challenge in the
field. Our approach thus shows the potential of self-distillation for becoming
an important tool in medical image analysis applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TRON: <span class="highlight-title">Transformer</span> Neural Network Acceleration with Non-Coherent Silicon
  Photonics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12914v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12914v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Salma Afifi, Febin Sunny, Mahdi Nikdast, Sudeep Pasricha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer neural networks are rapidly being integrated into
state-of-the-art solutions for natural language processing (NLP) and computer
vision. However, the complex structure of these models creates challenges for
accelerating their execution on conventional electronic platforms. We propose
the first silicon photonic hardware neural network accelerator called TRON for
transformer-based models such as BERT, and Vision Transformers. Our analysis
demonstrates that TRON exhibits at least 14x better throughput and 8x better
energy efficiency, in comparison to state-of-the-art transformer accelerators.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cross-Layer Design for AI Acceleration with Non-Coherent Optical
  Computing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12910v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12910v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Febin Sunny, Mahdi Nikdast, Sudeep Pasricha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emerging AI applications such as ChatGPT, graph convolutional networks, and
other deep neural networks require massive computational resources for training
and inference. Contemporary computing platforms such as CPUs, GPUs, and TPUs
are struggling to keep up with the demands of these AI applications.
Non-coherent optical computing represents a promising approach for light-speed
acceleration of AI workloads. In this paper, we show how cross-layer design can
overcome challenges in non-coherent optical computing platforms. We describe
approaches for optical device engineering, tuning circuit enhancements, and
architectural innovations to adapt optical computing to a variety of AI
workloads. We also discuss techniques for hardware/software co-design that can
intelligently map and adapt AI software to improve its performance on
non-coherent optical computing platforms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Feature Reduction Method Comparison Towards Explainability and
  Efficiency in Cybersecurity Intrusion Detection Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12891v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12891v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adam M. Lehavi, Seongtae Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realm of cybersecurity, intrusion detection systems (IDS) detect and
prevent attacks based on collected computer and network data. In recent
research, IDS models have been constructed using machine learning (ML) and deep
learning (DL) methods such as Random Forest (RF) and deep neural networks
(DNN). Feature selection (FS) can be used to construct faster, more
interpretable, and more accurate models. We look at three different FS
techniques; RF information gain (RF-IG), correlation feature selection using
the Bat Algorithm (CFS-BA), and CFS using the Aquila Optimizer (CFS-AO). Our
results show CFS-BA to be the most efficient of the FS methods, building in 55%
of the time of the best RF-IG model while achieving 99.99% of its accuracy.
This reinforces prior contributions attesting to CFS-BA's accuracy while
building upon the relationship between subset size, CFS score, and RF-IG score
in final results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in 2022 21st IEEE International Conference on Machine
  Learning and Applications. 8 pages. 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A dynamic risk score for early prediction of cardiogenic shock using
  machine learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12888v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12888v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Hu, Albert Lui, Mark Goldstein, Mukund Sudarshan, Andrea Tinsay, Cindy Tsui, Samuel Maidman, John Medamana, Neil Jethani, Aaalad Puli, Vuthy Nguy, Yindalon Aphinyanaphongs, Nicholas Kiefer, Nathaniel Smilowitz, James Horowitz, Tania Ahuja, Glenn Fishman, Judith Hochman, Stuart Katz, Samuel Bernard, Rajesh Ranganath
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Myocardial infarction and heart failure are major cardiovascular diseases
that affect millions of people in the US. The morbidity and mortality are
highest among patients who develop cardiogenic shock. Early recognition of
cardiogenic shock is critical. Prompt implementation of treatment measures can
prevent the deleterious spiral of ischemia, low blood pressure, and reduced
cardiac output due to cardiogenic shock. However, early identification of
cardiogenic shock has been challenging due to human providers' inability to
process the enormous amount of data in the cardiac intensive care unit (ICU)
and lack of an effective risk stratification tool. We developed a deep
learning-based risk stratification tool, called CShock, for patients admitted
into the cardiac ICU with acute decompensated heart failure and/or myocardial
infarction to predict onset of cardiogenic shock. To develop and validate
CShock, we annotated cardiac ICU datasets with physician adjudicated outcomes.
CShock achieved an area under the receiver operator characteristic curve
(AUROC) of 0.820, which substantially outperformed CardShock (AUROC 0.519), a
well-established risk score for cardiogenic shock prognosis. CShock was
externally validated in an independent patient cohort and achieved an AUROC of
0.800, demonstrating its generalizability in other cardiac ICUs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Consensus in Ranking Data Analysis: Definitions, Properties and
  Computational Issues 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12878v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12878v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Morgane Goibert, Clément Calauzènes, Ekhine Irurozki, Stéphan Clémençon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the issue of robustness in AI systems becomes vital, statistical learning
techniques that are reliable even in presence of partly contaminated data have
to be developed. Preference data, in the form of (complete) rankings in the
simplest situations, are no exception and the demand for appropriate concepts
and tools is all the more pressing given that technologies fed by or producing
this type of data (e.g. search engines, recommending systems) are now massively
deployed. However, the lack of vector space structure for the set of rankings
(i.e. the symmetric group $\mathfrak{S}_n$) and the complex nature of
statistics considered in ranking data analysis make the formulation of
robustness objectives in this domain challenging. In this paper, we introduce
notions of robustness, together with dedicated statistical methods, for
Consensus Ranking the flagship problem in ranking data analysis, aiming at
summarizing a probability distribution on $\mathfrak{S}_n$ by a median ranking.
Precisely, we propose specific extensions of the popular concept of breakdown
point, tailored to consensus ranking, and address the related computational
issues. Beyond the theoretical contributions, the relevance of the approach
proposed is supported by an experimental study.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Human Uncertainty in Concept-Based AI Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12872v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12872v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Katherine M. Collins, Matthew Barker, Mateo Espinosa Zarlenga, Naveen Raman, Umang Bhatt, Mateja Jamnik, Ilia Sucholutsky, Adrian Weller, Krishnamurthy Dvijotham
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Placing a human in the loop may abate the risks of deploying AI systems in
safety-critical settings (e.g., a clinician working with a medical AI system).
However, mitigating risks arising from human error and uncertainty within such
human-AI interactions is an important and understudied issue. In this work, we
study human uncertainty in the context of concept-based models, a family of AI
systems that enable human feedback via concept interventions where an expert
intervenes on human-interpretable concepts relevant to the task. Prior work in
this space often assumes that humans are oracles who are always certain and
correct. Yet, real-world decision-making by humans is prone to occasional
mistakes and uncertainty. We study how existing concept-based models deal with
uncertain interventions from humans using two novel datasets: UMNIST, a visual
dataset with controlled simulated uncertainty based on the MNIST dataset, and
CUB-S, a relabeling of the popular CUB concept dataset with rich,
densely-annotated soft labels from humans. We show that training with uncertain
concept labels may help mitigate weaknesses of concept-based systems when
handling uncertain interventions. These results allow us to identify several
open challenges, which we argue can be tackled through future multidisciplinary
research on building interactive uncertainty-aware systems. To facilitate
further research, we release a new elicitation platform, UElic, to collect
uncertain feedback from humans in collaborative prediction tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NeRF-GAN Distillation for Efficient 3D-Aware Generation with
  Convolutions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12865v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12865v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamad Shahbazi, Evangelos Ntavelis, Alessio Tonioni, Edo Collins, Danda Pani Paudel, Martin Danelljan, Luc Van Gool
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pose-conditioned convolutional generative models struggle with high-quality
3D-consistent image generation from single-view datasets, due to their lack of
sufficient 3D priors. Recently, the integration of Neural Radiance Fields
(NeRFs) and generative models, such as Generative Adversarial Networks (GANs),
has transformed 3D-aware generation from single-view images. NeRF-GANs exploit
the strong inductive bias of 3D neural representations and volumetric rendering
at the cost of higher computational complexity. This study aims at revisiting
pose-conditioned 2D GANs for efficient 3D-aware generation at inference time by
distilling 3D knowledge from pretrained NeRF-GANS. We propose a simple and
effective method, based on re-using the well-disentangled latent space of a
pre-trained NeRF-GAN in a pose-conditioned convolutional network to directly
generate 3D-consistent images corresponding to the underlying 3D
representations. Experiments on several datasets demonstrate that the proposed
method obtains results comparable with volumetric rendering in terms of quality
and 3D consistency while benefiting from the superior computational advantage
of convolutional networks. The code will be available at:
https://github.com/mshahbazi72/NeRF-GAN-Distillation
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cube-Based 3D Denoising Diffusion Probabilistic Model for Cone Beam
  Computed Tomography Reconstruction with Incomplete Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12861v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12861v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenjun Xia, Chuang Niu, Wenxiang Cong, Ge Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning (DL) has been extensively researched in the field of computed
tomography (CT) reconstruction with incomplete data, particularly in
sparse-view CT reconstruction. However, applying DL to sparse-view cone beam CT
(CBCT) remains challenging. Many models learn the mapping from sparse-view CT
images to ground truth but struggle to achieve satisfactory performance in
terms of global artifact removal. Incorporating sinogram data and utilizing
dual-domain information can enhance anti-artifact performance, but this
requires storing the entire sinogram in memory. This presents a memory issue
for high-resolution CBCT sinograms, limiting further research and application.
In this paper, we propose a cube-based 3D denoising diffusion probabilistic
model (DDPM) for CBCT reconstruction using down-sampled data. A DDPM network,
trained on cubes extracted from paired fully sampled sinograms and down-sampled
sinograms, is employed to inpaint down-sampled sinograms. Our method divides
the entire sinogram into overlapping cubes and processes these cubes in
parallel using multiple GPUs, overcoming memory limitations. Experimental
results demonstrate that our approach effectively suppresses few-view artifacts
while preserving textural details faithfully.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Anti-symmetric Barron functions and their approximation with sums of
  determinants 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12856v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12856v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nilin Abrahamsen, Lin Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A fundamental problem in quantum physics is to encode functions that are
completely anti-symmetric under permutations of identical particles. The Barron
space consists of high-dimensional functions that can be parameterized by
infinite neural networks with one hidden layer. By explicitly encoding the
anti-symmetric structure, we prove that the anti-symmetric functions which
belong to the Barron space can be efficiently approximated with sums of
determinants. This yields a factorial improvement in complexity compared to the
standard representation in the Barron space and provides a theoretical
explanation for the effectiveness of determinant-based architectures in
ab-initio quantum chemistry.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Three iterations of $(1-d)$-WL test distinguish non isometric clouds of
  $d$-dimensional points 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12853v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12853v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valentino Delle Rose, Alexander Kozachinskiy, Cristóbal Rojas, Mircea Petrache, Pablo Barceló
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Weisfeiler--Lehman (WL) test is a fundamental iterative algorithm for
checking isomorphism of graphs. It has also been observed that it underlies the
design of several graph neural network architectures, whose capabilities and
performance can be understood in terms of the expressive power of this test.
Motivated by recent developments in machine learning applications to datasets
involving three-dimensional objects, we study when the WL test is {\em
complete} for clouds of euclidean points represented by complete distance
graphs, i.e., when it can distinguish, up to isometry, any arbitrary such
cloud.
  Our main result states that the $(d-1)$-dimensional WL test is complete for
point clouds in $d$-dimensional Euclidean space, for any $d\ge 2$, and that
only three iterations of the test suffice. Our result is tight for $d = 2, 3$.
We also observe that the $d$-dimensional WL test only requires one iteration to
achieve completeness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Test-time Defense against Adversarial Attacks: Detection and
  Reconstruction of Adversarial Examples via Masked Autoencoder 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12848v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12848v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yun-Yun Tsai, Ju-Chin Chao, Albert Wen, Zhaoyuan Yang, Chengzhi Mao, Tapan Shah, Junfeng Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing defense methods against adversarial attacks can be categorized into
training time and test time defenses. Training time defense, i.e., adversarial
training, requires a significant amount of extra time for training and is often
not able to be generalized to unseen attacks. On the other hand, test time
defense by test time weight adaptation requires access to perform gradient
descent on (part of) the model weights, which could be infeasible for models
with frozen weights. To address these challenges, we propose DRAM, a novel
defense method to Detect and Reconstruct multiple types of Adversarial attacks
via Masked autoencoder (MAE). We demonstrate how to use MAE losses to build a
KS-test to detect adversarial attacks. Moreover, the MAE losses can be used to
repair adversarial samples from unseen attack types. In this sense, DRAM
neither requires model weight updates in test time nor augments the training
set with more adversarial samples. Evaluating DRAM on the large-scale ImageNet
data, we achieve the best detection rate of 82% on average on eight types of
adversarial attacks compared with other detection baselines. For
reconstruction, DRAM improves the robust accuracy by 6% ~ 41% for Standard
ResNet50 and 3% ~ 8% for Robust ResNet50 compared with other self-supervision
tasks, such as rotation prediction and contrastive learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The power and limitations of learning quantum dynamics incoherently 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12834v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12834v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sofiene Jerbi, Joe Gibbs, Manuel S. Rudolph, Matthias C. Caro, Patrick J. Coles, Hsin-Yuan Huang, Zoë Holmes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantum process learning is emerging as an important tool to study quantum
systems. While studied extensively in coherent frameworks, where the target and
model system can share quantum information, less attention has been paid to
whether the dynamics of quantum systems can be learned without the system and
target directly interacting. Such incoherent frameworks are practically
appealing since they open up methods of transpiling quantum processes between
the different physical platforms without the need for technically challenging
hybrid entanglement schemes. Here we provide bounds on the sample complexity of
learning unitary processes incoherently by analyzing the number of measurements
that are required to emulate well-established coherent learning strategies. We
prove that if arbitrary measurements are allowed, then any efficiently
representable unitary can be efficiently learned within the incoherent
framework; however, when restricted to shallow-depth measurements only
low-entangling unitaries can be learned. We demonstrate our incoherent learning
algorithm for low entangling unitaries by successfully learning a 16-qubit
unitary on \texttt{ibmq\_kolkata}, and further demonstrate the scalabilty of
our proposed algorithm through extensive numerical experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6+9 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards A Visual Programming Tool to Create Deep Learning Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12821v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12821v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tommaso Calò, Luigi De Russis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Learning (DL) developers come from different backgrounds, e.g.,
medicine, genomics, finance, and computer science. To create a DL model, they
must learn and use high-level programming languages (e.g., Python), thus
needing to handle related setups and solve programming errors. This paper
presents DeepBlocks, a visual programming tool that allows DL developers to
design, train, and evaluate models without relying on specific programming
languages. DeepBlocks works by building on the typical model structure: a
sequence of learnable functions whose arrangement defines the specific
characteristics of the model. We derived DeepBlocks' design goals from a
5-participants formative interview, and we validated the first implementation
of the tool through a typical use case. Results are promising and show that
developers could visually design complex DL architectures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EfficientTrain: Exploring Generalized Curriculum Learning for Training
  Visual Backbones 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.09703v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.09703v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yulin Wang, Yang Yue, Rui Lu, Tianjiao Liu, Zhao Zhong, Shiji Song, Gao Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The superior performance of modern deep networks usually comes with a costly
training procedure. This paper presents a new curriculum learning approach for
the efficient training of visual backbones (e.g., vision Transformers). Our
work is inspired by the inherent learning dynamics of deep networks: we
experimentally show that at an earlier training stage, the model mainly learns
to recognize some 'easier-to-learn' discriminative patterns within each
example, e.g., the lower-frequency components of images and the original
information before data augmentation. Driven by this phenomenon, we propose a
curriculum where the model always leverages all the training data at each
epoch, while the curriculum starts with only exposing the 'easier-to-learn'
patterns of each example, and introduces gradually more difficult patterns. To
implement this idea, we 1) introduce a cropping operation in the Fourier
spectrum of the inputs, which enables the model to learn from only the
lower-frequency components efficiently, 2) demonstrate that exposing the
features of original images amounts to adopting weaker data augmentation, and
3) integrate 1) and 2) and design a curriculum learning schedule with a
greedy-search algorithm. The resulting approach, EfficientTrain, is simple,
general, yet surprisingly effective. In the absence of hyper-parameter tuning,
it reduces the training wall-time of a wide variety of popular models (e.g.,
ResNet, ConvNeXt, DeiT, PVT, Swin, and CSWin) by >1.5x on ImageNet-1K/22K
without sacrificing the accuracy. It is also effective for self-supervised
learning (e.g., MAE). Code is available at
https://github.com/LeapLabTHU/EfficientTrain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SemDeDup: Data-efficient learning at web-scale through semantic
  deduplication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.09540v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.09540v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amro Abbas, Kushal Tirumala, Dániel Simig, Surya Ganguli, Ari S. Morcos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Progress in machine learning has been driven in large part by massive
increases in data. However, large web-scale datasets such as LAION are largely
uncurated beyond searches for exact duplicates, potentially leaving much
redundancy. Here, we introduce SemDeDup, a method which leverages embeddings
from pre-trained models to identify and remove semantic duplicates: data pairs
which are semantically similar, but not exactly identical. Removing semantic
duplicates preserves performance and speeds up learning. Analyzing a subset of
LAION, we show that SemDeDup can remove 50% of the data with minimal
performance loss, effectively halving training time. Moreover, performance
increases out of distribution. Also, analyzing language models trained on C4, a
partially curated dataset, we show that SemDeDup improves over prior approaches
while providing efficiency gains. SemDeDup provides an example of how simple
ways of leveraging quality embeddings can be used to make models learn faster
with less data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Estimating the randomness of quantum circuit ensembles up to 50 qubits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.09900v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.09900v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minzhao Liu, Junyu Liu, Yuri Alexeev, Liang Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Random quantum circuits have been utilized in the contexts of quantum
supremacy demonstrations, variational quantum algorithms for chemistry and
machine learning, and blackhole information. The ability of random circuits to
approximate any random unitaries has consequences on their complexity,
expressibility, and trainability. To study this property of random circuits, we
develop numerical protocols for estimating the frame potential, the distance
between a given ensemble and the exact randomness. Our tensor-network-based
algorithm has polynomial complexity for shallow circuits and is high-performing
using CPU and GPU parallelism. We study 1. local and parallel random circuits
to verify the linear growth in complexity as stated by the Brown-Susskind
conjecture, and; 2. hardware-efficient ans\"atze to shed light on its
expressibility and the barren plateau problem in the context of variational
algorithms. Our work shows that large-scale tensor network simulations could
provide important hints toward open problems in quantum information science.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, many figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RoBIC: A benchmark suite for assessing classifiers robustness <span class="chip">ICIP 2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2102.05368v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2102.05368v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thibault Maho, Benoît Bonnet, Teddy Furon, Erwan Le Merrer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many defenses have emerged with the development of adversarial attacks.
Models must be objectively evaluated accordingly. This paper systematically
tackles this concern by proposing a new parameter-free benchmark we coin RoBIC.
RoBIC fairly evaluates the robustness of image classifiers using a new
half-distortion measure. It gauges the robustness of the network against white
and black box attacks, independently of its accuracy. RoBIC is faster than the
other available benchmarks. We present the significant differences in the
robustness of 16 recent models as assessed by RoBIC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, accepted to ICIP 2021</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Multi-view Clustering via Unified and Discrete Bipartite Graph
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.04187v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.04187v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Si-Guo Fang, Dong Huang, Xiao-Sha Cai, Chang-Dong Wang, Chaobo He, Yong Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although previous graph-based multi-view clustering algorithms have gained
significant progress, most of them are still faced with three limitations.
First, they often suffer from high computational complexity, which restricts
their applications in large-scale scenarios. Second, they usually perform graph
learning either at the single-view level or at the view-consensus level, but
often neglect the possibility of the joint learning of single-view and
consensus graphs. Third, many of them rely on the k-means for discretization of
the spectral embeddings, which lack the ability to directly learn the graph
with discrete cluster structure. In light of this, this paper presents an
efficient multi-view clustering approach via unified and discrete bipartite
graph learning (UDBGL). Specifically, the anchor-based subspace learning is
incorporated to learn the view-specific bipartite graphs from multiple views,
upon which the bipartite graph fusion is leveraged to learn a view-consensus
bipartite graph with adaptive weight learning. Further, the Laplacian rank
constraint is imposed to ensure that the fused bipartite graph has discrete
cluster structures (with a specific number of connected components). By
simultaneously formulating the view-specific bipartite graph learning, the
view-consensus bipartite graph learning, and the discrete cluster structure
learning into a unified objective function, an efficient minimization algorithm
is then designed to tackle this optimization problem and directly achieve a
discrete clustering solution without requiring additional partitioning, which
notably has linear time complexity in data size. Experiments on a variety of
multi-view datasets demonstrate the robustness and efficiency of our UDBGL
approach. The code is available at https://github.com/huangdonghere/UDBGL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Neural Networks and Learning Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CgAT: Center-Guided Adversarial Training for Deep Hashing-Based
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.10779v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.10779v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xunguang Wang, Yiqun Lin, Xiaomeng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep hashing has been extensively utilized in massive image retrieval because
of its efficiency and effectiveness. However, deep hashing models are
vulnerable to adversarial examples, making it essential to develop adversarial
defense methods for image retrieval. Existing solutions achieved limited
defense performance because of using weak adversarial samples for training and
lacking discriminative optimization objectives to learn robust features. In
this paper, we present a min-max based Center-guided Adversarial Training,
namely CgAT, to improve the robustness of deep hashing networks through worst
adversarial examples. Specifically, we first formulate the center code as a
semantically-discriminative representative of the input image content, which
preserves the semantic similarity with positive samples and dissimilarity with
negative examples. We prove that a mathematical formula can calculate the
center code immediately. After obtaining the center codes in each optimization
iteration of the deep hashing network, they are adopted to guide the
adversarial training process. On the one hand, CgAT generates the worst
adversarial examples as augmented data by maximizing the Hamming distance
between the hash codes of the adversarial examples and the center codes. On the
other hand, CgAT learns to mitigate the effects of adversarial samples by
minimizing the Hamming distance to the center codes. Extensive experiments on
the benchmark datasets demonstrate the effectiveness of our adversarial
training algorithm in defending against adversarial attacks for deep
hashing-based retrieval. Compared with the current state-of-the-art defense
method, we significantly improve the defense performance by an average of
18.61\%, 12.35\%, and 11.56\% on FLICKR-25K, NUS-WIDE, and MS-COCO,
respectively. The code is available at https://github.com/xunguangwang/CgAT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Membership Inference Attacks against Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.03262v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.03262v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tomoya Matsumoto, Takayuki Miura, Naoto Yanai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have attracted attention in recent years as innovative
generative models. In this paper, we investigate whether a diffusion model is
resistant to a membership inference attack, which evaluates the privacy leakage
of a machine learning model. We primarily discuss the diffusion model from the
standpoints of comparison with a generative adversarial network (GAN) as
conventional models and hyperparameters unique to the diffusion model, i.e.,
time steps, sampling steps, and sampling variances. We conduct extensive
experiments with DDIM as a diffusion model and DCGAN as a GAN on the CelebA and
CIFAR-10 datasets in both white-box and black-box settings and then confirm if
the diffusion model is comparably resistant to a membership inference attack as
GAN. Next, we demonstrate that the impact of time steps is significant and
intermediate steps in a noise schedule are the most vulnerable to the attack.
We also found two key insights through further analysis. First, we identify
that DDIM is vulnerable to the attack for small sample sizes instead of
achieving a lower FID. Second, sampling steps in hyperparameters are important
for resistance to the attack, whereas the impact of sampling variances is quite
limited.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rotating without Seeing: Towards In-hand Dexterity through Touch 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10880v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10880v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhao-Heng Yin, Binghao Huang, Yuzhe Qin, Qifeng Chen, Xiaolong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tactile information plays a critical role in human dexterity. It reveals
useful contact information that may not be inferred directly from vision. In
fact, humans can even perform in-hand dexterous manipulation without using
vision. Can we enable the same ability for the multi-finger robot hand? In this
paper, we present Touch Dexterity, a new system that can perform in-hand object
rotation using only touching without seeing the object. Instead of relying on
precise tactile sensing in a small region, we introduce a new system design
using dense binary force sensors (touch or no touch) overlaying one side of the
whole robot hand (palm, finger links, fingertips). Such a design is low-cost,
giving a larger coverage of the object, and minimizing the Sim2Real gap at the
same time. We train an in-hand rotation policy using Reinforcement Learning on
diverse objects in simulation. Relying on touch-only sensing, we can directly
deploy the policy in a real robot hand and rotate novel objects that are not
presented in training. Extensive ablations are performed on how tactile
information help in-hand manipulation.Our project is available at
https://touchdexterity.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://touchdexterity.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Interpretable Reinforcement Learning via Neural Additive Models for
  Inventory Management 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10382v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10382v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julien Siems, Maximilian Schambach, Sebastian Schulze, Johannes S. Otterbach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The COVID-19 pandemic has highlighted the importance of supply chains and the
role of digital management to react to dynamic changes in the environment. In
this work, we focus on developing dynamic inventory ordering policies for a
multi-echelon, i.e. multi-stage, supply chain. Traditional inventory
optimization methods aim to determine a static reordering policy. Thus, these
policies are not able to adjust to dynamic changes such as those observed
during the COVID-19 crisis. On the other hand, conventional strategies offer
the advantage of being interpretable, which is a crucial feature for supply
chain managers in order to communicate decisions to their stakeholders. To
address this limitation, we propose an interpretable reinforcement learning
approach that aims to be as interpretable as the traditional static policies
while being as flexible and environment-agnostic as other deep learning-based
reinforcement learning solutions. We propose to use Neural Additive Models as
an interpretable dynamic policy of a reinforcement learning agent, showing that
this approach is competitive with a standard full connected policy. Finally, we
use the interpretability property to gain insights into a complex ordering
strategy for a simple, linear three-echelon inventory supply chain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unbiased Supervised Contrastive Learning <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.05568v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.05568v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carlo Alberto Barbano, Benoit Dufumier, Enzo Tartaglione, Marco Grangetto, Pietro Gori
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many datasets are biased, namely they contain easy-to-learn features that are
highly correlated with the target class only in the dataset but not in the true
underlying distribution of the data. For this reason, learning unbiased models
from biased data has become a very relevant research topic in the last years.
In this work, we tackle the problem of learning representations that are robust
to biases. We first present a margin-based theoretical framework that allows us
to clarify why recent contrastive losses (InfoNCE, SupCon, etc.) can fail when
dealing with biased data. Based on that, we derive a novel formulation of the
supervised contrastive loss (epsilon-SupInfoNCE), providing more accurate
control of the minimal distance between positive and negative samples.
Furthermore, thanks to our theoretical framework, we also propose FairKL, a new
debiasing regularization loss, that works well even with extremely biased data.
We validate the proposed losses on standard vision datasets including CIFAR10,
CIFAR100, and ImageNet, and we assess the debiasing capability of FairKL with
epsilon-SupInfoNCE, reaching state-of-the-art performance on a number of biased
datasets, including real instances of biases in the wild.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic Relevance Learning for Few-Shot Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2108.02235v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2108.02235v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weijie Liu, Chong Wang, Haohe Li, Shenghao Yu, Jiafei Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Expensive bounding-box annotations have limited the development of object
detection task. Thus, it is necessary to focus on more challenging task of
few-shot object detection. It requires the detector to recognize objects of
novel classes with only a few training samples. Nowadays, many existing popular
methods adopting training way similar to meta-learning have achieved promising
performance, such as Meta R-CNN series. However, support data is only used as
the class attention to guide the detecting of query images each time. Their
relevance to each other remains unexploited. Moreover, a lot of recent works
treat the support data and query images as independent branch without
considering the relationship between them. To address this issue, we propose a
dynamic relevance learning model, which utilizes the relationship between all
support images and Region of Interest (RoI) on the query images to construct a
dynamic graph convolutional network (GCN). By adjusting the prediction
distribution of the base detector using the output of this GCN, the proposed
model serves as a hard auxiliary classification task, which guides the detector
to improve the class representation implicitly. Comprehensive experiments have
been conducted on Pascal VOC and MS-COCO dataset. The proposed model achieves
the best overall performance, which shows its effectiveness of learning more
generalized features. Our code is available at
https://github.com/liuweijie19980216/DRL-for-FSOD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 8 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards AI-controlled FES-restoration of movements: Learning cycling
  stimulation pattern with reinforcement learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.09986v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.09986v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nat Wannawas, A. Aldo Faisal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Functional electrical stimulation (FES) has been increasingly integrated with
other rehabilitation devices, including robots. FES cycling is one of the
common FES applications in rehabilitation, which is performed by stimulating
leg muscles in a certain pattern. The appropriate pattern varies across
individuals and requires manual tuning which can be time-consuming and
challenging for the individual user. Here, we present an AI-based method for
finding the patterns, which requires no extra hardware or sensors. Our method
has two phases, starting with finding model-based patterns using reinforcement
learning and detailed musculoskeletal models. The models, built using
open-source software, can be customised through our automated script and can be
therefore used by non-technical individuals without extra cost. Next, our
method fine-tunes the pattern using real cycling data. We test our both in
simulation and experimentally on a stationary tricycle. In the simulation test,
our method can robustly deliver model-based patterns for different cycling
configurations. The experimental evaluation shows that our method can find a
model-based pattern that induces higher cycling speed than an EMG-based
pattern. By using just 100 seconds of cycling data, our method can deliver a
fine-tuned pattern that gives better cycling performance. Beyond FES cycling,
this work is a showcase, displaying the feasibility and potential of
human-in-the-loop AI in real-world rehabilitation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">GPT</span>Q: Accurate Post-Training Quantization for Generative <span class="highlight-title">Pre-train</span>ed
  <span class="highlight-title">Transformer</span>s <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.17323v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.17323v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elias Frantar, Saleh Ashkboos, Torsten Hoefler, Dan Alistarh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative Pre-trained Transformer models, known as GPT or OPT, set
themselves apart through breakthrough performance across complex language
modelling tasks, but also by their extremely high computational and storage
costs. Specifically, due to their massive size, even inference for large,
highly-accurate GPT models may require multiple performant GPUs, which limits
the usability of such models. While there is emerging work on relieving this
pressure via model compression, the applicability and performance of existing
compression techniques is limited by the scale and complexity of GPT models. In
this paper, we address this challenge, and propose GPTQ, a new one-shot weight
quantization method based on approximate second-order information, that is both
highly-accurate and highly-efficient. Specifically, GPTQ can quantize GPT
models with 175 billion parameters in approximately four GPU hours, reducing
the bitwidth down to 3 or 4 bits per weight, with negligible accuracy
degradation relative to the uncompressed baseline. Our method more than doubles
the compression gains relative to previously-proposed one-shot quantization
methods, preserving accuracy, allowing us for the first time to execute an 175
billion-parameter model inside a single GPU for generative inference. Moreover,
we also show that our method can still provide reasonable accuracy in the
extreme quantization regime, in which weights are quantized to 2-bit or even
ternary quantization levels. We show experimentally that these improvements can
be leveraged for end-to-end inference speedups over FP16, of around 3.25x when
using high-end GPUs (NVIDIA A100) and 4.5x when using more cost-effective ones
(NVIDIA A6000). The implementation is available at
https://github.com/IST-DASLab/gptq.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On The Effects Of Data Normalisation For Domain Adaptation On EEG Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.01081v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.01081v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Apicella, Francesco Isgrò, Andrea Pollastro, Roberto Prevete
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the Machine Learning (ML) literature, a well-known problem is the Dataset
Shift problem where, differently from the ML standard hypothesis, the data in
the training and test sets can follow different probability distributions,
leading ML systems toward poor generalisation performances. This problem is
intensely felt in the Brain-Computer Interface (BCI) context, where bio-signals
as Electroencephalographic (EEG) are often used. In fact, EEG signals are
highly non-stationary both over time and between different subjects. To
overcome this problem, several proposed solutions are based on recent transfer
learning approaches such as Domain Adaption (DA). In several cases, however,
the actual causes of the improvements remain ambiguous. This paper focuses on
the impact of data normalisation, or standardisation strategies applied
together with DA methods. In particular, using \textit{SEED}, \textit{DEAP},
and \textit{BCI Competition IV 2a} EEG datasets, we experimentally evaluated
the impact of different normalization strategies applied with and without
several well-known DA methods, comparing the obtained performances. It results
that the choice of the normalisation strategy plays a key role on the
classifier performances in DA scenarios, and interestingly, in several cases,
the use of only an appropriate normalisation schema outperforms the DA
technique.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to be published in its final version on Engineering
  Applications of Artificial Intelligence (EAAI)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LargeKernel3D: Scaling up Kernels in 3D Sparse CNNs <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.10555v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.10555v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yukang Chen, Jianhui Liu, Xiangyu Zhang, Xiaojuan Qi, Jiaya Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advance in 2D CNNs has revealed that large kernels are important.
However, when directly applying large convolutional kernels in 3D CNNs, severe
difficulties are met, where those successful module designs in 2D become
surprisingly ineffective on 3D networks, including the popular depth-wise
convolution. To address this vital challenge, we instead propose the
spatial-wise partition convolution and its large-kernel module. As a result, it
avoids the optimization and efficiency issues of naive 3D large kernels. Our
large-kernel 3D CNN network, LargeKernel3D, yields notable improvement in 3D
tasks of semantic segmentation and object detection. It achieves 73.9% mIoU on
the ScanNetv2 semantic segmentation and 72.8% NDS nuScenes object detection
benchmarks, ranking 1st on the nuScenes LIDAR leaderboard. The performance
further boosts to 74.2% NDS with a simple multi-modal fusion. In addition,
LargeKernel3D can be scaled to 17x17x17 kernel size on Waymo 3D object
detection. For the first time, we show that large kernels are feasible and
essential for 3D visual tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In CVPR 2023. Code is at
  https://github.com/dvlab-research/LargeKernel3D</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sparse<span class="highlight-title">GPT</span>: Massive Language Models Can Be Accurately Pruned in One-Shot 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.00774v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.00774v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elias Frantar, Dan Alistarh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We show for the first time that large-scale generative pretrained transformer
(GPT) family models can be pruned to at least 50% sparsity in one-shot, without
any retraining, at minimal loss of accuracy. This is achieved via a new pruning
method called SparseGPT, specifically designed to work efficiently and
accurately on massive GPT-family models. We can execute SparseGPT on the
largest available open-source models, OPT-175B and BLOOM-176B, in under 4.5
hours, and can reach 60% unstructured sparsity with negligible increase in
perplexity: remarkably, more than 100 billion weights from these models can be
ignored at inference time. SparseGPT generalizes to semi-structured (2:4 and
4:8) patterns, and is compatible with weight quantization approaches. The code
is available at: https://github.com/IST-DASLab/sparsegpt.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ExpressivE: A Spatio-Functional Embedding For Knowledge Graph Completion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.04192v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.04192v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aleksandar Pavlović, Emanuel Sallinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge graphs are inherently incomplete. Therefore substantial research
has been directed toward knowledge graph completion (KGC), i.e., predicting
missing triples from the information represented in the knowledge graph (KG).
KG embedding models (KGEs) have yielded promising results for KGC, yet any
current KGE is incapable of: (1) fully capturing vital inference patterns
(e.g., composition), (2) capturing prominent patterns jointly (e.g., hierarchy
and composition), and (3) providing an intuitive interpretation of captured
patterns. In this work, we propose ExpressivE, a fully expressive
spatio-functional KGE that solves all these challenges simultaneously.
ExpressivE embeds pairs of entities as points and relations as
hyper-parallelograms in the virtual triple space $\mathbb{R}^{2d}$. This model
design allows ExpressivE not only to capture a rich set of inference patterns
jointly but additionally to display any supported inference pattern through the
spatial relation of hyper-parallelograms, offering an intuitive and consistent
geometric interpretation of ExpressivE embeddings and their captured patterns.
Experimental results on standard KGC benchmarks reveal that ExpressivE is
competitive with state-of-the-art KGEs and even significantly outperforms them
on WN18RR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DeepProphet2 -- A Deep Learning Gene Recommendation Engine 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.01918v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.01918v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniele Brambilla, Davide Maria Giacomini, Luca Muscarnera, Andrea Mazzoleni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  New powerful tools for tackling life science problems have been created by
recent advances in machine learning. The purpose of the paper is to discuss the
potential advantages of gene recommendation performed by artificial
intelligence (AI). Indeed, gene recommendation engines try to solve this
problem: if the user is interested in a set of genes, which other genes are
likely to be related to the starting set and should be investigated? This task
was solved with a custom deep learning recommendation engine, DeepProphet2
(DP2), which is freely available to researchers worldwide via
https://www.generecommender.com?utm_source=DeepProphet2_paper&utm_medium=pdf.
Hereafter, insights behind the algorithm and its practical applications are
illustrated.
  The gene recommendation problem can be addressed by mapping the genes to a
metric space where a distance can be defined to represent the real semantic
distance between them. To achieve this objective a transformer-based model has
been trained on a well-curated freely available paper corpus, PubMed. The paper
describes multiple optimization procedures that were employed to obtain the
best bias-variance trade-off, focusing on embedding size and network depth. In
this context, the model's ability to discover sets of genes implicated in
diseases and pathways was assessed through cross-validation. A simple
assumption guided the procedure: the network had no direct knowledge of
pathways and diseases but learned genes' similarities and the interactions
among them. Moreover, to further investigate the space where the neural network
represents genes, the dimensionality of the embedding was reduced, and the
results were projected onto a human-comprehensible space. In conclusion, a set
of use cases illustrates the algorithm's potential applications in a real word
setting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MedNeXt: <span class="highlight-title">Transformer</span>-driven Scaling of ConvNets for Medical Image
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.09975v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.09975v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saikat Roy, Gregor Koehler, Constantin Ulrich, Michael Baumgartner, Jens Petersen, Fabian Isensee, Paul F. Jaeger, Klaus Maier-Hein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There has been exploding interest in embracing Transformer-based
architectures for medical image segmentation. However, the lack of large-scale
annotated medical datasets make achieving performances equivalent to those in
natural images challenging. Convolutional networks, in contrast, have higher
inductive biases and consequently, are easily trainable to high performance.
Recently, the ConvNeXt architecture attempted to modernize the standard ConvNet
by mirroring Transformer blocks. In this work, we improve upon this to design a
modernized and scalable convolutional architecture customized to challenges of
data-scarce medical settings. We introduce MedNeXt, a Transformer-inspired
large kernel segmentation network which introduces - 1) A fully ConvNeXt 3D
Encoder-Decoder Network for medical image segmentation, 2) Residual ConvNeXt up
and downsampling blocks to preserve semantic richness across scales, 3) A novel
technique to iteratively increase kernel sizes by upsampling small kernel
networks, to prevent performance saturation on limited medical data, 4)
Compound scaling at multiple levels (depth, width, kernel size) of MedNeXt.
This leads to state-of-the-art performance on 4 tasks on CT and MRI modalities
and varying dataset sizes, representing a modernized deep architecture for
medical image segmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ μSplit: efficient image decomposition for microscopy data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.12872v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.12872v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         Ashesh, Alexander Krull, Moises Di Sante, Francesco Silvio Pasqualini, Florian Jug
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present uSplit, a dedicated approach for trained image decomposition in
the context of fluorescence microscopy images. We find that best results using
regular deep architectures are achieved when large image patches are used
during training, making memory consumption the limiting factor to further
improving performance. We therefore introduce lateral contextualization (LC), a
memory efficient way to train powerful networks and show that LC leads to
consistent and significant improvements on the task at hand. We integrate LC
with U-Nets, Hierarchical AEs, and Hierarchical VAEs, for which we formulate a
modified ELBO loss. Additionally, LC enables training deeper hierarchical
models than otherwise possible and, interestingly, helps to reduce tiling
artefacts that are inherently impossible to avoid when using tiled VAE
predictions. We apply uSplit to five decomposition tasks, one on a synthetic
dataset, four others derived from real microscopy data. LC achieves SOTA
results (average improvements to the best baseline of 2.36 dB PSNR), while
simultaneously requiring considerably less GPU memory.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 7 figures, 9 pages supplement, 8 supplementary figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhanced Sampling of Configuration and Path Space in a Generalized
  Ensemble by Shooting Point Exchange 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.08757v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.08757v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastian Falkner, Alessandro Coretti, Christoph Dellago
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The computer simulation of many molecular processes is complicated by long
time scales caused by rare transitions between long-lived states. Here, we
propose a new approach to simulate such rare events, which combines transition
path sampling with enhanced exploration of configuration space. The method
relies on exchange moves between configuration and trajectory space, carried
out based on a generalized ensemble. This scheme substantially enhances the
efficiency of the transition path sampling simulations, particularly for
systems with multiple transition channels, and yields information on
thermodynamics, kinetics and reaction coordinates of molecular processes
without distorting their dynamics. The method is illustrated using the
isomerization of proline in the KPTP tetrapeptide.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Added Supplementary Information for simulation details and network
  parameters</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Guiding Online Reinforcement Learning with Action-Free Offline
  <span class="highlight-title">Pretrain</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.12876v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.12876v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deyao Zhu, Yuhui Wang, Jürgen Schmidhuber, Mohamed Elhoseiny
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Offline RL methods have been shown to reduce the need for environment
interaction by training agents using offline collected episodes. However, these
methods typically require action information to be logged during data
collection, which can be difficult or even impossible in some practical cases.
In this paper, we investigate the potential of using action-free offline
datasets to improve online reinforcement learning, name this problem
Reinforcement Learning with Action-Free Offline Pretraining (AFP-RL). We
introduce Action-Free Guide (AF-Guide), a method that guides online training by
extracting knowledge from action-free offline datasets. AF-Guide consists of an
Action-Free Decision Transformer (AFDT) implementing a variant of Upside-Down
Reinforcement Learning. It learns to plan the next states from the offline
dataset, and a Guided Soft Actor-Critic (Guided SAC) that learns online with
guidance from AFDT. Experimental results show that AF-Guide can improve sample
efficiency and performance in online training thanks to the knowledge from the
action-free offline dataset. Code is available at
https://github.com/Vision-CAIR/AF-Guide.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Instance-Dependent Bounds for Zeroth-order Lipschitz Optimization with
  Error Certificates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2102.01977v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2102.01977v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        François Bachoc, Tommaso R Cesari, Sébastien Gerchinovitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of zeroth-order (black-box) optimization of a Lipschitz
function $f$ defined on a compact subset $\mathcal X$ of $\mathbb R^d$, with
the additional constraint that algorithms must certify the accuracy of their
recommendations. We characterize the optimal number of evaluations of any
Lipschitz function $f$ to find and certify an approximate maximizer of $f$ at
accuracy $\varepsilon$. Under a weak assumption on $\mathcal X$, this optimal
sample complexity is shown to be nearly proportional to the integral
$\int_{\mathcal X} \mathrm{d}\boldsymbol x/( \max(f) - f(\boldsymbol x) +
\varepsilon )^d$. This result, which was only (and partially) known in
dimension $d=1$, solves an open problem dating back to 1991. In terms of
techniques, our upper bound relies on a packing bound by Bouttier al. (2020)
for the Piyavskii-Shubert algorithm that we link to the above integral. We also
show that a certified version of the computationally tractable DOO algorithm
matches these packing and integral bounds. Our instance-dependent lower bound
differs from traditional worst-case lower bounds in the Lipschitz setting and
relies on a local worst-case analysis that could likely prove useful for other
learning tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Active Learning for Deep Neural Networks on Edge Devices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.10836v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.10836v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuya Senzaki, Christian Hamelain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When dealing with deep neural network (DNN) applications on edge devices,
continuously updating the model is important. Although updating a model with
real incoming data is ideal, using all of them is not always feasible due to
limits, such as labeling and communication costs. Thus, it is necessary to
filter and select the data to use for training (i.e., active learning) on the
device. In this paper, we formalize a practical active learning problem for
DNNs on edge devices and propose a general task-agnostic framework to tackle
this problem, which reduces it to a stream submodular maximization. This
framework is light enough to be run with low computational resources, yet
provides solutions whose quality is theoretically guaranteed thanks to the
submodular property. Through this framework, we can configure data selection
criteria flexibly, including using methods proposed in previous active learning
studies. We evaluate our approach on both classification and object detection
tasks in a practical setting to simulate a real-life scenario. The results of
our study show that the proposed framework outperforms all other methods in
both tasks, while running at a practical speed on real devices.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Shape-Guided Diffusion with Inside-Outside Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.00210v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.00210v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dong Huk Park, Grace Luo, Clayton Toste, Samaneh Azadi, Xihui Liu, Maka Karalashvili, Anna Rohrbach, Trevor Darrell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When manipulating an object, existing text-to-image diffusion models often
ignore the shape of the object and generate content that is incorrectly scaled,
cut off, or replaced with background content. We propose a training-free
method, Shape-Guided Diffusion, that modifies pretrained diffusion models to be
sensitive to shape input specified by a user or automatically inferred from
text. We use a novel Inside-Outside Attention mechanism during the inversion
and generation process to apply this shape constraint to the cross- and
self-attention maps. Our mechanism designates which spatial region is the
object (inside) vs. background (outside) then associates edits specified by
text prompts to the correct region. We demonstrate the efficacy of our method
on the shape-guided editing task, where the model must replace an object
according to a text prompt and object mask. We curate a new ShapePrompts
benchmark derived from MS-COCO and achieve SOTA results in shape faithfulness
without a degradation in text alignment or image realism according to both
automatic metrics and annotator ratings. Our data and code will be made
available at https://shape-guided-diffusion.github.io.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Legs as Manipulator: Pushing Quadrupedal Agility Beyond Locomotion <span class="chip">ICRA 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11330v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11330v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuxin Cheng, Ashish Kumar, Deepak Pathak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Locomotion has seen dramatic progress for walking or running across
challenging terrains. However, robotic quadrupeds are still far behind their
biological counterparts, such as dogs, which display a variety of agile skills
and can use the legs beyond locomotion to perform several basic manipulation
tasks like interacting with objects and climbing. In this paper, we take a step
towards bridging this gap by training quadruped robots not only to walk but
also to use the front legs to climb walls, press buttons, and perform object
interaction in the real world. To handle this challenging optimization, we
decouple the skill learning broadly into locomotion, which involves anything
that involves movement whether via walking or climbing a wall, and
manipulation, which involves using one leg to interact while balancing on the
other three legs. These skills are trained in simulation using curriculum and
transferred to the real world using our proposed sim2real variant that builds
upon recent locomotion success. Finally, we combine these skills into a robust
long-term plan by learning a behavior tree that encodes a high-level task
hierarchy from one clean expert demonstration. We evaluate our method in both
simulation and real-world showing successful executions of both short as well
as long-range tasks and how robustness helps confront external perturbations.
Videos at https://robot-skills.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICRA 2023. Videos at https://robot-skills.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A numerical approximation method for the Fisher-Rao distance between
  multivariate normal distributions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.08175v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.08175v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Frank Nielsen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a simple method to approximate Rao's distance between multivariate
normal distributions based on discretizing curves joining normal distributions
and approximating Rao's distances between successive nearby normal
distributions on the curves by the square root of Jeffreys divergence, the
symmetrized Kullback-Leibler divergence. We consider experimentally the linear
interpolation curves in the ordinary, natural and expectation parameterizations
of the normal distributions, and compare these curves with a curve derived from
the Calvo and Oller's isometric embedding of the Fisher-Rao $d$-variate normal
manifold into the cone of $(d+1)\times (d+1)$ symmetric positive-definite
matrices [Journal of multivariate analysis 35.2 (1990): 223-242]. We report on
our experiments and assess the quality of our approximation technique by
comparing the numerical approximations with both lower and upper bounds.
Finally, we present several information-geometric properties of the Calvo and
Oller's isometric embedding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>42 pages, 17 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Joint Liver and Hepatic Lesion Segmentation in MRI using a Hybrid CNN
  with <span class="highlight-title">Transformer</span> Layers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.10981v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.10981v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georg Hille, Shubham Agrawal, Pavan Tummala, Christian Wybranski, Maciej Pech, Alexey Surov, Sylvia Saalfeld
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning-based segmentation of the liver and hepatic lesions therein
steadily gains relevance in clinical practice due to the increasing incidence
of liver cancer each year. Whereas various network variants with overall
promising results in the field of medical image segmentation have been
successfully developed over the last years, almost all of them struggle with
the challenge of accurately segmenting hepatic lesions in magnetic resonance
imaging (MRI). This led to the idea of combining elements of convolutional and
transformer-based architectures to overcome the existing limitations. This work
presents a hybrid network called SWTR-Unet, consisting of a pretrained ResNet,
transformer blocks as well as a common Unet-style decoder path. This network
was primarily applied to single-modality non-contrast-enhanced liver MRI and
additionally to the publicly available computed tomography (CT) data of the
liver tumor segmentation (LiTS) challenge to verify the applicability on other
modalities. For a broader evaluation, multiple state-of-the-art networks were
implemented and applied, ensuring a direct comparability. Furthermore,
correlation analysis and an ablation study were carried out, to investigate
various influencing factors on the segmentation accuracy of the presented
method. With Dice scores of averaged 98+-2% for liver and 81+-28% lesion
segmentation on the MRI dataset and 97+-2% and 79+-25%, respectively on the CT
dataset, the proposed SWTR-Unet proved to be a precise approach for liver and
hepatic lesion segmentation with state-of-the-art results for MRI and competing
accuracy in CT imaging. The achieved segmentation accuracy was found to be on
par with manually performed expert segmentations as indicated by inter-observer
variabilities for liver lesion segmentation. In conclusion, the presented
method could save valuable time and resources in clinical practice.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Long-term Causal Effects Estimation via Latent Surrogates Representation
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.04589v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.04589v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruichu Cai, Weilin Chen, Zeqin Yang, Shu Wan, Chen Zheng, Xiaoqing Yang, Jiecheng Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating long-term causal effects based on short-term surrogates is a
significant but challenging problem in many real-world applications, e.g.,
marketing and medicine. Despite its success in certain domains, most existing
methods estimate causal effects in an idealistic and simplistic way - ignoring
the causal structure among short-term outcomes and treating all of them as
surrogates. However, such methods cannot be well applied to real-world
scenarios, in which the partially observed surrogates are mixed with their
proxies among short-term outcomes. To this end, we develop our flexible method,
Laser, to estimate long-term causal effects in the more realistic situation
that the surrogates are observed or have observed proxies.Given the
indistinguishability between the surrogates and proxies, we utilize
identifiable variational auto-encoder (iVAE) to recover the whole valid
surrogates on all the surrogates candidates without the need of distinguishing
the observed surrogates or the proxies of latent surrogates. With the help of
the recovered surrogates, we further devise an unbiased estimation of long-term
causal effects. Extensive experimental results on the real-world and
semi-synthetic datasets demonstrate the effectiveness of our proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative Bias for Robust Visual Question Answering <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.00690v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.00690v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jae Won Cho, Dong-jin Kim, Hyeonggon Ryu, In So Kweon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of Visual Question Answering (VQA) is known to be plagued by the
issue of VQA models exploiting biases within the dataset to make its final
prediction. Various previous ensemble based debiasing methods have been
proposed where an additional model is purposefully trained to be biased in
order to train a robust target model. However, these methods compute the bias
for a model simply from the label statistics of the training data or from
single modal branches. In this work, in order to better learn the bias a target
VQA model suffers from, we propose a generative method to train the bias model
directly from the target model, called GenB. In particular, GenB employs a
generative network to learn the bias in the target model through a combination
of the adversarial objective and knowledge distillation. We then debias our
target model with GenB as a bias model, and show through extensive experiments
the effects of our method on various VQA bias datasets including VQA-CP2,
VQA-CP1, GQA-OOD, and VQA-CE, and show state-of-the-art results with the LXMERT
architecture on VQA-CP2.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Feature-adjacent multi-fidelity physics-informed machine learning for
  partial differential equations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11577v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11577v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenqian Chen, Panos Stinis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Physics-informed neural networks have emerged as an alternative method for
solving partial differential equations. However, for complex problems, the
training of such networks can still require high-fidelity data which can be
expensive to generate. To reduce or even eliminate the dependency on
high-fidelity data, we propose a novel multi-fidelity architecture which is
based on a feature space shared by the low- and high-fidelity solutions. In the
feature space, the projections of the low-fidelity and high-fidelity solutions
are adjacent by constraining their relative distance. The feature space is
represented with an encoder and its mapping to the original solution space is
effected through a decoder. The proposed multi-fidelity approach is validated
on forward and inverse problems for steady and unsteady problems described by
partial differential equations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lossy Compression of Noisy Data for Private and Data-Efficient Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.02892v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.02892v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Berivan Isik, Tsachy Weissman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Storage-efficient privacy-preserving learning is crucial due to increasing
amounts of sensitive user data required for modern learning tasks. We propose a
framework for reducing the storage cost of user data while at the same time
providing privacy guarantees, without essential loss in the utility of the data
for learning. Our method comprises noise injection followed by lossy
compression. We show that, when appropriately matching the lossy compression to
the distribution of the added noise, the compressed examples converge, in
distribution, to that of the noise-free training data as the sample size of the
training data (or the dimension of the training data) increases. In this sense,
the utility of the data for learning is essentially maintained, while reducing
storage and privacy leakage by quantifiable amounts. We present experimental
results on the CelebA dataset for gender classification and find that our
suggested pipeline delivers in practice on the promise of the theory: the
individuals in the images are unrecognizable (or less recognizable, depending
on the noise level), overall storage of the data is substantially reduced, with
no essential loss (and in some cases a slight boost) to the classification
accuracy. As an added bonus, our experiments suggest that our method yields a
substantial boost to robustness in the face of adversarial test data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at the IEEE Journal on Selected Areas in Information Theory
  (JSAIT). Preliminary version was presented at the IEEE International
  Symposium on Information Theory (ISIT), 2022, with a slightly different
  title, "Learning under Storage and Privacy Constraints."</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Orthogonal Non-negative Matrix Factorization: a
  Maximum-Entropy-Principle Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.02672v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.02672v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Salar Basiri, Mustafa Kapadia, Srinivasa Salapaka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce a new methodology to solve the orthogonal
nonnegative matrix factorization (ONMF) problem, where the objective is to
approximate an input data matrix by a product of two nonnegative matrices, the
features matrix and the mixing matrix, where one of them is orthogonal. We show
how the ONMF can be interpreted as a specific facility-location problem (FLP),
and adapt a maximum-entropy-principle based solution for FLP to the ONMF
problem. The proposed approach guarantees orthogonality and sparsity of the
features or the mixing matrix, while ensuring nonnegativity of both.
Additionally, our methodology develops a quantitative characterization of
``true" number of underlying features - a hyperparameter required for the ONMF.
An evaluation of the proposed method conducted on synthetic datasets, as well
as a standard genetic microarray dataset indicates significantly better
sparsity, orthogonality, and performance speed compared to similar methods in
the literature, with comparable or improved reconstruction errors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ New-Onset Diabetes Assessment Using Artificial Intelligence-Enhanced
  Electrocardiography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.02900v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.02900v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neil Jethani, Aahlad Puli, Hao Zhang, Leonid Garber, Lior Jankelson, Yindalon Aphinyanaphongs, Rajesh Ranganath
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Undiagnosed diabetes is present in 21.4% of adults with diabetes. Diabetes
can remain asymptomatic and undetected due to limitations in screening rates.
To address this issue, questionnaires, such as the American Diabetes
Association (ADA) Risk test, have been recommended for use by physicians and
the public. Based on evidence that blood glucose concentration can affect
cardiac electrophysiology, we hypothesized that an artificial intelligence
(AI)-enhanced electrocardiogram (ECG) could identify adults with new-onset
diabetes. We trained a neural network to estimate HbA1c using a 12-lead ECG and
readily available demographics. We retrospectively assembled a dataset
comprised of patients with paired ECG and HbA1c data. The population of
patients who receive both an ECG and HbA1c may a biased sample of the complete
outpatient population, so we adjusted the importance placed on each patient to
generate a more representative pseudo-population. We found ECG-based assessment
outperforms the ADA Risk test, achieving a higher area under the curve (0.80
vs. 0.68) and positive predictive value (13% vs. 9%) -- 2.6 times the
prevalence of diabetes in the cohort. The AI-enhanced ECG significantly
outperforms electrophysiologist interpretation of the ECG, suggesting that the
task is beyond current clinical capabilities. Given the prevalence of ECGs in
clinics and via wearable devices, such a tool would make precise, automated
diabetes assessment widely accessible.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Solving Bilevel Knapsack Problem using Graph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.13436v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.13436v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunhyeon Kwon, Hwayong Choi, Sungsoo Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Bilevel Optimization Problem is a hierarchical optimization problem with
two agents, a leader and a follower. The leader make their own decisions first,
and the followers make the best choices accordingly. The leader knows the
information of the followers, and the goal of the problem is to find the
optimal solution by considering the reactions of the followers from the
leader's point of view. For the Bilevel Optimization Problem, there are no
general and efficient algorithms or commercial solvers to get an optimal
solution, and it is very difficult to get a good solution even for a simple
problem. In this paper, we propose a deep learning approach using Graph Neural
Networks to solve the bilevel knapsack problem. We train the model to predict
the leader's solution and use it to transform the hierarchical optimization
problem into a single-level optimization problem to get the solution. Our model
found the feasible solution that was about 500 times faster than the exact
algorithm with $1.7\%$ optimal gap. Also, our model performed well on problems
of different size from the size it was trained on.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sequence Generation via Subsequence Similarity: Theory and Application
  to UAV Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08403v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08403v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amir Kazemi, Salar Basiri, Volodymyr Kindratenko, Srinivasa Salapaka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to generate synthetic sequences is crucial for a wide range of
applications, and recent advances in deep learning architectures and generative
frameworks have greatly facilitated this process. Particularly, unconditional
one-shot generative models constitute an attractive line of research that
focuses on capturing the internal information of a single image or video to
generate samples with similar contents. Since many of those one-shot models are
shifting toward efficient non-deep and non-adversarial approaches, we examine
the versatility of a one-shot generative model for augmenting whole datasets.
In this work, we focus on how similarity at the subsequence level affects
similarity at the sequence level, and derive bounds on the optimal transport of
real and generated sequences based on that of corresponding subsequences. We
use a one-shot generative model to sample from the vicinity of individual
sequences and generate subsequence-similar ones and demonstrate the improvement
of this approach by applying it to the problem of Unmanned Aerial Vehicle (UAV)
identification using limited radio-frequency (RF) signals. In the context of
UAV identification, RF fingerprinting is an effective method for distinguishing
legitimate devices from malicious ones, but heterogenous environments and
channel impairments can impose data scarcity and affect the performance of
classification models. By using subsequence similarity to augment sequences of
RF data with a low ratio (5%-20%) of training dataset, we achieve significant
improvements in performance metrics such as accuracy, precision, recall, and F1
score.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 8 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Experimental Study of Dimension Reduction Methods on Machine Learning
  Algorithms with Applications to Psychometrics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.13230v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.13230v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sean H. Merritt, Alexander P. Christensen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing interpretable machine learning models has become an increasingly
important issue. One way in which data scientists have been able to develop
interpretable models has been to use dimension reduction techniques. In this
paper, we examine several dimension reduction techniques including two recent
approaches developed in the network psychometrics literature called exploratory
graph analysis (EGA) and unique variable analysis (UVA). We compared EGA and
UVA with two other dimension reduction techniques common in the machine
learning literature (principal component analysis and independent component
analysis) as well as no reduction to the variables real data. We show that EGA
and UVA perform as well as the other reduction techniques or no reduction.
Consistent with previous literature, we show that dimension reduction can
decrease, increase, or provide the same accuracy as no reduction of variables.
Our tentative results find that dimension reduction tends to lead to better
performance when used for classification tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scalable Bayesian optimization with high-dimensional outputs using
  randomized prior networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.07260v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.07260v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Aziz Bhouri, Michael Joly, Robert Yu, Soumalya Sarkar, Paris Perdikaris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Several fundamental problems in science and engineering consist of global
optimization tasks involving unknown high-dimensional (black-box) functions
that map a set of controllable variables to the outcomes of an expensive
experiment. Bayesian Optimization (BO) techniques are known to be effective in
tackling global optimization problems using a relatively small number objective
function evaluations, but their performance suffers when dealing with
high-dimensional outputs. To overcome the major challenge of dimensionality,
here we propose a deep learning framework for BO and sequential decision making
based on bootstrapped ensembles of neural architectures with randomized priors.
Using appropriate architecture choices, we show that the proposed framework can
approximate functional relationships between design variables and quantities of
interest, even in cases where the latter take values in high-dimensional vector
spaces or even infinite-dimensional function spaces. In the context of BO, we
augmented the proposed probabilistic surrogates with re-parameterized Monte
Carlo approximations of multiple-point (parallel) acquisition functions, as
well as methodological extensions for accommodating black-box constraints and
multi-fidelity information sources. We test the proposed framework against
state-of-the-art methods for BO and demonstrate superior performance across
several challenging tasks with high-dimensional outputs, including a
constrained optimization task involving shape optimization of rotor blades in
turbo-machinery.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WDiscOOD: Out-of-Distribution Detection via Whitened Linear Discriminant
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07543v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07543v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiye Chen, Yunzhi Lin, Ruinian Xu, Patricio A. Vela
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks are susceptible to generating overconfident yet
erroneous predictions when presented with data beyond known concepts. This
challenge underscores the importance of detecting out-of-distribution (OOD)
samples in the open world. In this work, we propose a novel feature-space OOD
detection score that jointly reasons with both class-specific and
class-agnostic information. Specifically, our approach utilizes Whitened Linear
Discriminant Analysis to project features into two subspaces - the
discriminative and residual subspaces - in which the ID classes are maximally
separated and closely clustered, respectively. The OOD score is then determined
by combining the deviation from the input data to the ID distribution in both
subspaces. The efficacy of our method, named WDiscOOD, is verified on the
large-scale ImageNet-1k benchmark, with six OOD datasets that covers a variety
of distribution shifts. WDiscOOD demonstrates superior performance on deep
classifiers with diverse backbone architectures, including CNN and vision
transformer. Furthermore, we also show that our method can more effectively
detect novel concepts in representation space trained with contrastive
objectives, including supervised contrastive loss and multi-modality
contrastive loss.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Spatio-Temporal Aggregations for Large-Scale Capacity Expansion
  Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08996v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08996v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aron Brenner, Rahman Khorramfar, Saurabh Amin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective investment planning decisions are crucial to ensure cyber-physical
infrastructures satisfy performance requirements over an extended time horizon.
Computing these decisions often requires solving Capacity Expansion Problems
(CEPs). In the context of regional-scale energy systems, these problems are
prohibitively expensive to solve due to large network sizes, heterogeneous node
characteristics, and a large number of operational periods. To maintain
tractability, traditional approaches aggregate network nodes and/or select a
set of representative time periods. Often, these reductions do not capture
supply-demand variations that crucially impact CEP costs and constraints,
leading to suboptimal decisions. Here, we propose a novel graph convolutional
autoencoder approach for spatio-temporal aggregation of a generic CEP with
heterogeneous nodes (CEPHN). Our architecture leverages graph pooling to
identify nodes with similar characteristics and minimizes a multi-objective
loss function. This loss function is tailored to induce desirable spatial and
temporal aggregations with regard to tractability and optimality. In
particular, the output of the graph pooling provides a spatial aggregation
while clustering the low-dimensional encoded representations yields a temporal
aggregation. We apply our approach to generation expansion planning of a
coupled 88-node power and natural gas system in New England. The resulting
aggregation leads to a simpler CEPHN with 6 nodes and a small set of
representative days selected from one year. We evaluate aggregation outcomes
over a range of hyperparameters governing the loss function and compare
resulting upper bounds on the original problem with those obtained using
benchmark methods. We show that our approach provides upper bounds that are 33%
(resp. 10%) lower those than obtained from benchmark spatial (resp. temporal)
aggregation approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Causal Reasoning Meets Visual Representation Learning: A Prospective
  Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.12037v8">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.12037v8.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Liu, Yushen Wei, Hong Yan, Guanbin Li, Liang Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual representation learning is ubiquitous in various real-world
applications, including visual comprehension, video understanding, multi-modal
analysis, human-computer interaction, and urban computing. Due to the emergence
of huge amounts of multi-modal heterogeneous spatial/temporal/spatial-temporal
data in big data era, the lack of interpretability, robustness, and
out-of-distribution generalization are becoming the challenges of the existing
visual models. The majority of the existing methods tend to fit the original
data/variable distributions and ignore the essential causal relations behind
the multi-modal knowledge, which lacks unified guidance and analysis about why
modern visual representation learning methods easily collapse into data bias
and have limited generalization and cognitive abilities. Inspired by the strong
inference ability of human-level agents, recent years have therefore witnessed
great effort in developing causal reasoning paradigms to realize robust
representation and model learning with good cognitive ability. In this paper,
we conduct a comprehensive review of existing causal reasoning methods for
visual representation learning, covering fundamental theories, models, and
datasets. The limitations of current methods and datasets are also discussed.
Moreover, we propose some prospective challenges, opportunities, and future
research directions for benchmarking causal reasoning algorithms in visual
representation learning. This paper aims to provide a comprehensive overview of
this emerging field, attract attention, encourage discussions, bring to the
forefront the urgency of developing novel causal reasoning methods, publicly
available benchmarks, and consensus-building standards for reliable visual
representation learning and related real-world applications more efficiently.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 14 figures. This work has been accepted by Machine
  Intelligence Research. The arxiv version is kept updating by adding more
  novel methods, datasets and insights. The official video interpretation of
  this paper can be referred at https://youtu.be/2lfNaTkcTHI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Stationary Nash Equilibrium Policies in $n$-Player Stochastic
  Games with Independent Chains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.12224v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.12224v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        S. Rasoul Etesami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider a subclass of $n$-player stochastic games, in which players have
their own internal state/action spaces while they are coupled through their
payoff functions. It is assumed that players' internal chains are driven by
independent transition probabilities. Moreover, players can receive only
realizations of their payoffs, not the actual functions, and cannot observe
each other's states/actions. For this class of games, we first show that
finding a stationary Nash equilibrium (NE) policy without any assumption on the
reward functions is interactable. However, for general reward functions, we
develop polynomial-time learning algorithms based on dual averaging and dual
mirror descent, which converge in terms of the averaged Nikaido-Isoda distance
to the set of $\epsilon$-NE policies almost surely or in expectation. In
particular, under extra assumptions on the reward functions such as social
concavity, we derive polynomial upper bounds on the number of iterates to
achieve an $\epsilon$-NE policy with high probability. Finally, we evaluate the
effectiveness of the proposed algorithms in learning $\epsilon$-NE policies
using numerical experiments for energy management in smart grids.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A physics-aware deep learning model for energy localization in
  multiscale shock-to-detonation simulations of heterogeneous energetic
  materials 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.04561v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.04561v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Phong C. H. Nguyen, Yen-Thi Nguyen, Pradeep K. Seshadri, Joseph B. Choi, H. S. Udaykumar, Stephen Baek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predictive simulations of the shock-to-detonation transition (SDT) in
heterogeneous energetic materials (EM) are vital to the design and control of
their energy release and sensitivity. Due to the complexity of the
thermo-mechanics of EM during the SDT, both macro-scale response and sub-grid
mesoscale energy localization must be captured accurately. This work proposes
an efficient and accurate multiscale framework for SDT simulations of EM. We
introduce a new approach for SDT simulation by using deep learning to model the
mesoscale energy localization of shock-initiated EM microstructures. The
proposed multiscale modeling framework is divided into two stages. First, a
physics-aware recurrent convolutional neural network (PARC) is used to model
the mesoscale energy localization of shock-initiated heterogeneous EM
microstructures. PARC is trained using direct numerical simulations (DNS) of
hotspot ignition and growth within microstructures of pressed HMX material
subjected to different input shock strengths. After training, PARC is employed
to supply hotspot ignition and growth rates for macroscale SDT simulations. We
show that PARC can play the role of a surrogate model in a multiscale
simulation framework, while drastically reducing the computation cost and
providing improved representations of the sub-grid physics. The proposed
multiscale modeling approach will provide a new tool for material scientists in
designing high-performance and safer energetic materials.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic Vertex Replacement Grammars 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11553v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11553v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Gonzalez Cedre, Justus Isaiah Hibshman, Timothy La Fond, Grant Boquet, Tim Weninger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Context-free graph grammars have shown a remarkable ability to model
structures in real-world relational data. However, graph grammars lack the
ability to capture time-changing phenomena since the left-to-right transitions
of a production rule do not represent temporal change. In the present work, we
describe dynamic vertex-replacement grammars (DyVeRG), which generalize vertex
replacement grammars in the time domain by providing a formal framework for
updating a learned graph grammar in accordance with modifications to its
underlying data. We show that DyVeRG grammars can be learned from, and used to
generate, real-world dynamic graphs faithfully while remaining
human-interpretable. We also demonstrate their ability to forecast by computing
dyvergence scores, a novel graph similarity measurement exposed by this
framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fix the Noise: Disentangling Source Feature for Transfer Learning of
  StyleGAN <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.14079v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.14079v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongyeun Lee, Jae Young Lee, Doyeon Kim, Jaehyun Choi, Junmo Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transfer learning of StyleGAN has recently shown great potential to solve
diverse tasks, especially in domain translation. Previous methods utilized a
source model by swapping or freezing weights during transfer learning, however,
they have limitations on visual quality and controlling source features. In
other words, they require additional models that are computationally demanding
and have restricted control steps that prevent a smooth transition. In this
paper, we propose a new approach to overcome these limitations. Instead of
swapping or freezing, we introduce a simple feature matching loss to improve
generation quality. In addition, to control the degree of source features, we
train a target model with the proposed strategy, FixNoise, to preserve the
source features only in a disentangled subspace of a target feature space.
Owing to the disentangled feature space, our method can smoothly control the
degree of the source features in a single model. Extensive experiments
demonstrate that the proposed method can generate more consistent and realistic
images than previous works.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Full CVPR 2023 paper is available at arXiv:2303.11545. Best paper of
  CVPRW AICC 2022 (CVPR 2022 Workshop on AI for Content Creation). The code is
  available at https://github.com/LeeDongYeun/FixNoise</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SDFusion: Multimodal 3D Shape Completion, Reconstruction, and Generation <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.04493v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.04493v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yen-Chi Cheng, Hsin-Ying Lee, Sergey Tulyakov, Alexander Schwing, Liangyan Gui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we present a novel framework built to simplify 3D asset
generation for amateur users. To enable interactive generation, our method
supports a variety of input modalities that can be easily provided by a human,
including images, text, partially observed shapes and combinations of these,
further allowing to adjust the strength of each input. At the core of our
approach is an encoder-decoder, compressing 3D shapes into a compact latent
representation, upon which a diffusion model is learned. To enable a variety of
multi-modal inputs, we employ task-specific encoders with dropout followed by a
cross-attention mechanism. Due to its flexibility, our model naturally supports
a variety of tasks, outperforming prior works on shape completion, image-based
3D reconstruction, and text-to-3D. Most interestingly, our model can combine
all these tasks into one swiss-army-knife tool, enabling the user to perform
shape generation using incomplete shapes, images, and textual descriptions at
the same time, providing the relative weights for each input and facilitating
interactivity. Despite our approach being shape-only, we further show an
efficient method to texture the generated shape using large-scale text-to-image
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In CVPR 2023. Project page and code is available at:
  https://yccyenchicheng.github.io/SDFusion/. Fix some typos</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Matrix Completion with Cross-Concentrated Sampling: Bridging Uniform
  Sampling and CUR Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.09723v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.09723v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        HanQin Cai, Longxiu Huang, Pengyu Li, Deanna Needell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While uniform sampling has been widely studied in the matrix completion
literature, CUR sampling approximates a low-rank matrix via row and column
samples. Unfortunately, both sampling models lack flexibility for various
circumstances in real-world applications. In this work, we propose a novel and
easy-to-implement sampling strategy, coined Cross-Concentrated Sampling (CCS).
By bridging uniform sampling and CUR sampling, CCS provides extra flexibility
that can potentially save sampling costs in applications. In addition, we also
provide a sufficient condition for CCS-based matrix completion. Moreover, we
propose a highly efficient non-convex algorithm, termed Iterative CUR
Completion (ICURC), for the proposed CCS model. Numerical experiments verify
the empirical advantages of CCS and ICURC against uniform sampling and its
baseline algorithms, on both synthetic and real-world datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Attribution-Scores and Causal Counterfactuals as Explanations in
  Artificial Intelligence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02829v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02829v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leopoldo Bertossi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this expository article we highlight the relevance of explanations for
artificial intelligence, in general, and for the newer developments in {\em
explainable AI}, referring to origins and connections of and among different
approaches. We describe in simple terms, explanations in data management and
machine learning that are based on attribution-scores, and counterfactuals as
found in the area of causality. We elaborate on the importance of logical
reasoning when dealing with counterfactuals, and their use for score
computation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted as chapter contribution. In this version some additional
  comments were added, and some wrong equation references corrected</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Proximal Reinforcement Learning: Efficient Off-Policy Evaluation in
  Partially Observed Markov Decision Processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.15332v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.15332v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Bennett, Nathan Kallus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In applications of offline reinforcement learning to observational data, such
as in healthcare or education, a general concern is that observed actions might
be affected by unobserved factors, inducing confounding and biasing estimates
derived under the assumption of a perfect Markov decision process (MDP) model.
Here we tackle this by considering off-policy evaluation in a partially
observed MDP (POMDP). Specifically, we consider estimating the value of a given
target policy in a POMDP given trajectories with only partial state
observations generated by a different and unknown policy that may depend on the
unobserved state. We tackle two questions: what conditions allow us to identify
the target policy value from the observed data and, given identification, how
to best estimate it. To answer these, we extend the framework of proximal
causal inference to our POMDP setting, providing a variety of settings where
identification is made possible by the existence of so-called bridge functions.
We then show how to construct semiparametrically efficient estimators in these
settings. We term the resulting framework proximal reinforcement learning
(PRL). We demonstrate the benefits of PRL in an extensive simulation study and
on the problem of sepsis management.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Focusing on Potential Named Entities During Active Label Acquisition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.03837v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.03837v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Osman Berk Sapci, Oznur Tastan, Reyyan Yeniterzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Named entity recognition (NER) aims to identify mentions of named entities in
an unstructured text and classify them into predefined named entity classes.
While deep learning-based pre-trained language models help to achieve good
predictive performances in NER, many domain-specific NER applications still
call for a substantial amount of labeled data. Active learning (AL), a general
framework for the label acquisition problem, has been used for NER tasks to
minimize the annotation cost without sacrificing model performance. However,
the heavily imbalanced class distribution of tokens introduces challenges in
designing effective AL querying methods for NER. We propose several AL sentence
query evaluation functions that pay more attention to potential positive
tokens, and evaluate these proposed functions with both sentence-based and
token-based cost evaluation strategies. We also propose a better data-driven
normalization approach to penalize sentences that are too long or too short.
Our experiments on three datasets from different domains reveal that the
proposed approach reduces the number of annotated tokens while achieving better
or comparable prediction performance with conventional methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CA$^2$T-Net: Category-Agnostic 3D Articulation Transfer from Single
  Image 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.02232v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.02232v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jasmine Collins, Anqi Liang, Jitendra Malik, Hao Zhang, Frédéric Devernay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a neural network approach to transfer the motion from a single
image of an articulated object to a rest-state (i.e., unarticulated) 3D model.
Our network learns to predict the object's pose, part segmentation, and
corresponding motion parameters to reproduce the articulation shown in the
input image. The network is composed of three distinct branches that take a
shared joint image-shape embedding and is trained end-to-end. Unlike previous
methods, our approach is independent of the topology of the object and can work
with objects from arbitrary categories. Our method, trained with only synthetic
data, can be used to automatically animate a mesh, infer motion from real
images, and transfer articulation to functionally similar but geometrically
distinct 3D models at test time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Variational Method of Moments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2012.09422v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2012.09422v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Bennett, Nathan Kallus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The conditional moment problem is a powerful formulation for describing
structural causal parameters in terms of observables, a prominent example being
instrumental variable regression. A standard approach reduces the problem to a
finite set of marginal moment conditions and applies the optimally weighted
generalized method of moments (OWGMM), but this requires we know a finite set
of identifying moments, can still be inefficient even if identifying, or can be
theoretically efficient but practically unwieldy if we use a growing sieve of
moment conditions. Motivated by a variational minimax reformulation of OWGMM,
we define a very general class of estimators for the conditional moment
problem, which we term the variational method of moments (VMM) and which
naturally enables controlling infinitely-many moments. We provide a detailed
theoretical analysis of multiple VMM estimators, including ones based on kernel
methods and neural nets, and provide conditions under which these are
consistent, asymptotically normal, and semiparametrically efficient in the full
conditional moment model. We additionally provide algorithms for valid
statistical inference based on the same kind of variational reformulations,
both for kernel- and neural-net-based varieties. Finally, we demonstrate the
strong performance of our proposed estimation and inference algorithms in a
detailed series of synthetic experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Faith-Shap: The Faithful Shapley Interaction Index 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.00870v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.00870v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Che-Ping Tsai, Chih-Kuan Yeh, Pradeep Ravikumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Shapley values, which were originally designed to assign attributions to
individual players in coalition games, have become a commonly used approach in
explainable machine learning to provide attributions to input features for
black-box machine learning models. A key attraction of Shapley values is that
they uniquely satisfy a very natural set of axiomatic properties. However,
extending the Shapley value to assigning attributions to interactions rather
than individual players, an interaction index, is non-trivial: as the natural
set of axioms for the original Shapley values, extended to the context of
interactions, no longer specify a unique interaction index. Many proposals thus
introduce additional less ''natural'' axioms, while sacrificing the key axiom
of efficiency, in order to obtain unique interaction indices. In this work,
rather than introduce additional conflicting axioms, we adopt the viewpoint of
Shapley values as coefficients of the most faithful linear approximation to the
pseudo-Boolean coalition game value function. By extending linear to
$\ell$-order polynomial approximations, we can then define the general family
of faithful interaction indices. We show that by additionally requiring the
faithful interaction indices to satisfy interaction-extensions of the standard
individual Shapley axioms (dummy, symmetry, linearity, and efficiency), we
obtain a unique Faithful Shapley Interaction index, which we denote Faith-Shap,
as a natural generalization of the Shapley value to interactions. We then
provide some illustrative contrasts of Faith-Shap with previously proposed
interaction indices, and further investigate some of its interesting algebraic
properties. We further show the computational efficiency of computing
Faith-Shap, together with some additional qualitative insights, via some
illustrative experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Joint Inference of Diffusion and Structure in Partially Observed Social
  Networks Using Coupled Matrix Factorization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2010.01400v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2010.01400v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maryam Ramezani, Aryan Ahadinia, Amirmohammad Ziaei, Hamid R. Rabiee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Access to complete data in large-scale networks is often infeasible.
Therefore, the problem of missing data is a crucial and unavoidable issue in
the analysis and modeling of real-world social networks. However, most of the
research on different aspects of social networks does not consider this
limitation. One effective way to solve this problem is to recover the missing
data as a pre-processing step. In this paper, a model is learned from partially
observed data to infer unobserved diffusion and structure networks. To jointly
discover omitted diffusion activities and hidden network structures, we develop
a probabilistic generative model called "DiffStru." The interrelations among
links of nodes and cascade processes are utilized in the proposed method via
learning coupled with low-dimensional latent factors. Besides inferring unseen
data, latent factors such as community detection may also aid in network
classification problems. We tested different missing data scenarios on
simulated independent cascades over LFR networks and real datasets, including
Twitter and Memtracker. Experiments on these synthetic and real-world datasets
show that the proposed method successfully detects invisible social behaviors,
predicts links, and identifies latent features.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-lingual Evaluation of Code Generation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.14868v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.14868v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ben Athiwaratkun, Sanjay Krishna Gouda, Zijian Wang, Xiaopeng Li, Yuchen Tian, Ming Tan, Wasi Uddin Ahmad, Shiqi Wang, Qing Sun, Mingyue Shang, Sujan Kumar Gonugondla, Hantian Ding, Varun Kumar, Nathan Fulton, Arash Farahani, Siddhartha Jain, Robert Giaquinto, Haifeng Qian, Murali Krishna Ramanathan, Ramesh Nallapati, Baishakhi Ray, Parminder Bhatia, Sudipta Sengupta, Dan Roth, Bing Xiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present new benchmarks on evaluation code generation models: MBXP and
Multilingual HumanEval, and MathQA-X. These datasets cover over 10 programming
languages and are generated using a scalable conversion framework that
transpiles prompts and test cases from the original Python datasets into the
corresponding data in the target language. Using these benchmarks, we are able
to assess the performance of code generation models in a multi-lingual fashion,
and discovered generalization ability of language models on out-of-domain
languages, advantages of multi-lingual models over mono-lingual, the ability of
few-shot prompting to teach the model new languages, and zero-shot translation
abilities even on mono-lingual settings. Furthermore, we use our code
generation model to perform large-scale bootstrapping to obtain synthetic
canonical solutions in several languages, which can be used for other
code-related evaluations such as code insertion, robustness, or summarization
tasks. Overall, our benchmarks represents a significant step towards a deeper
understanding of language models' code generation abilities. We publicly
release our code and datasets at https://github.com/amazon-research/mxeval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and data release: https://github.com/amazon-research/mxeval</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Self-Supervised</span> Object Segmentation with a Cut-and-Pasting GAN 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.00366v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.00366v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kunal Chaturvedi, Ali Braytee, Jun Li, Mukesh Prasad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a novel self-supervised based Cut-and-Paste GAN to
perform foreground object segmentation and generate realistic composite images
without manual annotations. We accomplish this goal by a simple yet effective
self-supervised approach coupled with the U-Net based discriminator. The
proposed method extends the ability of the standard discriminators to learn not
only the global data representations via classification (real/fake) but also
learn semantic and structural information through pseudo labels created using
the self-supervised task. The proposed method empowers the generator to create
meaningful masks by forcing it to learn informative per-pixel as well as global
image feedback from the discriminator. Our experiments demonstrate that our
proposed method significantly outperforms the state-of-the-art methods on the
standard benchmark datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Convex mixed-integer optimization with Frank-Wolfe methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.11010v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.11010v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deborah Hendrych, Hannah Troppens, Mathieu Besançon, Sebastian Pokutta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mixed-integer nonlinear optimization encompasses a broad class of problems
that present both theoretical and computational challenges. We propose a new
type of method to solve these problems based on a branch-and-bound algorithm
with convex node relaxations. These relaxations are solved with a Frank-Wolfe
algorithm over the convex hull of mixed-integer feasible points instead of the
continuous relaxation via calls to a mixed-integer linear solver as the linear
oracle. The proposed method computes feasible solutions while working on a
single representation of the polyhedral constraints, leveraging the full extent
of mixed-integer linear solvers without an outer approximation scheme and can
exploit inexact solutions of node subproblems.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">6</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reliable and Efficient Evaluation of Adversarial Robustness for Deep
  Hashing-Based Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12658v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12658v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xunguang Wang, Jiawang Bai, Xinyue Xu, Xiaomeng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep hashing has been extensively applied to massive image retrieval due to
its efficiency and effectiveness. Recently, several adversarial attacks have
been presented to reveal the vulnerability of deep hashing models against
adversarial examples. However, existing attack methods suffer from degraded
performance or inefficiency because they underutilize the semantic relations
between original samples or spend a lot of time learning these relations with a
deep neural network. In this paper, we propose a novel Pharos-guided Attack,
dubbed PgA, to evaluate the adversarial robustness of deep hashing networks
reliably and efficiently. Specifically, we design pharos code to represent the
semantics of the benign image, which preserves the similarity to semantically
relevant samples and dissimilarity to irrelevant ones. It is proven that we can
quickly calculate the pharos code via a simple math formula. Accordingly, PgA
can directly conduct a reliable and efficient attack on deep hashing-based
retrieval by maximizing the similarity between the hash code of the adversarial
example and the pharos code. Extensive experiments on the benchmark datasets
verify that the proposed algorithm outperforms the prior state-of-the-arts in
both attack strength and speed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2204.10779</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Music-Driven Group Choreography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12337v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12337v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nhat Le, Thang Pham, Tuong Do, Erman Tjiputra, Quang D. Tran, Anh Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Music-driven choreography is a challenging problem with a wide variety of
industrial applications. Recently, many methods have been proposed to
synthesize dance motions from music for a single dancer. However, generating
dance motion for a group remains an open problem. In this paper, we present
$\rm AIOZ-GDANCE$, a new large-scale dataset for music-driven group dance
generation. Unlike existing datasets that only support single dance, our new
dataset contains group dance videos, hence supporting the study of group
choreography. We propose a semi-autonomous labeling method with humans in the
loop to obtain the 3D ground truth for our dataset. The proposed dataset
consists of $16.7$ hours of paired music and 3D motion from in-the-wild videos,
covering $7$ dance styles and $16$ music genres. We show that naively applying
single dance generation technique to creating group dance motion may lead to
unsatisfactory results, such as inconsistent movements and collisions between
dancers. Based on our new dataset, we propose a new method that takes an input
music sequence and a set of 3D positions of dancers to efficiently produce
multiple group-coherent choreographies. We propose new evaluation metrics for
measuring group dance quality and perform intensive experiments to demonstrate
the effectiveness of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted in cvpr 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Prototype Helps Federated Learning: Towards Faster Convergence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12296v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12296v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Qiao, Seong-Bae Park, Sun Moo Kang, Choong Seon Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) is a distributed machine learning technique in which
multiple clients cooperate to train a shared model without exchanging their raw
data. However, heterogeneity of data distribution among clients usually leads
to poor model inference. In this paper, a prototype-based federated learning
framework is proposed, which can achieve better inference performance with only
a few changes to the last global iteration of the typical federated learning
process. In the last iteration, the server aggregates the prototypes
transmitted from distributed clients and then sends them back to local clients
for their respective model inferences. Experiments on two baseline datasets
show that our proposal can achieve higher accuracy (at least 1%) and relatively
efficient communication than two popular baselines under different
heterogeneous settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>3 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dense-Localizing Audio-Visual Events in Untrimmed Videos: A Large-Scale
  Benchmark and Baseline <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12930v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12930v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tiantian Geng, Teng Wang, Jinming Duan, Runmin Cong, Feng Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing audio-visual event localization (AVE) handles manually trimmed
videos with only a single instance in each of them. However, this setting is
unrealistic as natural videos often contain numerous audio-visual events with
different categories. To better adapt to real-life applications, in this paper
we focus on the task of dense-localizing audio-visual events, which aims to
jointly localize and recognize all audio-visual events occurring in an
untrimmed video. The problem is challenging as it requires fine-grained
audio-visual scene and context understanding. To tackle this problem, we
introduce the first Untrimmed Audio-Visual (UnAV-100) dataset, which contains
10K untrimmed videos with over 30K audio-visual events. Each video has 2.8
audio-visual events on average, and the events are usually related to each
other and might co-occur as in real-life scenes. Next, we formulate the task
using a new learning-based framework, which is capable of fully integrating
audio and visual modalities to localize audio-visual events with various
lengths and capture dependencies between them in a single pass. Extensive
experiments demonstrate the effectiveness of our method as well as the
significance of multi-scale cross-modal perception and dependency modeling for
this task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GA2MIF: Graph and Attention based Two-stage Multi-source Information
  Fusion for Conversational Emotion Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.11900v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.11900v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiang Li, Xiaoping Wang, Guoqing Lv, Zhigang Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Emotion Recognition in Conversation (ERC) plays an influential
role in the field of human-computer interaction and conversational robotics
since it can motivate machines to provide empathetic services. Multimodal data
modeling is an up-and-coming research area in recent years, which is inspired
by human capability to integrate multiple senses. Several graph-based
approaches claim to capture interactive information between modalities, but the
heterogeneity of multimodal data makes these methods prohibit optimal
solutions. In this work, we introduce a multimodal fusion approach named Graph
and Attention based Two-stage Multi-source Information Fusion (GA2MIF) for
emotion detection in conversation. Our proposed method circumvents the problem
of taking heterogeneous graph as input to the model while eliminating complex
redundant connections in the construction of graph. GA2MIF focuses on
contextual modeling and cross-modal modeling through leveraging Multi-head
Directed Graph ATtention networks (MDGATs) and Multi-head Pairwise Cross-modal
ATtention networks (MPCATs), respectively. Extensive experiments on two public
datasets (i.e., IEMOCAP and MELD) demonstrate that the proposed GA2MIF has the
capacity to validly capture intra-modal long-range contextual information and
inter-modal complementary information, as well as outperforms the prevalent
State-Of-The-Art (SOTA) models by a remarkable margin.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Causal Reasoning Meets Visual Representation Learning: A Prospective
  Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.12037v8">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.12037v8.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Liu, Yushen Wei, Hong Yan, Guanbin Li, Liang Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual representation learning is ubiquitous in various real-world
applications, including visual comprehension, video understanding, multi-modal
analysis, human-computer interaction, and urban computing. Due to the emergence
of huge amounts of multi-modal heterogeneous spatial/temporal/spatial-temporal
data in big data era, the lack of interpretability, robustness, and
out-of-distribution generalization are becoming the challenges of the existing
visual models. The majority of the existing methods tend to fit the original
data/variable distributions and ignore the essential causal relations behind
the multi-modal knowledge, which lacks unified guidance and analysis about why
modern visual representation learning methods easily collapse into data bias
and have limited generalization and cognitive abilities. Inspired by the strong
inference ability of human-level agents, recent years have therefore witnessed
great effort in developing causal reasoning paradigms to realize robust
representation and model learning with good cognitive ability. In this paper,
we conduct a comprehensive review of existing causal reasoning methods for
visual representation learning, covering fundamental theories, models, and
datasets. The limitations of current methods and datasets are also discussed.
Moreover, we propose some prospective challenges, opportunities, and future
research directions for benchmarking causal reasoning algorithms in visual
representation learning. This paper aims to provide a comprehensive overview of
this emerging field, attract attention, encourage discussions, bring to the
forefront the urgency of developing novel causal reasoning methods, publicly
available benchmarks, and consensus-building standards for reliable visual
representation learning and related real-world applications more efficiently.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 14 figures. This work has been accepted by Machine
  Intelligence Research. The arxiv version is kept updating by adding more
  novel methods, datasets and insights. The official video interpretation of
  this paper can be referred at https://youtu.be/2lfNaTkcTHI</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-03-21T00:00:00Z">2023-03-21</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">39</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VideoXum: Cross-modal Visual and Textural Summarization of Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12060v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12060v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyang Lin, Hang Hua, Ming Chen, Yikang Li, Jenhao Hsiao, Chiuman Ho, Jiebo Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video summarization aims to distill the most important information from a
source video to produce either an abridged clip or a textual narrative.
Traditionally, different methods have been proposed depending on whether the
output is a video or text, thus ignoring the correlation between the two
semantically related tasks of visual summarization and textual summarization.
We propose a new joint video and text summarization task. The goal is to
generate both a shortened video clip along with the corresponding textual
summary from a long video, collectively referred to as a cross-modal summary.
The generated shortened video clip and text narratives should be semantically
well aligned. To this end, we first build a large-scale human-annotated dataset
-- VideoXum (X refers to different modalities). The dataset is reannotated
based on ActivityNet. After we filter out the videos that do not meet the
length requirements, 14,001 long videos remain in our new dataset. Each video
in our reannotated dataset has human-annotated video summaries and the
corresponding narrative summaries. We then design a novel end-to-end model --
VTSUM-BILP to address the challenges of our proposed task. Moreover, we propose
a new metric called VT-CLIPScore to help evaluate the semantic consistency of
cross-modality summary. The proposed model achieves promising performance on
this new task and establishes a benchmark for future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models Can Be Used to Estimate the Ideologies of
  Politicians in a Zero-Shot Learning Setting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12057v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12057v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patrick Y. Wu, Joshua A. Tucker, Jonathan Nagler, Solomon Messing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The mass aggregation of knowledge embedded in large language models (LLMs)
holds the promise of new solutions to problems of observability and measurement
in the social sciences. We examine the utility of one such model for a
particularly difficult measurement task: measuring the latent ideology of
lawmakers, which allows us to better understand functions that are core to
democracy, such as how politics shape policy and how political actors represent
their constituents. We scale the senators of the 116th United States Congress
along the liberal-conservative spectrum by prompting ChatGPT to select the more
liberal (or conservative) senator in pairwise comparisons. We show that the LLM
produced stable answers across repeated iterations, did not hallucinate, and
was not simply regurgitating information from a single source. This new scale
strongly correlates with pre-existing liberal-conservative scales such as
NOMINATE, but also differs in several important ways, such as correctly placing
senators who vote against their party for far-left or far-right ideological
reasons on the extreme ends. The scale also highly correlates with ideological
measures based on campaign giving and political activists' perceptions of these
senators. In addition to the potential for better-automated data collection and
information retrieval, our results suggest LLMs are likely to open new avenues
for measuring latent constructs like ideology that rely on aggregating large
quantities of data from public sources.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Wearing Masks Implies Refuting Trump?: Towards Target-specific User
  Stance Prediction across Events in COVID-19 and US Election 2020 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12029v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12029v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hong Zhang, Haewoon Kwak, Wei Gao, Jisun An
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  People who share similar opinions towards controversial topics could form an
echo chamber and may share similar political views toward other topics as well.
The existence of such connections, which we call connected behavior, gives
researchers a unique opportunity to predict how one would behave for a future
event given their past behaviors. In this work, we propose a framework to
conduct connected behavior analysis. Neural stance detection models are trained
on Twitter data collected on three seemingly independent topics, i.e., wearing
a mask, racial equality, and Trump, to detect people's stance, which we
consider as their online behavior in each topic-related event. Our results
reveal a strong connection between the stances toward the three topical events
and demonstrate the power of past behaviors in predicting one's future
behavior.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 2 pages, WebSci 2023, April 30-May 1, 2023, Evanston, TX,
  USA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ cTBL: Augmenting Large Language Models for Conversational Tables 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12024v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12024v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anirudh S Sundar, Larry Heck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An open challenge in multimodal conversational AI requires augmenting large
language models with information from textual and non-textual sources for
multi-turn dialogue. To address this problem, this paper introduces
Conversational Tables (cTBL), a three-step encoder-decoder approach to retrieve
tabular information and generate dialogue responses grounded on the retrieved
information. cTBL uses Transformer encoder embeddings for Dense Table Retrieval
and obtains up to 5% relative improvement in Top-1 and Top-3 accuracy over
sparse retrieval on the HyrbiDialogue dataset. Additionally, cTBL performs
tabular knowledge retrieval using both encoder and decoder models, resulting in
up to 46% relative improvement in ROUGE scores and better human evaluation for
response generation on HyrbiDialogue.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Logical Reasoning over Natural Language as Knowledge Representation: A
  <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12023v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12023v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zonglin Yang, Xinya Du, Rui Mao, Jinjie Ni, Erik Cambria
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Logical reasoning is central to human cognition and intelligence. Past
research of logical reasoning within AI uses formal language as knowledge
representation~(and symbolic reasoners). However, reasoning with formal
language has proved challenging~(e.g., brittleness and knowledge-acquisition
bottleneck). This paper provides a comprehensive overview on a new paradigm of
logical reasoning, which uses natural language as knowledge representation~(and
pretrained language models as reasoners), including philosophical definition
and categorization of logical reasoning, advantages of the new paradigm,
benchmarks and methods, challenges of the new paradigm, desirable tasks &
methods in the future, and relation to related NLP fields. This new paradigm is
promising since it not only alleviates many challenges of formal representation
but also has advantages over end-to-end neural methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Chinese Intermediate English Learners outdid Chat<span class="highlight-title">GPT</span> in deep cohesion:
  Evidence from English narrative writing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11812v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11812v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tongquan Zhou, Siyi Cao, Siruo Zhou, Yao Zhang, Aijing He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ChatGPT is a publicly available chatbot that can quickly generate texts on
given topics, but it is unknown whether the chatbot is really superior to human
writers in all aspects of writing and whether its writing quality can be
prominently improved on the basis of updating commands. Consequently, this
study compared the writing performance on a narrative topic by ChatGPT and
Chinese intermediate English (CIE) learners so as to reveal the chatbot's
advantage and disadvantage in writing. The data were analyzed in terms of five
discourse components using Coh-Metrix (a special instrument for analyzing
language discourses), and the results revealed that ChatGPT performed better
than human writers in narrativity, word concreteness, and referential cohesion,
but worse in syntactic simplicity and deep cohesion in its initial version.
After more revision commands were updated, while the resulting version was
facilitated in syntactic simplicity, yet it is still lagged far behind CIE
learners' writing in deep cohesion. In addition, the correlation analysis of
the discourse components suggests that narrativity was correlated with
referential cohesion in both ChatGPT and human writers, but the correlations
varied within each group.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LEAPT: Learning Adaptive Prefix-to-prefix Translation For Simultaneous
  Machine Translation <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11750v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11750v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Lin, Shuangtao Li, Xiaodong Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simultaneous machine translation, which aims at a real-time translation, is
useful in many live scenarios but very challenging due to the trade-off between
accuracy and latency. To achieve the balance for both, the model needs to wait
for appropriate streaming text (READ policy) and then generates its translation
(WRITE policy). However, WRITE policies of previous work either are specific to
the method itself due to the end-to-end training or suffer from the input
mismatch between training and decoding for the non-end-to-end training.
Therefore, it is essential to learn a generic and better WRITE policy for
simultaneous machine translation. Inspired by strategies utilized by human
interpreters and "wait" policies, we propose a novel adaptive prefix-to-prefix
training policy called LEAPT, which allows our machine translation model to
learn how to translate source sentence prefixes and make use of the future
context. Experiments show that our proposed methods greatly outperform
competitive baselines and achieve promising results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Open-domain Paradox for Chatbots: Common Ground as the Basis for
  Human-like Dialogue 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11708v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11708v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriel Skantze, A. Seza Doğruöz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is a surge in interest in the development of open-domain chatbots,
driven by the recent advancements of large language models. The "openness" of
the dialogue is expected to be maximized by providing minimal information to
the users about the common ground they can expect, including the presumed joint
activity. However, evidence suggests that the effect is the opposite. Asking
users to "just chat about anything" results in a very narrow form of dialogue,
which we refer to as the "open-domain paradox". In this paper, we explain this
paradox through the theory of common ground as the basis for human-like
communication. Furthermore, we question the assumptions behind open-domain
chatbots and identify paths forward for enabling common ground in
human-computer dialogue.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Simple Yet Effective Synthetic <span class="highlight-title">Dataset</span> Construction for Unsupervised
  Opinion Summarization <span class="chip">EACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11660v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11660v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ming Shen, Jie Ma, Shuai Wang, Yogarshi Vyas, Kalpit Dixit, Miguel Ballesteros, Yassine Benajiba
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Opinion summarization provides an important solution for summarizing opinions
expressed among a large number of reviews. However, generating aspect-specific
and general summaries is challenging due to the lack of annotated data. In this
work, we propose two simple yet effective unsupervised approaches to generate
both aspect-specific and general opinion summaries by training on synthetic
datasets constructed with aspect-related review contents. Our first approach,
Seed Words Based Leave-One-Out (SW-LOO), identifies aspect-related portions of
reviews simply by exact-matching aspect seed words and outperforms existing
methods by 3.4 ROUGE-L points on SPACE and 0.5 ROUGE-1 point on OPOSUM+ for
aspect-specific opinion summarization. Our second approach, Natural Language
Inference Based Leave-One-Out (NLI-LOO) identifies aspect-related sentences
utilizing an NLI model in a more general setting without using seed words and
outperforms existing approaches by 1.2 ROUGE-L points on SPACE for
aspect-specific opinion summarization and remains competitive on other metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EACL 2023 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Content Retrievability in Search with Controllable Query
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11648v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11648v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gustavo Penha, Enrico Palumbo, Maryam Aziz, Alice Wang, Hugues Bouchard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An important goal of online platforms is to enable content discovery, i.e.
allow users to find a catalog entity they were not familiar with. A
pre-requisite to discover an entity, e.g. a book, with a search engine is that
the entity is retrievable, i.e. there are queries for which the system will
surface such entity in the top results. However, machine-learned search engines
have a high retrievability bias, where the majority of the queries return the
same entities. This happens partly due to the predominance of narrow intent
queries, where users create queries using the title of an already known entity,
e.g. in book search 'harry potter'. The amount of broad queries where users
want to discover new entities, e.g. in music search 'chill lyrical electronica
with an atmospheric feeling to it', and have a higher tolerance to what they
might find, is small in comparison. We focus here on two factors that have a
negative impact on the retrievability of the entities (I) the training data
used for dense retrieval models and (II) the distribution of narrow and broad
intent queries issued in the system. We propose CtrlQGen, a method that
generates queries for a chosen underlying intent-narrow or broad. We can use
CtrlQGen to improve factor (I) by generating training data for dense retrieval
models comprised of diverse synthetic queries. CtrlQGen can also be used to
deal with factor (II) by suggesting queries with broader intents to users. Our
results on datasets from the domains of music, podcasts, and books reveal that
we can significantly decrease the retrievability bias of a dense retrieval
model when using CtrlQGen. First, by using the generated queries as training
data for dense models we make 9% of the entities retrievable (go from zero to
non-zero retrievability). Second, by suggesting broader queries to users, we
can make 12% of the entities retrievable in the best case.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in the International World Wide Web
  Conference 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Heterogeneous-Branch Collaborative Learning for Dialogue Generation <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11621v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11621v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwei Li, Shaoxiong Feng, Bin Sun, Kan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the development of deep learning, advanced dialogue generation methods
usually require a greater amount of computational resources. One promising
approach to obtaining a high-performance and lightweight model is knowledge
distillation, which relies heavily on the pre-trained powerful teacher.
Collaborative learning, also known as online knowledge distillation, is an
effective way to conduct one-stage group distillation in the absence of a
well-trained large teacher model. However, previous work has a severe branch
homogeneity problem due to the same training objective and the independent
identical training sets. To alleviate this problem, we consider the dialogue
attributes in the training of network branches. Each branch learns the
attribute-related features based on the selected subset. Furthermore, we
propose a dual group-based knowledge distillation method, consisting of
positive distillation and negative distillation, to further diversify the
features of different branches in a steadily and interpretable way. The
proposed approach significantly improves branch heterogeneity and outperforms
state-of-the-art collaborative learning methods on two widely used open-domain
dialogue datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Transformer</span>s in Speech Processing: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11607v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11607v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siddique Latif, Aun Zaidi, Heriberto Cuayahuitl, Fahad Shamshad, Moazzam Shoukat, Junaid Qadir
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The remarkable success of transformers in the field of natural language
processing has sparked the interest of the speech-processing community, leading
to an exploration of their potential for modeling long-range dependencies
within speech sequences. Recently, transformers have gained prominence across
various speech-related domains, including automatic speech recognition, speech
synthesis, speech translation, speech para-linguistics, speech enhancement,
spoken dialogue systems, and numerous multimodal applications. In this paper,
we present a comprehensive survey that aims to bridge research studies from
diverse subfields within speech technology. By consolidating findings from
across the speech technology landscape, we provide a valuable resource for
researchers interested in harnessing the power of transformers to advance the
field. We identify the challenges encountered by transformers in speech
processing while also offering insights into potential solutions to address
these issues.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under-review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Difficulty in learning chirality for <span class="highlight-title">Transformer</span> fed with SMILES 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11593v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11593v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yasuhiro Yoshikai, Tadahaya Mizuno, Shumpei Nemoto, Hiroyuki Kusuhara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have seen development of descriptor generation based on
representation learning of extremely diverse molecules, especially those that
apply natural language processing (NLP) models to SMILES, a literal
representation of molecular structure. However, little research has been done
on how these models understand chemical structure. To address this, we
investigated the relationship between the learning progress of SMILES and
chemical structure using a representative NLP model, the Transformer. The
results suggest that while the Transformer learns partial structures of
molecules quickly, it requires extended training to understand overall
structures. Consistently, the accuracy of molecular property predictions using
descriptors generated from models at different learning steps was similar from
the beginning to the end of training. Furthermore, we found that the
Transformer requires particularly long training to learn chirality and
sometimes stagnates with low translation accuracy due to misunderstanding of
enantiomers. These findings are expected to deepen understanding of NLP models
in chemistry.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SIFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11525v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11525v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shreyas Saxena, Vithursan Thangarasa, Abhay Gupta, Sean Lie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works have explored the use of weight sparsity to improve the training
efficiency (test accuracy w.r.t training FLOPs) of deep neural networks (DNNs).
These works aim to reduce training FLOPs but training with sparse weights often
leads to accuracy loss or requires longer train schedules, making the resulting
training efficiency less clear. In contrast, we focus on using sparsity to
increase accuracy while using the same FLOPS as the dense model and show
training efficiency gains through higher accuracy. In this work, we introduce
SIFT, a family of Sparse Iso-FLOP Transformations which are used as drop-in
replacements for dense layers to improve their representational capacity and
FLOP efficiency. Each transformation is parameterized by a single parameter
(sparsity level) and provides a larger search space to find optimal sparse
masks. Without changing any training hyperparameters, replacing dense layers
with SIFT leads to significant improvements across computer vision (CV) and
natural language processing (NLP) tasks, including ResNet-18 on ImageNet
(+3.5%) and GPT-3 Small on WikiText-103 (-0.4 PPL), both matching larger dense
model variants with 2x or more FLOPs. To the best of our knowledge, this is the
first work to demonstrate the use of sparsity for improving accuracy of dense
models via a simple-to-use set of sparse transformations. Code is available at:
https://github.com/CerebrasResearch/SIFT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Unified Taxonomy of Deep Syntactic Relations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12220v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12220v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kira Droganova, Daniel Zeman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper analyzes multiple deep-syntactic frameworks with the goal of
creating a proposal for a set of universal semantic role labels. The proposal
examines various theoretic linguistic perspectives and focuses on Meaning-Text
Theory and Functional Generative Description frameworks.
  For the purpose of this research, data from four languages is used -- Spanish
and Catalan (Taule et al., 2011), Czech (Hajic et al., 2017), and English
(Hajic et al., 2012). This proposal is oriented towards Universal Dependencies
(de Marneffe et al., 2021) with a further intention of applying the universal
semantic role labels to the UD data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MAGVLT: Masked Generative Vision-and-Language <span class="highlight-title">Transformer</span> <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12208v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12208v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sungwoong Kim, Daejin Jo, Donghoon Lee, Jongmin Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While generative modeling on multimodal image-text data has been actively
developed with large-scale paired datasets, there have been limited attempts to
generate both image and text data by a single model rather than a generation of
one fixed modality conditioned on the other modality. In this paper, we explore
a unified generative vision-and-language (VL) model that can produce both
images and text sequences. Especially, we propose a generative VL transformer
based on the non-autoregressive mask prediction, named MAGVLT, and compare it
with an autoregressive generative VL transformer (ARGVLT). In comparison to
ARGVLT, the proposed MAGVLT enables bidirectional context encoding, fast
decoding by parallel token predictions in an iterative refinement, and extended
editing capabilities such as image and text infilling. For rigorous training of
our MAGVLT with image-text pairs from scratch, we combine the image-to-text,
text-to-image, and joint image-and-text mask prediction tasks. Moreover, we
devise two additional tasks based on the step-unrolled mask prediction and the
selective prediction on the mixture of two image-text pairs. Experimental
results on various downstream generation tasks of VL benchmarks show that our
MAGVLT outperforms ARGVLT by a large margin even with significant inference
speedup. Particularly, MAGVLT achieves competitive results on both zero-shot
image-to-text and text-to-image generation tasks from MS-COCO by one
moderate-sized model (fewer than 500M parameters) even without the use of
monomodal data and networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understand Legal Documents with Contextualized Large Language Models <span class="chip">SemEval 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12135v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12135v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Jin, Yuchen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growth of pending legal cases in populous countries, such as India, has
become a major issue. Developing effective techniques to process and understand
legal documents is extremely useful in resolving this problem. In this paper,
we present our systems for SemEval-2023 Task 6: understanding legal texts (Modi
et al., 2023). Specifically, we first develop the Legal-BERT-HSLN model that
considers the comprehensive context information in both intra- and
inter-sentence levels to predict rhetorical roles (subtask A) and then train a
Legal-LUKE model, which is legal-contextualized and entity-aware, to recognize
legal entities (subtask B). Our evaluations demonstrate that our designed
models are more accurate than baselines, e.g., with an up to 15.0% better F1
score in subtask B. We achieved notable performance in the task leaderboard,
e.g., 0.834 micro F1 score, and ranked No.5 out of 27 teams in subtask A.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SemEval 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fundamentals of Generative Large Language Models and Perspectives in
  Cyber-Defense 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12132v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12132v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrei Kucharavy, Zachary Schillaci, Loïc Maréchal, Maxime Würsch, Ljiljana Dolamic, Remi Sabonnadiere, Dimitri Percia David, Alain Mermoud, Vincent Lenders
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative Language Models gained significant attention in late 2022 / early
2023, notably with the introduction of models refined to act consistently with
users' expectations of interactions with AI (conversational models). Arguably
the focal point of public attention has been such a refinement of the GPT3
model -- the ChatGPT and its subsequent integration with auxiliary
capabilities, including search as part of Microsoft Bing. Despite extensive
prior research invested in their development, their performance and
applicability to a range of daily tasks remained unclear and niche. However,
their wider utilization without a requirement for technical expertise, made in
large part possible through conversational fine-tuning, revealed the extent of
their true capabilities in a real-world environment. This has garnered both
public excitement for their potential applications and concerns about their
capabilities and potential malicious uses. This review aims to provide a brief
overview of the history, state of the art, and implications of Generative
Language Models in terms of their principles, abilities, limitations, and
future prospects -- especially in the context of cyber-defense, with a focus on
the Swiss operational environment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>41 pages (without references), 13 figures; public report of
  Cyber-Defence Campus</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Positive-Augmented Constrastive Learning for Image and Video Captioning
  Evaluation <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12112v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12112v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sara Sarto, Manuele Barraco, Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The CLIP model has been recently proven to be very effective for a variety of
cross-modal tasks, including the evaluation of captions generated from
vision-and-language architectures. In this paper, we propose a new recipe for a
contrastive-based evaluation metric for image captioning, namely
Positive-Augmented Contrastive learning Score (PAC-S), that in a novel way
unifies the learning of a contrastive visual-semantic space with the addition
of generated images and text on curated data. Experiments spanning several
datasets demonstrate that our new metric achieves the highest correlation with
human judgments on both images and videos, outperforming existing
reference-based metrics like CIDEr and SPICE and reference-free metrics like
CLIP-Score. Finally, we test the system-level correlation of the proposed
metric when considering popular image captioning approaches, and assess the
impact of employing different cross-modal features. Our source code and trained
models are publicly available at: https://github.com/aimagelab/pacscore.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023 (highlight paper)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Is <span class="highlight-title">BERT</span> Blind? Exploring the Effect of Vision-and-Language <span class="highlight-title">Pretrain</span>ing
  on Visual Language Understanding <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12513v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12513v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Morris Alper, Michael Fiman, Hadar Averbuch-Elor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most humans use visual imagination to understand and reason about language,
but models such as BERT reason about language using knowledge acquired during
text-only pretraining. In this work, we investigate whether vision-and-language
pretraining can improve performance on text-only tasks that involve implicit
visual reasoning, focusing primarily on zero-shot probing methods. We propose a
suite of visual language understanding (VLU) tasks for probing the visual
reasoning abilities of text encoder models, as well as various non-visual
natural language understanding (NLU) tasks for comparison. We also contribute a
novel zero-shot knowledge probing method, Stroop probing, for applying models
such as CLIP to text-only tasks without needing a prediction head such as the
masked language modelling head of models like BERT. We show that SOTA
multimodally trained text encoders outperform unimodally trained text encoders
on the VLU tasks while being underperformed by them on the NLU tasks, lending
new context to previously mixed results regarding the NLU capabilities of
multimodal models. We conclude that exposure to images during pretraining
affords inherent visual reasoning knowledge that is reflected in language-only
tasks that require implicit visual reasoning. Our findings bear importance in
the broader context of multimodal learning, providing principled guidelines for
the choice of text encoders used in such contexts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be presented in CVPR 2023. Project webpage:
  https://isbertblind.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Chat<span class="highlight-title">GPT</span> and a New Academic Reality: AI-Written Research Papers and the
  Ethics of the Large Language Models in Scholarly Publishing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13367v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13367v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brady Lund, Ting Wang, Nishith Reddy Mannuru, Bing Nie, Somipam Shimray, Ziang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper discusses OpenAIs ChatGPT, a generative pre-trained transformer,
which uses natural language processing to fulfill text-based user requests
(i.e., a chatbot). The history and principles behind ChatGPT and similar models
are discussed. This technology is then discussed in relation to its potential
impact on academia and scholarly research and publishing. ChatGPT is seen as a
potential model for the automated preparation of essays and other types of
scholarly manuscripts. Potential ethical issues that could arise with the
emergence of large language models like GPT-3, the underlying technology behind
ChatGPT, and its usage by academics and researchers, are discussed and situated
within the context of broader advancements in artificial intelligence, machine
learning, and natural language processing for research and scholarly
publishing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fine-tuning Climate<span class="highlight-title">Bert</span> <span class="highlight-title">transformer</span> with ClimaText for the disclosure
  analysis of climate-related financial risks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13373v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13373v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eduardo C. Garrido-Merchán, Cristina González-Barthe, María Coronado Vaca
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years there has been a growing demand from financial agents,
especially from particular and institutional investors, for companies to report
on climate-related financial risks. A vast amount of information, in text
format, can be expected to be disclosed in the short term by firms in order to
identify these types of risks in their financial and non financial reports,
particularly in response to the growing regulation that is being passed on the
matter. To this end, this paper applies state-of-the-art NLP techniques to
achieve the detection of climate change in text corpora. We use transfer
learning to fine-tune two transformer models, BERT and ClimateBert -a recently
published DistillRoBERTa-based model that has been specifically tailored for
climate text classification-. These two algorithms are based on the transformer
architecture which enables learning the contextual relationships between words
in a text. We carry out the fine-tuning process of both models on the novel
Clima-Text database, consisting of data collected from Wikipedia, 10K Files
Reports and web-based claims. Our text classification model obtained from the
ClimateBert fine-tuning process on ClimaText, outperforms the models created
with BERT and the current state-of-the-art transformer in this particular
problem. Our study is the first one to implement on the ClimaText database the
recently published ClimateBert algorithm. Based on our results, it can be said
that ClimateBert fine-tuned on ClimaText is an outstanding tool within the NLP
pre-trained transformer models that may and should be used by investors,
institutional agents and companies themselves to monitor the disclosure of
climate risk in financial reports. In addition, our transfer learning
methodology is cheap in computational terms, thus allowing any organization to
perform it.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ QAID: Question Answering Inspired Few-shot Intent Detection <span class="chip">ICLR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.01593v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.01593v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Asaf Yehudai, Matan Vetzler, Yosi Mass, Koren Lazar, Doron Cohen, Boaz Carmeli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intent detection with semantically similar fine-grained intents is a
challenging task. To address it, we reformulate intent detection as a
question-answering retrieval task by treating utterances and intent names as
questions and answers. To that end, we utilize a question-answering retrieval
architecture and adopt a two stages training schema with batch contrastive
loss. In the pre-training stage, we improve query representations through
self-supervised training. Then, in the fine-tuning stage, we increase
contextualized token-level similarity scores between queries and answers from
the same intent. Our results on three few-shot intent detection benchmarks
achieve state-of-the-art performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Token-level Contrastive Framework for Sign Language Translation <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.04916v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.04916v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Biao Fu, Peigen Ye, Liang Zhang, Pei Yu, Cong Hu, Yidong Chen, Xiaodong Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sign Language Translation (SLT) is a promising technology to bridge the
communication gap between the deaf and the hearing people. Recently,
researchers have adopted Neural Machine Translation (NMT) methods, which
usually require large-scale corpus for training, to achieve SLT. However, the
publicly available SLT corpus is very limited, which causes the collapse of the
token representations and the inaccuracy of the generated tokens. To alleviate
this issue, we propose ConSLT, a novel token-level \textbf{Con}trastive
learning framework for \textbf{S}ign \textbf{L}anguage \textbf{T}ranslation ,
which learns effective token representations by incorporating token-level
contrastive learning into the SLT decoding process. Concretely, ConSLT treats
each token and its counterpart generated by different dropout masks as positive
pairs during decoding, and then randomly samples $K$ tokens in the vocabulary
that are not in the current sentence to construct negative examples. We conduct
comprehensive experiments on two benchmarks (PHOENIX14T and CSL-Daily) for both
end-to-end and cascaded settings. The experimental results demonstrate that
ConSLT can achieve better translation quality than the strong baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Prompt</span>Cap: <span class="highlight-title">Prompt</span>-Guided Image Captioning for VQA with <span class="highlight-title">GPT</span>-3 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.09699v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.09699v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yushi Hu, Hang Hua, Zhengyuan Yang, Weijia Shi, Noah A. Smith, Jiebo Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge-based visual question answering (VQA) involves questions that
require world knowledge beyond the image to yield the correct answer. Large
language models (LMs) like GPT-3 are particularly helpful for this task because
of their strong knowledge retrieval and reasoning capabilities. To enable LM to
understand images, prior work uses a captioning model to convert images into
text. However, when summarizing an image in a single caption sentence, which
visual entities to describe are often underspecified. Generic image captions
often miss visual details essential for the LM to answer visual questions
correctly. To address this challenge, we propose PromptCap (Prompt-guided image
Captioning), a captioning model designed to serve as a better connector between
images and black-box LMs. Different from generic captions, PromptCap takes a
natural-language prompt to control the visual entities to describe in the
generated caption. The prompt contains a question that the caption should aid
in answering. To avoid extra annotation, PromptCap is trained by examples
synthesized with GPT-3 and existing datasets. We demonstrate PromptCap's
effectiveness on an existing pipeline in which GPT-3 is prompted with image
captions to carry out VQA. PromptCap outperforms generic captions by a large
margin and achieves state-of-the-art accuracy on knowledge-based VQA tasks
(60.4% on OK-VQA and 59.6% on A-OKVQA). Zero-shot results on WebQA show that
PromptCap generalizes well to unseen domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vid2Seq: Large-Scale <span class="highlight-title">Pretrain</span>ing of a Visual Language Model for Dense
  Video Captioning <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.14115v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.14115v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antoine Yang, Arsha Nagrani, Paul Hongsuck Seo, Antoine Miech, Jordi Pont-Tuset, Ivan Laptev, Josef Sivic, Cordelia Schmid
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we introduce Vid2Seq, a multi-modal single-stage dense event
captioning model pretrained on narrated videos which are readily-available at
scale. The Vid2Seq architecture augments a language model with special time
tokens, allowing it to seamlessly predict event boundaries and textual
descriptions in the same output sequence. Such a unified model requires
large-scale training data, which is not available in current annotated
datasets. We show that it is possible to leverage unlabeled narrated videos for
dense video captioning, by reformulating sentence boundaries of transcribed
speech as pseudo event boundaries, and using the transcribed speech sentences
as pseudo event captions. The resulting Vid2Seq model pretrained on the
YT-Temporal-1B dataset improves the state of the art on a variety of dense
video captioning benchmarks including YouCook2, ViTT and ActivityNet Captions.
Vid2Seq also generalizes well to the tasks of video paragraph captioning and
video clip captioning, and to few-shot settings. Our code is publicly available
at https://antoyang.github.io/vid2seq.html.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023 Camera-Ready; Project Webpage:
  https://antoyang.github.io/vid2seq.html ; 18 pages; 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DocRED-FE: A Document-Level Fine-Grained Entity And Relation Extraction
  <span class="highlight-title">Dataset</span> <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11141v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11141v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongbo Wang, Weimin Xiong, Yifan Song, Dawei Zhu, Yu Xia, Sujian Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Joint entity and relation extraction (JERE) is one of the most important
tasks in information extraction. However, most existing works focus on
sentence-level coarse-grained JERE, which have limitations in real-world
scenarios. In this paper, we construct a large-scale document-level
fine-grained JERE dataset DocRED-FE, which improves DocRED with Fine-Grained
Entity Type. Specifically, we redesign a hierarchical entity type schema
including 11 coarse-grained types and 119 fine-grained types, and then
re-annotate DocRED manually according to this schema. Through comprehensive
experiments we find that: (1) DocRED-FE is challenging to existing JERE models;
(2) Our fine-grained entity types promote relation classification. We make
DocRED-FE with instruction and the code for our baselines publicly available at
https://github.com/PKU-TANGENT/DOCRED-FE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE ICASSP 2023. The first two authors contribute
  equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Maximum Linear Arrangement Problem for trees under projectivity and
  planarity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.06924v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.06924v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lluís Alemany-Puig, Juan Luis Esteban, Ramon Ferrer-i-Cancho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A linear arrangement is a mapping $\pi$ from the $n$ vertices of a graph $G$
to $n$ distinct consecutive integers. Linear arrangements can be represented by
drawing the vertices along a horizontal line and drawing the edges as
semicircles above said line. In this setting, the length of an edge is defined
as the absolute value of the difference between the positions of its two
vertices in the arrangement, and the cost of an arrangement as the sum of all
edge lengths. Here we study two variants of the Maximum Linear Arrangement
problem (MaxLA), which consists of finding an arrangement that maximizes the
cost. In the planar variant for free trees, vertices have to be arranged in
such a way that there are no edge crossings. In the projective variant for
rooted trees, arrangements have to be planar and the root of the tree cannot be
covered by any edge. In this paper we present algorithms that are linear in
time and space to solve planar and projective MaxLA for trees. We also prove
several properties of maximum projective and planar arrangements, and show that
caterpillar trees maximize planar MaxLA over all trees of a fixed size thereby
generalizing a previous extremal result on trees.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The fourth version is incorrect. We are sure the right files were
  uploaded but for whatever reason, it looks like we uploaded the wrong files.
  The abstract in the fourth version is correct though</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Pre-train</span>ed Token-replaced Detection Model as Few-shot Learner <span class="chip">COLING 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.03235v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.03235v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zicheng Li, Shoushan Li, Guodong Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained masked language models have demonstrated remarkable ability as
few-shot learners. In this paper, as an alternative, we propose a novel
approach to few-shot learning with pre-trained token-replaced detection models
like ELECTRA. In this approach, we reformulate a classification or a regression
task as a token-replaced detection problem. Specifically, we first define a
template and label description words for each task and put them into the input
to form a natural language prompt. Then, we employ the pre-trained
token-replaced detection model to predict which label description word is the
most original (i.e., least replaced) among all label description words in the
prompt. A systematic evaluation on 16 datasets demonstrates that our approach
outperforms few-shot learners with pre-trained masked language models in both
one-sentence and two-sentence learning tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to COLING 2022. The code is publicly available at
  https://github.com/cjfarmer/TRD_FSL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tracking, exploring and analyzing recent developments in German-language
  online press in the face of the coronavirus crisis: cOWIDplus Analysis and
  cOWIDplus Viewer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2005.13316v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2005.13316v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sascha Wolfer, Alexander Koplenig, Frank Michaelis, Carolin Müller-Spitzer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The coronavirus pandemic may be the largest crisis the world has had to face
since World War II. It does not come as a surprise that it is also having an
impact on language as our primary communication tool. We present three
inter-connected resources that are designed to capture and illustrate these
effects on a subset of the German language: An RSS corpus of German-language
newsfeeds (with freely available untruncated unigram frequency lists), a static
but continuously updated HTML page tracking the diversity of the used
vocabulary and a web application that enables other researchers and the broader
public to explore these effects without any or with little knowledge of corpus
representation/exploration or statistical analyses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 6 figures, 1 table, 3852 words</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Collecting Interactive Multi-modal <span class="highlight-title">Dataset</span>s for Grounded Language
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.06552v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.06552v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shrestha Mohanty, Negar Arabzadeh, Milagro Teruel, Yuxuan Sun, Artem Zholus, Alexey Skrynnik, Mikhail Burtsev, Kavya Srinet, Aleksandr Panov, Arthur Szlam, Marc-Alexandre Côté, Julia Kiseleva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human intelligence can remarkably adapt quickly to new tasks and
environments. Starting from a very young age, humans acquire new skills and
learn how to solve new tasks either by imitating the behavior of others or by
following provided natural language instructions. To facilitate research which
can enable similar capabilities in machines, we made the following
contributions (1) formalized the collaborative embodied agent using natural
language task; (2) developed a tool for extensive and scalable data collection;
and (3) collected the first dataset for interactive grounded language
understanding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Chat<span class="highlight-title">GPT</span> Is on the Horizon: Could a Large Language Model Be All We Need
  for Intelligent Transportation? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05382v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05382v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ou Zheng, Mohamed Abdel-Aty, Dongdong Wang, Zijin Wang, Shengxuan Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ChatGPT, developed by OpenAI, is one of the milestone large language models
(LLMs) with 6 billion parameters. ChatGPT has demonstrated the impressive
language understanding capability of LLM, particularly in generating
conversational response. As LLMs start to gain more attention in various
research or engineering domains, it is time to envision how LLM may
revolutionize the way we approach intelligent transportation systems. This
paper explores the future applications of LLM in addressing key transportation
problems. By leveraging LLM with cross-modal encoder, an intelligent system can
also process traffic data from different modalities and execute transportation
operations through an LLM. We present and validate these potential
transportation applications equipped by LLM. To further demonstrate this
potential, we also provide a concrete smartphone-based crash report
auto-generation and analysis framework as a use case. Despite the potential
benefits, challenges related to data privacy, data quality, and model bias must
be considered. Overall, the use of LLM in intelligent transport systems holds
promise for more efficient, intelligent, and sustainable transportation systems
that further improve daily life around the world.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to Nature - Machine Intelligence (13 Pages, 8 Figures)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ De<span class="highlight-title">BERT</span>aV3: Improving De<span class="highlight-title">BERT</span>a using ELECTRA-Style <span class="highlight-title">Pre-Train</span>ing with
  Gradient-Disentangled Embedding Sharing <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.09543v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.09543v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengcheng He, Jianfeng Gao, Weizhu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a new pre-trained language model, DeBERTaV3, which
improves the original DeBERTa model by replacing mask language modeling (MLM)
with replaced token detection (RTD), a more sample-efficient pre-training task.
Our analysis shows that vanilla embedding sharing in ELECTRA hurts training
efficiency and model performance. This is because the training losses of the
discriminator and the generator pull token embeddings in different directions,
creating the "tug-of-war" dynamics. We thus propose a new gradient-disentangled
embedding sharing method that avoids the tug-of-war dynamics, improving both
training efficiency and the quality of the pre-trained model. We have
pre-trained DeBERTaV3 using the same settings as DeBERTa to demonstrate its
exceptional performance on a wide range of downstream natural language
understanding (NLU) tasks. Taking the GLUE benchmark with eight tasks as an
example, the DeBERTaV3 Large model achieves a 91.37% average score, which is
1.37% over DeBERTa and 1.91% over ELECTRA, setting a new state-of-the-art
(SOTA) among the models with a similar structure. Furthermore, we have
pre-trained a multi-lingual model mDeBERTa and observed a larger improvement
over strong baselines compared to English models. For example, the mDeBERTa
Base achieves a 79.8% zero-shot cross-lingual accuracy on XNLI and a 3.6%
improvement over XLM-R Base, creating a new SOTA on this benchmark. We have
made our pre-trained models and inference code publicly available at
https://github.com/microsoft/DeBERTa.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 10 tables, 2 Figures. The DeBERTaV3 model significantly
  improves performance of the downstream NLU tasks over models with a similar
  structure, e.g. DeBERTaV3 large achieves 91.37% average GLUE score which is
  1.37% over DeBERTa large. XSmall has only 22M backbone parameters, but
  significantly outperforms RoBERTa/XLNet-base. Paper is published as a
  conference paper at ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ I Can't Believe There's No Images! Learning Visual Tasks Using only
  Language Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.09778v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.09778v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sophia Gu, Christopher Clark, Aniruddha Kembhavi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many high-level skills that are required for computer vision tasks, such as
parsing questions, comparing and contrasting semantics, and writing
descriptions, are also required in other domains such as natural language
processing. In this paper, we ask whether it is possible to learn those skills
from textual data and then transfer them to vision tasks without ever training
on visual training data. Key to our approach is exploiting the joint embedding
space of contrastively trained vision and language encoders. In practice, there
can be systematic differences between embedding spaces for different modalities
in contrastive models, and we analyze how these differences affect our approach
and study strategies to mitigate this concern. We produce models using only
text training data on four representative tasks: image captioning, visual
entailment, visual question answering and visual news, and evaluate them on
standard benchmarks using images. We find these models generally perform close
to models trained on images, while surpassing prior work for captioning and
visual entailment in this text only setting by over 9 points, and outperforming
all prior work on visual news by over 30 points. We also showcase a variety of
stylistic image captioning models that are trained using no image data and no
human-curated language data, but instead using readily-available text data from
books, the web, or language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>website (https://prior.allenai.org/projects/close), code
  (https://github.com/allenai/close)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data-Efficient Learning of Natural Language to Linear Temporal Logic
  Translators for Robot Task Specification <span class="chip">ICRA 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08006v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08006v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayi Pan, Glen Chou, Dmitry Berenson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To make robots accessible to a broad audience, it is critical to endow them
with the ability to take universal modes of communication, like commands given
in natural language, and extract a concrete desired task specification, defined
using a formal language like linear temporal logic (LTL). In this paper, we
present a learning-based approach for translating from natural language
commands to LTL specifications with very limited human-labeled training data.
This is in stark contrast to existing natural-language to LTL translators,
which require large human-labeled datasets, often in the form of labeled pairs
of LTL formulas and natural language commands, to train the translator. To
reduce reliance on human data, our approach generates a large synthetic
training dataset through algorithmic generation of LTL formulas, conversion to
structured English, and then exploiting the paraphrasing capabilities of modern
large language models (LLMs) to synthesize a diverse corpus of natural language
commands corresponding to the LTL formulas. We use this generated data to
finetune an LLM and apply a constrained decoding procedure at inference time to
ensure the returned LTL formula is syntactically correct. We evaluate our
approach on three existing LTL/natural language datasets and show that we can
translate natural language commands at 75\% accuracy with far less human data
($\le$12 annotations). Moreover, when training on large human-annotated
datasets, our method achieves higher test accuracy (95\% on average) than prior
work. Finally, we show the translated formulas can be used to plan
long-horizon, multi-stage tasks on a 12D quadrotor.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICRA 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating Inclusivity, Equity, and Accessibility of NLP Technology: A
  Case Study for Indian Languages <span class="chip">EACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.12676v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.12676v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simran Khanuja, Sebastian Ruder, Partha Talukdar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In order for NLP technology to be widely applicable, fair, and useful, it
needs to serve a diverse set of speakers across the world's languages, be
equitable, i.e., not unduly biased towards any particular language, and be
inclusive of all users, particularly in low-resource settings where compute
constraints are common. In this paper, we propose an evaluation paradigm that
assesses NLP technologies across all three dimensions. While diversity and
inclusion have received attention in recent literature, equity is currently
unexplored. We propose to address this gap using the Gini coefficient, a
well-established metric used for estimating societal wealth inequality. Using
our paradigm, we highlight the distressed state of current technologies for
Indian (IN) languages (a linguistically large and diverse set, with a varied
speaker population), across all three dimensions. To improve upon these
metrics, we demonstrate the importance of region-specific choices in model
building and dataset creation, and more importantly, propose a novel,
generalisable approach to optimal resource allocation during fine-tuning.
Finally, we discuss steps to mitigate these biases and encourage the community
to employ multi-faceted evaluation when building linguistically diverse and
equitable technologies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EACL Findings, 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Counteracts: Testing Stereotypical Representation in <span class="highlight-title">Pre-train</span>ed
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.04347v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.04347v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Damin Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models have demonstrated strong performance on various natural
language understanding tasks. Similar to humans, language models could also
have their own bias that is learned from the training data. As more and more
downstream tasks integrate language models as part of the pipeline, it is
necessary to understand the internal stereotypical representation and the
methods to mitigate the negative effects. In this paper, we proposed a simple
method to test the internal stereotypical representation in pre-trained
language models using counterexamples. We mainly focused on gender bias, but
the method can be extended to other types of bias. We evaluated models on 9
different cloze-style prompts consisting of knowledge and base prompts. Our
results indicate that pre-trained language models show a certain amount of
robustness when using unrelated knowledge, and prefer shallow linguistic cues,
such as word position and syntactic structure, to alter the internal
stereotypical representation. Such findings shed light on how to manipulate
language models in a neutral approach for both finetuning and evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>FACCT; to be submitted to</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Is <span class="highlight-title">Prompt</span> All You Need? No. A Comprehensive and Broader View of
  Instruction Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10475v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10475v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renze Lou, Kai Zhang, Wenpeng Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Task semantics can be expressed by a set of input-to-output examples or a
piece of textual instruction. Conventional machine learning approaches for
natural language processing (NLP) mainly rely on the availability of
large-scale sets of task-specific examples. Two issues arise: first, collecting
task-specific labeled examples does not apply to scenarios where tasks may be
too complicated or costly to annotate, or the system is required to handle a
new task immediately; second, this is not user-friendly since end-users are
probably more willing to provide task description rather than a set of examples
before using the system. Therefore, the community is paying increasing interest
in a new supervision-seeking paradigm for NLP: learning from task instructions.
Despite its impressive progress, there are some common issues that the
community struggles with. This survey paper tries to summarize the current
research on instruction learning, particularly, by answering the following
questions: (i) what is task instruction, and what instruction types exist? (ii)
how to model instructions? (iii) what factors influence and explain the
instructions' performance? (iv) what challenges remain in instruction learning?
To our knowledge, this is the first comprehensive survey about textual
instructions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work is still in progress. The paper list is available at
  https://github.com/RenzeLou/awesome-instruction-learning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dual-Stream <span class="highlight-title">Transformer</span> for Generic Event Boundary Captioning <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.03038v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.03038v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Gu, Hanhua Ye, Guang Chen, Yufei Wang, Libo Zhang, Longyin Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes our champion solution for the CVPR2022 Generic Event
Boundary Captioning (GEBC) competition. GEBC requires the captioning model to
have a comprehension of instantaneous status changes around the given video
boundary, which makes it much more challenging than conventional video
captioning task. In this paper, a Dual-Stream Transformer with improvements on
both video content encoding and captions generation is proposed: (1) We utilize
three pre-trained models to extract the video features from different
granularities. Moreover, we exploit the types of boundary as hints to help the
model generate captions. (2) We particularly design an model, termed as
Dual-Stream Transformer, to learn discriminative representations for boundary
captioning. (3) Towards generating content-relevant and human-like captions, we
improve the description quality by designing a word-level ensemble strategy.
The promising results on the GEBC test split demonstrate the efficacy of our
proposed model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2023 LOVEU Workshop</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">163</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OmniTracker: Unifying Object Tracking by Tracking-with-Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12079v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12079v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junke Wang, Dongdong Chen, Zuxuan Wu, Chong Luo, Xiyang Dai, Lu Yuan, Yu-Gang Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object tracking (OT) aims to estimate the positions of target objects in a
video sequence. Depending on whether the initial states of target objects are
specified by provided annotations in the first frame or the categories, OT
could be classified as instance tracking (e.g., SOT and VOS) and category
tracking (e.g., MOT, MOTS, and VIS) tasks. Combing the advantages of the best
practices developed in both communities, we propose a novel
tracking-with-detection paradigm, where tracking supplements appearance priors
for detection and detection provides tracking with candidate bounding boxes for
association. Equipped with such a design, a unified tracking model,
OmniTracker, is further presented to resolve all the tracking tasks with a
fully shared network architecture, model weights, and inference pipeline.
Extensive experiments on 7 tracking datasets, including LaSOT, TrackingNet,
DAVIS16-17, MOT17, MOTS20, and YTVIS19, demonstrate that OmniTracker achieves
on-par or even better results than both task-specific and unified tracking
models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Natural Language-Assisted Sign Language Recognition <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12080v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12080v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ronglai Zuo, Fangyun Wei, Brian Mak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sign languages are visual languages which convey information by signers'
handshape, facial expression, body movement, and so forth. Due to the inherent
restriction of combinations of these visual ingredients, there exist a
significant number of visually indistinguishable signs (VISigns) in sign
languages, which limits the recognition capacity of vision neural networks. To
mitigate the problem, we propose the Natural Language-Assisted Sign Language
Recognition (NLA-SLR) framework, which exploits semantic information contained
in glosses (sign labels). First, for VISigns with similar semantic meanings, we
propose language-aware label smoothing by generating soft labels for each
training sign whose smoothing weights are computed from the normalized semantic
similarities among the glosses to ease training. Second, for VISigns with
distinct semantic meanings, we present an inter-modality mixup technique which
blends vision and gloss features to further maximize the separability of
different signs under the supervision of blended labels. Besides, we also
introduce a novel backbone, video-keypoint network, which not only models both
RGB videos and human body keypoints but also derives knowledge from sign videos
of different temporal receptive fields. Empirically, our method achieves
state-of-the-art performance on three widely-adopted benchmarks: MSASL, WLASL,
and NMFs-CSL. Codes are available at https://github.com/FangyunWei/SLRT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2023. Codes are available at
  https://github.com/FangyunWei/SLRT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Two-shot Video Object Segmentation <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12078v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12078v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kun Yan, Xiao Li, Fangyun Wei, Jinglu Wang, Chenbin Zhang, Ping Wang, Yan Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous works on video object segmentation (VOS) are trained on densely
annotated videos. Nevertheless, acquiring annotations in pixel level is
expensive and time-consuming. In this work, we demonstrate the feasibility of
training a satisfactory VOS model on sparsely annotated videos-we merely
require two labeled frames per training video while the performance is
sustained. We term this novel training paradigm as two-shot video object
segmentation, or two-shot VOS for short. The underlying idea is to generate
pseudo labels for unlabeled frames during training and to optimize the model on
the combination of labeled and pseudo-labeled data. Our approach is extremely
simple and can be applied to a majority of existing frameworks. We first
pre-train a VOS model on sparsely annotated videos in a semi-supervised manner,
with the first frame always being a labeled one. Then, we adopt the pre-trained
VOS model to generate pseudo labels for all unlabeled frames, which are
subsequently stored in a pseudo-label bank. Finally, we retrain a VOS model on
both labeled and pseudo-labeled data without any restrictions on the first
frame. For the first time, we present a general way to train VOS models on
two-shot VOS datasets. By using 7.3% and 2.9% labeled data of YouTube-VOS and
DAVIS benchmarks, our approach achieves comparable results in contrast to the
counterparts trained on fully labeled set. Code and models are available at
https://github.com/yk-pku/Two-shot-Video-Object-Segmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2023. Code and models are available at
  https://github.com/yk-pku/Two-shot-Video-Object-Segmentation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VAD: Vectorized Scene Representation for Efficient Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12077v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12077v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Jiang, Shaoyu Chen, Qing Xu, Bencheng Liao, Jiajie Chen, Helong Zhou, Qian Zhang, Wenyu Liu, Chang Huang, Xinggang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous driving requires a comprehensive understanding of the surrounding
environment for reliable trajectory planning. Previous works rely on dense
rasterized scene representation (e.g., agent occupancy and semantic map) to
perform planning, which is computationally intensive and misses the
instance-level structure information. In this paper, we propose VAD, an
end-to-end vectorized paradigm for autonomous driving, which models the driving
scene as fully vectorized representation. The proposed vectorized paradigm has
two significant advantages. On one hand, VAD exploits the vectorized agent
motion and map elements as explicit instance-level planning constraints which
effectively improves planning safety. On the other hand, VAD runs much faster
than previous end-to-end planning methods by getting rid of
computation-intensive rasterized representation and hand-designed
post-processing steps. VAD achieves state-of-the-art end-to-end planning
performance on the nuScenes dataset, outperforming the previous best method by
a large margin (reducing the average collision rate by 48.4%). Besides, VAD
greatly improves the inference speed (up to 9.3x), which is critical for the
real-world deployment of an autonomous driving system. Code and models will be
released for facilitating future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code&Demos: https://github.com/hustvl/VAD</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dexterity from Touch: <span class="highlight-title">Self-Supervised</span> <span class="highlight-title">Pre-Train</span>ing of Tactile
  Representations with Robotic Play 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12076v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12076v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Irmak Guzey, Ben Evans, Soumith Chintala, Lerrel Pinto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Teaching dexterity to multi-fingered robots has been a longstanding challenge
in robotics. Most prominent work in this area focuses on learning controllers
or policies that either operate on visual observations or state estimates
derived from vision. However, such methods perform poorly on fine-grained
manipulation tasks that require reasoning about contact forces or about objects
occluded by the hand itself. In this work, we present T-Dex, a new approach for
tactile-based dexterity, that operates in two phases. In the first phase, we
collect 2.5 hours of play data, which is used to train self-supervised tactile
encoders. This is necessary to bring high-dimensional tactile readings to a
lower-dimensional embedding. In the second phase, given a handful of
demonstrations for a dexterous task, we learn non-parametric policies that
combine the tactile observations with visual ones. Across five challenging
dexterous tasks, we show that our tactile-based dexterity models outperform
purely vision and torque-based models by an average of 1.7X. Finally, we
provide a detailed analysis on factors critical to T-Dex including the
importance of play data, architectures, and representation learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Video and code can be accessed here:
  https://tactile-dexterity.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CC3D: Layout-Conditioned Generation of Compositional 3D Scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12074v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12074v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sherwin Bahmani, Jeong Joon Park, Despoina Paschalidou, Xingguang Yan, Gordon Wetzstein, Leonidas Guibas, Andrea Tagliasacchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we introduce CC3D, a conditional generative model that
synthesizes complex 3D scenes conditioned on 2D semantic scene layouts, trained
using single-view images. Different from most existing 3D GANs that limit their
applicability to aligned single objects, we focus on generating complex scenes
with multiple objects, by modeling the compositional nature of 3D scenes. By
devising a 2D layout-based approach for 3D synthesis and implementing a new 3D
field representation with a stronger geometric inductive bias, we have created
a 3D GAN that is both efficient and of high quality, while allowing for a more
controllable generation process. Our evaluations on synthetic 3D-FRONT and
real-world KITTI-360 datasets demonstrate that our model generates scenes of
improved visual and geometric quality in comparison to previous works.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Webpage: https://sherwinbahmani.github.io/cc3d/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3D Mitochondria Instance Segmentation with Spatio-Temporal <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12073v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12073v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Omkar Thawakar, Rao Muhammad Anwer, Jorma Laaksonen, Orly Reiner, Mubarak Shah, Fahad Shahbaz Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate 3D mitochondria instance segmentation in electron microscopy (EM) is
a challenging problem and serves as a prerequisite to empirically analyze their
distributions and morphology. Most existing approaches employ 3D convolutions
to obtain representative features. However, these convolution-based approaches
struggle to effectively capture long-range dependencies in the volume
mitochondria data, due to their limited local receptive field. To address this,
we propose a hybrid encoder-decoder framework based on a split spatio-temporal
attention module that efficiently computes spatial and temporal self-attentions
in parallel, which are later fused through a deformable convolution. Further,
we introduce a semantic foreground-background adversarial loss during training
that aids in delineating the region of mitochondria instances from the
background clutter. Our extensive experiments on three benchmarks, Lucchi,
MitoEM-R and MitoEM-H, reveal the benefits of the proposed contributions
achieving state-of-the-art results on all three datasets. Our code and models
are available at https://github.com/OmkarThawakar/STT-UNET.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 3 figures, 5 Tables, 2 page references</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ProphNet: Efficient Agent-Centric Motion Forecasting with
  Anchor-Informed Proposals <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12071v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12071v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xishun Wang, Tong Su, Fang Da, Xiaodong Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motion forecasting is a key module in an autonomous driving system. Due to
the heterogeneous nature of multi-sourced input, multimodality in agent
behavior, and low latency required by onboard deployment, this task is
notoriously challenging. To cope with these difficulties, this paper proposes a
novel agent-centric model with anchor-informed proposals for efficient
multimodal motion prediction. We design a modality-agnostic strategy to
concisely encode the complex input in a unified manner. We generate diverse
proposals, fused with anchors bearing goal-oriented scene context, to induce
multimodal prediction that covers a wide range of future trajectories. Our
network architecture is highly uniform and succinct, leading to an efficient
model amenable for real-world driving deployment. Experiments reveal that our
agent-centric network compares favorably with the state-of-the-art methods in
prediction accuracy, while achieving scene-centric level inference latency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Machine Learning for Brain Disorders: <span class="highlight-title">Transformer</span>s and Visual
  <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12068v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12068v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robin Courant, Maika Edberg, Nicolas Dufour, Vicky Kalogeiton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers were initially introduced for natural language processing (NLP)
tasks, but fast they were adopted by most deep learning fields, including
computer vision. They measure the relationships between pairs of input tokens
(words in the case of text strings, parts of images for visual Transformers),
termed attention. The cost is exponential with the number of tokens. For image
classification, the most common Transformer Architecture uses only the
Transformer Encoder in order to transform the various input tokens. However,
there are also numerous other applications in which the decoder part of the
traditional Transformer Architecture is also used. Here, we first introduce the
Attention mechanism (Section 1), and then the Basic Transformer Block including
the Vision Transformer (Section 2). Next, we discuss some improvements of
visual Transformers to account for small datasets or less computation(Section
3). Finally, we introduce Visual Transformers applied to tasks other than image
classification, such as detection, segmentation, generation and training
without labels (Section 4) and other domains, such as video or multimodality
using text or audio data (Section 5).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in O. Colliot (Ed.), Machine Learning for Brain Disorders,
  Springer</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Motion Matters: Neural Motion Transfer for Better Camera Physiological
  Sensing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12059v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12059v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akshay Paruchuri, Xin Liu, Yulu Pan, Shwetak Patel, Daniel McDuff, Soumyadip Sengupta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning models for camera-based physiological measurement can have
weak generalization due to a lack of representative training data. Body motion
is one of the most significant sources of noise when attempting to recover the
subtle cardiac pulse from a video. We explore motion transfer as a form of data
augmentation to introduce motion variation while preserving physiological
changes. We adapt a neural video synthesis approach to augment videos for the
task of remote photoplethysmography (PPG) and study the effects of motion
augmentation with respect to 1) the magnitude and 2) the type of motion. After
training on motion-augmented versions of publicly available datasets, the
presented inter-dataset results on five benchmark datasets show improvements of
up to 75% over existing state-of-the-art results. Our findings illustrate the
utility of motion transfer as a data augmentation technique for improving the
generalization of models for camera-based physiological sensing. We release our
code and pre-trained models for using motion transfer as a data augmentation
technique on our project page: https://motion-matters.github.io/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VideoXum: Cross-modal Visual and Textural Summarization of Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12060v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12060v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyang Lin, Hang Hua, Ming Chen, Yikang Li, Jenhao Hsiao, Chiuman Ho, Jiebo Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video summarization aims to distill the most important information from a
source video to produce either an abridged clip or a textual narrative.
Traditionally, different methods have been proposed depending on whether the
output is a video or text, thus ignoring the correlation between the two
semantically related tasks of visual summarization and textual summarization.
We propose a new joint video and text summarization task. The goal is to
generate both a shortened video clip along with the corresponding textual
summary from a long video, collectively referred to as a cross-modal summary.
The generated shortened video clip and text narratives should be semantically
well aligned. To this end, we first build a large-scale human-annotated dataset
-- VideoXum (X refers to different modalities). The dataset is reannotated
based on ActivityNet. After we filter out the videos that do not meet the
length requirements, 14,001 long videos remain in our new dataset. Each video
in our reannotated dataset has human-annotated video summaries and the
corresponding narrative summaries. We then design a novel end-to-end model --
VTSUM-BILP to address the challenges of our proposed task. Moreover, we propose
a new metric called VT-CLIPScore to help evaluate the semantic consistency of
cross-modality summary. The proposed model achieves promising performance on
this new task and establishes a benchmark for future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Influencer Backdoor Attack on Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12054v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12054v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoheng Lan, Jindong Gu, Philip Torr, Hengshuang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When a small number of poisoned samples are injected into the training
dataset of a deep neural network, the network can be induced to exhibit
malicious behavior during inferences, which poses potential threats to
real-world applications. While they have been intensively studied in
classification, backdoor attacks on semantic segmentation have been largely
overlooked. Unlike classification, semantic segmentation aims to classify every
pixel within a given image. In this work, we explore backdoor attacks on
segmentation models to misclassify all pixels of a victim class by injecting a
specific trigger on non-victim pixels during inferences, which is dubbed
Influencer Backdoor Attack (IBA). IBA is expected to maintain the
classification accuracy of non-victim pixels and misleads classifications of
all victim pixels in every single inference. Specifically, we consider two
types of IBA scenarios, i.e., 1) Free-position IBA: the trigger can be
positioned freely except for pixels of the victim class, and 2) Long-distance
IBA: the trigger can only be positioned somewhere far from victim pixels, given
the possible practical constraint. Based on the context aggregation ability of
segmentation models, we propose techniques to improve IBA for the scenarios.
Concretely, for free-position IBA, we propose a simple, yet effective Nearest
Neighbor trigger injection strategy for poisoned sample creation. For
long-distance IBA, we propose a novel Pixel Random Labeling strategy. Our
extensive experiments reveal that current segmentation models do suffer from
backdoor attacks, and verify that our proposed techniques can further increase
attack performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Novel and Optimal Spectral Method for Permutation Synchronization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12051v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12051v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Duc Nguyen, Anderson Ye Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Permutation synchronization is an important problem in computer science that
constitutes the key step of many computer vision tasks. The goal is to recover
$n$ latent permutations from their noisy and incomplete pairwise measurements.
In recent years, spectral methods have gained increasing popularity thanks to
their simplicity and computational efficiency. Spectral methods utilize the
leading eigenspace $U$ of the data matrix and its block submatrices
$U_1,U_2,\ldots, U_n$ to recover the permutations. In this paper, we propose a
novel and statistically optimal spectral algorithm. Unlike the existing methods
which use $\{U_jU_1^\top\}_{j\geq 2}$, ours constructs an anchor matrix $M$ by
aggregating useful information from all the block submatrices and estimates the
latent permutations through $\{U_jM^\top\}_{j\geq 1}$. This modification
overcomes a crucial limitation of the existing methods caused by the repetitive
use of $U_1$ and leads to an improved numerical performance. To establish the
optimality of the proposed method, we carry out a fine-grained spectral
analysis and obtain a sharp exponential error bound that matches the minimax
rate.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CurveCloudNet: Processing Point Clouds with 1D Structure 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12050v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12050v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Colton Stearns, Jiateng Liu, Davis Rempe, Despoina Paschalidou, Jeong Joon Park, Sebastien Mascha, Leonidas J. Guibas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern depth sensors such as LiDAR operate by sweeping laser-beams across the
scene, resulting in a point cloud with notable 1D curve-like structures. In
this work, we introduce a new point cloud processing scheme and backbone,
called CurveCloudNet, which takes advantage of the curve-like structure
inherent to these sensors. While existing backbones discard the rich 1D
traversal patterns and rely on Euclidean operations, CurveCloudNet
parameterizes the point cloud as a collection of polylines (dubbed a "curve
cloud"), establishing a local surface-aware ordering on the points. Our method
applies curve-specific operations to process the curve cloud, including a
symmetric 1D convolution, a ball grouping for merging points along curves, and
an efficient 1D farthest point sampling algorithm on curves. By combining these
curve operations with existing point-based operations, CurveCloudNet is an
efficient, scalable, and accurate backbone with low GPU memory requirements.
Evaluations on the ShapeNet, Kortx, Audi Driving, and nuScenes datasets
demonstrate that CurveCloudNet outperforms both point-based and sparse-voxel
backbones in various segmentation settings, notably scaling better to large
scenes than point-based alternatives while exhibiting better single object
performance than sparse-voxel alternatives.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vox-E: Text-guided Voxel Editing of 3D Objects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12048v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12048v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Etai Sella, Gal Fiebelman, Peter Hedman, Hadar Averbuch-Elor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large scale text-guided diffusion models have garnered significant attention
due to their ability to synthesize diverse images that convey complex visual
concepts. This generative power has more recently been leveraged to perform
text-to-3D synthesis. In this work, we present a technique that harnesses the
power of latent diffusion models for editing existing 3D objects. Our method
takes oriented 2D images of a 3D object as input and learns a grid-based
volumetric representation of it. To guide the volumetric representation to
conform to a target text prompt, we follow unconditional text-to-3D methods and
optimize a Score Distillation Sampling (SDS) loss. However, we observe that
combining this diffusion-guided loss with an image-based regularization loss
that encourages the representation not to deviate too strongly from the input
object is challenging, as it requires achieving two conflicting goals while
viewing only structure-and-appearance coupled 2D projections. Thus, we
introduce a novel volumetric regularization loss that operates directly in 3D
space, utilizing the explicit nature of our 3D representation to enforce
correlation between the global structure of the original and edited object.
Furthermore, we present a technique that optimizes cross-attention volumetric
grids to refine the spatial extent of the edits. Extensive experiments and
comparisons demonstrate the effectiveness of our approach in creating a myriad
of edits which cannot be achieved by prior works.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project webpage: https://tau-vailab.github.io/Vox-E/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semantic Latent Space Regression of Diffusion Autoencoders for Vertebral
  Fracture Grading 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12031v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12031v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthias Keicher, Matan Atad, David Schinz, Alexandra S. Gersing, Sarah C. Foreman, Sophia S. Goller, Juergen Weissinger, Jon Rischewski, Anna-Sophia Dietrich, Benedikt Wiestler, Jan S. Kirschke, Nassir Navab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vertebral fractures are a consequence of osteoporosis, with significant
health implications for affected patients. Unfortunately, grading their
severity using CT exams is hard and subjective, motivating automated grading
methods. However, current approaches are hindered by imbalance and scarcity of
data and a lack of interpretability. To address these challenges, this paper
proposes a novel approach that leverages unlabelled data to train a generative
Diffusion Autoencoder (DAE) model as an unsupervised feature extractor. We
model fracture grading as a continuous regression, which is more reflective of
the smooth progression of fractures. Specifically, we use a binary, supervised
fracture classifier to construct a hyperplane in the DAE's latent space. We
then regress the severity of the fracture as a function of the distance to this
hyperplane, calibrating the results to the Genant scale. Importantly, the
generative nature of our method allows us to visualize different grades of a
given vertebra, providing interpretability and insight into the features that
contribute to automated grading.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Joint Visual Grounding and Tracking with Natural Language Specification <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12027v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12027v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Zhou, Zikun Zhou, Kaige Mao, Zhenyu He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tracking by natural language specification aims to locate the referred target
in a sequence based on the natural language description. Existing algorithms
solve this issue in two steps, visual grounding and tracking, and accordingly
deploy the separated grounding model and tracking model to implement these two
steps, respectively. Such a separated framework overlooks the link between
visual grounding and tracking, which is that the natural language descriptions
provide global semantic cues for localizing the target for both two steps.
Besides, the separated framework can hardly be trained end-to-end. To handle
these issues, we propose a joint visual grounding and tracking framework, which
reformulates grounding and tracking as a unified task: localizing the referred
target based on the given visual-language references. Specifically, we propose
a multi-source relation modeling module to effectively build the relation
between the visual-language references and the test image. In addition, we
design a temporal modeling module to provide a temporal clue with the guidance
of the global semantic information for our model, which effectively improves
the adaptability to the appearance variations of the target. Extensive
experimental results on TNL2K, LaSOT, OTB99, and RefCOCOg demonstrate that our
method performs favorably against state-of-the-art algorithms for both tracking
and grounding. Code is available at https://github.com/lizhou-cs/JointNLT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Optical Flow and Scene Flow with Bidirectional Camera-LiDAR
  Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12017v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12017v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haisong Liu, Tao Lu, Yihui Xu, Jia Liu, Limin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we study the problem of jointly estimating the optical flow
and scene flow from synchronized 2D and 3D data. Previous methods either employ
a complex pipeline that splits the joint task into independent stages, or fuse
2D and 3D information in an ``early-fusion'' or ``late-fusion'' manner. Such
one-size-fits-all approaches suffer from a dilemma of failing to fully utilize
the characteristic of each modality or to maximize the inter-modality
complementarity. To address the problem, we propose a novel end-to-end
framework, which consists of 2D and 3D branches with multiple bidirectional
fusion connections between them in specific layers. Different from previous
work, we apply a point-based 3D branch to extract the LiDAR features, as it
preserves the geometric structure of point clouds. To fuse dense image features
and sparse point features, we propose a learnable operator named bidirectional
camera-LiDAR fusion module (Bi-CLFM). We instantiate two types of the
bidirectional fusion pipeline, one based on the pyramidal coarse-to-fine
architecture (dubbed CamLiPWC), and the other one based on the recurrent
all-pairs field transforms (dubbed CamLiRAFT). On FlyingThings3D, both CamLiPWC
and CamLiRAFT surpass all existing methods and achieve up to a 47.9\% reduction
in 3D end-point-error from the best published result. Our best-performing
model, CamLiRAFT, achieves an error of 4.26\% on the KITTI Scene Flow
benchmark, ranking 1st among all submissions with much fewer parameters.
Besides, our methods have strong generalization performance and the ability to
handle non-rigid motion. Code is available at
https://github.com/MCG-NJU/CamLiFlow.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2111.10502</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatic evaluation of herding behavior in towed fishing gear using
  end-to-end training of CNN and attention-based networks <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12016v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12016v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Orri Steinn Guðfinnsson, Týr Vilhjálmsson, Martin Eineborg, Torfi Thorhallsson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper considers the automatic classification of herding behavior in the
cluttered low-visibility environment that typically surrounds towed fishing
gear. The paper compares three convolutional and attention-based deep action
recognition network architectures trained end-to-end on a small set of video
sequences captured by a remotely controlled camera and classified by an expert
in fishing technology. The sequences depict a scene in front of a fishing trawl
where the conventional herding mechanism has been replaced by directed laser
light. The goal is to detect the presence of a fish in the sequence and
classify whether or not the fish reacts to the lasers. A two-stream CNN model,
a CNN-transformer hybrid, and a pure transformer model were trained end-to-end
to achieve 63%, 54%, and 60% 10-fold classification accuracy on the three-class
task when compared to the human expert. Inspection of the activation maps
learned by the three networks raises questions about the attributes of the
sequences the models may be learning, specifically whether changes in viewpoint
introduced by human camera operators that affect the position of laser lines in
the video frames may interfere with the classification. This underlines the
importance of careful experimental design when capturing scientific data for
automatic end-to-end evaluation and the usefulness of inspecting the trained
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 10 figures. To appear in Proceedings of the 5th Workshop on
  Computer Vision for Analysis of Underwater Imagery (CVAUI) 2022, published as
  part of the CVPR 2022 Proceedings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NeAT: Learning Neural Implicit Surfaces with Arbitrary Topologies from
  Multi-view Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12012v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12012v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoxu Meng, Weikai Chen, Bo Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent progress in neural implicit functions has set new state-of-the-art in
reconstructing high-fidelity 3D shapes from a collection of images. However,
these approaches are limited to closed surfaces as they require the surface to
be represented by a signed distance field. In this paper, we propose NeAT, a
new neural rendering framework that can learn implicit surfaces with arbitrary
topologies from multi-view images. In particular, NeAT represents the 3D
surface as a level set of a signed distance function (SDF) with a validity
branch for estimating the surface existence probability at the query positions.
We also develop a novel neural volume rendering method, which uses SDF and
validity to calculate the volume opacity and avoids rendering points with low
validity. NeAT supports easy field-to-mesh conversion using the classic
Marching Cubes algorithm. Extensive experiments on DTU, MGN, and Deep Fashion
3D datasets indicate that our approach is able to faithfully reconstruct both
watertight and non-watertight surfaces. In particular, NeAT significantly
outperforms the state-of-the-art methods in the task of open surface
reconstruction both quantitatively and qualitatively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visual Representation Learning from Unlabeled Video using Contrastive
  Masked Autoencoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12001v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12001v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jefferson Hernandez, Ruben Villegas, Vicente Ordonez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Masked Autoencoders (MAEs) learn self-supervised representations by randomly
masking input image patches and a reconstruction loss. Alternatively,
contrastive learning self-supervised methods encourage two versions of the same
input to have a similar representation, while pulling apart the representations
for different inputs. We propose ViC-MAE, a general method that combines both
MAE and contrastive learning by pooling the local feature representations
learned under the MAE reconstruction objective and leveraging this global
representation under a contrastive objective across video frames. We show that
visual representations learned under ViC-MAE generalize well to both video
classification and image classification tasks. Using a backbone ViT-B/16
network pre-trained on the Moments in Time (MiT) dataset, we obtain
state-of-the-art transfer learning from video to images on Imagenet-1k by
improving 1.58% in absolute top-1 accuracy from a recent previous work.
Moreover, our method maintains a competitive transfer-learning performance of
81.50% top-1 accuracy on the Kinetics-400 video classification benchmark. In
addition, we show that despite its simplicity, ViC-MAE yields improved results
compared to combining MAE pre-training with previously proposed contrastive
objectives such as VicReg and SiamSiam.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ E-MLB: Multilevel Benchmark for Event-Based Camera Denoising 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11997v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11997v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saizhe Ding, Jinze Chen, Yang Wang, Yu Kang, Weiguo Song, Jie Cheng, Yang Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event cameras, such as dynamic vision sensors (DVS), are biologically
inspired vision sensors that have advanced over conventional cameras in high
dynamic range, low latency and low power consumption, showing great application
potential in many fields. Event cameras are more sensitive to junction leakage
current and photocurrent as they output differential signals, losing the
smoothing function of the integral imaging process in the RGB camera. The
logarithmic conversion further amplifies noise, especially in low-contrast
conditions. Recently, researchers proposed a series of datasets and evaluation
metrics but limitations remain: 1) the existing datasets are small in scale and
insufficient in noise diversity, which cannot reflect the authentic working
environments of event cameras; and 2) the existing denoising evaluation metrics
are mostly referenced evaluation metrics, relying on APS information or manual
annotation. To address the above issues, we construct a large-scale event
denoising dataset (multilevel benchmark for event denoising, E-MLB) for the
first time, which consists of 100 scenes, each with four noise levels, that is
12 times larger than the largest existing denoising dataset. We also propose
the first nonreference event denoising metric, the event structural ratio
(ESR), which measures the structural intensity of given events. ESR is inspired
by the contrast metric, but is independent of the number of events and
projection direction. Based on the proposed benchmark and ESR, we evaluate the
most representative denoising algorithms, including classic and SOTA, and
provide denoising baselines under various scenes and noise levels. The
corresponding results and codes are available at
https://github.com/KugaMaxx/cuke-emlb.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Text2Room: Extracting Textured 3D Meshes from 2D Text-to-Image Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11989v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11989v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Höllein, Ang Cao, Andrew Owens, Justin Johnson, Matthias Nießner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Text2Room, a method for generating room-scale textured 3D meshes
from a given text prompt as input. To this end, we leverage pre-trained 2D
text-to-image models to synthesize a sequence of images from different poses.
In order to lift these outputs into a consistent 3D scene representation, we
combine monocular depth estimation with a text-conditioned inpainting model.
The core idea of our approach is a tailored viewpoint selection such that the
content of each image can be fused into a seamless, textured 3D mesh. More
specifically, we propose a continuous alignment strategy that iteratively fuses
scene frames with the existing geometry to create a seamless mesh. Unlike
existing works that focus on generating single objects or zoom-out trajectories
from text, our method generates complete 3D scenes with multiple objects and
explicit 3D geometry. We evaluate our approach using qualitative and
quantitative metrics, demonstrating it as the first method to generate
room-scale 3D geometry with compelling textures from only text as input.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>video: https://youtu.be/fjRnFL91EZc project page:
  https://lukashoel.github.io/text-to-room/ code:
  https://github.com/lukasHoel/text2room</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Defect Detection Approaches Based on Simulated Reference Image 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11971v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11971v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nati Ofir, Yotam Ben Shoshan, Ran Badanes, Boris Sherman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work is addressing the problem of defect anomaly detection based on a
clean reference image. Specifically, we focus on SEM semiconductor defects in
addition to several natural image anomalies. There are well-known methods to
create a simulation of an artificial reference image by its defect specimen. In
this work, we introduce several applications for this capability, that the
simulated reference is beneficial for improving their results. Among these
defect detection methods are classic computer vision applied on
difference-image, supervised deep-learning (DL) based on human labels, and
unsupervised DL which is trained on feature-level patterns of normal reference
images. We show in this study how to incorporate correctly the simulated
reference image for these defect and anomaly detection applications. As our
experiment demonstrates, simulated reference achieves higher performance than
the real reference of an image of a defect and anomaly. This advantage of
simulated reference occurs mainly due to the less noise and geometric
variations together with better alignment and registration to the original
defect background.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explain To Me: Salience-Based Explainability for Synthetic Face
  Detection Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11969v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11969v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Colton Crum, Patrick Tinsley, Aidan Boyd, Jacob Piland, Christopher Sweet, Timothy Kelley, Kevin Bowyer, Adam Czajka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The performance of convolutional neural networks has continued to improve
over the last decade. At the same time, as model complexity grows, it becomes
increasingly more difficult to explain model decisions. Such explanations may
be of critical importance for reliable operation of human-machine pairing
setups, or for model selection when the "best" model among many
equally-accurate models must be established. Saliency maps represent one
popular way of explaining model decisions by highlighting image regions models
deem important when making a prediction. However, examining salience maps at
scale is not practical. In this paper, we propose five novel methods of
leveraging model salience to explain a model behavior at scale. These methods
ask: (a) what is the average entropy for a model's salience maps, (b) how does
model salience change when fed out-of-set samples, (c) how closely does model
salience follow geometrical transformations, (d) what is the stability of model
salience across independent training runs, and (e) how does model salience
react to salience-guided image degradations. To assess the proposed measures on
a concrete and topical problem, we conducted a series of experiments for the
task of synthetic face detection with two types of models: those trained
traditionally with cross-entropy loss, and those guided by human salience when
training to increase model generalizability. These two types of models are
characterized by different, interpretable properties of their salience maps,
which allows for the evaluation of the correctness of the proposed measures. We
offer source codes for each measure along with this paper.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NEMTO: Neural Environment Matting for Novel View and Relighting
  Synthesis of Transparent Objects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11963v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11963v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongqing Wang, Tong Zhang, Sabine Süsstrunk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose NEMTO, the first end-to-end neural rendering pipeline to model 3D
transparent objects with complex geometry and unknown indices of refraction.
Commonly used appearance modeling such as the Disney BSDF model cannot
accurately address this challenging problem due to the complex light paths
bending through refractions and the strong dependency of surface appearance on
illumination. With 2D images of the transparent object as input, our method is
capable of high-quality novel view and relighting synthesis. We leverage
implicit Signed Distance Functions (SDF) to model the object geometry and
propose a refraction-aware ray bending network to model the effects of light
refraction within the object. Our ray bending network is more tolerant to
geometric inaccuracies than traditional physically-based methods for rendering
transparent objects. We provide extensive evaluations on both synthetic and
real-world datasets to demonstrate our high-quality synthesis and the
applicability of our method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning A Sparse <span class="highlight-title">Transformer</span> Network for Effective Image Deraining <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11950v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11950v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Chen, Hao Li, Mingqiang Li, Jinshan Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers-based methods have achieved significant performance in image
deraining as they can model the non-local information which is vital for
high-quality image reconstruction. In this paper, we find that most existing
Transformers usually use all similarities of the tokens from the query-key
pairs for the feature aggregation. However, if the tokens from the query are
different from those of the key, the self-attention values estimated from these
tokens also involve in feature aggregation, which accordingly interferes with
the clear image restoration. To overcome this problem, we propose an effective
DeRaining network, Sparse Transformer (DRSformer) that can adaptively keep the
most useful self-attention values for feature aggregation so that the
aggregated features better facilitate high-quality image reconstruction.
Specifically, we develop a learnable top-k selection operator to adaptively
retain the most crucial attention scores from the keys for each query for
better feature aggregation. Simultaneously, as the naive feed-forward network
in Transformers does not model the multi-scale information that is important
for latent clear image restoration, we develop an effective mixed-scale
feed-forward network to generate better features for image deraining. To learn
an enriched set of hybrid features, which combines local context from CNN
operators, we equip our model with mixture of experts feature compensator to
present a cooperation refinement deraining scheme. Extensive experimental
results on the commonly used benchmarks demonstrate that the proposed method
achieves favorable performance against state-of-the-art approaches. The source
code and trained models are available at
https://github.com/cschenxiang/DRSformer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a highlight paper in CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3D-CLFusion: Fast Text-to-3D Rendering with Contrastive Latent Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11938v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11938v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu-Jhe Li, Kris Kitani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We tackle the task of text-to-3D creation with pre-trained latent-based NeRFs
(NeRFs that generate 3D objects given input latent code). Recent works such as
DreamFusion and Magic3D have shown great success in generating 3D content using
NeRFs and text prompts, but the current approach of optimizing a NeRF for every
text prompt is 1) extremely time-consuming and 2) often leads to low-resolution
outputs. To address these challenges, we propose a novel method named
3D-CLFusion which leverages the pre-trained latent-based NeRFs and performs
fast 3D content creation in less than a minute. In particular, we introduce a
latent diffusion prior network for learning the w latent from the input CLIP
text/image embeddings. This pipeline allows us to produce the w latent without
further optimization during inference and the pre-trained NeRF is able to
perform multi-view high-resolution 3D synthesis based on the latent. We note
that the novelty of our model lies in that we introduce contrastive learning
during training the diffusion prior which enables the generation of the valid
view-invariant latent code. We demonstrate through experiments the
effectiveness of our proposed view-invariant diffusion process for fast
text-to-3D creation, e.g., 100 times faster than DreamFusion. We note that our
model is able to serve as the role of a plug-and-play tool for text-to-3D with
pre-trained NeRFs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages. Non-CMU authors are currently hidden due to an internal
  legal review in progress of their company</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Using Explanations to Guide Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11932v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11932v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sukrut Rao, Moritz Böhle, Amin Parchami-Araghi, Bernt Schiele
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks are highly performant, but might base their decision on
spurious or background features that co-occur with certain classes, which can
hurt generalization. To mitigate this issue, the usage of 'model guidance' has
gained popularity recently: for this, models are guided to be "right for the
right reasons" by regularizing the models' explanations to highlight the right
features. Experimental validation of these approaches has thus far however been
limited to relatively simple and / or synthetic datasets. To gain a better
understanding of which model-guiding approaches actually transfer to more
challenging real-world datasets, in this work we conduct an in-depth evaluation
across various loss functions, attribution methods, models, and 'guidance
depths' on the PASCAL VOC 2007 and MS COCO 2014 datasets, and show that model
guidance can sometimes even improve model performance. In this context, we
further propose a novel energy loss, show its effectiveness in directing the
model to focus on object features. We also show that these gains can be
achieved even with a small fraction (e.g. 1%) of bounding box annotations,
highlighting the cost effectiveness of this approach. Lastly, we show that this
approach can also improve generalization under distribution shifts. Code will
be made available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages, 35 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Object-Centric Temporal Modeling for Efficient Multi-View 3D
  Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11926v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11926v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shihao Wang, Yingfei Liu, Tiancai Wang, Ying Li, Xiangyu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a long-sequence modeling framework, named
StreamPETR, for multi-view 3D object detection. Built upon the sparse query
design in the PETR series, we systematically develop an object-centric temporal
mechanism. The model is performed in an online manner and the long-term
historical information is propagated through object queries frame by frame.
Besides, we introduce a motion-aware layer normalization to model the movement
of the objects. StreamPETR achieves significant performance improvements only
with negligible computation cost, compared to the single-frame baseline. On the
standard nuScenes benchmark, it reaches a new state-of-the-art performance
(63.6% NDS). The lightweight version realizes 45.0% mAP and 31.7 FPS,
outperforming the state-of-the-art method (SOLOFusion) by 2.3% mAP and 1.8x
faster FPS. Code will be available at
https://github.com/exiawsh/StreamPETR.git.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Performance-aware Approximation of Global Channel Pruning for Multitask
  CNNs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11923v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11923v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hancheng Ye, Bo Zhang, Tao Chen, Jiayuan Fan, Bin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Global channel pruning (GCP) aims to remove a subset of channels (filters)
across different layers from a deep model without hurting the performance.
Previous works focus on either single task model pruning or simply adapting it
to multitask scenario, and still face the following problems when handling
multitask pruning: 1) Due to the task mismatch, a well-pruned backbone for
classification task focuses on preserving filters that can extract
category-sensitive information, causing filters that may be useful for other
tasks to be pruned during the backbone pruning stage; 2) For multitask
predictions, different filters within or between layers are more closely
related and interacted than that for single task prediction, making multitask
pruning more difficult. Therefore, aiming at multitask model compression, we
propose a Performance-Aware Global Channel Pruning (PAGCP) framework. We first
theoretically present the objective for achieving superior GCP, by considering
the joint saliency of filters from intra- and inter-layers. Then a sequentially
greedy pruning strategy is proposed to optimize the objective, where a
performance-aware oracle criterion is developed to evaluate sensitivity of
filters to each task and preserve the globally most task-related filters.
Experiments on several multitask datasets show that the proposed PAGCP can
reduce the FLOPs and parameters by over 60% with minor performance drop, and
achieves 1.2x$\sim$3.3x acceleration on both cloud and mobile platforms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in T-PAMI, our code is available at
  http://www.github.com/HankYe/PAGCP.git</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Context De-confounded Emotion Recognition <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11921v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11921v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dingkang Yang, Zhaoyu Chen, Yuzheng Wang, Shunli Wang, Mingcheng Li, Siao Liu, Xiao Zhao, Shuai Huang, Zhiyan Dong, Peng Zhai, Lihua Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Context-Aware Emotion Recognition (CAER) is a crucial and challenging task
that aims to perceive the emotional states of the target person with contextual
information. Recent approaches invariably focus on designing sophisticated
architectures or mechanisms to extract seemingly meaningful representations
from subjects and contexts. However, a long-overlooked issue is that a context
bias in existing datasets leads to a significantly unbalanced distribution of
emotional states among different context scenarios. Concretely, the harmful
bias is a confounder that misleads existing models to learn spurious
correlations based on conventional likelihood estimation, significantly
limiting the models' performance. To tackle the issue, this paper provides a
causality-based perspective to disentangle the models from the impact of such
bias, and formulate the causalities among variables in the CAER task via a
tailored causal graph. Then, we propose a Contextual Causal Intervention Module
(CCIM) based on the backdoor adjustment to de-confound the confounder and
exploit the true causal effect for model training. CCIM is plug-in and
model-agnostic, which improves diverse state-of-the-art approaches by
considerable margins. Extensive experiments on three benchmark datasets
demonstrate the effectiveness of our CCIM and the significance of causal
insight.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Decision-based Black-box Patch Attacks on Video Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11917v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11917v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaixun Jiang, Zhaoyu Chen, Tony Huang, Jiafeng Wang, Dingkang Yang, Bo Li, Yan Wang, Wenqiang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although Deep Neural Networks (DNNs) have demonstrated excellent performance,
they are vulnerable to adversarial patches that introduce perceptible and
localized perturbations to the input. Generating adversarial patches on images
has received much attention, while adversarial patches on videos have not been
well investigated. Further, decision-based attacks, where attackers only access
the predicted hard labels by querying threat models, have not been well
explored on video models either, even if they are practical in real-world video
recognition scenes. The absence of such studies leads to a huge gap in the
robustness assessment for video models. To bridge this gap, this work first
explores decision-based patch attacks on video models. We analyze that the huge
parameter space brought by videos and the minimal information returned by
decision-based models both greatly increase the attack difficulty and query
burden. To achieve a query-efficient attack, we propose a spatial-temporal
differential evolution (STDE) framework. First, STDE introduces target videos
as patch textures and only adds patches on keyframes that are adaptively
selected by temporal difference. Second, STDE takes minimizing the patch area
as the optimization objective and adopts spatialtemporal mutation and crossover
to search for the global optimum without falling into the local optimum.
Experiments show STDE has demonstrated state-of-the-art performance in terms of
threat, efficiency and imperceptibility. Hence, STDE has the potential to be a
powerful tool for evaluating the robustness of video recognition models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CompoDiff: Versatile Composed Image Retrieval With Latent Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11916v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11916v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Geonmo Gu, Sanghyuk Chun, Wonjae Kim, HeeJae Jun, Yoohoon Kang, Sangdoo Yun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a novel diffusion-based model, CompoDiff, for solving
Composed Image Retrieval (CIR) with latent diffusion and presents a newly
created dataset of 18 million reference images, conditions, and corresponding
target image triplets to train the model. CompoDiff not only achieves a new
zero-shot state-of-the-art on a CIR benchmark such as FashionIQ but also
enables a more versatile CIR by accepting various conditions, such as negative
text and image mask conditions, which are unavailable with existing CIR
methods. In addition, the CompoDiff features are on the intact CLIP embedding
space so that they can be directly used for all existing models exploiting the
CLIP space. The code and dataset used for the training, and the pre-trained
weights are available at https://github.com/navervision/CompoDiff
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>First two authors contributed equally; 23 pages, 4.8MB</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 360BEV: Panoramic Semantic Mapping for Indoor Bird's-Eye View 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11910v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11910v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhifeng Teng, Jiaming Zhang, Kailun Yang, Kunyu Peng, Hao Shi, Simon Reiß, Ke Cao, Rainer Stiefelhagen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Seeing only a tiny part of the whole is not knowing the full circumstance.
Bird's-eye-view (BEV) perception, a process of obtaining allocentric maps from
egocentric views, is restricted when using a narrow Field of View (FoV) alone.
In this work, mapping from 360{\deg} panoramas to BEV semantics, the 360BEV
task, is established for the first time to achieve holistic representations of
indoor scenes in a top-down view. Instead of relying on narrow-FoV image
sequences, a panoramic image with depth information is sufficient to generate a
holistic BEV semantic map. To benchmark 360BEV, we present two indoor datasets,
360BEV-Matterport and 360BEV-Stanford, both of which include egocentric
panoramic images and semantic segmentation labels, as well as allocentric
semantic maps. Besides delving deep into different mapping paradigms, we
propose a dedicated solution for panoramic semantic mapping, namely 360Mapper.
Through extensive experiments, our methods achieve 44.32% and 45.78% in mIoU on
both datasets respectively, surpassing previous counterparts with gains of
+7.60% and +9.70% in mIoU. Code and datasets will be available at:
\url{https://jamycheung.github.io/360BEV.html}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and datasets will be available at:
  \url{https://jamycheung.github.io/360BEV.html}</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Multiscale Surface Vision <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11909v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11909v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon Dahan, Abdulah Fawaz, Mohamed A. Suliman, Mariana da Silva, Logan Z. J. Williams, Daniel Rueckert, Emma C. Robinson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Surface meshes are a favoured domain for representing structural and
functional information on the human cortex, but their complex topology and
geometry pose significant challenges for deep learning analysis. While
Transformers have excelled as domain-agnostic architectures for
sequence-to-sequence learning, notably for structures where the translation of
the convolution operation is non-trivial, the quadratic cost of the
self-attention operation remains an obstacle for many dense prediction tasks.
Inspired by some of the latest advances in hierarchical modelling with vision
transformers, we introduce the Multiscale Surface Vision Transformer (MS-SiT)
as a backbone architecture for surface deep learning. The self-attention
mechanism is applied within local-mesh-windows to allow for high-resolution
sampling of the underlying data, while a shifted-window strategy improves the
sharing of information between windows. Neighbouring patches are successively
merged, allowing the MS-SiT to learn hierarchical representations suitable for
any prediction task. Results demonstrate that the MS-SiT outperforms existing
surface deep learning methods for neonatal phenotyping prediction tasks using
the Developing Human Connectome Project (dHCP) dataset. Furthermore, building
the MS-SiT backbone into a U-shaped architecture for surface segmentation
demonstrates competitive results on cortical parcellation using the UK Biobank
(UKB) and manually-annotated MindBoggle datasets. Code and trained models are
publicly available at
https://github.com/metrics-lab/surface-vision-transformers .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Solving Oscillation Problem in Post-Training Quantization Through a
  Theoretical Perspective <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11906v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11906v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuexiao Ma, Huixia Li, Xiawu Zheng, Xuefeng Xiao, Rui Wang, Shilei Wen, Xin Pan, Fei Chao, Rongrong Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Post-training quantization (PTQ) is widely regarded as one of the most
efficient compression methods practically, benefitting from its data privacy
and low computation costs. We argue that an overlooked problem of oscillation
is in the PTQ methods. In this paper, we take the initiative to explore and
present a theoretical proof to explain why such a problem is essential in PTQ.
And then, we try to solve this problem by introducing a principled and
generalized framework theoretically. In particular, we first formulate the
oscillation in PTQ and prove the problem is caused by the difference in module
capacity. To this end, we define the module capacity (ModCap) under
data-dependent and data-free scenarios, where the differentials between
adjacent modules are used to measure the degree of oscillation. The problem is
then solved by selecting top-k differentials, in which the corresponding
modules are jointly optimized and quantized. Extensive experiments demonstrate
that our method successfully reduces the performance drop and is generalized to
different neural networks and PTQ methods. For example, with 2/4 bit ResNet-50
quantization, our method surpasses the previous state-of-the-art method by
1.9%. It becomes more significant on small model quantization, e.g. surpasses
BRECQ method by 6.61% on MobileNetV2*0.5.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-time volumetric rendering of dynamic humans 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11898v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11898v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ignacio Rocco, Iurii Makarov, Filippos Kokkinos, David Novotny, Benjamin Graham, Natalia Neverova, Andrea Vedaldi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a method for fast 3D reconstruction and real-time rendering of
dynamic humans from monocular videos with accompanying parametric body fits.
Our method can reconstruct a dynamic human in less than 3h using a single GPU,
compared to recent state-of-the-art alternatives that take up to 72h. These
speedups are obtained by using a lightweight deformation model solely based on
linear blend skinning, and an efficient factorized volumetric representation
for modeling the shape and color of the person in canonical pose. Moreover, we
propose a novel local ray marching rendering which, by exploiting standard GPU
hardware and without any baking or conversion of the radiance field, allows
visualizing the neural human on a mobile VR device at 40 frames per second with
minimal loss of visual quality. Our experimental evaluation shows superior or
competitive results with state-of-the art methods while obtaining large
training speedup, using a simple model, and achieving real-time rendering.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://real-time-humans.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TIFA: Accurate and Interpretable Text-to-Image Faithfulness Evaluation
  with Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11897v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11897v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yushi Hu, Benlin Liu, Jungo Kasai, Yizhong Wang, Mari Ostendorf, Ranjay Krishna, Noah A. Smith
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite thousands of researchers, engineers, and artists actively working on
improving text-to-image generation models, systems often fail to produce images
that accurately align with the text inputs. We introduce TIFA (Text-to-Image
Faithfulness evaluation with question Answering), an automatic evaluation
metric that measures the faithfulness of a generated image to its text input
via visual question answering (VQA). Specifically, given a text input, we
automatically generate several question-answer pairs using a language model. We
calculate image faithfulness by checking whether existing VQA models can answer
these questions using the generated image. TIFA is a reference-free metric that
allows for fine-grained and interpretable evaluations of generated images. TIFA
also has better correlations with human judgments than existing metrics. Based
on this approach, we introduce TIFA v1.0, a benchmark consisting of 4K diverse
text inputs and 25K questions across 12 categories (object, counting, etc.). We
present a comprehensive evaluation of existing text-to-image models using TIFA
v1.0 and highlight the limitations and challenges of current models. For
instance, we find that current text-to-image models, despite doing well on
color and material, still struggle in counting, spatial relations, and
composing multiple objects. We hope our benchmark will help carefully measure
the research progress in text-to-image synthesis and provide valuable insights
for further research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Better Understanding Differences in Attribution Methods via Systematic
  Evaluations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11884v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11884v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sukrut Rao, Moritz Böhle, Bernt Schiele
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks are very successful on many vision tasks, but hard to
interpret due to their black box nature. To overcome this, various post-hoc
attribution methods have been proposed to identify image regions most
influential to the models' decisions. Evaluating such methods is challenging
since no ground truth attributions exist. We thus propose three novel
evaluation schemes to more reliably measure the faithfulness of those methods,
to make comparisons between them more fair, and to make visual inspection more
systematic. To address faithfulness, we propose a novel evaluation setting
(DiFull) in which we carefully control which parts of the input can influence
the output in order to distinguish possible from impossible attributions. To
address fairness, we note that different methods are applied at different
layers, which skews any comparison, and so evaluate all methods on the same
layers (ML-Att) and discuss how this impacts their performance on quantitative
metrics. For more systematic visualizations, we propose a scheme (AggAtt) to
qualitatively evaluate the methods on complete datasets. We use these
evaluation schemes to study strengths and shortcomings of some widely used
attribution methods over a wide range of models. Finally, we propose a
post-processing smoothing step that significantly improves the performance of
some attribution methods, and discuss its applicability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 37 figures, 2 tables, extended version of arXiv:2205.10435</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Protective Self-Adaptive Pruning to Better Compress DNNs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11881v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11881v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liang Li, Pengfei Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adaptive network pruning approach has recently drawn significant attention
due to its excellent capability to identify the importance and redundancy of
layers and filters and customize a suitable pruning solution. However, it
remains unsatisfactory since current adaptive pruning methods rely mostly on an
additional monitor to score layer and filter importance, and thus faces high
complexity and weak interpretability. To tackle these issues, we have deeply
researched the weight reconstruction process in iterative prune-train process
and propose a Protective Self-Adaptive Pruning (PSAP) method. First of all,
PSAP can utilize its own information, weight sparsity ratio, to adaptively
adjust pruning ratio of layers before each pruning step. Moreover, we propose a
protective reconstruction mechanism to prevent important filters from being
pruned through supervising gradients and to avoid unrecoverable information
loss as well. Our PSAP is handy and explicit because it merely depends on
weights and gradients of model itself, instead of requiring an additional
monitor as in early works. Experiments on ImageNet and CIFAR-10 also
demonstrate its superiority to current works in both accuracy and compression
ratio, especially for compressing with a high ratio or pruning from scratch.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Focused and Collaborative Feedback Integration for Interactive Image
  Segmentation <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11880v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11880v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiaoqiao Wei, Hui Zhang, Jun-Hai Yong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interactive image segmentation aims at obtaining a segmentation mask for an
image using simple user annotations. During each round of interaction, the
segmentation result from the previous round serves as feedback to guide the
user's annotation and provides dense prior information for the segmentation
model, effectively acting as a bridge between interactions. Existing methods
overlook the importance of feedback or simply concatenate it with the original
input, leading to underutilization of feedback and an increase in the number of
required annotations. To address this, we propose an approach called Focused
and Collaborative Feedback Integration (FCFI) to fully exploit the feedback for
click-based interactive image segmentation. FCFI first focuses on a local area
around the new click and corrects the feedback based on the similarities of
high-level features. It then alternately and collaboratively updates the
feedback and deep features to integrate the feedback into the features. The
efficacy and efficiency of FCFI were validated on four benchmarks, namely
GrabCut, Berkeley, SBD, and DAVIS. Experimental results show that FCFI achieved
new state-of-the-art performance with less computational overhead than previous
methods. The source code is available at
https://github.com/veizgyauzgyauz/FCFI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contrastive Alignment of Vision to Language Through Parameter-Efficient
  Transfer Learning <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11866v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11866v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zaid Khan, Yun Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive vision-language models (e.g. CLIP) are typically created by
updating all the parameters of a vision model and language model through
contrastive training. Can such models be created by a small number of parameter
updates to an already-trained language model and vision model? The literature
describes techniques that can create vision-language models by updating a small
number of parameters in a language model, but these require already aligned
visual representations and are non-contrastive, hence unusable for
latency-sensitive applications such as neural search. We explore the
feasibility and benefits of parameter-efficient contrastive vision-language
alignment through transfer learning: creating a model such as CLIP by minimally
updating an already-trained vision and language model. We find that a minimal
set of parameter updates ($<$7%) can achieve the same performance as full-model
training, and updating specific components ($<$1% of parameters) can match 75%
of full-model training. We describe a series of experiments: we show that
existing knowledge is conserved more strongly in parameter-efficient training
and that parameter-efficient scaling scales with model and dataset size. Where
paired-image text data is scarce but strong multilingual language models exist
(e.g. low resource languages), parameter-efficient training is even preferable
to full-model training. Given a fixed compute budget, parameter-efficient
training allows training larger models on the same hardware, achieving
equivalent performance in less time. Parameter-efficient training hence
constitutes an energy-efficient and effective training strategy for contrastive
vision-language models that may be preferable to the full-model training
paradigm for common use cases. Code and weights at
https://github.com/codezakh/LilT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LEAPS: End-to-End One-Step Person Search With Learnable Proposals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11859v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11859v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiqiang Dong, Jiale Cao, Rao Muhammad Anwer, Jin Xie, Fahad Khan, Yanwei Pang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose an end-to-end one-step person search approach with learnable
proposals, named LEAPS. Given a set of sparse and learnable proposals, LEAPS
employs a dynamic person search head to directly perform person detection and
corresponding re-id feature generation without non-maximum suppression
post-processing. The dynamic person search head comprises a detection head and
a novel flexible re-id head. Our flexible re-id head first employs a dynamic
region-of-interest (RoI) operation to extract discriminative RoI features of
the proposals. Then, it generates re-id features using a plain and a
hierarchical interaction re-id module. To better guide discriminative re-id
feature learning, we introduce a diverse re-id sample matching strategy,
instead of bipartite matching in detection head. Comprehensive experiments
reveal the benefit of the proposed LEAPS, achieving a favorable performance on
two public person search benchmarks: CUHK-SYSU and PRW. When using the same
ResNet50 backbone, our LEAPS obtains a mAP score of 55.0%, outperforming the
best reported results in literature by 1.7%, while achieving around a two-fold
speedup on the challenging PRW dataset. Our source code and models will be
released.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CLIP-ReIdent: Contrastive Training for Player Re-Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11855v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11855v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Konrad Habel, Fabian Deuser, Norbert Oswald
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sports analytics benefits from recent advances in machine learning providing
a competitive advantage for teams or individuals. One important task in this
context is the performance measurement of individual players to provide reports
and log files for subsequent analysis. During sport events like basketball,
this involves the re-identification of players during a match either from
multiple camera viewpoints or from a single camera viewpoint at different
times. In this work, we investigate whether it is possible to transfer the
out-standing zero-shot performance of pre-trained CLIP models to the domain of
player re-identification. For this purpose we reformulate the contrastive
language-to-image pre-training approach from CLIP to a contrastive
image-to-image training approach using the InfoNCE loss as training objective.
Unlike previous work, our approach is entirely class-agnostic and benefits from
large-scale pre-training. With a fine-tuned CLIP ViT-L/14 model we achieve
98.44 % mAP on the MMSports 2022 Player Re-Identification challenge.
Furthermore we show that the CLIP Vision Transformers have already strong OCR
capabilities to identify useful player features like shirt numbers in a
zero-shot manner without any fine-tuning on the dataset. By applying the
Score-CAM algorithm we visualise the most important image regions that our
fine-tuned model identifies when calculating the similarity score between two
images of a player.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sample4Geo: Hard Negative Sampling For Cross-View Geo-Localisation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11851v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11851v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabian Deuser, Konrad Habel, Norbert Oswald
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-View Geo-Localisation is still a challenging task where additional
modules, specific pre-processing or zooming strategies are necessary to
determine accurate positions of images. Since different views have different
geometries, pre-processing like polar transformation helps to merge them.
However, this results in distorted images which then have to be rectified.
Adding hard negatives to the training batch could improve the overall
performance but with the default loss functions in geo-localisation it is
difficult to include them. In this article, we present a simplified but
effective architecture based on contrastive learning with symmetric InfoNCE
loss that outperforms current state-of-the-art results. Our framework consists
of a narrow training pipeline that eliminates the need of using aggregation
modules, avoids further pre-processing steps and even increases the
generalisation capability of the model to unknown regions. We introduce two
types of sampling strategies for hard negatives. The first explicitly exploits
geographically neighboring locations to provide a good starting point. The
second leverages the visual similarity between the image embeddings in order to
mine hard negative samples. Our work shows excellent performance on common
cross-view datasets like CVUSA, CVACT, University-1652 and VIGOR. A comparison
between cross-area and same-area settings demonstrate the good generalisation
capability of our model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dens-PU: PU Learning with Density-Based Positive Labeled Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11848v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11848v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vasileios Sevetlidis, George Pavlidis, Spyridon Mouroutsos, Antonios Gasteratos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study proposes a novel approach for solving the PU learning problem
based on an anomaly-detection strategy. Latent encodings extracted from
positive-labeled data are linearly combined to acquire new samples. These new
samples are used as embeddings to increase the density of positive-labeled data
and, thus, define a boundary that approximates the positive class. The further
a sample is from the boundary the more it is considered as a negative sample.
Once a set of negative samples is obtained, the PU learning problem reduces to
binary classification. The approach, named Dens-PU due to its reliance on the
density of positive-labeled data, was evaluated using benchmark image datasets,
and state-of-the-art results were attained.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Paced Neutral Expression-Disentangled Learning for Facial
  Expression Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11840v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11840v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenqian Wu, Xiaoyuan Li, Yazhou Ren, Xiaorong Pu, Xiaofeng Zhu, Lifang He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The accuracy of facial expression recognition is typically affected by the
following factors: high similarities across different expressions, disturbing
factors, and micro-facial movement of rapid and subtle changes. One potentially
viable solution for addressing these barriers is to exploit the neutral
information concealed in neutral expression images. To this end, in this paper
we propose a self-Paced Neutral Expression-Disentangled Learning (SPNDL) model.
SPNDL disentangles neutral information from facial expressions, making it
easier to extract key and deviation features. Specifically, it allows to
capture discriminative information among similar expressions and perceive
micro-facial movements. In order to better learn these neutral
expression-disentangled features (NDFs) and to alleviate the non-convex
optimization problem, a self-paced learning (SPL) strategy based on NDFs is
proposed in the training stage. SPL learns samples from easy to complex by
increasing the number of samples selected into the training process, which
enables to effectively suppress the negative impacts introduced by low-quality
samples and inconsistently distributed NDFs. Experiments on three popular
databases (i.e., CK+, Oulu-CASIA, and RAF-DB) show the effectiveness of our
proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ADCNet: End-to-end perception with raw radar ADC data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11420v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11420v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Yang, Ishan Khatri, Michael Happold, Chulong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is a renewed interest in radar sensors in the autonomous driving
industry. As a relatively mature technology, radars have seen steady
improvement over the last few years, making them an appealing alternative or
complement to the commonly used LiDARs. An emerging trend is to leverage rich,
low-level radar data for perception. In this work we push this trend to the
extreme -- we propose a method to perform end-to-end learning on the raw radar
analog-to-digital (ADC) data. Specifically, we design a learnable signal
processing module inside the neural network, and a pre-training method guided
by traditional signal processing algorithms. Experiment results corroborate the
overall efficacy of the end-to-end learning method, while an ablation study
validates the effectiveness of our individual innovations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Self-supervised</span> learning of a tailored Convolutional Auto Encoder for
  histopathological prostate grading 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11837v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11837v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zahra Tabatabaei, Adrian colomer, Kjersti Engan, Javier Oliver, Valery Naranjo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  According to GLOBOCAN 2020, prostate cancer is the second most common cancer
in men worldwide and the fourth most prevalent cancer overall. For
pathologists, grading prostate cancer is challenging, especially when
discriminating between Grade 3 (G3) and Grade 4 (G4). This paper proposes a
Self-Supervised Learning (SSL) framework to classify prostate histopathological
images when labeled images are scarce. In particular, a tailored Convolutional
Auto Encoder (CAE) is trained to reconstruct 128x128x3 patches of prostate
cancer Whole Slide Images (WSIs) as a pretext task. The downstream task of the
proposed SSL paradigm is the automatic grading of histopathological patches of
prostate cancer. The presented framework reports promising results on the
validation set, obtaining an overall accuracy of 83% and on the test set,
achieving an overall accuracy value of 76% with F1-score of 77% in G4.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GLADE: Gradient Loss Augmented Degradation Enhancement for Unpaired
  Super-Resolution of Anisotropic MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11831v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11831v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michele Pascale, Vivek Muthurangu, Javier Montalt Tordera, Heather E Fitzke, Gauraang Bhatnagar, Stuart Taylor, Jennifer Steeden
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel approach to synthesise high-resolution isotropic 3D
abdominal MR images, from anisotropic 3D images in an unpaired fashion. Using a
modified CycleGAN architecture with a gradient mapping loss, we leverage
disjoint patches from the high-resolution (in-plane) data of an anisotropic
volume to enforce the network generator to increase the resolution of the
low-resolution (through-plane) slices. This will enable accelerated
whole-abdomen scanning with high-resolution isotropic images within short
breath-hold times.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Treasure Beneath Multiple Annotations: An Uncertainty-aware Edge
  Detector <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11828v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11828v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Caixia Zhou, Yaping Huang, Mengyang Pu, Qingji Guan, Li Huang, Haibin Ling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning-based edge detectors heavily rely on pixel-wise labels which
are often provided by multiple annotators. Existing methods fuse multiple
annotations using a simple voting process, ignoring the inherent ambiguity of
edges and labeling bias of annotators. In this paper, we propose a novel
uncertainty-aware edge detector (UAED), which employs uncertainty to
investigate the subjectivity and ambiguity of diverse annotations.
Specifically, we first convert the deterministic label space into a learnable
Gaussian distribution, whose variance measures the degree of ambiguity among
different annotations. Then we regard the learned variance as the estimated
uncertainty of the predicted edge maps, and pixels with higher uncertainty are
likely to be hard samples for edge detection. Therefore we design an adaptive
weighting loss to emphasize the learning from those pixels with high
uncertainty, which helps the network to gradually concentrate on the important
pixels. UAED can be combined with various encoder-decoder backbones, and the
extensive experiments demonstrate that UAED achieves superior performance
consistently across multiple edge detection benchmarks. The source code is
available at \url{https://github.com/ZhouCX117/UAED}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fighting over-fitting with quantization for learning deep neural
  networks on noisy labels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11803v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11803v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gauthier Tallec, Edouard Yvinec, Arnaud Dapogny, Kevin Bailly
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rising performance of deep neural networks is often empirically
attributed to an increase in the available computational power, which allows
complex models to be trained upon large amounts of annotated data. However,
increased model complexity leads to costly deployment of modern neural
networks, while gathering such amounts of data requires huge costs to avoid
label noise. In this work, we study the ability of compression methods to
tackle both of these problems at once. We hypothesize that quantization-aware
training, by restricting the expressivity of neural networks, behaves as a
regularization. Thus, it may help fighting overfitting on noisy data while also
allowing for the compression of the model at inference. We first validate this
claim on a controlled test with manually introduced label noise. Furthermore,
we also test the proposed method on Facial Action Unit detection, where labels
are typically noisy due to the subtlety of the task. In all cases, our results
suggests that quantization significantly improve the results compared with
existing baselines, regularization as well as other compression methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CAT-Seg: Cost Aggregation for Open-Vocabulary Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11797v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11797v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seokju Cho, Heeseong Shin, Sunghwan Hong, Seungjun An, Seungjun Lee, Anurag Arnab, Paul Hongsuck Seo, Seungryong Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing works on open-vocabulary semantic segmentation have utilized
large-scale vision-language models, such as CLIP, to leverage their exceptional
open-vocabulary recognition capabilities. However, the problem of transferring
these capabilities learned from image-level supervision to the pixel-level task
of segmentation and addressing arbitrary unseen categories at inference makes
this task challenging. To address these issues, we aim to attentively relate
objects within an image to given categories by leveraging relational
information among class categories and visual semantics through aggregation,
while also adapting the CLIP representations to the pixel-level task. However,
we observe that direct optimization of the CLIP embeddings can harm its
open-vocabulary capabilities. In this regard, we propose an alternative
approach to optimize the image-text similarity map, i.e. the cost map, using a
novel cost aggregation-based method. Our framework, namely CAT-Seg, achieves
state-of-the-art performance across all benchmarks. We provide extensive
ablation studies to validate our choices. Project page:
https://ku-cvlab.github.io/CAT-Seg/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://ku-cvlab.github.io/CAT-Seg/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OTJR: Optimal Transport Meets Optimal Jacobian Regularization for
  Adversarial Robustness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11793v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11793v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Binh M. Le, Shahroz Tariq, Simon S. Woo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks are widely recognized as being vulnerable to adversarial
perturbation. To overcome this challenge, developing a robust classifier is
crucial. So far, two well-known defenses have been adopted to improve the
learning of robust classifiers, namely adversarial training (AT) and Jacobian
regularization. However, each approach behaves differently against adversarial
perturbations. First, our work carefully analyzes and characterizes these two
schools of approaches, both theoretically and empirically, to demonstrate how
each approach impacts the robust learning of a classifier. Next, we propose our
novel Optimal Transport with Jacobian regularization method, dubbed OTJR,
jointly incorporating the input-output Jacobian regularization into the AT by
leveraging the optimal transport theory. In particular, we employ the Sliced
Wasserstein (SW) distance that can efficiently push the adversarial samples'
representations closer to those of clean samples, regardless of the number of
classes within the dataset. The SW distance provides the adversarial samples'
movement directions, which are much more informative and powerful for the
Jacobian regularization. Our extensive experiments demonstrate the
effectiveness of our proposed method, which jointly incorporates Jacobian
regularization into AT. Furthermore, we demonstrate that our proposed method
consistently enhances the model's robustness with CIFAR-100 dataset under
various adversarial attack settings, achieving up to 28.49% under AutoAttack.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Propagate And Calibrate: Real-time Passive Non-line-of-sight Tracking <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11791v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11791v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihao Wang, Zhigang Wang, Bin Zhao, Dong Wang, Mulin Chen, Xuelong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Non-line-of-sight (NLOS) tracking has drawn increasing attention in recent
years, due to its ability to detect object motion out of sight. Most previous
works on NLOS tracking rely on active illumination, e.g., laser, and suffer
from high cost and elaborate experimental conditions. Besides, these techniques
are still far from practical application due to oversimplified settings. In
contrast, we propose a purely passive method to track a person walking in an
invisible room by only observing a relay wall, which is more in line with real
application scenarios, e.g., security. To excavate imperceptible changes in
videos of the relay wall, we introduce difference frames as an essential
carrier of temporal-local motion messages. In addition, we propose PAC-Net,
which consists of alternating propagation and calibration, making it capable of
leveraging both dynamic and static messages on a frame-level granularity. To
evaluate the proposed method, we build and publish the first dynamic passive
NLOS tracking dataset, NLOS-Track, which fills the vacuum of realistic NLOS
datasets. NLOS-Track contains thousands of NLOS video clips and corresponding
trajectories. Both real-shot and synthetic data are included.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023 camera-ready version. Codes and dataset are available at
  https://againstentropy.github.io/NLOS-Track/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Probabilistic Domain Adaptation for Biomedical Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11790v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11790v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anwai Archit, Constantin Pape
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Segmentation is a key analysis tasks in biomedical imaging. Given the many
different experimental settings in this field, the lack of generalization
limits the use of deep learning in practice. Domain adaptation is a promising
remedy: it trains a model for a given task on a source dataset with labels and
adapts it to a target dataset without additional labels. We introduce a
probabilistic domain adaptation method, building on self-training approaches
and the Probabilistic UNet. We use the latter to sample multiple segmentation
hypothesis to implement better pseudo-label filtering. We further study joint
and separate source-target training strategies and evaluate our method on three
challenging domain adaptation tasks for biomedical segmentation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Sufficient Framework for Continuous Sign Language Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11771v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11771v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youngjoon Jang, Youngtaek Oh, Jae Won Cho, Myungchul Kim, Dong-Jin Kim, In So Kweon, Joon Son Chung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of this work is to develop self-sufficient framework for Continuous
Sign Language Recognition (CSLR) that addresses key issues of sign language
recognition. These include the need for complex multi-scale features such as
hands, face, and mouth for understanding, and absence of frame-level
annotations. To this end, we propose (1) Divide and Focus Convolution (DFConv)
which extracts both manual and non-manual features without the need for
additional networks or annotations, and (2) Dense Pseudo-Label Refinement
(DPLR) which propagates non-spiky frame-level pseudo-labels by combining the
ground truth gloss sequence labels with the predicted sequence. We demonstrate
that our model achieves state-of-the-art performance among RGB-based methods on
large-scale CSLR benchmarks, PHOENIX-2014 and PHOENIX-2014-T, while showing
comparable results with better efficiency when compared to other approaches
that use multi-modality or extra annotations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Simulating Malaria Detection in Laboratories using Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11759v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11759v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Onyekachukwu R. Okonji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Malaria is usually diagnosed by a microbiologist by examining a small sample
of blood smear. Reducing mortality from malaria infection is possible if it is
diagnosed early and followed with appropriate treatment. While the WHO has set
audacious goals of reducing malaria incidence and mortality rates by 90% in
2030 and eliminating malaria in 35 countries by that time, it still remains a
difficult challenge. Computer-assisted diagnostics are on the rise these days
as they can be used effectively as a primary test in the absence of or
providing assistance to a physician or pathologist. The purpose of this paper
is to describe an approach to detecting, localizing and counting parasitic
cells in blood sample images towards easing the burden on healthcare workers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LIMITR: Leveraging Local Information for Medical Image-Text
  Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11755v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11755v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gefen Dawidowicz, Elad Hirsch, Ayellet Tal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical imaging analysis plays a critical role in the diagnosis and treatment
of various medical conditions. This paper focuses on chest X-ray images and
their corresponding radiological reports. It presents a new model that learns a
joint X-ray image & report representation. The model is based on a novel
alignment scheme between the visual data and the text, which takes into account
both local and global information. Furthermore, the model integrates
domain-specific information of two types -- lateral images and the consistent
visual structure of chest images. Our representation is shown to benefit three
types of retrieval tasks: text-image retrieval, class-based retrieval, and
phrase-grounding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detecting Everything in the Open World: Towards Universal Object
  Detection <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11749v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11749v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenyu Wang, Yali Li, Xi Chen, Ser-Nam Lim, Antonio Torralba, Hengshuang Zhao, Shengjin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we formally address universal object detection, which aims to
detect every scene and predict every category. The dependence on human
annotations, the limited visual information, and the novel categories in the
open world severely restrict the universality of traditional detectors. We
propose \textbf{UniDetector}, a universal object detector that has the ability
to recognize enormous categories in the open world. The critical points for the
universality of UniDetector are: 1) it leverages images of multiple sources and
heterogeneous label spaces for training through the alignment of image and text
spaces, which guarantees sufficient information for universal representations.
2) it generalizes to the open world easily while keeping the balance between
seen and unseen classes, thanks to abundant information from both vision and
language modalities. 3) it further promotes the generalization ability to novel
categories through our proposed decoupling training manner and probability
calibration. These contributions allow UniDetector to detect over 7k
categories, the largest measurable category size so far, with only about 500
classes participating in training. Our UniDetector behaves the strong zero-shot
generalization ability on large-vocabulary datasets like LVIS, ImageNetBoxes,
and VisualGenome - it surpasses the traditional supervised baselines by more
than 4\% on average without seeing any corresponding images. On 13 public
detection datasets with various scenes, UniDetector also achieves
state-of-the-art performance with only a 3\% amount of training data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data-efficient Large Scale Place Recognition with Graded Similarity
  Supervision <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11739v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11739v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maria Leyva-Vallina, Nicola Strisciuglio, Nicolai Petkov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual place recognition (VPR) is a fundamental task of computer vision for
visual localization. Existing methods are trained using image pairs that either
depict the same place or not. Such a binary indication does not consider
continuous relations of similarity between images of the same place taken from
different positions, determined by the continuous nature of camera pose. The
binary similarity induces a noisy supervision signal into the training of VPR
methods, which stall in local minima and require expensive hard mining
algorithms to guarantee convergence. Motivated by the fact that two images of
the same place only partially share visual cues due to camera pose differences,
we deploy an automatic re-annotation strategy to re-label VPR datasets. We
compute graded similarity labels for image pairs based on available
localization metadata. Furthermore, we propose a new Generalized Contrastive
Loss (GCL) that uses graded similarity labels for training contrastive
networks. We demonstrate that the use of the new labels and GCL allow to
dispense from hard-pair mining, and to train image descriptors that perform
better in VPR by nearest neighbor search, obtaining superior or comparable
results than methods that require expensive hard-pair mining and re-ranking
techniques. Code and models available at:
https://github.com/marialeyvallina/generalized_contrastive_loss
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-modal <span class="highlight-title">Prompt</span>ing for Low-Shot Temporal Action Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11732v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11732v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Ju, Zeqian Li, Peisen Zhao, Ya Zhang, Xiaopeng Zhang, Qi Tian, Yanfeng Wang, Weidi Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we consider the problem of temporal action localization under
low-shot (zero-shot & few-shot) scenario, with the goal of detecting and
classifying the action instances from arbitrary categories within some
untrimmed videos, even not seen at training time. We adopt a Transformer-based
two-stage action localization architecture with class-agnostic action proposal,
followed by open-vocabulary classification. We make the following
contributions. First, to compensate image-text foundation models with temporal
motions, we improve category-agnostic action proposal by explicitly aligning
embeddings of optical flows, RGB and texts, which has largely been ignored in
existing low-shot methods. Second, to improve open-vocabulary action
classification, we construct classifiers with strong discriminative power,
i.e., avoid lexical ambiguities. To be specific, we propose to prompt the
pre-trained CLIP text encoder either with detailed action descriptions
(acquired from large-scale language models), or visually-conditioned
instance-specific prompt vectors. Third, we conduct thorough experiments and
ablation studies on THUMOS14 and ActivityNet1.3, demonstrating the superior
performance of our proposed model, outperforming existing state-of-the-art
approaches by one significant margin.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Abstract Visual Reasoning: An Algebraic Approach for Solving Raven's
  Progressive Matrices <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11730v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11730v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyi Xu, Tushar Vaidya, Yufei Wu, Saket Chandra, Zhangsheng Lai, Kai Fong Ernest Chong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce algebraic machine reasoning, a new reasoning framework that is
well-suited for abstract reasoning. Effectively, algebraic machine reasoning
reduces the difficult process of novel problem-solving to routine algebraic
computation. The fundamental algebraic objects of interest are the ideals of
some suitably initialized polynomial ring. We shall explain how solving Raven's
Progressive Matrices (RPMs) can be realized as computational problems in
algebra, which combine various well-known algebraic subroutines that include:
Computing the Gr\"obner basis of an ideal, checking for ideal containment, etc.
Crucially, the additional algebraic structure satisfied by ideals allows for
more operations on ideals beyond set-theoretic operations.
  Our algebraic machine reasoning framework is not only able to select the
correct answer from a given answer set, but also able to generate the correct
answer with only the question matrix given. Experiments on the I-RAVEN dataset
yield an overall $93.2\%$ accuracy, which significantly outperforms the current
state-of-the-art accuracy of $77.0\%$ and exceeds human performance at $84.4\%$
accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR) 2023. 30 pages, 7 figures (including supplementary
  material). First three authors contributed equally. Code is available at:
  https://github.com/Xu-Jingyi/AlgebraicMR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ExtremeNeRF: Few-shot Neural Radiance Fields Under Unconstrained
  Illumination 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11728v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11728v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        SeokYeong Lee, JunYong Choi, Seungryong Kim, Ig-Jae Kim, Junghyun Cho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a new challenge that synthesizes a novel view in a
more practical environment, where the number of input multi-view images is
limited and illumination variations are significant. Despite recent success,
neural radiance fields (NeRF) require a massive amount of input multi-view
images taken under constrained illuminations. To address the problem, we
suggest ExtremeNeRF, which utilizes occlusion-aware multiview albedo
consistency, supported by geometric alignment and depth consistency. We extract
intrinsic image components that should be illumination-invariant across
different views, enabling direct appearance comparison between the input and
novel view under unconstrained illumination. We provide extensive experimental
results for an evaluation of the task, using the newly built NeRF Extreme
benchmark, which is the first in-the-wild novel view synthesis benchmark taken
under multiple viewing directions and varying illuminations. The project page
is at https://seokyeong94.github.io/ExtremeNeRF/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3D Human Mesh Estimation from Virtual Markers <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11726v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11726v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoxuan Ma, Jiajun Su, Chunyu Wang, Wentao Zhu, Yizhou Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inspired by the success of volumetric 3D pose estimation, some recent human
mesh estimators propose to estimate 3D skeletons as intermediate
representations, from which, the dense 3D meshes are regressed by exploiting
the mesh topology. However, body shape information is lost in extracting
skeletons, leading to mediocre performance. The advanced motion capture systems
solve the problem by placing dense physical markers on the body surface, which
allows to extract realistic meshes from their non-rigid motions. However, they
cannot be applied to wild images without markers. In this work, we present an
intermediate representation, named virtual markers, which learns 64 landmark
keypoints on the body surface based on the large-scale mocap data in a
generative style, mimicking the effects of physical markers. The virtual
markers can be accurately detected from wild images and can reconstruct the
intact meshes with realistic shapes by simple interpolation. Our approach
outperforms the state-of-the-art methods on three datasets. In particular, it
surpasses the existing methods by a notable margin on the SURREAL dataset,
which has diverse body shapes. Code is available at
https://github.com/ShirleyMaxx/VirtualMarker.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Task-based Generation of Optimized Projection Sets using Differentiable
  Ranking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11724v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11724v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linda-Sophie Schneider, Mareike Thies, Christopher Syben, Richard Schielein, Mathias Unberath, Andreas Maier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a method for selecting valuable projections in computed tomography
(CT) scans to enhance image reconstruction and diagnosis. The approach
integrates two important factors, projection-based detectability and data
completeness, into a single feed-forward neural network. The network evaluates
the value of projections, processes them through a differentiable ranking
function and makes the final selection using a straight-through estimator. Data
completeness is ensured through the label provided during training. The
approach eliminates the need for heuristically enforcing data completeness,
which may exclude valuable projections. The method is evaluated on simulated
data in a non-destructive testing scenario, where the aim is to maximize the
reconstruction quality within a specified region of interest. We achieve
comparable results to previous methods, laying the foundation for using
reconstruction-based loss functions to learn the selection of projections.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Implicit Neural Representation for Cooperative Low-light Image
  Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11722v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11722v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuzhou Yang, Moxuan Ding, Yanmin Wu, Zihan Li, Jian Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The following three factors restrict the application of existing low-light
image enhancement methods: unpredictable brightness degradation and noise,
inherent gap between metric-favorable and visual-friendly versions, and the
limited paired training data. To address these limitations, we propose an
implicit Neural Representation method for Cooperative low-light image
enhancement, dubbed NeRCo. It robustly recovers perceptual-friendly results in
an unsupervised manner. Concretely, NeRCo unifies the diverse degradation
factors of real-world scenes with a controllable fitting function, leading to
better robustness. In addition, for the output results, we introduce
semantic-orientated supervision with priors from the pre-trained
vision-language model. Instead of merely following reference images, it
encourages results to meet subjective expectations, finding more
visual-friendly solutions. Further, to ease the reliance on paired data and
reduce solution space, we develop a dual-closed-loop constrained enhancement
module. It is trained cooperatively with other affiliated modules in a
self-supervised manner. Finally, extensive experiments demonstrate the
robustness and superior effectiveness of our proposed NeRCo. Our code is
available at https://github.com/Ysz2022/NeRCo.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lidar Line Selection with Spatially-Aware Shapley Value for
  Cost-Efficient Depth Completion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11720v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11720v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kamil Adamczewski, Christos Sakaridis, Vaishakh Patil, Luc Van Gool
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lidar is a vital sensor for estimating the depth of a scene. Typical spinning
lidars emit pulses arranged in several horizontal lines and the monetary cost
of the sensor increases with the number of these lines. In this work, we
present the new problem of optimizing the positioning of lidar lines to find
the most effective configuration for the depth completion task. We propose a
solution to reduce the number of lines while retaining the up-to-the-mark
quality of depth completion. Our method consists of two components, (1) line
selection based on the marginal contribution of a line computed via the Shapley
value and (2) incorporating line position spread to take into account its need
to arrive at image-wide depth completion. Spatially-aware Shapley values (SaS)
succeed in selecting line subsets that yield a depth accuracy comparable to the
full lidar input while using just half of the lines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Complete <span class="highlight-title">Survey</span> on Generative AI (AIGC): Is Chat<span class="highlight-title">GPT</span> from <span class="highlight-title">GPT</span>-4 to
  <span class="highlight-title">GPT</span>-5 All You Need? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11717v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11717v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaoning Zhang, Chenshuang Zhang, Sheng Zheng, Yu Qiao, Chenghao Li, Mengchun Zhang, Sumit Kumar Dam, Chu Myaet Thwal, Ye Lin Tun, Le Luang Huy, Donguk kim, Sung-Ho Bae, Lik-Hang Lee, Yang Yang, Heng Tao Shen, In So Kweon, Choong Seon Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As ChatGPT goes viral, generative AI (AIGC, a.k.a AI-generated content) has
made headlines everywhere because of its ability to analyze and create text,
images, and beyond. With such overwhelming media coverage, it is almost
impossible for us to miss the opportunity to glimpse AIGC from a certain angle.
In the era of AI transitioning from pure analysis to creation, it is worth
noting that ChatGPT, with its most recent language model GPT-4, is just a tool
out of numerous AIGC tasks. Impressed by the capability of the ChatGPT, many
people are wondering about its limits: can GPT-5 (or other future GPT variants)
help ChatGPT unify all AIGC tasks for diversified content creation? Toward
answering this question, a comprehensive review of existing AIGC tasks is
needed. As such, our work comes to fill this gap promptly by offering a first
look at AIGC, ranging from its techniques to applications. Modern generative AI
relies on various technical foundations, ranging from model architecture and
self-supervised pretraining to generative modeling methods (like GAN and
diffusion models). After introducing the fundamental techniques, this work
focuses on the technological development of various AIGC tasks based on their
output type, including text, images, videos, 3D content, etc., which depicts
the full potential of ChatGPT's future. Moreover, we summarize their
significant applications in some mainstream industries, such as education and
creativity content. Finally, we discuss the challenges currently faced and
present an outlook on how generative AI might evolve in the near future.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>56 pages, 548 citations</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Single-Step Multiclass SVM based on Quantum Annealing for Remote
  Sensing Data Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11705v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11705v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amer Delilbasic, Bertrand Le Saux, Morris Riedel, Kristel Michielsen, Gabriele Cavallaro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, the development of quantum annealers has enabled
experimental demonstrations and has increased research interest in applications
of quantum annealing, such as in quantum machine learning and in particular for
the popular quantum SVM. Several versions of the quantum SVM have been
proposed, and quantum annealing has been shown to be effective in them.
Extensions to multiclass problems have also been made, which consist of an
ensemble of multiple binary classifiers. This work proposes a novel quantum SVM
formulation for direct multiclass classification based on quantum annealing,
called Quantum Multiclass SVM (QMSVM). The multiclass classification problem is
formulated as a single Quadratic Unconstrained Binary Optimization (QUBO)
problem solved with quantum annealing. The main objective of this work is to
evaluate the feasibility, accuracy, and time performance of this approach.
Experiments have been performed on the D-Wave Advantage quantum annealer for a
classification problem on remote sensing data. The results indicate that,
despite the memory demands of the quantum annealer, QMSVM can achieve accuracy
that is comparable to standard SVM methods and, more importantly, it scales
much more efficiently with the number of training examples, resulting in nearly
constant time. This work shows an approach for bringing together classical and
quantum computation, solving practical problems in remote sensing with current
hardware.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 10 figures, 3 tables. Submitted to IEEE JSTARS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Linking generative semi-supervised learning and generative open-set
  recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11702v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11702v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emile Reyn Engelbrecht, Johan du Preez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study investigates the relationship between semi-supervised learning
(SSL) and open-set recognition (OSR) in the context of generative adversarial
networks (GANs). Although no previous study has formally linked SSL and OSR,
their respective methods share striking similarities. Specifically, SSL-GANs
and OSR-GANs require generator to produce samples in the complementary space.
Subsequently, by regularising networks with generated samples, both SSL and OSR
classifiers generalize the open space. To demonstrate the connection between
SSL and OSR, we theoretically and experimentally compare state-of-the-art
SSL-GAN methods with state-of-the-art OSR-GAN methods. Our results indicate
that the SSL optimised margin-GANs, which have a stronger foundation in
literature, set the new standard for the combined SSL-OSR task and achieves new
state-of-other art results in certain general OSR experiments. However, the OSR
optimised adversarial reciprocal point (ARP)-GANs still slightly out-performed
margin-GANs at other OSR experiments. This result indicates unique insights for
the combined optimisation task of SSL-OSR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A High-Frequency Focused Network for Lightweight Single Image
  Super-Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11701v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11701v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaotian Weng, Yi Chen, Zhichao Zheng, Yanhui Gu, Junsheng Zhou, Yudong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lightweight neural networks for single-image super-resolution (SISR) tasks
have made substantial breakthroughs in recent years. Compared to low-frequency
information, high-frequency detail is much more difficult to reconstruct. Most
SISR models allocate equal computational resources for low-frequency and
high-frequency information, which leads to redundant processing of simple
low-frequency information and inadequate recovery of more challenging
high-frequency information. We propose a novel High-Frequency Focused Network
(HFFN) through High-Frequency Focused Blocks (HFFBs) that selectively enhance
high-frequency information while minimizing redundant feature computation of
low-frequency information. The HFFB effectively allocates more computational
resources to the more challenging reconstruction of high-frequency information.
Moreover, we propose a Local Feature Fusion Block (LFFB) effectively fuses
features from multiple HFFBs in a local region, utilizing complementary
information across layers to enhance feature representativeness and reduce
artifacts in reconstructed images. We assess the efficacy of our proposed HFFN
on five benchmark datasets and show that it significantly enhances the
super-resolution performance of the network. Our experimental results
demonstrate state-of-the-art performance in reconstructing high-frequency
information while using a low number of parameters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Anchor Free remote sensing detector based on solving discrete polar
  coordinate equation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11694v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11694v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linfeng Shi, Yan Li, Xi Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the rapid development of depth learning, object detection in aviatic
remote sensing images has become increasingly popular in recent years. Most of
the current Anchor Free detectors based on key point detection sampling
directly regression and classification features, with the design of object loss
function based on the horizontal bounding box. It is more challenging for
complex and diverse aviatic remote sensing object. In this paper, we propose an
Anchor Free aviatic remote sensing object detector (BWP-Det) to detect rotating
and multi-scale object. Specifically, we design a interactive
double-branch(IDB) up-sampling network, in which one branch gradually
up-sampling is used for the prediction of Heatmap, and the other branch is used
for the regression of boundary box parameters. We improve a weighted
multi-scale convolution (WmConv) in order to highlight the difference between
foreground and background. We extracted Pixel level attention features from the
middle layer to guide the two branches to pay attention to effective object
information in the sampling process. Finally, referring to the calculation idea
of horizontal IoU, we design a rotating IoU based on the split polar coordinate
plane, namely JIoU, which is expressed as the intersection ratio following
discretization of the inner ellipse of the rotating bounding box, to solve the
correlation between angle and side length in the regression process of the
rotating bounding box. Ultimately, BWP-Det, our experiments on DOTA, UCAS-AOD
and NWPU VHR-10 datasets show, achieves advanced performance with simpler
models and fewer regression parameters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning a 3D Morphable Face Reflectance Model from Low-cost Data <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11686v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11686v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Han, Zhibo Wang, Feng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modeling non-Lambertian effects such as facial specularity leads to a more
realistic 3D Morphable Face Model. Existing works build parametric models for
diffuse and specular albedo using Light Stage data. However, only diffuse and
specular albedo cannot determine the full BRDF. In addition, the requirement of
Light Stage data is hard to fulfill for the research communities. This paper
proposes the first 3D morphable face reflectance model with spatially varying
BRDF using only low-cost publicly-available data. We apply linear shiness
weighting into parametric modeling to represent spatially varying specular
intensity and shiness. Then an inverse rendering algorithm is developed to
reconstruct the reflectance parameters from non-Light Stage data, which are
used to train an initial morphable reflectance model. To enhance the model's
generalization capability and expressive power, we further propose an
update-by-reconstruction strategy to finetune it on an in-the-wild dataset.
Experimental results show that our method obtains decent rendering results with
plausible facial specularities. Our code is released
\href{https://yxuhan.github.io/ReflectanceMM/index.html}{\textcolor{magenta}{here}}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023. Project page:
  https://yxuhan.github.io/ReflectanceMM/index.html</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SpikeCV: Open a Continuous Computer Vision Era 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11684v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11684v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yajing Zheng, Jiyuan Zhang, Rui Zhao, Jianhao Ding, Shiyan Chen, Ruiqin Xiong, Zhaofei Yu, Tiejun Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  SpikeCV is a new open-source computer vision platform for the spike camera,
which is a neuromorphic visual sensor that has developed rapidly in recent
years. In the spike camera, each pixel position directly accumulates the light
intensity and asynchronously fires spikes. The output binary spikes can reach a
frequency of 40,000 Hz. As a new type of visual expression, spike sequence has
high spatiotemporal completeness and preserves the continuous visual
information of the external world. Taking advantage of the low latency and high
dynamic range of the spike camera, many spike-based algorithms have made
significant progress, such as high-quality imaging and ultra-high-speed target
detection.
  To build up a community ecology for the spike vision to facilitate more users
to take advantage of the spike camera, SpikeCV provides a variety of
ultra-high-speed scene datasets, hardware interfaces, and an easy-to-use
modules library. SpikeCV focuses on encapsulation for spike data,
standardization for dataset interfaces, modularization for vision tasks, and
real-time applications for challenging scenes. With the advent of the
open-source Python ecosystem, modules of SpikeCV can be used as a Python
library to fulfilled most of the numerical analysis needs of researchers. We
demonstrate the efficiency of the SpikeCV on offline inference and real-time
applications. The project repository address are
\url{https://openi.pcl.ac.cn/Cordium/SpikeCV} and
\url{https://github.com/Zyj061/SpikeCV
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffuMask: Synthesizing Images with Pixel-level Annotations for Semantic
  Segmentation Using Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11681v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11681v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weijia Wu, Yuzhong Zhao, Mike Zheng Shou, Hong Zhou, Chunhua Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collecting and annotating images with pixel-wise labels is time-consuming and
laborious. In contrast, synthetic data can be freely available using a
generative model (e.g., DALL-E, Stable Diffusion). In this paper, we show that
it is possible to automatically obtain accurate semantic masks of synthetic
images generated by the Off-the-shelf Stable Diffusion model, which uses only
text-image pairs during training. Our approach, called DiffuMask, exploits the
potential of the cross-attention map between text and image, which is natural
and seamless to extend the text-driven image synthesis to semantic mask
generation. DiffuMask uses text-guided cross-attention information to localize
class/word-specific regions, which are combined with practical techniques to
create a novel high-resolution and class-discriminative pixel-wise mask. The
methods help to reduce data collection and annotation costs obviously.
Experiments demonstrate that the existing segmentation methods trained on
synthetic data of DiffuMask can achieve a competitive performance over the
counterpart of real data (VOC 2012, Cityscapes). For some classes (e.g., bird),
DiffuMask presents promising performance, close to the stateof-the-art result
of real data (within 3% mIoU gap). Moreover, in the open-vocabulary
segmentation (zero-shot) setting, DiffuMask achieves a new SOTA result on
Unseen class of VOC 2012. The project website can be found at
https://weijiawu.github.io/DiffusionMask/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Full or Weak annotations? An adaptive strategy for budget-constrained
  annotation campaigns <span class="chip">CVPR23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11678v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11678v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Javier Gamazo Tejero, Martin S. Zinkernagel, Sebastian Wolf, Raphael Sznitman, Pablo Márquez Neila
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Annotating new datasets for machine learning tasks is tedious,
time-consuming, and costly. For segmentation applications, the burden is
particularly high as manual delineations of relevant image content are often
extremely expensive or can only be done by experts with domain-specific
knowledge. Thanks to developments in transfer learning and training with weak
supervision, segmentation models can now also greatly benefit from annotations
of different kinds. However, for any new domain application looking to use weak
supervision, the dataset builder still needs to define a strategy to distribute
full segmentation and other weak annotations. Doing so is challenging, however,
as it is a priori unknown how to distribute an annotation budget for a given
new dataset. To this end, we propose a novel approach to determine annotation
strategies for segmentation datasets, whereby estimating what proportion of
segmentation and classification annotations should be collected given a fixed
budget. To do so, our method sequentially determines proportions of
segmentation and classification annotations to collect for budget-fractions by
modeling the expected improvement of the final segmentation model. We show in
our experiments that our approach yields annotations that perform very close to
the optimal for a number of different annotation budgets and datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning Pipeline for Preprocessing and Segmenting Cardiac Magnetic
  Resonance of Single Ventricle Patients from an Image Registry 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11676v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11676v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tina Yao, Nicole St. Clair, Gabriel F. Miller, Adam L. Dorfman, Mark A. Fogel, Sunil Ghelani, Rajesh Krishnamurthy, Christopher Z. Lam, Joshua D. Robinson, David Schidlow, Timothy C. Slesnick, Justin Weigand, Michael Quail, Rahul Rathod, Jennifer A. Steeden, Vivek Muthurangu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Purpose: To develop and evaluate an end-to-end deep learning pipeline for
segmentation and analysis of cardiac magnetic resonance images to provide
core-lab processing for a multi-centre registry of Fontan patients.
  Materials and Methods: This retrospective study used training (n = 175),
validation (n = 25) and testing (n = 50) cardiac magnetic resonance image exams
collected from 13 institutions in the UK, US and Canada. The data was used to
train and evaluate a pipeline containing three deep-learning models. The
pipeline's performance was assessed on the Dice and IoU score between the
automated and reference standard manual segmentation. Cardiac function values
were calculated from both the automated and manual segmentation and evaluated
using Bland-Altman analysis and paired t-tests. The overall pipeline was
further evaluated qualitatively on 475 unseen patient exams.
  Results: For the 50 testing dataset, the pipeline achieved a median Dice
score of 0.91 (0.89-0.94) for end-diastolic volume, 0.86 (0.82-0.89) for
end-systolic volume, and 0.74 (0.70-0.77) for myocardial mass. The deep
learning-derived end-diastolic volume, end-systolic volume, myocardial mass,
stroke volume and ejection fraction had no statistical difference compared to
the same values derived from manual segmentation with p values all greater than
0.05. For the 475 unseen patient exams, the pipeline achieved 68% adequate
segmentation in both systole and diastole, 26% needed minor adjustments in
either systole or diastole, 5% needed major adjustments, and the cropping model
only failed in 0.4%.
  Conclusion: Deep learning pipeline can provide standardised 'core-lab'
segmentation for Fontan patients. This pipeline can now be applied to the >4500
cardiac magnetic resonance exams currently in the FORCE registry as well as any
new patients that are recruited.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BoPR: Body-aware Part Regressor for Human Shape and Pose Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11675v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11675v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongkang Cheng, Shaoli Huang, Jifeng Ning, Ying Shan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel approach for estimating human body shape and pose
from monocular images that effectively addresses the challenges of occlusions
and depth ambiguity. Our proposed method BoPR, the Body-aware Part Regressor,
first extracts features of both the body and part regions using an
attention-guided mechanism. We then utilize these features to encode extra
part-body dependency for per-part regression, with part features as queries and
body feature as a reference. This allows our network to infer the spatial
relationship of occluded parts with the body by leveraging visible parts and
body reference information. Our method outperforms existing state-of-the-art
methods on two benchmark datasets, and our experiments show that it
significantly surpasses existing methods in terms of depth ambiguity and
occlusion handling. These results provide strong evidence of the effectiveness
of our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ALOFT: A Lightweight MLP-like Architecture with Dynamic Low-frequency
  Transform for Domain Generalization <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11674v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11674v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jintao Guo, Na Wang, Lei Qi, Yinghuan Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain generalization (DG) aims to learn a model that generalizes well to
unseen target domains utilizing multiple source domains without re-training.
Most existing DG works are based on convolutional neural networks (CNNs).
However, the local operation of the convolution kernel makes the model focus
too much on local representations (e.g., texture), which inherently causes the
model more prone to overfit to the source domains and hampers its
generalization ability. Recently, several MLP-based methods have achieved
promising results in supervised learning tasks by learning global interactions
among different patches of the image. Inspired by this, in this paper, we first
analyze the difference between CNN and MLP methods in DG and find that MLP
methods exhibit a better generalization ability because they can better capture
the global representations (e.g., structure) than CNN methods. Then, based on a
recent lightweight MLP method, we obtain a strong baseline that outperforms
most state-of-the-art CNN-based methods. The baseline can learn global
structure representations with a filter to suppress structure irrelevant
information in the frequency space. Moreover, we propose a dynAmic
LOw-Frequency spectrum Transform (ALOFT) that can perturb local texture
features while preserving global structure features, thus enabling the filter
to remove structure-irrelevant information sufficiently. Extensive experiments
on four benchmarks have demonstrated that our method can achieve great
performance improvement with a small number of parameters compared to SOTA
CNN-based DG methods. Our code is available at
https://github.com/lingeringlight/ALOFT/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2023. The code is available at
  https://github.com/lingeringlight/ALOFT/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Focus or Not: A Baseline for Anomaly Event Detection On the Open Public
  Places with Satellite Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11668v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11668v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongjin Jeon, Youngtack Oh, Doyoung Jeong, Hyunguk Choi, Junsik Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, monitoring the world wide area with satellite images has
been emerged as an important issue.
  Site monitoring task can be divided into two independent tasks; 1) Change
Detection and 2) Anomaly Event Detection.
  Unlike to change detection research is actively conducted based on the
numerous datasets(\eg LEVIR-CD, WHU-CD, S2Looking, xView2 and etc...) to meet
up the expectations of industries or governments, research on AI models for
detecting anomaly events is passively and rarely conducted.
  In this paper, we introduce a novel satellite imagery dataset(AED-RS) for
detecting anomaly events on the open public places.
  AED-RS Dataset contains satellite images of normal and abnormal situations of
8 open public places from all over the world.
  Each places are labeled with different criteria based on the difference of
characteristics of each places.
  With this dataset, we introduce a baseline model for our dataset TB-FLOW,
which can be trained in weakly-supervised manner and shows reasonable
performance on the AED-RS Dataset compared with the other NF(Normalizing-Flow)
based anomaly detection models. Our dataset and code will be publicly open in
\url{https://github.com/SIAnalytics/RS_AnomalyDetection.git}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advanced Multi-Microscopic Views Cell Semi-supervised Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11661v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11661v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fang Hu, Xuexue Sun, Ke Qing, Fenxi Xiao, Zhi Wang, Xiaolu Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although deep learning (DL) shows powerful potential in cell segmentation
tasks, it suffers from poor generalization as DL-based methods originally
simplified cell segmentation in detecting cell membrane boundary, lacking
prominent cellular structures to position overall differentiating. Moreover,
the scarcity of annotated cell images limits the performance of DL models.
Segmentation limitations of a single category of cell make massive practice
difficult, much less, with varied modalities. In this paper, we introduce a
novel semi-supervised cell segmentation method called Multi-Microscopic-view
Cell semi-supervised Segmentation (MMCS), which can train cell segmentation
models utilizing less labeled multi-posture cell images with different
microscopy well. Technically, MMCS consists of Nucleus-assisted global
recognition, Self-adaptive diameter filter, and Temporal-ensembling models.
Nucleus-assisted global recognition adds additional cell nucleus channel to
improve the global distinguishing performance of fuzzy cell membrane boundaries
even when cells aggregate. Besides, self-adapted cell diameter filter can help
separate multi-resolution cells with different morphology properly. It further
leverages the temporal-ensembling models to improve the semi-supervised
training process, achieving effective training with less labeled data.
Additionally, optimizing the weight of unlabeled loss contributed to total loss
also improve the model performance. Evaluated on the Tuning Set of NeurIPS 2022
Cell Segmentation Challenge (NeurIPS CellSeg), MMCS achieves an F1-score of
0.8239 and the running time for all cases is within the time tolerance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating climate and health impact of small-scale kiln industry using
  multi-spectral classifier and deep learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11654v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11654v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Usman Nazir, Murtaza Taj, Momin Uppal, Sara Khalid
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Industrial air pollution has a direct health impact and is a major
contributor to climate change. Small scale industries particularly bull-trench
brick kilns are one of the major causes of air pollution in South Asia often
creating hazardous levels of smog that is injurious to human health. To
mitigate the climate and health impact of the kiln industry, fine-grained kiln
localization at different geographic locations is needed. Kiln localization
using multi-spectral remote sensing data such as vegetation index results in a
noisy estimates whereas use of high-resolution imagery is infeasible due to
cost and compute complexities. This paper proposes a fusion of spatio-temporal
multi-spectral data with high-resolution imagery for detection of brick kilns
within the "Brick-Kiln-Belt" of South Asia. We first perform classification
using low-resolution spatio-temporal multi-spectral data from Sentinel-2
imagery by combining vegetation, burn, build up and moisture indices. Then
orientation aware object detector: YOLOv3 (with theta value) is implemented for
removal of false detections and fine-grained localization. Our proposed
technique, when compared with other benchmarks, results in a 21x improvement in
speed with comparable or higher accuracy when tested over multiple countries.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoopInit: Initializing Generative Adversarial Networks via Cooperative
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11649v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11649v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Zhao, Jianwen Xie, Ping Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Numerous research efforts have been made to stabilize the training of the
Generative Adversarial Networks (GANs), such as through regularization and
architecture design. However, we identify the instability can also arise from
the fragile balance at the early stage of adversarial learning. This paper
proposes the CoopInit, a simple yet effective cooperative learning-based
initialization strategy that can quickly learn a good starting point for GANs,
with a very small computation overhead during training. The proposed algorithm
consists of two learning stages: (i) Cooperative initialization stage: The
discriminator of GAN is treated as an energy-based model (EBM) and is optimized
via maximum likelihood estimation (MLE), with the help of the GAN's generator
to provide synthetic data to approximate the learning gradients. The EBM also
guides the MLE learning of the generator via MCMC teaching; (ii) Adversarial
finalization stage: After a few iterations of initialization, the algorithm
seamlessly transits to the regular mini-max adversarial training until
convergence. The motivation is that the MLE-based initialization stage drives
the model towards mode coverage, which is helpful in alleviating the issue of
mode dropping during the adversarial learning stage. We demonstrate the
effectiveness of the proposed approach on image generation and one-sided
unpaired image-to-image translation tasks through extensive experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages of main text, 2 pages of references</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visibility Constrained Wide-band Illumination Spectrum Design for
  Seeing-in-the-Dark <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11642v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11642v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muyao Niu, Zhuoxiao Li, Zhihang Zhong, Yinqiang Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Seeing-in-the-dark is one of the most important and challenging computer
vision tasks due to its wide applications and extreme complexities of
in-the-wild scenarios. Existing arts can be mainly divided into two threads: 1)
RGB-dependent methods restore information using degraded RGB inputs only (\eg,
low-light enhancement), 2) RGB-independent methods translate images captured
under auxiliary near-infrared (NIR) illuminants into RGB domain (\eg, NIR2RGB
translation). The latter is very attractive since it works in complete darkness
and the illuminants are visually friendly to naked eyes, but tends to be
unstable due to its intrinsic ambiguities. In this paper, we try to robustify
NIR2RGB translation by designing the optimal spectrum of auxiliary illumination
in the wide-band VIS-NIR range, while keeping visual friendliness. Our core
idea is to quantify the visibility constraint implied by the human vision
system and incorporate it into the design pipeline. By modeling the formation
process of images in the VIS-NIR range, the optimal multiplexing of a wide
range of LEDs is automatically designed in a fully differentiable manner,
within the feasible region defined by the visibility constraint. We also
collect a substantially expanded VIS-NIR hyperspectral image dataset for
experiments by using a customized 50-band filter wheel. Experimental results
show that the task can be significantly improved by using the optimized
wide-band illumination than using NIR only. Codes Available:
https://github.com/MyNiuuu/VCSD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Human Pose as Compositional Tokens <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11638v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11638v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zigang Geng, Chunyu Wang, Yixuan Wei, Ze Liu, Houqiang Li, Han Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human pose is typically represented by a coordinate vector of body joints or
their heatmap embeddings. While easy for data processing, unrealistic pose
estimates are admitted due to the lack of dependency modeling between the body
joints. In this paper, we present a structured representation, named Pose as
Compositional Tokens (PCT), to explore the joint dependency. It represents a
pose by M discrete tokens with each characterizing a sub-structure with several
interdependent joints. The compositional design enables it to achieve a small
reconstruction error at a low cost. Then we cast pose estimation as a
classification task. In particular, we learn a classifier to predict the
categories of the M tokens from an image. A pre-learned decoder network is used
to recover the pose from the tokens without further post-processing. We show
that it achieves better or comparable pose estimation results as the existing
methods in general scenarios, yet continues to work well when occlusion occurs,
which is ubiquitous in practice. The code and models are publicly available at
https://github.com/Gengzigang/PCT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Equiangular Basis Vectors <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11637v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11637v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Shen, Xuhao Sun, Xiu-Shen Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Equiangular Basis Vectors (EBVs) for classification tasks. In deep
neural networks, models usually end with a k-way fully connected layer with
softmax to handle different classification tasks. The learning objective of
these methods can be summarized as mapping the learned feature representations
to the samples' label space. While in metric learning approaches, the main
objective is to learn a transformation function that maps training data points
from the original space to a new space where similar points are closer while
dissimilar points become farther apart. Different from previous methods, our
EBVs generate normalized vector embeddings as "predefined classifiers" which
are required to not only be with the equal status between each other, but also
be as orthogonal as possible. By minimizing the spherical distance of the
embedding of an input between its categorical EBV in training, the predictions
can be obtained by identifying the categorical EBV with the smallest distance
during inference. Various experiments on the ImageNet-1K dataset and other
downstream tasks demonstrate that our method outperforms the general fully
connected classifier while it does not introduce huge additional computation
compared with classical metric learning methods. Our EBVs won the first place
in the 2022 DIGIX Global AI Challenge, and our code is open-source and
available at https://github.com/NJUST-VIPGroup/Equiangular-Basis-Vectors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Context-aware Classifier for Semantic Segmentation <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11633v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11633v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuotao Tian, Jiequan Cui, Li Jiang, Xiaojuan Qi, Xin Lai, Yixin Chen, Shu Liu, Jiaya Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic segmentation is still a challenging task for parsing diverse
contexts in different scenes, thus the fixed classifier might not be able to
well address varying feature distributions during testing. Different from the
mainstream literature where the efficacy of strong backbones and effective
decoder heads has been well studied, in this paper, additional contextual hints
are instead exploited via learning a context-aware classifier whose content is
data-conditioned, decently adapting to different latent distributions. Since
only the classifier is dynamically altered, our method is model-agnostic and
can be easily applied to generic segmentation models. Notably, with only
negligible additional parameters and +2\% inference time, decent performance
gain has been achieved on both small and large models with challenging
benchmarks, manifesting substantial practical merits brought by our simple yet
effective method. The implementation is available at
\url{https://github.com/tianzhuotao/CAC}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2023. Code and models are available at
  https://github.com/tianzhuotao/CAC</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Embarrassingly Simple Approach for Wafer Feature Extraction and
  Defect Pattern Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11632v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11632v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nitish Shukla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Identifying defect patterns in a wafer map during manufacturing is crucial to
find the root cause of the underlying issue and provides valuable insights on
improving yield in the foundry. Currently used methods use deep neural networks
to identify the defects. These methods are generally very huge and have
significant inference time. They also require GPU support to efficiently
operate. All these issues make these models not fit for on-line prediction in
the manufacturing foundry. In this paper, we propose an extremely simple yet
effective technique to extract features from wafer images. The proposed method
is extremely fast, intuitive, and non-parametric while being explainable. The
experiment results show that the proposed pipeline outperforms conventional
deep learning models. Our feature extraction requires no training or
fine-tuning while preserving the relative shape and location of data points as
revealed by our interpretability analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BoxSnake: Polygonal Instance Segmentation with Box Supervision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11630v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11630v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Yang, Lin Song, Yixiao Ge, Xiu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Box-supervised instance segmentation has gained much attention as it requires
only simple box annotations instead of costly mask or polygon annotations.
However, existing box-supervised instance segmentation models mainly focus on
mask-based frameworks. We propose a new end-to-end training technique, termed
BoxSnake, to achieve effective polygonal instance segmentation using only box
annotations for the first time. Our method consists of two loss functions: (1)
a point-based unary loss that constrains the bounding box of predicted polygons
to achieve coarse-grained segmentation; and (2) a distance-aware pairwise loss
that encourages the predicted polygons to fit the object boundaries. Compared
with the mask-based weakly-supervised methods, BoxSnake further reduces the
performance gap between the predicted segmentation and the bounding box, and
shows significant superiority on the Cityscapes dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TMA: Temporal Motion Aggregation for Event-based Optical Flow 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11629v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11629v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haotian Liu, Guang Chen, Sanqing Qu, Yanping Zhang, Zhijun Li, Alois Knoll, Changjun Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event cameras have the ability to record continuous and detailed trajectories
of objects with high temporal resolution, thereby providing intuitive motion
cues for optical flow estimation. Nevertheless, most existing learning-based
approaches for event optical flow estimation directly remould the paradigm of
conventional images by representing the consecutive event stream as static
frames, ignoring the inherent temporal continuity of event data. In this paper,
we argue that temporal continuity is a vital element of event-based optical
flow and propose a novel Temporal Motion Aggregation (TMA) approach to unlock
its potential. Technically, TMA comprises three components: an event splitting
strategy to incorporate intermediate motion information underlying the temporal
context, a linear lookup strategy to align temporally continuous motion
features and a novel motion pattern aggregation module to emphasize consistent
patterns for motion feature enhancement. By incorporating temporally continuous
motion information, TMA can derive better flow estimates than existing methods
at early stages, which not only enables TMA to obtain more accurate final
predictions, but also greatly reduces the demand for a number of refinements.
Extensive experiments on DESC-Flow and MVSEC datasets verify the effectiveness
and superiority of our TMA. Remarkably, compared to E-RAFT, TMA achieves a 6%
improvement in accuracy and a 40% reduction in inference time on DSEC-Flow.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Information-containing Adversarial Perturbation for Combating Facial
  Manipulation Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11625v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11625v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yao Zhu, Yuefeng Chen, Xiaodan Li, Rong Zhang, Xiang Tian, Bolun Zheng, Yaowu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the development of deep learning technology, the facial manipulation
system has become powerful and easy to use. Such systems can modify the
attributes of the given facial images, such as hair color, gender, and age.
Malicious applications of such systems pose a serious threat to individuals'
privacy and reputation. Existing studies have proposed various approaches to
protect images against facial manipulations. Passive defense methods aim to
detect whether the face is real or fake, which works for posterior forensics
but can not prevent malicious manipulation. Initiative defense methods protect
images upfront by injecting adversarial perturbations into images to disrupt
facial manipulation systems but can not identify whether the image is fake. To
address the limitation of existing methods, we propose a novel two-tier
protection method named Information-containing Adversarial Perturbation (IAP),
which provides more comprehensive protection for {facial images}. We use an
encoder to map a facial image and its identity message to a cross-model
adversarial example which can disrupt multiple facial manipulation systems to
achieve initiative protection. Recovering the message in adversarial examples
with a decoder serves passive protection, contributing to provenance tracking
and fake image detection. We introduce a feature-level correlation measurement
that is more suitable to measure the difference between the facial images than
the commonly used mean squared error. Moreover, we propose a spectral diffusion
method to spread messages to different frequency channels, thereby improving
the robustness of the message against facial manipulation. Extensive
experimental results demonstrate that our proposed IAP can recover the messages
from the adversarial examples with high average accuracy and effectively
disrupt the facial manipulation systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>\copyright 20XX IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detecting the open-world objects with the help of the Brain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11623v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11623v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuailei Ma, Yuefeng Wang, Ying Wei, Peihao Chen, Zhixiang Ye, Jiaqi Fan, Enming Zhang, Thomas H. Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open World Object Detection (OWOD) is a novel computer vision task with a
considerable challenge, bridging the gap between classic object detection (OD)
benchmarks and real-world object detection. In addition to detecting and
classifying seen/known objects, OWOD algorithms are expected to detect
unseen/unknown objects and incrementally learn them. The natural instinct of
humans to identify unknown objects in their environments mainly depends on
their brains' knowledge base. It is difficult for a model to do this only by
learning from the annotation of several tiny datasets. The large pre-trained
grounded language-image models - VL (\ie GLIP) have rich knowledge about the
open world but are limited to the text prompt. We propose leveraging the VL as
the ``Brain'' of the open-world detector by simply generating unknown labels.
Leveraging it is non-trivial because the unknown labels impair the model's
learning of known objects. In this paper, we alleviate these problems by
proposing the down-weight loss function and decoupled detection structure.
Moreover, our detector leverages the ``Brain'' to learn novel objects beyond VL
through our pseudo-labeling scheme.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2301.01970</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HRDFuse: Monocular 360°Depth Estimation by Collaboratively Learning
  Holistic-with-Regional Depth Distributions <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11616v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11616v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Ai, Zidong cao, Yan-pei Cao, Ying Shan, Lin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Depth estimation from a monocular 360{\deg} image is a burgeoning problem
owing to its holistic sensing of a scene. Recently, some methods, \eg,
OmniFusion, have applied the tangent projection (TP) to represent a
360{\deg}image and predicted depth values via patch-wise regressions, which are
merged to get a depth map with equirectangular projection (ERP) format.
However, these methods suffer from 1) non-trivial process of merging plenty of
patches; 2) capturing less holistic-with-regional contextual information by
directly regressing the depth value of each pixel. In this paper, we propose a
novel framework, \textbf{HRDFuse}, that subtly combines the potential of
convolutional neural networks (CNNs) and transformers by collaboratively
learning the \textit{holistic} contextual information from the ERP and the
\textit{regional} structural information from the TP. Firstly, we propose a
spatial feature alignment (\textbf{SFA}) module that learns feature
similarities between the TP and ERP to aggregate the TP features into a
complete ERP feature map in a pixel-wise manner. Secondly, we propose a
collaborative depth distribution classification (\textbf{CDDC}) module that
learns the \textbf{holistic-with-regional} histograms capturing the ERP and TP
depth distributions. As such, the final depth values can be predicted as a
linear combination of histogram bin centers. Lastly, we adaptively combine the
depth predictions from ERP and TP to obtain the final depth map. Extensive
experiments show that our method predicts\textbf{ more smooth and accurate
depth} results while achieving \textbf{favorably better} results than the SOTA
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at CVPR2023, 20 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Table Structure Recognition with Dynamic Queries Enhanced
  Detection <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11615v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11615v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawei Wang, Weihong Lin, Chixiang Ma, Mingze Li, Zheng Sun, Lei Sun, Qiang Huo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a new table structure recognition (TSR) approach, called
TSRFormer, to robustly recognizing the structures of complex tables with
geometrical distortions from various table images. Unlike previous methods, we
formulate table separation line prediction as a line regression problem instead
of an image segmentation problem and propose a new two-stage dynamic queries
enhanced DETR based separation line regression approach, named DQ-DETR, to
predict separation lines from table images directly. Compared to Vallina DETR,
we propose three improvements in DQ-DETR to make the two-stage DETR framework
work efficiently and effectively for the separation line prediction task: 1) A
new query design, named Dynamic Query, to decouple single line query into
separable point queries which could intuitively improve the localization
accuracy for regression tasks; 2) A dynamic queries based progressive line
regression approach to progressively regressing points on the line which
further enhances localization accuracy for distorted tables; 3) A
prior-enhanced matching strategy to solve the slow convergence issue of DETR.
After separation line prediction, a simple relation network based cell merging
module is used to recover spanning cells. With these new techniques, our
TSRFormer achieves state-of-the-art performance on several benchmark datasets,
including SciTSR, PubTabNet, WTW and FinTabNet. Furthermore, we have validated
the robustness and high localization accuracy of our approach to tables with
complex structures, borderless cells, large blank spaces, empty or spanning
cells as well as distorted or even curved shapes on a more challenging
real-world in-house dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 11 figures, Preprint. arXiv admin note: substantial text
  overlap with arXiv:2208.04921</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Model Robustness Meets Data Privacy: Adversarial Robustness Distillation
  without Original Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11611v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11611v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuzheng Wang, Zhaoyu Chen, Dingkang Yang, Pinxue Guo, Kaixun Jiang, Wenqiang Zhang, Lizhe Qi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale deep learning models have achieved great performance based on
large-scale datasets. Moreover, the existing Adversarial Training (AT) can
further improve the robustness of these large models. However, these large
models are difficult to deploy to mobile devices, and the effect of AT on small
models is very limited. In addition, the data privacy issue (e.g., face data
and diagnosis report) may lead to the original data being unavailable, which
relies on data-free knowledge distillation technology for training. To tackle
these issues, we propose a challenging novel task called Data-Free Adversarial
Robustness Distillation (DFARD), which tries to train small, easily deployable,
robust models without relying on the original data. We find the combination of
existing techniques resulted in degraded model performance due to fixed
training objectives and scarce information content. First, an interactive
strategy is designed for more efficient knowledge transfer to find more
suitable training objectives at each epoch. Then, we explore an adaptive
balance method to suppress information loss and obtain more data information
than previous methods. Experiments show that our method improves baseline
performance on the novel task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Novel Class Discovery for 3D Point Cloud Semantic Segmentation <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11610v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11610v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luigi Riz, Cristiano Saltori, Elisa Ricci, Fabio Poiesi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Novel class discovery (NCD) for semantic segmentation is the task of learning
a model that can segment unlabelled (novel) classes using only the supervision
from labelled (base) classes. This problem has recently been pioneered for 2D
image data, but no work exists for 3D point cloud data. In fact, the
assumptions made for 2D are loosely applicable to 3D in this case. This paper
is presented to advance the state of the art on point cloud data analysis in
four directions. Firstly, we address the new problem of NCD for point cloud
semantic segmentation. Secondly, we show that the transposition of the only
existing NCD method for 2D semantic segmentation to 3D data is suboptimal.
Thirdly, we present a new method for NCD based on online clustering that
exploits uncertainty quantification to produce prototypes for pseudo-labelling
the points of the novel classes. Lastly, we introduce a new evaluation protocol
to assess the performance of NCD for point cloud semantic segmentation. We
thoroughly evaluate our method on SemanticKITTI and SemanticPOSS datasets,
showing that it can significantly outperform the baseline. Project page at this
link: https://github.com/LuigiRiz/NOPS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper accepted at CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CAFS: Class Adaptive Framework for Semi-Supervised Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11606v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11606v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingi Ju, Hyeoncheol Noh, Yooseung Wang, Minseok Seo, Dong-Geol Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semi-supervised semantic segmentation learns a model for classifying pixels
into specific classes using a few labeled samples and numerous unlabeled
images. The recent leading approach is consistency regularization by
selftraining with pseudo-labeling pixels having high confidences for unlabeled
images. However, using only highconfidence pixels for self-training may result
in losing much of the information in the unlabeled datasets due to poor
confidence calibration of modern deep learning networks. In this paper, we
propose a class-adaptive semisupervision framework for semi-supervised semantic
segmentation (CAFS) to cope with the loss of most information that occurs in
existing high-confidence-based pseudolabeling methods. Unlike existing
semi-supervised semantic segmentation frameworks, CAFS constructs a validation
set on a labeled dataset, to leverage the calibration performance for each
class. On this basis, we propose a calibration aware class-wise adaptive
thresholding and classwise adaptive oversampling using the analysis results
from the validation set. Our proposed CAFS achieves state-ofthe-art performance
on the full data partition of the base PASCAL VOC 2012 dataset and on the 1/4
data partition of the Cityscapes dataset with significant margins of 83.0% and
80.4%, respectively. The code is available at https://github.com/cjf8899/CAFS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning for Video-based Person Re-Identification: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11332v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11332v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khawar Islam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video-based person re-identification (video re-ID) has lately fascinated
growing attention due to its broad practical applications in various areas,
such as surveillance, smart city, and public safety. Nevertheless, video re-ID
is quite difficult and is an ongoing stage due to numerous uncertain challenges
such as viewpoint, occlusion, pose variation, and uncertain video sequence,
etc. In the last couple of years, deep learning on video re-ID has continuously
achieved surprising results on public datasets, with various approaches being
developed to handle diverse problems in video re-ID. Compared to image-based
re-ID, video re-ID is much more challenging and complex. To encourage future
research and challenges, this first comprehensive paper introduces a review of
up-to-date advancements in deep learning approaches for video re-ID. It broadly
covers three important aspects, including brief video re-ID methods with their
limitations, major milestones with technical challenges, and architectural
design. It offers comparative performance analysis on various available
datasets, guidance to improve video re-ID with valuable thoughts, and exciting
research directions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Low-complexity Deep Video Compression with A Distributed Coding
  Architecture <span class="chip">ICME 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11599v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11599v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinjie Zhang, Jiawei Shao, Jun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prevalent predictive coding-based video compression methods rely on a heavy
encoder to reduce the temporal redundancy, which makes it challenging to deploy
them on resource-constrained devices. Meanwhile, as early as the 1970s,
distributed source coding theory has indicated that independent encoding and
joint decoding with side information (SI) can achieve high-efficient
compression of correlated sources. This has inspired a distributed coding
architecture aiming at reducing the encoding complexity. However, traditional
distributed coding methods suffer from a substantial performance gap to
predictive coding ones. Inspired by the great success of learning-based
compression, we propose the first end-to-end distributed deep video compression
framework to improve the rate-distortion performance. A key ingredient is an
effective SI generation module at the decoder, which helps to effectively
exploit inter-frame correlations without computation-intensive encoder-side
motion estimation and compensation. Experiments show that our method
significantly outperforms conventional distributed video coding and H.264.
Meanwhile, it enjoys 6-7x encoding speedup against DVC [1] with comparable
compression performance. Code is released at
https://github.com/Xinjie-Q/Distributed-DVC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICME 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lightweight Hybrid Video Compression Framework Using Reference-Guided
  Restoration Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11592v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11592v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hochang Rhee, Seyun Kim, Nam Ik Cho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent deep-learning-based video compression methods brought coding gains
over conventional codecs such as AVC and HEVC. However, learning-based codecs
generally require considerable computation time and model complexity. In this
paper, we propose a new lightweight hybrid video codec consisting of a
conventional video codec(HEVC / VVC), a lossless image codec, and our new
restoration network. Precisely, our encoder consists of the conventional video
encoder and a lossless image encoder, transmitting a lossy-compressed video
bitstream along with a losslessly-compressed reference frame. The decoder is
constructed with corresponding video/image decoders and a new restoration
network, which enhances the compressed video in two-step processes. In the
first step, a network trained with a large video dataset restores the details
lost by the conventional encoder. Then, we further boost the video quality with
the guidance of a reference image, which is a losslessly compressed video
frame. The reference image provides video-specific information, which can be
utilized to better restore the details of a compressed video. Experimental
results show that the proposed method achieves comparable performance to
top-tier methods, even when applied to HEVC. Nevertheless, our method has lower
complexity, a faster run time, and can be easily integrated into existing
conventional codecs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SVCNet: Scribble-based Video Colorization Network with Temporal
  Aggregation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11591v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11591v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuzhi Zhao, Lai-Man Po, Kangcheng Liu, Xuehui Wang, Wing-Yin Yu, Pengfei Xian, Yujia Zhang, Mengyang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a scribble-based video colorization network with
temporal aggregation called SVCNet. It can colorize monochrome videos based on
different user-given color scribbles. It addresses three common issues in the
scribble-based video colorization area: colorization vividness, temporal
consistency, and color bleeding. To improve the colorization quality and
strengthen the temporal consistency, we adopt two sequential sub-networks in
SVCNet for precise colorization and temporal smoothing, respectively. The first
stage includes a pyramid feature encoder to incorporate color scribbles with a
grayscale frame, and a semantic feature encoder to extract semantics. The
second stage finetunes the output from the first stage by aggregating the
information of neighboring colorized frames (as short-range connections) and
the first colorized frame (as a long-range connection). To alleviate the color
bleeding artifacts, we learn video colorization and segmentation
simultaneously. Furthermore, we set the majority of operations on a fixed small
image resolution and use a Super-resolution Module at the tail of SVCNet to
recover original sizes. It allows the SVCNet to fit different image resolutions
at the inference. Finally, we evaluate the proposed SVCNet on DAVIS and Videvo
benchmarks. The experimental results demonstrate that SVCNet produces both
higher-quality and more temporally consistent videos than other well-known
video colorization approaches. The codes and models can be found at
https://github.com/zhaoyuzhi/SVCNet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under revision of IEEE Transactions on Image Processing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LayoutDiffusion: Improving Graphic Layout Generation by Discrete
  Diffusion Probabilistic Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11589v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11589v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyi Zhang, Jiaqi Guo, Shizhao Sun, Jian-Guang Lou, Dongmei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Creating graphic layouts is a fundamental step in graphic designs. In this
work, we present a novel generative model named LayoutDiffusion for automatic
layout generation. As layout is typically represented as a sequence of discrete
tokens, LayoutDiffusion models layout generation as a discrete denoising
diffusion process. It learns to reverse a mild forward process, in which
layouts become increasingly chaotic with the growth of forward steps and
layouts in the neighboring steps do not differ too much. Designing such a mild
forward process is however very challenging as layout has both categorical
attributes and ordinal attributes. To tackle the challenge, we summarize three
critical factors for achieving a mild forward process for the layout, i.e.,
legality, coordinate proximity and type disruption. Based on the factors, we
propose a block-wise transition matrix coupled with a piece-wise linear noise
schedule. Experiments on RICO and PubLayNet datasets show that LayoutDiffusion
outperforms state-of-the-art approaches significantly. Moreover, it enables two
conditional layout generation tasks in a plug-and-play manner without
re-training and achieves better performance than existing methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 20 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automated deep learning segmentation of high-resolution 7 T ex vivo MRI
  for quantitative analysis of structure-pathology correlations in
  neurodegenerative diseases 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12237v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12237v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pulkit Khandelwal, Michael Tran Duong, Shokufeh Sadaghiani, Sydney Lim, Amanda Denning, Eunice Chung, Sadhana Ravikumar, Sanaz Arezoumandan, Claire Peterson, Madigan Bedard, Noah Capp, Ranjit Ittyerah, Elyse Migdal, Grace Choi, Emily Kopp, Bridget Loja, Eusha Hasan, Jiacheng Li, Karthik Prabhakaran, Gabor Mizsei, Marianna Gabrielyan, Theresa Schuck, Winifred Trotman, John Robinson, Daniel Ohm, Edward B. Lee, John Q. Trojanowski, Corey McMillan, Murray Grossman, David J. Irwin, John Detre, M. Dylan Tisdall, Sandhitsu R. Das, Laura E. M. Wisse, David A. Wolk, Paul A. Yushkevich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ex vivo MRI of the brain provides remarkable advantages over in vivo MRI for
visualizing and characterizing detailed neuroanatomy, and helps to link
microscale histology studies with morphometric measurements. However, automated
segmentation methods for brain mapping in ex vivo MRI are not well developed,
primarily due to limited availability of labeled datasets, and heterogeneity in
scanner hardware and acquisition protocols. In this work, we present a high
resolution dataset of 37 ex vivo post-mortem human brain tissue specimens
scanned on a 7T whole-body MRI scanner. We developed a deep learning pipeline
to segment the cortical mantle by benchmarking the performance of nine deep
neural architectures. We then segment the four subcortical structures: caudate,
putamen, globus pallidus, and thalamus; white matter hyperintensities, and the
normal appearing white matter. We show excellent generalizing capabilities
across whole brain hemispheres in different specimens, and also on unseen
images acquired at different magnetic field strengths and different imaging
sequence. We then compute volumetric and localized cortical thickness
measurements across key regions, and link them with semi-quantitative
neuropathological ratings. Our code, containerized executables, and the
processed datasets are publicly available at:
https://github.com/Pulkit-Khandelwal/upenn-picsl-brain-ex-vivo.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint submitted to NeuroImage Project website:
  https://github.com/Pulkit-Khandelwal/upenn-picsl-brain-ex-vivo</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SALAD: Part-Level Latent Diffusion for 3D Shape Generation and
  Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12236v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12236v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juil Koo, Seungwoo Yoo, Minh Hieu Nguyen, Minhyuk Sung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a cascaded diffusion model based on a part-level implicit 3D
representation. Our model achieves state-of-the-art generation quality and also
enables part-level shape editing and manipulation without any additional
training in conditional setup. Diffusion models have demonstrated impressive
capabilities in data generation as well as zero-shot completion and editing via
a guided reverse process. Recent research on 3D diffusion models has focused on
improving their generation capabilities with various data representations,
while the absence of structural information has limited their capability in
completion and editing tasks. We thus propose our novel diffusion model using a
part-level implicit representation. To effectively learn diffusion with
high-dimensional embedding vectors of parts, we propose a cascaded framework,
learning diffusion first on a low-dimensional subspace encoding extrinsic
parameters of parts and then on the other high-dimensional subspace encoding
intrinsic attributes. In the experiments, we demonstrate the outperformance of
our method compared with the previous ones both in generation and part-level
completion and manipulation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://salad3d.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pre-NeRF 360: Enriching Unbounded Appearances for Neural Radiance Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12234v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12234v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmad AlMughrabi, Umair Haroon, Ricardo Marques, Petia Radeva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural radiance fields (NeRF) appeared recently as a powerful tool to
generate realistic views of objects and confined areas. Still, they face
serious challenges with open scenes, where the camera has unrestricted movement
and content can appear at any distance. In such scenarios, current
NeRF-inspired models frequently yield hazy or pixelated outputs, suffer slow
training times, and might display irregularities, because of the challenging
task of reconstructing an extensive scene from a limited number of images. We
propose a new framework to boost the performance of NeRF-based architectures
yielding significantly superior outcomes compared to the prior work. Our
solution overcomes several obstacles that plagued earlier versions of NeRF,
including handling multiple video inputs, selecting keyframes, and extracting
poses from real-world frames that are ambiguous and symmetrical. Furthermore,
we applied our framework, dubbed as "Pre-NeRF 360", to enable the use of the
Nutrition5k dataset in NeRF and introduce an updated version of this dataset,
known as the N5k360 dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Compositional 3D Scene Generation using Locally Conditioned Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12218v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12218v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryan Po, Gordon Wetzstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Designing complex 3D scenes has been a tedious, manual process requiring
domain expertise. Emerging text-to-3D generative models show great promise for
making this task more intuitive, but existing approaches are limited to
object-level generation. We introduce \textbf{locally conditioned diffusion} as
an approach to compositional scene diffusion, providing control over semantic
parts using text prompts and bounding boxes while ensuring seamless transitions
between these parts. We demonstrate a score distillation sampling--based
text-to-3D synthesis pipeline that enables compositional 3D scene generation at
a higher fidelity than relevant baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>For project page, see https://ryanpo.com/comp3d/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Image Reconstruction without Explicit Priors <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12217v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12217v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Angela F. Gao, Oscar Leong, He Sun, Katherine L. Bouman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider solving ill-posed imaging inverse problems without access to an
explicit image prior or ground-truth examples. An overarching challenge in
inverse problems is that there are many undesired images that fit to the
observed measurements, thus requiring image priors to constrain the space of
possible solutions to more plausible reconstructions. However, in many
applications it is difficult or potentially impossible to obtain ground-truth
images to learn an image prior. Thus, inaccurate priors are often used, which
inevitably result in biased solutions. Rather than solving an inverse problem
using priors that encode the explicit structure of any one image, we propose to
solve a set of inverse problems jointly by incorporating prior constraints on
the collective structure of the underlying images.The key assumption of our
work is that the ground-truth images we aim to reconstruct share common,
low-dimensional structure. We show that such a set of inverse problems can be
solved simultaneously by learning a shared image generator with a
low-dimensional latent space. The parameters of the generator and latent
embedding are learned by maximizing a proxy for the Evidence Lower Bound
(ELBO). Once learned, the generator and latent embeddings can be combined to
provide reconstructions for each inverse problem. The framework we propose can
handle general forward model corruptions, and we show that measurements derived
from only a few ground-truth images (O(10)) are sufficient for image
reconstruction without explicit priors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Prompt</span>-MIL: Boosting Multi-Instance Learning Schemes via Task-specific
  <span class="highlight-title">Prompt</span> Tuning <span class="chip">MICCAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12214v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12214v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingwei Zhang, Saarthak Kapse, Ke Ma, Prateek Prasanna, Joel Saltz, Maria Vakalopoulou, Dimitris Samaras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Whole slide image (WSI) classification is a critical task in computational
pathology, requiring the processing of gigapixel-sized images, which is
challenging for current deep-learning methods. Current state of the art methods
are based on multi-instance learning schemes (MIL), which usually rely on
pretrained features to represent the instances. Due to the lack of
task-specific annotated data, these features are either obtained from
well-established backbones on natural images, or, more recently from
self-supervised models pretrained on histopathology. However, both approaches
yield task-agnostic features, resulting in performance loss compared to the
appropriate task-related supervision, if available. In this paper, we show that
when task-specific annotations are limited, we can inject such supervision into
downstream task training, to reduce the gap between fully task-tuned and task
agnostic features. We propose Prompt-MIL, an MIL framework that integrates
prompts into WSI classification. Prompt-MIL adopts a prompt tuning mechanism,
where only a small fraction of parameters calibrates the pretrained features to
encode task-specific information, rather than the conventional full fine-tuning
approaches. Extensive experiments on three WSI datasets, TCGA-BRCA, TCGA-CRC,
and BRIGHT, demonstrate the superiority of Prompt-MIL over conventional MIL
methods, achieving a relative improvement of 1.49%-4.03% in accuracy and
0.25%-8.97% in AUROC while using fewer than 0.3% additional parameters.
Compared to conventional full fine-tuning approaches, we fine-tune less than
1.3% of the parameters, yet achieve a relative improvement of 1.29%-13.61% in
accuracy and 3.22%-27.18% in AUROC and reduce GPU memory consumption by 38%-45%
while training 21%-27% faster.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to MICCAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MAGVLT: Masked Generative Vision-and-Language <span class="highlight-title">Transformer</span> <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12208v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12208v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sungwoong Kim, Daejin Jo, Donghoon Lee, Jongmin Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While generative modeling on multimodal image-text data has been actively
developed with large-scale paired datasets, there have been limited attempts to
generate both image and text data by a single model rather than a generation of
one fixed modality conditioned on the other modality. In this paper, we explore
a unified generative vision-and-language (VL) model that can produce both
images and text sequences. Especially, we propose a generative VL transformer
based on the non-autoregressive mask prediction, named MAGVLT, and compare it
with an autoregressive generative VL transformer (ARGVLT). In comparison to
ARGVLT, the proposed MAGVLT enables bidirectional context encoding, fast
decoding by parallel token predictions in an iterative refinement, and extended
editing capabilities such as image and text infilling. For rigorous training of
our MAGVLT with image-text pairs from scratch, we combine the image-to-text,
text-to-image, and joint image-and-text mask prediction tasks. Moreover, we
devise two additional tasks based on the step-unrolled mask prediction and the
selective prediction on the mixture of two image-text pairs. Experimental
results on various downstream generation tasks of VL benchmarks show that our
MAGVLT outperforms ARGVLT by a large margin even with significant inference
speedup. Particularly, MAGVLT achieves competitive results on both zero-shot
image-to-text and text-to-image generation tasks from MS-COCO by one
moderate-sized model (fewer than 500M parameters) even without the use of
monomodal data and networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Autofluorescence Bronchoscopy Video Analysis for Lesion Frame Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12198v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12198v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Chang, Rebecca Bascom, Jennifer Toth, Danish Ahmad, William E. Higgins
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Because of the significance of bronchial lesions as indicators of early lung
cancer and squamous cell carcinoma, a critical need exists for early detection
of bronchial lesions. Autofluorescence bronchoscopy (AFB) is a primary modality
used for bronchial lesion detection, as it shows high sensitivity to suspicious
lesions. The physician, however, must interactively browse a long video stream
to locate lesions, making the search exceedingly tedious and error prone.
Unfortunately, limited research has explored the use of automated AFB video
analysis for efficient lesion detection. We propose a robust automatic AFB
analysis approach that distinguishes informative and uninformative AFB video
frames in a video. In addition, for the informative frames, we determine the
frames containing potential lesions and delineate candidate lesion regions. Our
approach draws upon a combination of computer-based image analysis, machine
learning, and deep learning. Thus, the analysis of an AFB video stream becomes
more tractable. Tests with patient AFB video indicate that $\ge$97\% of frames
were correctly labeled as informative or uninformative. In addition, $\ge$97\%
of lesion frames were correctly identified, with false positive and false
negative rates $\le$3\%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LiDARFormer: A Unified <span class="highlight-title">Transformer</span>-based Multi-task Network for LiDAR
  Perception 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12194v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12194v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixiang Zhou, Dongqiangzi Ye, Weijia Chen, Yufei Xie, Yu Wang, Panqu Wang, Hassan Foroosh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is a recent trend in the LiDAR perception field towards unifying
multiple tasks in a single strong network with improved performance, as opposed
to using separate networks for each task. In this paper, we introduce a new
LiDAR multi-task learning paradigm based on the transformer. The proposed
LiDARFormer utilizes cross-space global contextual feature information and
exploits cross-task synergy to boost the performance of LiDAR perception tasks
across multiple large-scale datasets and benchmarks. Our novel
transformer-based framework includes a cross-space transformer module that
learns attentive features between the 2D dense Bird's Eye View (BEV) and 3D
sparse voxel feature maps. Additionally, we propose a transformer decoder for
the segmentation task to dynamically adjust the learned features by leveraging
the categorical feature representations. Furthermore, we combine the
segmentation and detection features in a shared transformer decoder with
cross-task attention layers to enhance and integrate the object-level and
class-level features. LiDARFormer is evaluated on the large-scale nuScenes and
the Waymo Open datasets for both 3D detection and semantic segmentation tasks,
and it outperforms all previously published methods on both tasks. Notably,
LiDARFormer achieves the state-of-the-art performance of 76.4% L2 mAPH and
74.3% NDS on the challenging Waymo and nuScenes detection benchmarks for a
single model LiDAR-only method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Black-box Backdoor Defense via Zero-shot Image Purification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12175v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12175v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yucheng Shi, Mengnan Du, Xuansheng Wu, Zihan Guan, Ninghao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Backdoor attacks inject poisoned data into the training set, resulting in
misclassification of the poisoned samples during model inference. Defending
against such attacks is challenging, especially in real-world black-box
settings where only model predictions are available. In this paper, we propose
a novel backdoor defense framework that can effectively defend against various
attacks through zero-shot image purification (ZIP). Our proposed framework can
be applied to black-box models without requiring any internal information about
the poisoned model or any prior knowledge of the clean/poisoned samples. Our
defense framework involves a two-step process. First, we apply a linear
transformation on the poisoned image to destroy the trigger pattern. Then, we
use a pre-trained diffusion model to recover the missing semantic information
removed by the transformation. In particular, we design a new reverse process
using the transformed image to guide the generation of high-fidelity purified
images, which can be applied in zero-shot settings. We evaluate our ZIP
backdoor defense framework on multiple datasets with different kinds of
attacks. Experimental results demonstrate the superiority of our ZIP framework
compared to state-of-the-art backdoor defense baselines. We believe that our
results will provide valuable insights for future defense methods for black-box
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Whac-A-Mole Dilemma: Shortcuts Come in Multiples Where Mitigating One
  Amplifies Others <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.04825v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.04825v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiheng Li, Ivan Evtimov, Albert Gordo, Caner Hazirbas, Tal Hassner, Cristian Canton Ferrer, Chenliang Xu, Mark Ibrahim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning models have been found to learn shortcuts -- unintended
decision rules that are unable to generalize -- undermining models'
reliability. Previous works address this problem under the tenuous assumption
that only a single shortcut exists in the training data. Real-world images are
rife with multiple visual cues from background to texture. Key to advancing the
reliability of vision systems is understanding whether existing methods can
overcome multiple shortcuts or struggle in a Whac-A-Mole game, i.e., where
mitigating one shortcut amplifies reliance on others. To address this
shortcoming, we propose two benchmarks: 1) UrbanCars, a dataset with precisely
controlled spurious cues, and 2) ImageNet-W, an evaluation set based on
ImageNet for watermark, a shortcut we discovered affects nearly every modern
vision model. Along with texture and background, ImageNet-W allows us to study
multiple shortcuts emerging from training on natural images. We find computer
vision models, including large foundation models -- regardless of training set,
architecture, and supervision -- struggle when multiple shortcuts are present.
Even methods explicitly designed to combat shortcuts struggle in a Whac-A-Mole
dilemma. To tackle this challenge, we propose Last Layer Ensemble, a
simple-yet-effective method to mitigate multiple shortcuts without Whac-A-Mole
behavior. Our results surface multi-shortcut mitigation as an overlooked
challenge critical to advancing the reliability of vision systems. The datasets
and code are released: https://github.com/facebookresearch/Whac-A-Mole.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023. Code is available at
  https://github.com/facebookresearch/Whac-A-Mole</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MixMask: Revisiting Masking Strategy for Siamese ConvNets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.11456v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.11456v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kirill Vishniakov, Eric Xing, Zhiqiang Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in self-supervised learning have integrated Masked Image
Modeling (MIM) and Siamese Networks into a unified framework that leverages the
benefits of both techniques. However, several issues remain unaddressed when
applying conventional erase-based masking with Siamese ConvNets. These include
(I) the inability to drop uninformative masked regions in ConvNets as they
process data continuously, resulting in low training efficiency compared to ViT
models; and (II) the mismatch between erase-based masking and the
contrastive-based objective in Siamese ConvNets, which differs from the MIM
approach. In this paper, we propose a filling-based masking strategy called
MixMask to prevent information incompleteness caused by the randomly erased
regions in an image in the vanilla masking method. Furthermore, we introduce a
flexible loss function design that considers the semantic distance change
between two different mixed views to adapt the integrated architecture and
prevent mismatches between the transformed input and objective in Masked
Siamese ConvNets (MSCN). We conducted extensive experiments on various
datasets, including CIFAR-100, Tiny-ImageNet, and ImageNet-1K. The results
demonstrate that our proposed framework achieves superior accuracy on linear
probing, semi-supervised, and supervised finetuning, outperforming the
state-of-the-art MSCN by a significant margin. Additionally, we demonstrate the
superiority of our approach in object detection and segmentation tasks. Our
source code is available at https://github.com/LightnessOfBeing/MixMask.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical report. Code is available at
  https://github.com/LightnessOfBeing/MixMask</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-modal reward for visual relationships-based image captioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10766v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10766v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Abedi, Hossein Karshenas, Peyman Adibi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks have achieved promising results in automatic image
captioning due to their effective representation learning and context-based
content generation capabilities. As a prominent type of deep features used in
many of the recent image captioning methods, the well-known bottomup features
provide a detailed representation of different objects of the image in
comparison with the feature maps directly extracted from the raw image.
However, the lack of high-level semantic information about the relationships
between these objects is an important drawback of bottom-up features, despite
their expensive and resource-demanding extraction procedure. To take advantage
of visual relationships in caption generation, this paper proposes a deep
neural network architecture for image captioning based on fusing the visual
relationships information extracted from an image's scene graph with the
spatial feature maps of the image. A multi-modal reward function is then
introduced for deep reinforcement learning of the proposed network using a
combination of language and vision similarities in a common embedding space.
The results of extensive experimentation on the MSCOCO dataset show the
effectiveness of using visual relationships in the proposed captioning method.
Moreover, the results clearly indicate that the proposed multi-modal reward in
deep reinforcement learning leads to better model optimization, outperforming
several state-of-the-art image captioning algorithms, while using light and
easy to extract image features. A detailed experimental study of the components
constituting the proposed method is also presented.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CoReS: Compatible Representations via Stationarity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.07632v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.07632v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niccolo Biondi, Federico Pernici, Matteo Bruni, Alberto Del Bimbo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a novel method to learn internal feature
representation models that are \textit{compatible} with previously learned
ones. Compatible features enable for direct comparison of old and new learned
features, allowing them to be used interchangeably over time. This eliminates
the need for visual search systems to extract new features for all previously
seen images in the gallery-set when sequentially upgrading the representation
model. Extracting new features is typically quite expensive or infeasible in
the case of very large gallery-sets and/or real time systems (i.e.,
face-recognition systems, social networks, life-long learning systems, robotics
and surveillance systems). Our approach, called Compatible Representations via
Stationarity (CoReS), achieves compatibility by encouraging stationarity to the
learned representation model without relying on previously learned models.
Stationarity allows features' statistical properties not to change under time
shift so that the current learned features are inter-operable with the old
ones. We evaluate single and sequential multi-model upgrading in growing
large-scale training datasets and we show that our method improves the
state-of-the-art in achieving compatible features by a large margin. In
particular, upgrading ten times with training data taken from CASIA-WebFace and
evaluating in Labeled Face in the Wild (LFW), we obtain a 49\% increase in
measuring the average number of times compatibility is achieved, which is a
544\% relative improvement over previous state-of-the-art.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>in IEEE Transactions on Pattern Analysis and Machine Intelligence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UKnow: A Unified Knowledge Protocol for Common-Sense Reasoning and
  Vision-Language <span class="highlight-title">Pre-train</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.06891v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.06891v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Biao Gong, Xiaoying Xie, Yutong Feng, Yiliang Lv, Yujun Shen, Deli Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work presents a unified knowledge protocol, called UKnow, which
facilitates knowledge-based studies from the perspective of data. Particularly
focusing on visual and linguistic modalities, we categorize data knowledge into
five unit types, namely, in-image, in-text, cross-image, cross-text, and
image-text, and set up an efficient pipeline to help construct the multimodal
knowledge graph from any data collection. Thanks to the logical information
naturally contained in knowledge graph, organizing datasets under UKnow format
opens up more possibilities of data usage compared to the commonly used
image-text pairs. Following UKnow protocol, we collect, from public
international news, a large-scale multimodal knowledge graph dataset that
consists of 1,388,568 nodes (with 571,791 vision-related ones) and 3,673,817
triplets. The dataset is also annotated with rich event tags, including 11
coarse labels and 9,185 fine labels. Experiments on four benchmarks demonstrate
the potential of UKnow in supporting common-sense reasoning and boosting
vision-language pre-training with a single dataset, benefiting from its unified
form of knowledge organization. Code, dataset, and models will be made publicly
available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Policy Adaptation from Foundation Model Feedback <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07398v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07398v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuying Ge, Annabella Macaluso, Li Erran Li, Ping Luo, Xiaolong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent progress on vision-language foundation models have brought significant
advancement to building general-purpose robots. By using the pre-trained models
to encode the scene and instructions as inputs for decision making, the
instruction-conditioned policy can generalize across different objects and
tasks. While this is encouraging, the policy still fails in most cases given an
unseen task or environment. In this work, we propose Policy Adaptation from
Foundation model Feedback (PAFF). When deploying the trained policy to a new
task or a new environment, we first let the policy play with randomly generated
instructions to record the demonstrations. While the execution could be wrong,
we can use the pre-trained foundation models to provide feedback to relabel the
demonstrations. This automatically provides new pairs of
demonstration-instruction data for policy fine-tuning. We evaluate our method
on a broad range of experiments with the focus on generalization on unseen
objects, unseen tasks, unseen environments, and sim-to-real transfer. We show
PAFF improves baselines by a large margin in all cases. Our project page is
available at https://geyuying.github.io/PAFF/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2023; Project page: https://geyuying.github.io/PAFF/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PartNeRF: Generating Part-Aware Editable 3D Shapes without 3D
  Supervision <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.09554v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.09554v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Konstantinos Tertikas, Despoina Paschalidou, Boxiao Pan, Jeong Joon Park, Mikaela Angelina Uy, Ioannis Emiris, Yannis Avrithis, Leonidas Guibas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Impressive progress in generative models and implicit representations gave
rise to methods that can generate 3D shapes of high quality. However, being
able to locally control and edit shapes is another essential property that can
unlock several content creation applications. Local control can be achieved
with part-aware models, but existing methods require 3D supervision and cannot
produce textures. In this work, we devise PartNeRF, a novel part-aware
generative model for editable 3D shape synthesis that does not require any
explicit 3D supervision. Our model generates objects as a set of locally
defined NeRFs, augmented with an affine transformation. This enables several
editing operations such as applying transformations on parts, mixing parts from
different objects etc. To ensure distinct, manipulable parts we enforce a hard
assignment of rays to parts that makes sure that the color of each ray is only
determined by a single NeRF. As a result, altering one part does not affect the
appearance of the others. Evaluations on various ShapeNet categories
demonstrate the ability of our model to generate editable 3D objects of
improved fidelity, compared to previous part-based generative approaches that
require 3D supervision or models relying on NeRFs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in CVPR 2023, Project Page:
  https://ktertikas.github.io/part_nerf</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Instance Relation Graph Guided Source-Free Domain Adaptive Object
  Detection <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.15793v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.15793v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vibashan VS, Poojan Oza, Vishal M. Patel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised Domain Adaptation (UDA) is an effective approach to tackle the
issue of domain shift. Specifically, UDA methods try to align the source and
target representations to improve the generalization on the target domain.
Further, UDA methods work under the assumption that the source data is
accessible during the adaptation process. However, in real-world scenarios, the
labelled source data is often restricted due to privacy regulations, data
transmission constraints, or proprietary data concerns. The Source-Free Domain
Adaptation (SFDA) setting aims to alleviate these concerns by adapting a
source-trained model for the target domain without requiring access to the
source data. In this paper, we explore the SFDA setting for the task of
adaptive object detection. To this end, we propose a novel training strategy
for adapting a source-trained object detector to the target domain without
source data. More precisely, we design a novel contrastive loss to enhance the
target representations by exploiting the objects relations for a given target
domain input. These object instance relations are modelled using an Instance
Relation Graph (IRG) network, which are then used to guide the contrastive
representation learning. In addition, we utilize a student-teacher based
knowledge distillation strategy to avoid overfitting to the noisy pseudo-labels
generated by the source-trained model. Extensive experiments on multiple object
detection benchmark datasets show that the proposed approach is able to
efficiently adapt source-trained object detectors to the target domain,
outperforming previous state-of-the-art domain adaptive detection methods. Code
and models are provided in
\href{https://viudomain.github.io/irg-sfda-web/}{https://viudomain.github.io/irg-sfda-web/}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2023. Project site:
  \href{https://viudomain.github.io/irg-sfda-web/}{https://viudomain.github.io/irg-sfda-web/}</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PointTAD: Multi-Label Temporal Action Detection with Learnable Query
  Points <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.11035v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.11035v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Tan, Xiaotong Zhao, Xintian Shi, Bin Kang, Limin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional temporal action detection (TAD) usually handles untrimmed videos
with small number of action instances from a single label (e.g., ActivityNet,
THUMOS). However, this setting might be unrealistic as different classes of
actions often co-occur in practice. In this paper, we focus on the task of
multi-label temporal action detection that aims to localize all action
instances from a multi-label untrimmed video. Multi-label TAD is more
challenging as it requires for fine-grained class discrimination within a
single video and precise localization of the co-occurring instances. To
mitigate this issue, we extend the sparse query-based detection paradigm from
the traditional TAD and propose the multi-label TAD framework of PointTAD.
Specifically, our PointTAD introduces a small set of learnable query points to
represent the important frames of each action instance. This point-based
representation provides a flexible mechanism to localize the discriminative
frames at boundaries and as well the important frames inside the action.
Moreover, we perform the action decoding process with the Multi-level
Interactive Module to capture both point-level and instance-level action
semantics. Finally, our PointTAD employs an end-to-end trainable framework
simply based on RGB input for easy deployment. We evaluate our proposed method
on two popular benchmarks and introduce the new metric of detection-mAP for
multi-label TAD. Our model outperforms all previous methods by a large margin
under the detection-mAP metric, and also achieves promising results under the
segmentation-mAP metric. Code is available at
https://github.com/MCG-NJU/PointTAD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2022 camera ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust Model-based Face Reconstruction through Weakly-Supervised Outlier
  Segmentation <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.09614v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.09614v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chunlu Li, Andreas Morel-Forster, Thomas Vetter, Bernhard Egger, Adam Kortylewski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we aim to enhance model-based face reconstruction by avoiding
fitting the model to outliers, i.e. regions that cannot be well-expressed by
the model such as occluders or make-up. The core challenge for localizing
outliers is that they are highly variable and difficult to annotate. To
overcome this challenging problem, we introduce a joint Face-autoencoder and
outlier segmentation approach (FOCUS).In particular, we exploit the fact that
the outliers cannot be fitted well by the face model and hence can be localized
well given a high-quality model fitting. The main challenge is that the model
fitting and the outlier segmentation are mutually dependent on each other, and
need to be inferred jointly. We resolve this chicken-and-egg problem with an
EM-type training strategy, where a face autoencoder is trained jointly with an
outlier segmentation network. This leads to a synergistic effect, in which the
segmentation network prevents the face encoder from fitting to the outliers,
enhancing the reconstruction quality. The improved 3D face reconstruction, in
turn, enables the segmentation network to better predict the outliers. To
resolve the ambiguity between outliers and regions that are difficult to fit,
such as eyebrows, we build a statistical prior from synthetic data that
measures the systematic bias in model fitting. Experiments on the NoW testset
demonstrate that FOCUS achieves SOTA 3D face reconstruction performance among
all baselines that are trained without 3D annotation. Moreover, our results on
CelebA-HQ and the AR database show that the segmentation network can localize
occluders accurately despite being trained without any segmentation annotation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, CVPR2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WALDO: Future Video Synthesis using Object Layer Decomposition and
  Parametric Flow Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.14308v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.14308v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guillaume Le Moing, Jean Ponce, Cordelia Schmid
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents WALDO (WArping Layer-Decomposed Objects), a novel
approach to the prediction of future video frames from past ones. Individual
images are decomposed into multiple layers combining object masks and a small
set of control points. The layer structure is shared across all frames in each
video to build dense inter-frame connections. Complex scene motions are modeled
by combining parametric geometric transformations associated with individual
layers, and video synthesis is broken down into discovering the layers
associated with past frames, predicting the corresponding transformations for
upcoming ones and warping the associated object regions accordingly, and
filling in the remaining image parts. Extensive experiments on multiple
benchmarks including urban videos (Cityscapes and KITTI) and videos featuring
nonrigid motions (UCF-Sports and H3.6M), show that our method consistently
outperforms the state of the art by a significant margin in every case. Code,
pretrained models, and video samples synthesized by our approach can be found
in the project webpage https://16lemoing.github.io/waldo.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stateless actor-critic for instance segmentation with high-level priors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2107.02600v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2107.02600v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Hilt, Maedeh Zarvandi, Edgar Kaziakhmedov, Sourabh Bhide, Maria Leptin, Constantin Pape, Anna Kreshuk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instance segmentation is an important computer vision problem which remains
challenging despite impressive recent advances due to deep learning-based
methods. Given sufficient training data, fully supervised methods can yield
excellent performance, but annotation of ground-truth data remains a major
bottleneck, especially for biomedical applications where it has to be performed
by domain experts. The amount of labels required can be drastically reduced by
using rules derived from prior knowledge to guide the segmentation. However,
these rules are in general not differentiable and thus cannot be used with
existing methods. Here, we relax this requirement by using stateless actor
critic reinforcement learning, which enables non-differentiable rewards. We
formulate the instance segmentation problem as graph partitioning and the actor
critic predicts the edge weights driven by the rewards, which are based on the
conformity of segmented instances to high-level priors on object shape,
position or size. The experiments on toy and real datasets demonstrate that we
can achieve excellent performance without any direct supervision based only on
a rich set of priors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DeepGraviLens: a Multi-Modal Architecture for Classifying Gravitational
  Lensing Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.00701v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.00701v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolò Oreste Pinciroli Vago, Piero Fraternali
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gravitational lensing is the relativistic effect generated by massive bodies,
which bend the space-time surrounding them. It is a deeply investigated topic
in astrophysics and allows validating theoretical relativistic results and
studying faint astrophysical objects that would not be visible otherwise. In
recent years Machine Learning methods have been applied to support the analysis
of the gravitational lensing phenomena by detecting lensing effects in data
sets consisting of images associated with brightness variation time series.
However, the state-of-art approaches either consider only images and neglect
time-series data or achieve relatively low accuracy on the most difficult data
sets. This paper introduces DeepGraviLens, a novel multi-modal network that
classifies spatio-temporal data belonging to one non-lensed system type and
three lensed system types. It surpasses the current state of the art accuracy
results by $\approx$ 19% to $\approx$ 43%, depending on the considered data
set. Such an improvement will enable the acceleration of the analysis of lensed
objects in upcoming astrophysical surveys, which will exploit the petabytes of
data collected, e.g., from the Vera C. Rubin Observatory.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MulGT: Multi-task Graph-<span class="highlight-title">Transformer</span> with Task-aware Knowledge Injection
  and Domain Knowledge-driven Pooling for Whole Slide Image Analysis <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.10574v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.10574v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiqin Zhao, Shujun Wang, Maximus Yeung, Tianye Niu, Lequan Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Whole slide image (WSI) has been widely used to assist automated diagnosis
under the deep learning fields. However, most previous works only discuss the
SINGLE task setting which is not aligned with real clinical setting, where
pathologists often conduct multiple diagnosis tasks simultaneously. Also, it is
commonly recognized that the multi-task learning paradigm can improve learning
efficiency by exploiting commonalities and differences across multiple tasks.
To this end, we present a novel multi-task framework (i.e., MulGT) for WSI
analysis by the specially designed Graph-Transformer equipped with Task-aware
Knowledge Injection and Domain Knowledge-driven Graph Pooling modules.
Basically, with the Graph Neural Network and Transformer as the building
commons, our framework is able to learn task-agnostic low-level local
information as well as task-specific high-level global representation.
Considering that different tasks in WSI analysis depend on different features
and properties, we also design a novel Task-aware Knowledge Injection module to
transfer the task-shared graph embedding into task-specific feature spaces to
learn more accurate representation for different tasks. Further, we elaborately
design a novel Domain Knowledge-driven Graph Pooling module for each task to
improve both the accuracy and robustness of different tasks by leveraging
different diagnosis patterns of multiple tasks. We evaluated our method on two
public WSI datasets from TCGA projects, i.e., esophageal carcinoma and kidney
carcinoma. Experimental results show that our method outperforms single-task
counterparts and the state-of-theart methods on both tumor typing and staging
tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Uncertainty-Aware Optimal Transport for Semantically Coherent
  Out-of-Distribution Detection <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10449v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10449v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Lu, Kai Zhu, Wei Zhai, Kecheng Zheng, Yang Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantically coherent out-of-distribution (SCOOD) detection aims to discern
outliers from the intended data distribution with access to unlabeled extra
set. The coexistence of in-distribution and out-of-distribution samples will
exacerbate the model overfitting when no distinction is made. To address this
problem, we propose a novel uncertainty-aware optimal transport scheme. Our
scheme consists of an energy-based transport (ET) mechanism that estimates the
fluctuating cost of uncertainty to promote the assignment of semantic-agnostic
representation, and an inter-cluster extension strategy that enhances the
discrimination of semantic property among different clusters by widening the
corresponding margin distance. Furthermore, a T-energy score is presented to
mitigate the magnitude gap between the parallel transport and classifier
branches. Extensive experiments on two standard SCOOD benchmarks demonstrate
the above-par OOD detection performance, outperforming the state-of-the-art
methods by a margin of 27.69% and 34.4% on FPR@95, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contrastive learning for regression in multi-site brain age prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.08326v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.08326v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carlo Alberto Barbano, Benoit Dufumier, Edouard Duchesnay, Marco Grangetto, Pietro Gori
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building accurate Deep Learning (DL) models for brain age prediction is a
very relevant topic in neuroimaging, as it could help better understand
neurodegenerative disorders and find new biomarkers. To estimate accurate and
generalizable models, large datasets have been collected, which are often
multi-site and multi-scanner. This large heterogeneity negatively affects the
generalization performance of DL models since they are prone to overfit
site-related noise. Recently, contrastive learning approaches have been shown
to be more robust against noise in data or labels. For this reason, we propose
a novel contrastive learning regression loss for robust brain age prediction
using MRI scans. Our method achieves state-of-the-art performance on the
OpenBHB challenge, yielding the best generalization capability and robustness
to site-related noise.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CMX: Cross-Modal Fusion for RGB-X Semantic Segmentation with
  <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.04838v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.04838v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaming Zhang, Huayao Liu, Kailun Yang, Xinxin Hu, Ruiping Liu, Rainer Stiefelhagen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scene understanding based on image segmentation is a crucial component for
autonomous vehicles. Pixel-wise semantic segmentation of RGB images can be
advanced by exploiting informative features from the supplementary modality
(X-modality). In this work, we propose CMX, a transformer-based cross-modal
fusion framework for RGB-X semantic segmentation. To generalize to different
sensing modalities encompassing various supplements and uncertainties, we
consider that comprehensive cross-modal interactions should be provided. CMX is
built with two streams to extract features from RGB images and the X-modality.
In each feature extraction stage, we design a Cross-Modal Feature Rectification
Module (CM-FRM) to calibrate the feature of the current modality by combining
the feature from the other modality, in spatial- and channel-wise dimensions.
With rectified feature pairs, we deploy a Feature Fusion Module (FFM) to mix
them for the final semantic prediction. FFM is constructed with a
cross-attention mechanism, which enables exchange of long-range contexts,
enhancing bi-modal features globally. Extensive experiments show that CMX
generalizes to diverse multi-modal combinations, achieving state-of-the-art
performances on five RGB-Depth benchmarks, as well as RGB-Thermal,
RGB-Polarization, and RGB-LiDAR datasets. Besides, to investigate the
generalizability to dense-sparse data fusion, we establish an RGB-Event
semantic segmentation benchmark based on the EventScape dataset, on which CMX
sets the new state-of-the-art. The source code of CMX is publicly available at
https://github.com/huaaaliu/RGBX_Semantic_Segmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at
  https://github.com/huaaaliu/RGBX_Semantic_Segmentation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CVT-SLR: Contrastive Visual-Textual Transformation for Sign Language
  Recognition with Variational Alignment <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05725v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05725v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiangbin Zheng, Yile Wang, Cheng Tan, Siyuan Li, Ge Wang, Jun Xia, Yidong Chen, Stan Z. Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sign language recognition (SLR) is a weakly supervised task that annotates
sign videos as textual glosses. Recent studies show that insufficient training
caused by the lack of large-scale available sign language datasets becomes the
main bottleneck for SLR. The majority of SLR works thereby adopt pretrained
visual modules and develop two mainstream solutions. The multi-stream
architectures extend multi-cue visual features, yielding the current SOTA
performances but requiring complex designs and might introduce potential noise.
Alternatively, the advanced single-cue SLR frameworks using explicit
cross-modal alignment between visual and textual modalities are simple and
effective, potentially competitive with the multi-cue framework. In this work,
we propose a novel contrastive visual-textual transformation for SLR, CVT-SLR,
to fully explore the pretrained knowledge of both the visual and language
modalities. Based on the single-cue cross-modal alignment framework, we propose
a variational autoencoder (VAE) for pretrained contextual knowledge while
introducing the complete pretrained language module. The VAE implicitly aligns
visual and textual modalities while benefiting from pretrained contextual
knowledge as the traditional contextual module. Meanwhile, a contrastive
cross-modal alignment algorithm is proposed to further enhance the explicit
consistency constraints. Extensive experiments conducted on the two most
popular public datasets, PHOENIX-2014 and PHOENIX-2014T, demonstrate that our
proposed SLR framework not only consistently outperforms existing single-cue
methods but even outperforms SOTA multi-cue methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2023 (highlight)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">GPT</span>4MIA: Utilizing Generative <span class="highlight-title">Pre-train</span>ed <span class="highlight-title">Transformer</span> (<span class="highlight-title">GPT</span>-3) as A
  Plug-and-Play Transductive Model for Medical Image Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.08722v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.08722v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhe Zhang, Danny Z. Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a novel approach (called GPT4MIA) that utilizes
Generative Pre-trained Transformer (GPT) as a plug-and-play transductive
inference tool for medical image analysis (MIA). We provide theoretical
analysis on why a large pre-trained language model such as GPT-3 can be used as
a plug-and-play transductive inference model for MIA. At the methodological
level, we develop several technical treatments to improve the efficiency and
effectiveness of GPT4MIA, including better prompt structure design, sample
selection, and prompt ordering of representative samples/features. We present
two concrete use cases (with workflow) of GPT4MIA: (1) detecting prediction
errors and (2) improving prediction accuracy, working in conjecture with
well-established vision-based models for image classification (e.g., ResNet).
Experiments validate that our proposed method is effective for these two tasks.
We further discuss the opportunities and challenges in utilizing
Transformer-based large language models for broader MIA applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Version 3: Added appendix with more results and visualizations.
  Questions and suggestions are welcome</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Recent Progress in <span class="highlight-title">Transformer</span>-based Medical Image Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.06643v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.06643v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaoshan Liu, Qiujie Lv, Ziduo Yang, Yifan Li, Chau Hung Lee, Lei Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The transformer is primarily used in the field of natural language
processing. Recently, it has been adopted and shows promise in the computer
vision (CV) field. Medical image analysis (MIA), as a critical branch of CV,
also greatly benefits from this state-of-the-art technique. In this review, we
first recap the core component of the transformer, the attention mechanism, and
the detailed structures of the transformer. After that, we depict the recent
progress of the transformer in the field of MIA. We organize the applications
in a sequence of different tasks, including classification, segmentation,
captioning, registration, detection, reconstruction, denoising, localization,
and synthesis. The mainstream classification and segmentation tasks are further
divided into eleven medical image modalities. Finally, We discuss the open
challenges and future opportunities in this field. This review with the latest
contents, detailed information, and task-modality organization mode may greatly
benefit the broad MIA community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>83 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Training Invertible Neural Networks as Autoencoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11239v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11239v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        The-Gia Leo Nguyen, Lynton Ardizzone, Ullrich Köthe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autoencoders are able to learn useful data representations in an unsupervised
matter and have been widely used in various machine learning and computer
vision tasks. In this work, we present methods to train Invertible Neural
Networks (INNs) as (variational) autoencoders which we call INN (variational)
autoencoders. Our experiments on MNIST, CIFAR and CelebA show that for low
bottleneck sizes our INN autoencoder achieves results similar to the classical
autoencoder. However, for large bottleneck sizes our INN autoencoder
outperforms its classical counterpart. Based on the empirical results, we
hypothesize that INN autoencoders might not have any intrinsic information loss
and thereby are not bounded to a maximal number of layers (depth) after which
only suboptimal results can be achieved.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Conference Paper at GCPR2019</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Prompt</span>Cap: <span class="highlight-title">Prompt</span>-Guided Image Captioning for VQA with <span class="highlight-title">GPT</span>-3 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.09699v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.09699v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yushi Hu, Hang Hua, Zhengyuan Yang, Weijia Shi, Noah A. Smith, Jiebo Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge-based visual question answering (VQA) involves questions that
require world knowledge beyond the image to yield the correct answer. Large
language models (LMs) like GPT-3 are particularly helpful for this task because
of their strong knowledge retrieval and reasoning capabilities. To enable LM to
understand images, prior work uses a captioning model to convert images into
text. However, when summarizing an image in a single caption sentence, which
visual entities to describe are often underspecified. Generic image captions
often miss visual details essential for the LM to answer visual questions
correctly. To address this challenge, we propose PromptCap (Prompt-guided image
Captioning), a captioning model designed to serve as a better connector between
images and black-box LMs. Different from generic captions, PromptCap takes a
natural-language prompt to control the visual entities to describe in the
generated caption. The prompt contains a question that the caption should aid
in answering. To avoid extra annotation, PromptCap is trained by examples
synthesized with GPT-3 and existing datasets. We demonstrate PromptCap's
effectiveness on an existing pipeline in which GPT-3 is prompted with image
captions to carry out VQA. PromptCap outperforms generic captions by a large
margin and achieves state-of-the-art accuracy on knowledge-based VQA tasks
(60.4% on OK-VQA and 59.6% on A-OKVQA). Zero-shot results on WebQA show that
PromptCap generalizes well to unseen domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ $L_2$BN: Enhancing Batch Normalization by Equalizing the $L_2$ Norms of
  Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.02625v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.02625v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhennan Wang, Kehan Li, Runyi Yu, Yian Zhao, Pengchong Qiao, Chang Liu, Fan Xu, Xiangyang Ji, Guoli Song, Jie Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we analyze batch normalization from the perspective of
discriminability and find the disadvantages ignored by previous studies: the
difference in $l_2$ norms of sample features can hinder batch normalization
from obtaining more distinguished inter-class features and more compact
intra-class features. To address this issue, we propose a simple yet effective
method to equalize the $l_2$ norms of sample features. Concretely, we
$l_2$-normalize each sample feature before feeding them into batch
normalization, and therefore the features are of the same magnitude. Since the
proposed method combines the $l_2$ normalization and batch normalization, we
name our method $L_2$BN. The $L_2$BN can strengthen the compactness of
intra-class features and enlarge the discrepancy of inter-class features. The
$L_2$BN is easy to implement and can exert its effect without any additional
parameters or hyper-parameters. We evaluate the effectiveness of $L_2$BN
through extensive experiments with various models on image classification and
acoustic scene classification tasks. The results demonstrate that the $L_2$BN
can boost the generalization ability of various neural network models and
achieve considerable performance improvements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Paced Learning for Open-Set Domain Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05933v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05933v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinghong Liu, Yi Zhou, Tao Zhou, Jie Qin, Shengcai Liao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain adaptation tackles the challenge of generalizing knowledge acquired
from a source domain to a target domain with different data distributions.
Traditional domain adaptation methods presume that the classes in the source
and target domains are identical, which is not always the case in real-world
scenarios. Open-set domain adaptation (OSDA) addresses this limitation by
allowing previously unseen classes in the target domain. Open-set domain
adaptation aims to not only recognize target samples belonging to common
classes shared by source and target domains but also perceive unknown class
samples. We propose a novel framework based on self-paced learning to
distinguish common and unknown class samples precisely, referred to as SPLOS
(self-paced learning for open-set). To utilize unlabeled target samples for
self-paced learning, we generate pseudo labels and design a cross-domain mixup
method tailored for OSDA scenarios. This strategy minimizes the noise from
pseudo labels and ensures our model progressively learns common class features
of the target domain, beginning with simpler examples and advancing to more
complex ones. Furthermore, unlike existing OSDA methods that require manual
hyperparameter $threshold$ tuning to separate common and unknown classes, our
approach self-tunes a suitable threshold, eliminating the need for empirical
tuning during testing. Comprehensive experiments illustrate that our method
consistently achieves superior performance on different benchmarks compared
with various state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Models that Can See and Read 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07389v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07389v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roy Ganz, Oren Nuriel, Aviad Aberdam, Yair Kittenplon, Shai Mazor, Ron Litman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Question Answering (VQA) and Image Captioning (CAP), which are among
the most popular vision-language tasks, have analogous scene-text versions that
require reasoning from the text in the image. Despite their obvious
resemblance, the two are treated independently and, as we show, yield
task-specific methods that can either see or read, but not both. In this work,
we conduct an in-depth analysis of this phenomenon and propose UniTNT, a
Unified Text-Non-Text approach, which grants existing multimodal architectures
scene-text understanding capabilities. Specifically, we treat scene-text
information as an additional modality, fusing it with any pretrained
encoder-decoder-based architecture via designated modules. Thorough experiments
reveal that UniTNT leads to the first single model that successfully handles
both task types. Moreover, we show that scene-text understanding capabilities
can boost vision-language models' performance on general VQA and CAP by up to
2.69% and 0.6 CIDEr, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Whose Emotion Matters? Speaking Activity Localisation without Prior
  Knowledge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.15377v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.15377v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hugo Carneiro, Cornelius Weber, Stefan Wermter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of emotion recognition in conversations (ERC) benefits from the
availability of multiple modalities, as provided, for example, in the
video-based Multimodal EmotionLines Dataset (MELD). However, only a few
research approaches use both acoustic and visual information from the MELD
videos. There are two reasons for this: First, label-to-video alignments in
MELD are noisy, making those videos an unreliable source of emotional speech
data. Second, conversations can involve several people in the same scene, which
requires the localisation of the utterance source. In this paper, we introduce
MELD with Fixed Audiovisual Information via Realignment (MELD-FAIR) by using
recent active speaker detection and automatic speech recognition models, we are
able to realign the videos of MELD and capture the facial expressions from
speakers in 96.92% of the utterances provided in MELD. Experiments with a
self-supervised voice recognition model indicate that the realigned MELD-FAIR
videos more closely match the transcribed utterances given in the MELD dataset.
Finally, we devise a model for emotion recognition in conversations trained on
the realigned MELD-FAIR videos, which outperforms state-of-the-art models for
ERC based on vision alone. This indicates that localising the source of
speaking activities is indeed effective for extracting facial expressions from
the uttering speakers and that faces provide more informative visual cues than
the visual features state-of-the-art models have been using so far. The
MELD-FAIR realignment data, and the code of the realignment procedure and of
the emotional recognition, are available at
https://github.com/knowledgetechnologyuhh/MELD-FAIR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 8 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 3D Video Loops from Asynchronous Input 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05312v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05312v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Ma, Xiaoyu Li, Jing Liao, Pedro V. Sander
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Looping videos are short video clips that can be looped endlessly without
visible seams or artifacts. They provide a very attractive way to capture the
dynamism of natural scenes. Existing methods have been mostly limited to 2D
representations. In this paper, we take a step forward and propose a practical
solution that enables an immersive experience on dynamic 3D looping scenes. The
key challenge is to consider the per-view looping conditions from asynchronous
input while maintaining view consistency for the 3D representation. We propose
a novel sparse 3D video representation, namely Multi-Tile Video (MTV), which
not only provides a view-consistent prior, but also greatly reduces memory
usage, making the optimization of a 4D volume tractable. Then, we introduce a
two-stage pipeline to construct the 3D looping MTV from completely asynchronous
multi-view videos with no time overlap. A novel looping loss based on video
temporal retargeting algorithms is adopted during the optimization to loop the
3D scene. Experiments of our framework have shown promise in successfully
generating and rendering photorealistic 3D looping videos in real time even on
mobile devices. The code, dataset, and live demos are available in
https://limacv.github.io/VideoLoop3D_web/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>For more information, please visit the homepage at
  https://limacv.github.io/VideoLoop3D_web/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vid2Seq: Large-Scale <span class="highlight-title">Pretrain</span>ing of a Visual Language Model for Dense
  Video Captioning <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.14115v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.14115v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antoine Yang, Arsha Nagrani, Paul Hongsuck Seo, Antoine Miech, Jordi Pont-Tuset, Ivan Laptev, Josef Sivic, Cordelia Schmid
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we introduce Vid2Seq, a multi-modal single-stage dense event
captioning model pretrained on narrated videos which are readily-available at
scale. The Vid2Seq architecture augments a language model with special time
tokens, allowing it to seamlessly predict event boundaries and textual
descriptions in the same output sequence. Such a unified model requires
large-scale training data, which is not available in current annotated
datasets. We show that it is possible to leverage unlabeled narrated videos for
dense video captioning, by reformulating sentence boundaries of transcribed
speech as pseudo event boundaries, and using the transcribed speech sentences
as pseudo event captions. The resulting Vid2Seq model pretrained on the
YT-Temporal-1B dataset improves the state of the art on a variety of dense
video captioning benchmarks including YouCook2, ViTT and ActivityNet Captions.
Vid2Seq also generalizes well to the tasks of video paragraph captioning and
video clip captioning, and to few-shot settings. Our code is publicly available
at https://antoyang.github.io/vid2seq.html.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023 Camera-Ready; Project Webpage:
  https://antoyang.github.io/vid2seq.html ; 18 pages; 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sharpness-aware Quantization for Deep Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.12273v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.12273v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Liu, Jianfei Cai, Bohan Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Network quantization is a dominant paradigm of model compression. However,
the abrupt changes in quantized weights during training often lead to severe
loss fluctuations and result in a sharp loss landscape, making the gradients
unstable and thus degrading the performance. Recently, Sharpness-Aware
Minimization (SAM) has been proposed to smooth the loss landscape and improve
the generalization performance of the models. Nevertheless, directly applying
SAM to the quantized models can lead to perturbation mismatch or diminishment
issues, resulting in suboptimal performance. In this paper, we propose a novel
method, dubbed Sharpness-Aware Quantization (SAQ), to explore the effect of SAM
in model compression, particularly quantization for the first time.
Specifically, we first provide a unified view of quantization and SAM by
treating them as introducing quantization noises and adversarial perturbations
to the model weights, respectively. According to whether the noise and
perturbation terms depend on each other, SAQ can be formulated into three
cases, which are analyzed and compared comprehensively. Furthermore, by
introducing an efficient training strategy, SAQ only incurs a little additional
training overhead compared with the default optimizer (e.g., SGD or AdamW).
Extensive experiments on both convolutional neural networks and Transformers
across various datasets (i.e., ImageNet, CIFAR-10/100, Oxford Flowers-102,
Oxford-IIIT Pets) show that SAQ improves the generalization performance of the
quantized models, yielding the SOTA results in uniform quantization. For
example, on ImageNet, SAQ outperforms AdamW by 1.2% on the Top-1 accuracy for
4-bit ViT-B/16. Our 4-bit ResNet-50 surpasses the previous SOTA method by 0.9%
on the Top-1 accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Tech report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Masked Wavelet Representation for Compact Neural Radiance Fields <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.09069v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.09069v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Rho, Byeonghyeon Lee, Seungtae Nam, Joo Chan Lee, Jong Hwan Ko, Eunbyung Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural radiance fields (NeRF) have demonstrated the potential of
coordinate-based neural representation (neural fields or implicit neural
representation) in neural rendering. However, using a multi-layer perceptron
(MLP) to represent a 3D scene or object requires enormous computational
resources and time. There have been recent studies on how to reduce these
computational inefficiencies by using additional data structures, such as grids
or trees. Despite the promising performance, the explicit data structure
necessitates a substantial amount of memory. In this work, we present a method
to reduce the size without compromising the advantages of having additional
data structures. In detail, we propose using the wavelet transform on
grid-based neural fields. Grid-based neural fields are for fast convergence,
and the wavelet transform, whose efficiency has been demonstrated in
high-performance standard codecs, is to improve the parameter efficiency of
grids. Furthermore, in order to achieve a higher sparsity of grid coefficients
while maintaining reconstruction quality, we present a novel trainable masking
approach. Experimental results demonstrate that non-spatial grid coefficients,
such as wavelet coefficients, are capable of attaining a higher level of
sparsity than spatial grid coefficients, resulting in a more compact
representation. With our proposed mask and compression pipeline, we achieved
state-of-the-art performance within a memory budget of 2 MB. Our code is
available at https://github.com/daniel03c1/masked_wavelet_nerf.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ElasticViT: Conflict-aware Supernet Training for Deploying Fast Vision
  <span class="highlight-title">Transformer</span> on Diverse Mobile Devices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.09730v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.09730v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Tang, Li Lyna Zhang, Huiqiang Jiang, Jiahang Xu, Ting Cao, Quanlu Zhang, Yuqing Yang, Zhi Wang, Mao Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Architecture Search (NAS) has shown promising performance in the
automatic design of vision transformers (ViT) exceeding 1G FLOPs. However,
designing lightweight and low-latency ViT models for diverse mobile devices
remains a big challenge. In this work, we propose ElasticViT, a two-stage NAS
approach that trains a high-quality ViT supernet over a very large search space
that supports a wide range of mobile devices, and then searches an optimal
sub-network (subnet) for direct deployment. However, prior supernet training
methods that rely on uniform sampling suffer from the gradient conflict issue:
the sampled subnets can have vastly different model sizes (e.g., 50M vs. 2G
FLOPs), leading to different optimization directions and inferior performance.
To address this challenge, we propose two novel sampling techniques:
complexity-aware sampling and performance-aware sampling. Complexity-aware
sampling limits the FLOPs difference among the subnets sampled across adjacent
training steps, while covering different-sized subnets in the search space.
Performance-aware sampling further selects subnets that have good accuracy,
which can reduce gradient conflicts and improve supernet quality. Our
discovered models, ElasticViT models, achieve top-1 accuracy from 67.2% to
80.0% on ImageNet from 60M to 800M FLOPs without extra retraining,
outperforming all prior CNNs and ViTs in terms of accuracy and latency. Our
tiny and small models are also the first ViT models that surpass
state-of-the-art CNNs with significantly lower latency on mobile devices. For
instance, ElasticViT-S1 runs 2.62x faster than EfficientNet-B0 with 0.1% higher
accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DeSTSeg: Segmentation Guided Denoising Student-Teacher for Anomaly
  Detection <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.11317v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.11317v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuan Zhang, Shiyu Li, Xi Li, Ping Huang, Jiulong Shan, Ting Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual anomaly detection, an important problem in computer vision, is usually
formulated as a one-class classification and segmentation task. The
student-teacher (S-T) framework has proved to be effective in solving this
challenge. However, previous works based on S-T only empirically applied
constraints on normal data and fused multi-level information. In this study, we
propose an improved model called DeSTSeg, which integrates a pre-trained
teacher network, a denoising student encoder-decoder, and a segmentation
network into one framework. First, to strengthen the constraints on anomalous
data, we introduce a denoising procedure that allows the student network to
learn more robust representations. From synthetically corrupted normal images,
we train the student network to match the teacher network feature of the same
images without corruption. Second, to fuse the multi-level S-T features
adaptively, we train a segmentation network with rich supervision from
synthetic anomaly masks, achieving a substantial performance improvement.
Experiments on the industrial inspection benchmark dataset demonstrate that our
method achieves state-of-the-art performance, 98.6% on image-level AUC, 75.8%
on pixel-level average precision, and 76.4% on instance-level average
precision.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FS-BAN: Born-Again Networks for Domain Generalization Few-Shot
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.10930v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.10930v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunqing Zhao, Ngai-Man Cheung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conventional Few-shot classification (FSC) aims to recognize samples from
novel classes given limited labeled data. Recently, domain generalization FSC
(DG-FSC) has been proposed with the goal to recognize novel class samples from
unseen domains. DG-FSC poses considerable challenges to many models due to the
domain shift between base classes (used in training) and novel classes
(encountered in evaluation). In this work, we make two novel contributions to
tackle DG-FSC. Our first contribution is to propose Born-Again Network (BAN)
episodic training and comprehensively investigate its effectiveness for DG-FSC.
As a specific form of knowledge distillation, BAN has been shown to achieve
improved generalization in conventional supervised classification with a
closed-set setup. This improved generalization motivates us to study BAN for
DG-FSC, and we show that BAN is promising to address the domain shift
encountered in DG-FSC. Building on the encouraging findings, our second (major)
contribution is to propose Few-Shot BAN (FS-BAN), a novel BAN approach for
DG-FSC. Our proposed FS-BAN includes novel multi-task learning objectives:
Mutual Regularization, Mismatched Teacher, and Meta-Control Temperature, each
of these is specifically designed to overcome central and unique challenges in
DG-FSC, namely overfitting and domain discrepancy. We analyze different design
choices of these techniques. We conduct comprehensive quantitative and
qualitative analysis and evaluation over six datasets and three baseline
models. The results suggest that our proposed FS-BAN consistently improves the
generalization performance of baseline models and achieves state-of-the-art
accuracy for DG-FSC. Project Page: https://yunqing-me.github.io/Born-Again-FS/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 9 figures, 15 tables. IEEE Transactions on Image Processing
  (TIP), 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Actionlet-Dependent Contrastive Learning for Unsupervised Skeleton-Based
  Action Recognition <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10904v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10904v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lilang Lin, Jiahang Zhang, Jiaying Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The self-supervised pretraining paradigm has achieved great success in
skeleton-based action recognition. However, these methods treat the motion and
static parts equally, and lack an adaptive design for different parts, which
has a negative impact on the accuracy of action recognition. To realize the
adaptive action modeling of both parts, we propose an Actionlet-Dependent
Contrastive Learning method (ActCLR). The actionlet, defined as the
discriminative subset of the human skeleton, effectively decomposes motion
regions for better action modeling. In detail, by contrasting with the static
anchor without motion, we extract the motion region of the skeleton data, which
serves as the actionlet, in an unsupervised manner. Then, centering on
actionlet, a motion-adaptive data transformation method is built. Different
data transformations are applied to actionlet and non-actionlet regions to
introduce more diversity while maintaining their own characteristics.
Meanwhile, we propose a semantic-aware feature pooling method to build feature
representations among motion and static regions in a distinguished manner.
Extensive experiments on NTU RGB+D and PKUMMD show that the proposed method
achieves remarkable action recognition performance. More visualization and
quantitative experiments demonstrate the effectiveness of our method. Our
project website is available at https://langlandslin.github.io/projects/ActCLR/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2023 (Highlight). The project page is at
  https://langlandslin.github.io/projects/ActCLR/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust Dynamic Radiance Fields <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.02239v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.02239v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu-Lun Liu, Chen Gao, Andreas Meuleman, Hung-Yu Tseng, Ayush Saraf, Changil Kim, Yung-Yu Chuang, Johannes Kopf, Jia-Bin Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dynamic radiance field reconstruction methods aim to model the time-varying
structure and appearance of a dynamic scene. Existing methods, however, assume
that accurate camera poses can be reliably estimated by Structure from Motion
(SfM) algorithms. These methods, thus, are unreliable as SfM algorithms often
fail or produce erroneous poses on challenging videos with highly dynamic
objects, poorly textured surfaces, and rotating camera motion. We address this
robustness issue by jointly estimating the static and dynamic radiance fields
along with the camera parameters (poses and focal length). We demonstrate the
robustness of our approach via extensive quantitative and qualitative
experiments. Our results show favorable performance over the state-of-the-art
dynamic view synthesis methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023. Project page: https://robust-dynrf.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-view Inverse Rendering for Large-scale Real-world Indoor Scenes <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.10206v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.10206v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Li, Lingli Wang, Mofang Cheng, Cihui Pan, Jiaqi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a efficient multi-view inverse rendering method for large-scale
real-world indoor scenes that reconstructs global illumination and
physically-reasonable SVBRDFs. Unlike previous representations, where the
global illumination of large scenes is simplified as multiple environment maps,
we propose a compact representation called Texture-based Lighting (TBL). It
consists of 3D mesh and HDR textures, and efficiently models direct and
infinite-bounce indirect lighting of the entire large scene. Based on TBL, we
further propose a hybrid lighting representation with precomputed irradiance,
which significantly improves the efficiency and alleviates the rendering noise
in the material optimization. To physically disentangle the ambiguity between
materials, we propose a three-stage material optimization strategy based on the
priors of semantic segmentation and room segmentation. Extensive experiments
show that the proposed method outperforms the state-of-the-art quantitatively
and qualitatively, and enables physically-reasonable mixed-reality applications
such as material editing, editable novel view synthesis and relighting. The
project page is at https://lzleejean.github.io/TexIR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2023. The project page is at:
  https://lzleejean.github.io/TexIR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Top-Down Beats Bottom-Up in 3D Instance Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.02871v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.02871v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maksim Kolodiazhnyi, Danila Rukhovich, Anna Vorontsova, Anton Konushin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most 3D instance segmentation methods exploit a bottom-up strategy, typically
including resource-exhaustive post-processing. For point grouping, bottom-up
methods rely on prior assumptions about the objects in the form of
hyperparameters, which are domain-specific and need to be carefully tuned. On
the contrary, we address 3D instance segmentation with a TD3D: top-down, fully
data-driven, simple approach trained in an end-to-end manner. With its
straightforward fully-convolutional pipeline, it performs surprisingly well on
the standard benchmarks: ScanNet v2, its extension ScanNet200, and S3DIS.
Besides, our method is much faster on inference than the current
state-of-the-art grouping-based approaches. Code is available at
https://github.com/SamsungLabs/td3d .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Explicit Visual <span class="highlight-title">Prompt</span>ing for Low-Level Structure Segmentations <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10883v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10883v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weihuang Liu, Xi Shen, Chi-Man Pun, Xiaodong Cun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the generic problem of detecting low-level structures in images,
which includes segmenting the manipulated parts, identifying out-of-focus
pixels, separating shadow regions, and detecting concealed objects. Whereas
each such topic has been typically addressed with a domain-specific solution,
we show that a unified approach performs well across all of them. We take
inspiration from the widely-used pre-training and then prompt tuning protocols
in NLP and propose a new visual prompting model, named Explicit Visual
Prompting (EVP). Different from the previous visual prompting which is
typically a dataset-level implicit embedding, our key insight is to enforce the
tunable parameters focusing on the explicit visual content from each individual
image, i.e., the features from frozen patch embeddings and the input's
high-frequency components. The proposed EVP significantly outperforms other
parameter-efficient tuning protocols under the same amount of tunable
parameters (5.7% extra trainable parameters of each task). EVP also achieves
state-of-the-art performances on diverse low-level structure segmentation tasks
compared to task-specific solutions. Our code is available at:
https://github.com/NiFangBaAGe/Explicit-Visual-Prompt.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Octuplet Loss: Make Face Recognition Robust to Image Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06726v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06726v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martin Knoche, Mohamed Elkadeem, Stefan Hörmann, Gerhard Rigoll
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image resolution, or in general, image quality, plays an essential role in
the performance of today's face recognition systems. To address this problem,
we propose a novel combination of the popular triplet loss to improve
robustness against image resolution via fine-tuning of existing face
recognition models. With octuplet loss, we leverage the relationship between
high-resolution images and their synthetically down-sampled variants jointly
with their identity labels. Fine-tuning several state-of-the-art approaches
with our method proves that we can significantly boost performance for
cross-resolution (high-to-low resolution) face verification on various datasets
without meaningfully exacerbating the performance on high-to-high resolution
images. Our method applied on the FaceTransformer network achieves 95.12% face
verification accuracy on the challenging XQLFW dataset while reaching 99.73% on
the LFW database. Moreover, the low-to-low face verification accuracy benefits
from our method. We release our code to allow seamless integration of the
octuplet loss into existing frameworks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ COLA: COarse LAbel <span class="highlight-title">pre-train</span>ing for 3D semantic segmentation of sparse
  LiDAR <span class="highlight-title">dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.06884v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.06884v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jules Sanchez, Jean-Emmanuel Deschaud, François Goulette
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transfer learning is a proven technique in 2D computer vision to leverage the
large amount of data available and achieve high performance with datasets
limited in size due to the cost of acquisition or annotation. In 3D, annotation
is known to be a costly task; nevertheless, pre-training methods have only
recently been investigated. Due to this cost, unsupervised pre-training has
been heavily favored. In this work, we tackle the case of real-time 3D semantic
segmentation of sparse autonomous driving LiDAR scans. Such datasets have been
increasingly released, but each has a unique label set. We propose here an
intermediate-level label set called coarse labels, which can easily be used on
any existing and future autonomous driving datasets, thus allowing all the data
available to be leveraged at once without any additional manual labeling. This
way, we have access to a larger dataset, alongside a simple task of semantic
segmentation. With it, we introduce a new pre-training task: coarse label
pre-training, also called COLA. We thoroughly analyze the impact of COLA on
various datasets and architectures and show that it yields a noticeable
performance improvement, especially when only a small dataset is available for
the finetuning task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Physics Driven Deep Retinex Fusion for Adaptive Infrared and Visible
  Image Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.02869v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.02869v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanjie Gu, Zhibo Xiao, Yinghan Guan, Haoran Dai, Cheng Liu, Liang Xue, Shouyu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Convolutional neural networks have turned into an illustrious tool for image
fusion and super-resolution. However, their excellent performance cannot work
without large fixed-paired datasets; and additionally, these high-demanded
ground truth data always cannot be obtained easily in fusion tasks. In this
study, we show that, the structures of generative networks capture a great deal
of image feature priors, and then these priors are sufficient to reconstruct
high-quality fused super-resolution result using only low-resolution inputs. By
this way, we propose a novel self-supervised dataset-free method for adaptive
infrared (IR) and visible (VIS) image super-resolution fusion named Deep
Retinex Fusion (DRF). The key idea of DRF is first generating component priors
which are disentangled from physical model using our designed generative
networks ZipperNet, LightingNet and AdjustingNet, then combining these priors
which captured by networks via adaptive fusion loss functions based on Retinex
theory, and finally reconstructing the super-resolution fusion results.
Furthermore, in order to verify the effectiveness of our reported DRF, both
qualitative and quantitative experiments via comparing with other
state-of-the-art methods are performed using different test sets. These results
prove that, comparing with large datasets trained methods, DRF which works
without any dataset achieves the best super-resolution fusion performance; and
more importantly, DRF can adaptively balance IR and VIS information and has
good noise immunity. DRF codes are open source available at
https://github.com/GuYuanjie/Deep-Retinex-fusion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TINC: Tree-structured Implicit Neural Compression <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.06689v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.06689v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Runzhao Yang, Tingxiong Xiao, Yuxiao Cheng, Jinli Suo, Qionghai Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Implicit neural representation (INR) can describe the target scenes with high
fidelity using a small number of parameters, and is emerging as a promising
data compression technique. However, limited spectrum coverage is intrinsic to
INR, and it is non-trivial to remove redundancy in diverse complex data
effectively. Preliminary studies can only exploit either global or local
correlation in the target data and thus of limited performance. In this paper,
we propose a Tree-structured Implicit Neural Compression (TINC) to conduct
compact representation for local regions and extract the shared features of
these local representations in a hierarchical manner. Specifically, we use
Multi-Layer Perceptrons (MLPs) to fit the partitioned local regions, and these
MLPs are organized in tree structure to share parameters according to the
spatial distance. The parameter sharing scheme not only ensures the continuity
between adjacent regions, but also jointly removes the local and non-local
redundancy. Extensive experiments show that TINC improves the compression
fidelity of INR, and has shown impressive compression capabilities over
commercial tools and other deep learning based methods. Besides, the approach
is of high flexibility and can be tailored for different data and parameter
settings. The source code can be found at https://github.com/RichealYoung/TINC .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hierarchical Knowledge Guided Learning for Real-world Retinal Diseases
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.08913v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.08913v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lie Ju, Zhen Yu, Lin Wang, Xin Zhao, Xin Wang, Paul Bonnington, Zongyuan Ge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the real world, medical datasets often exhibit a long-tailed data
distribution (i.e., a few classes occupy the majority of the data, while most
classes have only a limited number of samples), which results in a challenging
long-tailed learning scenario. Some recently published datasets in
ophthalmology AI consist of more than 40 kinds of retinal diseases with complex
abnormalities and variable morbidity. Nevertheless, more than 30 conditions are
rarely seen in global patient cohorts. From a modeling perspective, most deep
learning models trained on these datasets may lack the ability to generalize to
rare diseases where only a few available samples are presented for training. In
addition, there may be more than one disease for the presence of the retina,
resulting in a challenging label co-occurrence scenario, also known as
\textit{multi-label}, which can cause problems when some re-sampling strategies
are applied during training. To address the above two major challenges, this
paper presents a novel method that enables the deep neural network to learn
from a long-tailed fundus database for various retinal disease recognition.
Firstly, we exploit the prior knowledge in ophthalmology to improve the feature
representation using a hierarchy-aware pre-training. Secondly, we adopt an
instance-wise class-balanced sampling strategy to address the label
co-occurrence issue under the long-tailed medical dataset scenario. Thirdly, we
introduce a novel hybrid knowledge distillation to train a less biased
representation and classifier. We conducted extensive experiments on four
databases, including two public datasets and two in-house databases with more
than one million fundus images. The experimental results demonstrate the
superiority of our proposed methods with recognition accuracy outperforming the
state-of-the-art competitors, especially for these rare diseases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Commonsense Knowledge Assisted Deep Learning for Resource-constrained
  and Fine-grained Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.09026v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.09026v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pu Zhang, Bin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we consider fine-grained image object detection in
resource-constrained cases such as edge computing. Deep learning (DL), namely
learning with deep neural networks (DNNs), has become the dominating approach
to object detection. To achieve accurate fine-grained detection, one needs to
employ a large enough DNN model and a vast amount of data annotations, which
brings a challenge for using modern DL object detectors in resource-constrained
cases. To this end, we propose an approach, which leverages commonsense
knowledge to assist a coarse-grained object detector to get accurate
fine-grained detection results. Specifically, we introduce a commonsense
knowledge inference module (CKIM) to translate coarse-grained labels given by a
backbone lightweight coarse-grained DL detector to fine-grained labels. We
consider both crisp-rule and fuzzy-rule based inference in our CKIM; the latter
is used to handle ambiguity in the target semantic labels. We implement our
method based on several modern DL detectors, namely YOLOv4, Mobilenetv3-SSD and
YOLOv7-tiny. Experiment results show that our approach outperforms benchmark
detectors remarkably in terms of accuracy, model size and processing latency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ I Can't Believe There's No Images! Learning Visual Tasks Using only
  Language Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.09778v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.09778v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sophia Gu, Christopher Clark, Aniruddha Kembhavi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many high-level skills that are required for computer vision tasks, such as
parsing questions, comparing and contrasting semantics, and writing
descriptions, are also required in other domains such as natural language
processing. In this paper, we ask whether it is possible to learn those skills
from textual data and then transfer them to vision tasks without ever training
on visual training data. Key to our approach is exploiting the joint embedding
space of contrastively trained vision and language encoders. In practice, there
can be systematic differences between embedding spaces for different modalities
in contrastive models, and we analyze how these differences affect our approach
and study strategies to mitigate this concern. We produce models using only
text training data on four representative tasks: image captioning, visual
entailment, visual question answering and visual news, and evaluate them on
standard benchmarks using images. We find these models generally perform close
to models trained on images, while surpassing prior work for captioning and
visual entailment in this text only setting by over 9 points, and outperforming
all prior work on visual news by over 30 points. We also showcase a variety of
stylistic image captioning models that are trained using no image data and no
human-curated language data, but instead using readily-available text data from
books, the web, or language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>website (https://prior.allenai.org/projects/close), code
  (https://github.com/allenai/close)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VectorMapNet: End-to-end Vectorized HD Map Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.08920v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.08920v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yicheng Liu, Yuantian Yuan, Yue Wang, Yilun Wang, Hang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous driving systems require a good understanding of surrounding
environments, including moving obstacles and static High-Definition (HD)
semantic map elements. Existing methods approach the semantic map problem by
offline manual annotation, which suffers from serious scalability issues.
Recent learning-based methods produce dense rasterized segmentation predictions
to construct maps. However, these predictions do not include instance
information of individual map elements and require heuristic post-processing to
obtain vectorized maps. To tackle these challenges, we introduce an end-to-end
vectorized HD map learning pipeline, termed VectorMapNet. VectorMapNet takes
onboard sensor observations and predicts a sparse set of polylines in the
bird's-eye view. This pipeline can explicitly model the spatial relation
between map elements and generate vectorized maps that are friendly to
downstream autonomous driving tasks. Extensive experiments show that
VectorMapNet achieve strong map learning performance on both nuScenes and
Argoverse2 dataset, surpassing previous state-of-the-art methods by 14.2 mAP
and 14.6mAP. Qualitatively, we also show that VectorMapNet is capable of
generating comprehensive maps and capturing more fine-grained details of road
geometry. To the best of our knowledge, VectorMapNet is the first work designed
towards end-to-end vectorized map learning from onboard observations. Our
project website is available at
https://tsinghua-mars-lab.github.io/vectormapnet/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging Inpainting for Single-Image Shadow Removal 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.05361v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.05361v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoguang Li, Qing Guo, Rabab Abdelfattah, Di Lin, Wei Feng, Ivor Tsang, Song Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fully-supervised shadow removal methods achieve the best restoration
qualities on public datasets but still generate some shadow remnants. One of
the reasons is the lack of large-scale shadow & shadow-free image pairs.
Unsupervised methods can alleviate the issue but their restoration qualities
are much lower than those of fully-supervised methods. In this work, we find
that pretraining shadow removal networks on the image inpainting dataset can
reduce the shadow remnants significantly: a naive encoder-decoder network gets
competitive restoration quality w.r.t. the state-of-the-art methods via only
10% shadow & shadow-free image pairs. After analyzing networks with/without
inpainting pre-training via the information stored in the weight (IIW), we find
that inpainting pretraining improves restoration quality in non-shadow regions
and enhances the generalization ability of networks significantly.
Additionally, shadow removal fine-tuning enables networks to fill in the
details of shadow regions. Inspired by these observations we formulate shadow
removal as an adaptive fusion task that takes advantage of both shadow removal
and image inpainting. Specifically, we develop an adaptive fusion network
consisting of two encoders, an adaptive fusion block, and a decoder. The two
encoders are responsible for extracting the feature from the shadow image and
the shadow-masked image respectively. The adaptive fusion block is responsible
for combining these features in an adaptive manner. Finally, the decoder
converts the adaptive fused features to the desired shadow-free result. The
extensive experiments show that our method empowered with inpainting
outperforms all state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Subspace Perturbation Analysis for Data-Driven Radar Target Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08241v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08241v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shyam Venkatasubramanian, Sandeep Gogineni, Bosung Kang, Ali Pezeshki, Muralidhar Rangaswamy, Vahid Tarokh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works exploring data-driven approaches to classical problems in
adaptive radar have demonstrated promising results pertaining to the task of
radar target localization. Via the use of space-time adaptive processing (STAP)
techniques and convolutional neural networks, these data-driven approaches to
target localization have helped benchmark the performance of neural networks
for matched scenarios. However, the thorough bridging of these topics across
mismatched scenarios still remains an open problem. As such, in this work, we
augment our data-driven approach to radar target localization by performing a
subspace perturbation analysis, which allows us to benchmark the localization
accuracy of our proposed deep learning framework across mismatched scenarios.
To evaluate this framework, we generate comprehensive datasets by randomly
placing targets of variable strengths in mismatched constrained areas via
RFView, a high-fidelity, site-specific modeling and simulation tool. For the
radar returns from these constrained areas, we generate heatmap tensors in
range, azimuth, and elevation using the normalized adaptive matched filter
(NAMF) test statistic. We estimate target locations from these heatmap tensors
using a convolutional neural network, and demonstrate that the predictive
performance of our framework in the presence of mismatches can be
predetermined.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 3 figures. Submitted to 2023 IEEE Radar Conference
  (RadarConf). Extension of arXiv:2209.02890</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LidarMultiNet: Towards a Unified Multi-Task Network for LiDAR Perception <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.09385v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.09385v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongqiangzi Ye, Zixiang Zhou, Weijia Chen, Yufei Xie, Yu Wang, Panqu Wang, Hassan Foroosh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LiDAR-based 3D object detection, semantic segmentation, and panoptic
segmentation are usually implemented in specialized networks with distinctive
architectures that are difficult to adapt to each other. This paper presents
LidarMultiNet, a LiDAR-based multi-task network that unifies these three major
LiDAR perception tasks. Among its many benefits, a multi-task network can
reduce the overall cost by sharing weights and computation among multiple
tasks. However, it typically underperforms compared to independently combined
single-task models. The proposed LidarMultiNet aims to bridge the performance
gap between the multi-task network and multiple single-task networks. At the
core of LidarMultiNet is a strong 3D voxel-based encoder-decoder architecture
with a Global Context Pooling (GCP) module extracting global contextual
features from a LiDAR frame. Task-specific heads are added on top of the
network to perform the three LiDAR perception tasks. More tasks can be
implemented simply by adding new task-specific heads while introducing little
additional cost. A second stage is also proposed to refine the first-stage
segmentation and generate accurate panoptic segmentation results. LidarMultiNet
is extensively tested on both Waymo Open Dataset and nuScenes dataset,
demonstrating for the first time that major LiDAR perception tasks can be
unified in a single strong network that is trained end-to-end and achieves
state-of-the-art performance. Notably, LidarMultiNet reaches the official 1st
place in the Waymo Open Dataset 3D semantic segmentation challenge 2022 with
the highest mIoU and the best accuracy for most of the 22 classes on the test
set, using only LiDAR points as input. It also sets the new state-of-the-art
for a single model on the Waymo 3D object detection benchmark and three
nuScenes benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI 2023 (Oral). Full-length paper extending our
  previous technical report of the 1st place solution of the 2022 Waymo Open
  Dataset 3D Semantic Segmentation challenge, including evaluations on 5 major
  benchmarks. arXiv admin note: text overlap with arXiv:2206.11428</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">17</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Application of an ontology for model cards to generate computable
  artifacts for linking machine learning information from biomedical research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11991v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11991v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Amith, Licong Cui, Kirk Roberts, Cui Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model card reports provide a transparent description of machine learning
models which includes information about their evaluation, limitations, intended
use, etc. Federal health agencies have expressed an interest in model cards
report for research studies using machine-learning based AI. Previously, we
have developed an ontology model for model card reports to structure and
formalize these reports. In this paper, we demonstrate a Java-based library
(OWL API, FaCT++) that leverages our ontology to publish computable model card
reports. We discuss future directions and other use cases that highlight
applicability and feasibility of ontology-driven systems to support FAIR
challenges.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CompoDiff: Versatile Composed Image Retrieval With Latent Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11916v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11916v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Geonmo Gu, Sanghyuk Chun, Wonjae Kim, HeeJae Jun, Yoohoon Kang, Sangdoo Yun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a novel diffusion-based model, CompoDiff, for solving
Composed Image Retrieval (CIR) with latent diffusion and presents a newly
created dataset of 18 million reference images, conditions, and corresponding
target image triplets to train the model. CompoDiff not only achieves a new
zero-shot state-of-the-art on a CIR benchmark such as FashionIQ but also
enables a more versatile CIR by accepting various conditions, such as negative
text and image mask conditions, which are unavailable with existing CIR
methods. In addition, the CompoDiff features are on the intact CLIP embedding
space so that they can be directly used for all existing models exploiting the
CLIP space. The code and dataset used for the training, and the pre-trained
weights are available at https://github.com/navervision/CompoDiff
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>First two authors contributed equally; 23 pages, 4.8MB</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal <span class="highlight-title">Pre-train</span>ing Framework for Sequential Recommendation via
  Contrastive Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11879v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11879v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingzi Zhang, Xin Zhou, Zhiqi Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential recommendation systems utilize the sequential interactions of
users with items as their main supervision signals in learning users'
preferences. However, existing methods usually generate unsatisfactory results
due to the sparsity of user behavior data. To address this issue, we propose a
novel pre-training framework, named Multimodal Sequence Mixup for Sequential
Recommendation (MSM4SR), which leverages both users' sequential behaviors and
items' multimodal content (\ie text and images) for effectively recommendation.
Specifically, MSM4SR tokenizes each item image into multiple textual keywords
and uses the pre-trained BERT model to obtain initial textual and visual
features of items, for eliminating the discrepancy between the text and image
modalities. A novel backbone network, \ie Multimodal Mixup Sequence Encoder
(M$^2$SE), is proposed to bridge the gap between the item multimodal content
and the user behavior, using a complementary sequence mixup strategy. In
addition, two contrastive learning tasks are developed to assist M$^2$SE in
learning generalized multimodal representations of the user behavior sequence.
Extensive experiments on real-world datasets demonstrate that MSM4SR
outperforms state-of-the-art recommendation methods. Moreover, we further
verify the effectiveness of MSM4SR on other challenging tasks including
cold-start and cross-domain recommendation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Debiased Contrastive Learning for Sequential Recommendation <span class="chip">WWW'2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11780v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11780v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhao Yang, Chao Huang, Lianghao Xia, Chunzhen Huang, Da Luo, Kangyi Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current sequential recommender systems are proposed to tackle the dynamic
user preference learning with various neural techniques, such as Transformer
and Graph Neural Networks (GNNs). However, inference from the highly sparse
user behavior data may hinder the representation ability of sequential pattern
encoding. To address the label shortage issue, contrastive learning (CL)
methods are proposed recently to perform data augmentation in two fashions: (i)
randomly corrupting the sequence data (e.g. stochastic masking, reordering);
(ii) aligning representations across pre-defined contrastive views. Although
effective, we argue that current CL-based methods have limitations in
addressing popularity bias and disentangling of user conformity and real
interest. In this paper, we propose a new Debiased Contrastive learning
paradigm for Recommendation (DCRec) that unifies sequential pattern encoding
with global collaborative relation modeling through adaptive conformity-aware
augmentation. This solution is designed to tackle the popularity bias issue in
recommendation systems. Our debiased contrastive learning framework effectively
captures both the patterns of item transitions within sequences and the
dependencies between users across sequences. Our experiments on various
real-world datasets have demonstrated that DCRec significantly outperforms
state-of-the-art baselines, indicating its efficacy for recommendation. To
facilitate reproducibility of our results, we make our implementation of DCRec
publicly available at: https://github.com/HKUDS/DCRec.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted by WWW'2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Recommendation Systems in Libraries: an Application with Heterogeneous
  Data Sources <span class="chip">EDBT</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11746v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11746v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessandro Speciale, Greta Vallero, Luca Vassio, Marco Mellia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Reading&Machine project exploits the support of digitalization to
increase the attractiveness of libraries and improve the users' experience. The
project implements an application that helps the users in their decision-making
process, providing recommendation system (RecSys)-generated lists of books the
users might be interested in, and showing them through an interactive Virtual
Reality (VR)-based Graphical User Interface (GUI). In this paper, we focus on
the design and testing of the recommendation system, employing data about all
users' loans over the past 9 years from the network of libraries located in
Turin, Italy. In addition, we use data collected by the Anobii online social
community of readers, who share their feedback and additional information about
books they read. Armed with this heterogeneous data, we build and evaluate
Content Based (CB) and Collaborative Filtering (CF) approaches. Our results
show that the CF outperforms the CB approach, improving by up to 47\% the
relevant recommendations provided to a reader. However, the performance of the
CB approach is heavily dependent on the number of books the reader has already
read, and it can work even better than CF for users with a large history.
Finally, our evaluations highlight that the performances of both approaches are
significantly improved if the system integrates and leverages the information
from the Anobii dataset, which allows us to include more user readings (for CF)
and richer book metadata (for CB).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at 7th International workshop on Data Analytics solutions
  for Real-LIfe APplications - 28th March-31st March, 2023, Ioannina, Greece.
  The paper will be published in the Proceedings of EDBT/ICDT 2023 Joint
  Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamically Expandable Graph Convolution for Streaming Recommendation <span class="chip">WWW 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11700v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11700v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowei He, Xu He, Yingxue Zhang, Ruiming Tang, Chen Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Personalized recommender systems have been widely studied and deployed to
reduce information overload and satisfy users' diverse needs. However,
conventional recommendation models solely conduct a one-time training-test
fashion and can hardly adapt to evolving demands, considering user preference
shifts and ever-increasing users and items in the real world. To tackle such
challenges, the streaming recommendation is proposed and has attracted great
attention recently. Among these, continual graph learning is widely regarded as
a promising approach for the streaming recommendation by academia and industry.
However, existing methods either rely on the historical data replay which is
often not practical under increasingly strict data regulations, or can seldom
solve the \textit{over-stability} issue. To overcome these difficulties, we
propose a novel \textbf{D}ynamically \textbf{E}xpandable \textbf{G}raph
\textbf{C}onvolution (DEGC) algorithm from a \textit{model isolation}
perspective for the streaming recommendation which is orthogonal to previous
methods. Based on the motivation of disentangling outdated short-term
preferences from useful long-term preferences, we design a sequence of
operations including graph convolution pruning, refining, and expanding to only
preserve beneficial long-term preference-related parameters and extract fresh
short-term preferences. Moreover, we model the temporal user preference, which
is utilized as user embedding initialization, for better capturing the
individual-level preference shifts. Extensive experiments on the three most
representative GCN-based recommendation models and four industrial datasets
demonstrate the effectiveness and robustness of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 6 figures, published on The Web Conference 2023 (WWW 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ByteCover3: Accurate Cover Song Identification on Short Queries <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11692v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11692v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingjian Du, Zijie Wang, Xia Liang, Huidong Liang, Bilei Zhu, Zejun Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning based methods have become a paradigm for cover song
identification (CSI) in recent years, where the ByteCover systems have achieved
state-of-the-art results on all the mainstream datasets of CSI. However, with
the burgeon of short videos, many real-world applications require matching
short music excerpts to full-length music tracks in the database, which is
still under-explored and waiting for an industrial-level solution. In this
paper, we upgrade the previous ByteCover systems to ByteCover3 that utilizes
local features to further improve the identification performance of short music
queries. ByteCover3 is designed with a local alignment loss (LAL) module and a
two-stage feature retrieval pipeline, allowing the system to perform CSI in a
more precise and efficient way. We evaluated ByteCover3 on multiple datasets
with different benchmark settings, where ByteCover3 beat all the compared
methods including its previous versions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepeted by ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> on Causal Inference for Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11666v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11666v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huishi Luo, Fuzhen Zhuang, Ruobing Xie, Hengshu Zhu, Deqing Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, causal inference has attracted increasing attention from
researchers of recommender systems (RS), which analyzes the relationship
between a cause and its effect and has a wide range of real-world applications
in multiple fields. Causal inference can model the causality in recommender
systems like confounding effects and deal with counterfactual problems such as
offline policy evaluation and data augmentation. Although there are already
some valuable surveys on causal recommendations, these surveys introduce
approaches in a relatively isolated way and lack theoretical analysis of
existing methods. Due to the unfamiliarity with causality to RS researchers, it
is both necessary and challenging to comprehensively review the relevant
studies from the perspective of causal theory, which might be instructive for
the readers to propose new approaches in practice. This survey attempts to
provide a systematic review of up-to-date papers in this area from a
theoretical standpoint. Firstly, we introduce the fundamental concepts of
causal inference as the basis of the following review. Then we propose a new
taxonomy from the perspective of causal techniques and further discuss
technical details about how existing methods apply causal inference to address
specific recommender issues. Finally, we highlight some promising directions
for future research in this field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under peer review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Content Retrievability in Search with Controllable Query
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11648v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11648v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gustavo Penha, Enrico Palumbo, Maryam Aziz, Alice Wang, Hugues Bouchard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An important goal of online platforms is to enable content discovery, i.e.
allow users to find a catalog entity they were not familiar with. A
pre-requisite to discover an entity, e.g. a book, with a search engine is that
the entity is retrievable, i.e. there are queries for which the system will
surface such entity in the top results. However, machine-learned search engines
have a high retrievability bias, where the majority of the queries return the
same entities. This happens partly due to the predominance of narrow intent
queries, where users create queries using the title of an already known entity,
e.g. in book search 'harry potter'. The amount of broad queries where users
want to discover new entities, e.g. in music search 'chill lyrical electronica
with an atmospheric feeling to it', and have a higher tolerance to what they
might find, is small in comparison. We focus here on two factors that have a
negative impact on the retrievability of the entities (I) the training data
used for dense retrieval models and (II) the distribution of narrow and broad
intent queries issued in the system. We propose CtrlQGen, a method that
generates queries for a chosen underlying intent-narrow or broad. We can use
CtrlQGen to improve factor (I) by generating training data for dense retrieval
models comprised of diverse synthetic queries. CtrlQGen can also be used to
deal with factor (II) by suggesting queries with broader intents to users. Our
results on datasets from the domains of music, podcasts, and books reveal that
we can significantly decrease the retrievability bias of a dense retrieval
model when using CtrlQGen. First, by using the generated queries as training
data for dense models we make 9% of the entities retrievable (go from zero to
non-zero retrievability). Second, by suggesting broader queries to users, we
can make 12% of the entities retrievable in the best case.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in the International World Wide Web
  Conference 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bounding System-Induced Biases in Recommender Systems with A Randomized
  <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11574v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11574v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dugang Liu, Pengxiang Cheng, Zinan Lin, Xiaolian Zhang, Zhenhua Dong, Rui Zhang, Xiuqiang He, Weike Pan, Zhong Ming
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Debiased recommendation with a randomized dataset has shown very promising
results in mitigating the system-induced biases. However, it still lacks more
theoretical insights or an ideal optimization objective function compared with
the other more well studied route without a randomized dataset. To bridge this
gap, we study the debiasing problem from a new perspective and propose to
directly minimize the upper bound of an ideal objective function, which
facilitates a better potential solution to the system-induced biases. Firstly,
we formulate a new ideal optimization objective function with a randomized
dataset. Secondly, according to the prior constraints that an adopted loss
function may satisfy, we derive two different upper bounds of the objective
function, i.e., a generalization error bound with the triangle inequality and a
generalization error bound with the separability. Thirdly, we show that most
existing related methods can be regarded as the insufficient optimization of
these two upper bounds. Fourthly, we propose a novel method called debiasing
approximate upper bound with a randomized dataset (DUB), which achieves a more
sufficient optimization of these upper bounds. Finally, we conduct extensive
experiments on a public dataset and a real product dataset to verify the
effectiveness of our DUB.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM TOIS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ QAID: Question Answering Inspired Few-shot Intent Detection <span class="chip">ICLR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.01593v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.01593v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Asaf Yehudai, Matan Vetzler, Yosi Mass, Koren Lazar, Doron Cohen, Boaz Carmeli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intent detection with semantically similar fine-grained intents is a
challenging task. To address it, we reformulate intent detection as a
question-answering retrieval task by treating utterances and intent names as
questions and answers. To that end, we utilize a question-answering retrieval
architecture and adopt a two stages training schema with batch contrastive
loss. In the pre-training stage, we improve query representations through
self-supervised training. Then, in the fine-tuning stage, we increase
contextualized token-level similarity scores between queries and answers from
the same intent. Our results on three few-shot intent detection benchmarks
achieve state-of-the-art performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Query Representations for Dense Retrieval with Pseudo
  Relevance Feedback: A Reproducibility Study <span class="chip">ECIR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.06400v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.06400v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hang Li, Shengyao Zhuang, Ahmed Mourad, Xueguang Ma, Jimmy Lin, Guido Zuccon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pseudo-Relevance Feedback (PRF) utilises the relevance signals from the top-k
passages from the first round of retrieval to perform a second round of
retrieval aiming to improve search effectiveness. A recent research direction
has been the study and development of PRF methods for deep language models
based rankers, and in particular in the context of dense retrievers. Dense
retrievers, compared to more complex neural rankers, provide a trade-off
between effectiveness, which is often reduced compared to more complex neural
rankers, and query latency, which also is reduced making the retrieval pipeline
more efficient. The introduction of PRF methods for dense retrievers has been
motivated as an attempt to further improve their effectiveness.
  In this paper, we reproduce and study a recent method for PRF with dense
retrievers, called ANCE-PRF. This method concatenates the query text and that
of the top-k feedback passages to form a new query input, which is then encoded
into a dense representation using a newly trained query encoder based on the
original dense retriever used for the first round of retrieval. While the
method can potentially be applied to any of the existing dense retrievers,
prior work has studied it only in the context of the ANCE dense retriever.
  We study the reproducibility of ANCE-PRF in terms of both its training
(encoding of the PRF signal) and inference (ranking) steps. We further extend
the empirical analysis provided in the original work to investigate the effect
of the hyper-parameters that govern the training process and the robustness of
the method across these different settings. Finally, we contribute a study of
the generalisability of the ANCE-PRF method when dense retrievers other than
ANCE are used for the first round of retrieval and for encoding the PRF signal.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ECIR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adap-$τ$: Adaptively Modulating Embedding Magnitude for
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.04775v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.04775v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawei Chen, Junkang Wu, Jiancan Wu, Sheng Zhou, Xuezhi Cao, Xiangnan He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have witnessed the great successes of embedding-based methods in
recommender systems. Despite their decent performance, we argue one potential
limitation of these methods -- the embedding magnitude has not been explicitly
modulated, which may aggravate popularity bias and training instability,
hindering the model from making a good recommendation. It motivates us to
leverage the embedding normalization in recommendation. By normalizing
user/item embeddings to a specific value, we empirically observe impressive
performance gains (9\% on average) on four real-world datasets. Although
encouraging, we also reveal a serious limitation when applying normalization in
recommendation -- the performance is highly sensitive to the choice of the
temperature $\tau$ which controls the scale of the normalized embeddings.
  To fully foster the merits of the normalization while circumvent its
limitation, this work studied on how to adaptively set the proper $\tau$.
Towards this end, we first make a comprehensive analyses of $\tau$ to fully
understand its role on recommendation. We then accordingly develop an adaptive
fine-grained strategy Adap-$\tau$ for the temperature with satisfying four
desirable properties including adaptivity, personalized, efficiency and
model-agnostic. Extensive experiments have been conducted to validate the
effectiveness of the proposal. The code is available at
\url{https://github.com/junkangwu/Adap_tau}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Automated <span class="highlight-title">Self-Supervised</span> Learning for Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07797v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07797v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lianghao Xia, Chao Huang, Chunzhen Huang, Kangyi Lin, Tao Yu, Ben Kao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph neural networks (GNNs) have emerged as the state-of-the-art paradigm
for collaborative filtering (CF). To improve the representation quality over
limited labeled data, contrastive learning has attracted attention in
recommendation and benefited graph-based CF model recently. However, the
success of most contrastive methods heavily relies on manually generating
effective contrastive views for heuristic-based data augmentation. This does
not generalize across different datasets and downstream recommendation tasks,
which is difficult to be adaptive for data augmentation and robust to noise
perturbation. To fill this crucial gap, this work proposes a unified Automated
Collaborative Filtering (AutoCF) to automatically perform data augmentation for
recommendation. Specifically, we focus on the generative self-supervised
learning framework with a learnable augmentation paradigm that benefits the
automated distillation of important self-supervised signals. To enhance the
representation discrimination ability, our masked graph autoencoder is designed
to aggregate global information during the augmentation via reconstructing the
masked subgraph structures. Experiments and ablation studies are performed on
several public datasets for recommending products, venues, and locations.
Results demonstrate the superiority of AutoCF against various baseline methods.
We release the model implementation at https://github.com/HKUDS/AutoCF.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM The Web Conference, 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph-less Collaborative Filtering <span class="chip">WWW 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08537v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08537v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lianghao Xia, Chao Huang, Jiao Shi, Yong Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph neural networks (GNNs) have shown the power in representation learning
over graph-structured user-item interaction data for collaborative filtering
(CF) task. However, with their inherently recursive message propagation among
neighboring nodes, existing GNN-based CF models may generate indistinguishable
and inaccurate user (item) representations due to the over-smoothing and noise
effect with low-pass Laplacian smoothing operators. In addition, the recursive
information propagation with the stacked aggregators in the entire graph
structures may result in poor scalability in practical applications. Motivated
by these limitations, we propose a simple and effective collaborative filtering
model (SimRec) that marries the power of knowledge distillation and contrastive
learning. In SimRec, adaptive transferring knowledge is enabled between the
teacher GNN model and a lightweight student network, to not only preserve the
global collaborative signals, but also address the over-smoothing issue with
representation recalibration. Empirical results on public datasets show that
SimRec archives better efficiency while maintaining superior recommendation
performance compared with various strong baselines. Our implementations are
publicly available at: https://github.com/HKUDS/SimRec.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM WWW 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Recommendation Systems with User Personality Inferred from
  Product <span class="highlight-title">Review</span>s <span class="chip">WSDM'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05039v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05039v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyuan Lu, Min-Yen Kan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Personality is a psychological factor that reflects people's preferences,
which in turn influences their decision-making. We hypothesize that accurate
modeling of users' personalities improves recommendation systems' performance.
However, acquiring such personality profiles is both sensitive and expensive.
We address this problem by introducing a novel method to automatically extract
personality profiles from public product review text. We then design and assess
three context-aware recommendation architectures that leverage the profiles to
test our hypothesis.
  Experiments on our two newly contributed personality datasets --
Amazon-beauty and Amazon-music -- validate our hypothesis, showing performance
boosts of 3--28%.Our analysis uncovers that varying personality types
contribute differently to recommendation performance: open and extroverted
personalities are most helpful in music recommendation, while a conscientious
personality is most helpful in beauty product recommendation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IRS@WSDM'23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MFBE: Leveraging Multi-Field Information of FAQs for Efficient Dense
  Retrieval <span class="chip">PAKDD</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.11953v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.11953v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Debopriyo Banerjee, Mausam Jain, Ashish Kulkarni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the domain of question-answering in NLP, the retrieval of Frequently Asked
Questions (FAQ) is an important sub-area which is well researched and has been
worked upon for many languages. Here, in response to a user query, a retrieval
system typically returns the relevant FAQs from a knowledge-base. The efficacy
of such a system depends on its ability to establish semantic match between the
query and the FAQs in real-time. The task becomes challenging due to the
inherent lexical gap between queries and FAQs, lack of sufficient context in
FAQ titles, scarcity of labeled data and high retrieval latency. In this work,
we propose a bi-encoder-based query-FAQ matching model that leverages multiple
combinations of FAQ fields (like, question, answer, and category) both during
model training and inference. Our proposed Multi-Field Bi-Encoder (MFBE) model
benefits from the additional context resulting from multiple FAQ fields and
performs well even with minimal labeled data. We empirically support this claim
through experiments on proprietary as well as open-source public datasets in
both unsupervised and supervised settings. Our model achieves around 27% and
20% better top-1 accuracy for the FAQ retrieval task on internal and open
datasets, respectively over the best performing baseline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors contributed equally to this work. 12 pages, 3
  figures, 5 tables. Accepted at the 2023 Pacific-Asia Conference On Knowledge
  Discovery And Data Mining (PAKDD)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">152</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dexterity from Touch: <span class="highlight-title">Self-Supervised</span> <span class="highlight-title">Pre-Train</span>ing of Tactile
  Representations with Robotic Play 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12076v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12076v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Irmak Guzey, Ben Evans, Soumith Chintala, Lerrel Pinto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Teaching dexterity to multi-fingered robots has been a longstanding challenge
in robotics. Most prominent work in this area focuses on learning controllers
or policies that either operate on visual observations or state estimates
derived from vision. However, such methods perform poorly on fine-grained
manipulation tasks that require reasoning about contact forces or about objects
occluded by the hand itself. In this work, we present T-Dex, a new approach for
tactile-based dexterity, that operates in two phases. In the first phase, we
collect 2.5 hours of play data, which is used to train self-supervised tactile
encoders. This is necessary to bring high-dimensional tactile readings to a
lower-dimensional embedding. In the second phase, given a handful of
demonstrations for a dexterous task, we learn non-parametric policies that
combine the tactile observations with visual ones. Across five challenging
dexterous tasks, we show that our tactile-based dexterity models outperform
purely vision and torque-based models by an average of 1.7X. Finally, we
provide a detailed analysis on factors critical to T-Dex including the
importance of play data, architectures, and representation learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Video and code can be accessed here:
  https://tactile-dexterity.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Representational Status of Deep Learning Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12032v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12032v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eamon Duede
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper aims to clarify the representational status of Deep Learning
Models (DLMs). While commonly referred to as 'representations', what this
entails is ambiguous due to a conflation of functional and relational
conceptions of representation. This paper argues that while DLMs represent
their targets in a relational sense, they are best understood as highly
idealized models. This result has immediate implications for explainable AI
(XAI) and directs philosophical attention toward examining the idealized nature
of DLM representations and their role in future scientific investigation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semantic Latent Space Regression of Diffusion Autoencoders for Vertebral
  Fracture Grading 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12031v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12031v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthias Keicher, Matan Atad, David Schinz, Alexandra S. Gersing, Sarah C. Foreman, Sophia S. Goller, Juergen Weissinger, Jon Rischewski, Anna-Sophia Dietrich, Benedikt Wiestler, Jan S. Kirschke, Nassir Navab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vertebral fractures are a consequence of osteoporosis, with significant
health implications for affected patients. Unfortunately, grading their
severity using CT exams is hard and subjective, motivating automated grading
methods. However, current approaches are hindered by imbalance and scarcity of
data and a lack of interpretability. To address these challenges, this paper
proposes a novel approach that leverages unlabelled data to train a generative
Diffusion Autoencoder (DAE) model as an unsupervised feature extractor. We
model fracture grading as a continuous regression, which is more reflective of
the smooth progression of fractures. Specifically, we use a binary, supervised
fracture classifier to construct a hyperplane in the DAE's latent space. We
then regress the severity of the fracture as a function of the distance to this
hyperplane, calibrating the results to the Genant scale. Importantly, the
generative nature of our method allows us to visualize different grades of a
given vertebra, providing interpretability and insight into the features that
contribute to automated grading.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph Kalman Filters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12021v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12021v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cesare Alippi, Daniele Zambon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The well-known Kalman filters model dynamical systems by relying on
state-space representations with the next state updated, and its uncertainty
controlled, by fresh information associated with newly observed system outputs.
This paper generalizes, for the first time in the literature, Kalman and
extended Kalman filters to discrete-time settings where inputs, states, and
outputs are represented as attributed graphs whose topology and attributes can
change with time. The setup allows us to adapt the framework to cases where the
output is a vector or a scalar too (node/graph level tasks). Within the
proposed theoretical framework, the unknown state-transition and the readout
functions are learned end-to-end along with the downstream prediction task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ End-to-End Integration of Speech Separation and Voice Activity Detection
  for Low-Latency Diarization of Telephone Conversations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12002v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12002v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giovanni Morrone, Samuele Cornell, Luca Serafini, Enrico Zovato, Alessio Brutti, Stefano Squartini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works show that speech separation guided diarization (SSGD) is an
increasingly promising direction, mainly thanks to the recent progress in
speech separation. It performs diarization by first separating the speakers and
then applying voice activity detection (VAD) on each separated stream. In this
work we conduct an in-depth study of SSGD in the conversational telephone
speech (CTS) domain, focusing mainly on low-latency streaming diarization
applications. We consider three state-of-the-art speech separation (SSep)
algorithms and study their performance both in online and offline scenarios,
considering non-causal and causal implementations as well as continuous SSep
(CSS) windowed inference. We compare different SSGD algorithms on two widely
used CTS datasets: CALLHOME and Fisher Corpus (Part 1 and 2) and evaluate both
separation and diarization performance. To improve performance, a novel, causal
and computationally efficient leakage removal algorithm is proposed, which
significantly decreases false alarms. We also explore, for the first time,
fully end-to-end SSGD integration between SSep and VAD modules. Crucially, this
enables fine-tuning on real-world data for which oracle speakers sources are
not available. In particular, our best model achieves 8.8% DER on CALLHOME,
which outperforms the current state-of-the-art end-to-end neural diarization
model, despite being trained on an order of magnitude less data and having
significantly lower latency, i.e., 0.1 vs. 1 seconds. Finally, we also show
that the separated signals can be readily used also for automatic speech
recognition, reaching performance close to using oracle sources in some
configurations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bayesian Optimization for Function Compositions with Applications to
  Dynamic Pricing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11954v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11954v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kunal Jain, Prabuchandran K. J., Tejas Bodas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bayesian Optimization (BO) is used to find the global optima of black box
functions. In this work, we propose a practical BO method of function
compositions where the form of the composition is known but the constituent
functions are expensive to evaluate. By assuming an independent Gaussian
process (GP) model for each of the constituent black-box function, we propose
EI and UCB based BO algorithms and demonstrate their ability to outperform
vanilla BO and the current state-of-art algorithms. We demonstrate a novel
application of the proposed methods to dynamic pricing in revenue management
when the underlying demand function is expensive to evaluate.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 6 figures, 1 table, To be published in: 17th Learning And
  Intelligent Optimization Conference (LION17)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Using Explanations to Guide Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11932v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11932v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sukrut Rao, Moritz Böhle, Amin Parchami-Araghi, Bernt Schiele
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks are highly performant, but might base their decision on
spurious or background features that co-occur with certain classes, which can
hurt generalization. To mitigate this issue, the usage of 'model guidance' has
gained popularity recently: for this, models are guided to be "right for the
right reasons" by regularizing the models' explanations to highlight the right
features. Experimental validation of these approaches has thus far however been
limited to relatively simple and / or synthetic datasets. To gain a better
understanding of which model-guiding approaches actually transfer to more
challenging real-world datasets, in this work we conduct an in-depth evaluation
across various loss functions, attribution methods, models, and 'guidance
depths' on the PASCAL VOC 2007 and MS COCO 2014 datasets, and show that model
guidance can sometimes even improve model performance. In this context, we
further propose a novel energy loss, show its effectiveness in directing the
model to focus on object features. We also show that these gains can be
achieved even with a small fraction (e.g. 1%) of bounding box annotations,
highlighting the cost effectiveness of this approach. Lastly, we show that this
approach can also improve generalization under distribution shifts. Code will
be made available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages, 35 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do intermediate feature coalitions aid explainability of black-box
  models? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11920v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11920v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minal Suresh Patil, Kary Främling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work introduces the notion of intermediate concepts based on levels
structure to aid explainability for black-box models. The levels structure is a
hierarchical structure in which each level corresponds to features of a dataset
(i.e., a player-set partition). The level of coarseness increases from the
trivial set, which only comprises singletons, to the set, which only contains
the grand coalition. In addition, it is possible to establish meronomies, i.e.,
part-whole relationships, via a domain expert that can be utilised to generate
explanations at an abstract level. We illustrate the usability of this approach
in a real-world car model example and the Titanic dataset, where intermediate
concepts aid in explainability at different levels of abstraction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Time Series Contrastive Learning with Information-Aware Augmentations <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11911v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11911v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongsheng Luo, Wei Cheng, Yingheng Wang, Dongkuan Xu, Jingchao Ni, Wenchao Yu, Xuchao Zhang, Yanchi Liu, Yuncong Chen, Haifeng Chen, Xiang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Various contrastive learning approaches have been proposed in recent years
and achieve significant empirical success. While effective and prevalent,
contrastive learning has been less explored for time series data. A key
component of contrastive learning is to select appropriate augmentations
imposing some priors to construct feasible positive samples, such that an
encoder can be trained to learn robust and discriminative representations.
Unlike image and language domains where ``desired'' augmented samples can be
generated with the rule of thumb guided by prefabricated human priors, the
ad-hoc manual selection of time series augmentations is hindered by their
diverse and human-unrecognizable temporal structures. How to find the desired
augmentations of time series data that are meaningful for given contrastive
learning tasks and datasets remains an open question. In this work, we address
the problem by encouraging both high \textit{fidelity} and \textit{variety}
based upon information theory. A theoretical analysis leads to the criteria for
selecting feasible data augmentations. On top of that, we propose a new
contrastive learning approach with information-aware augmentations, InfoTS,
that adaptively selects optimal augmentations for time series representation
learning. Experiments on various datasets show highly competitive performance
with up to 12.0\% reduction in MSE on forecasting tasks and up to 3.7\%
relative improvement in accuracy on classification tasks over the leading
baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in AAAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Non-Asymptotic Pointwise and Worst-Case Bounds for Classical Spectrum
  Estimators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11908v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11908v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Lamperski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spectrum estimation is a fundamental methodology in the analysis of
time-series data, with applications including medicine, speech analysis, and
control design. The asymptotic theory of spectrum estimation is
well-understood, but the theory is limited when the number of samples is fixed
and finite. This paper gives non-asymptotic error bounds for a broad class of
spectral estimators, both pointwise (at specific frequencies) and in the worst
case over all frequencies. The general method is used to derive error bounds
for the classical Blackman-Tukey, Bartlett, and Welch estimators.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, under review in IEEE Transactions on Signal Processing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Better Understanding Differences in Attribution Methods via Systematic
  Evaluations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11884v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11884v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sukrut Rao, Moritz Böhle, Bernt Schiele
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks are very successful on many vision tasks, but hard to
interpret due to their black box nature. To overcome this, various post-hoc
attribution methods have been proposed to identify image regions most
influential to the models' decisions. Evaluating such methods is challenging
since no ground truth attributions exist. We thus propose three novel
evaluation schemes to more reliably measure the faithfulness of those methods,
to make comparisons between them more fair, and to make visual inspection more
systematic. To address faithfulness, we propose a novel evaluation setting
(DiFull) in which we carefully control which parts of the input can influence
the output in order to distinguish possible from impossible attributions. To
address fairness, we note that different methods are applied at different
layers, which skews any comparison, and so evaluate all methods on the same
layers (ML-Att) and discuss how this impacts their performance on quantitative
metrics. For more systematic visualizations, we propose a scheme (AggAtt) to
qualitatively evaluate the methods on complete datasets. We use these
evaluation schemes to study strengths and shortcomings of some widely used
attribution methods over a wide range of models. Finally, we propose a
post-processing smoothing step that significantly improves the performance of
some attribution methods, and discuss its applicability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 37 figures, 2 tables, extended version of arXiv:2205.10435</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Protective Self-Adaptive Pruning to Better Compress DNNs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11881v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11881v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liang Li, Pengfei Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adaptive network pruning approach has recently drawn significant attention
due to its excellent capability to identify the importance and redundancy of
layers and filters and customize a suitable pruning solution. However, it
remains unsatisfactory since current adaptive pruning methods rely mostly on an
additional monitor to score layer and filter importance, and thus faces high
complexity and weak interpretability. To tackle these issues, we have deeply
researched the weight reconstruction process in iterative prune-train process
and propose a Protective Self-Adaptive Pruning (PSAP) method. First of all,
PSAP can utilize its own information, weight sparsity ratio, to adaptively
adjust pruning ratio of layers before each pruning step. Moreover, we propose a
protective reconstruction mechanism to prevent important filters from being
pruned through supervising gradients and to avoid unrecoverable information
loss as well. Our PSAP is handy and explicit because it merely depends on
weights and gradients of model itself, instead of requiring an additional
monitor as in early works. Experiments on ImageNet and CIFAR-10 also
demonstrate its superiority to current works in both accuracy and compression
ratio, especially for compressing with a high ratio or pruning from scratch.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Tale of Two Circuits: Grokking as Competition of Sparse and Dense
  Subnetworks <span class="chip">ICLR
  2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11873v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11873v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William Merrill, Nikolaos Tsilivis, Aman Shukla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Grokking is a phenomenon where a model trained on an algorithmic task first
overfits but, then, after a large amount of additional training, undergoes a
phase transition to generalize perfectly. We empirically study the internal
structure of networks undergoing grokking on the sparse parity task, and find
that the grokking phase transition corresponds to the emergence of a sparse
subnetwork that dominates model predictions. On an optimization level, we find
that this subnetwork arises when a small subset of neurons undergoes rapid norm
growth, whereas the other neurons in the network decay slowly in norm. Thus, we
suggest that the grokking phase transition can be understood to emerge from
competition of two largely distinct subnetworks: a dense one that dominates
before the transition and generalizes poorly, and a sparse one that dominates
afterwards.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at the Workshop on Understanding Foundation Models at ICLR
  2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Continual Learning in the Presence of Spurious Correlation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11863v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11863v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Donggyu Lee, Sangwon Jung, Taesup Moon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most continual learning (CL) algorithms have focused on tackling the
stability-plasticity dilemma, that is, the challenge of preventing the
forgetting of previous tasks while learning new ones. However, they have
overlooked the impact of the knowledge transfer when the dataset in a certain
task is biased - namely, when some unintended spurious correlations of the
tasks are learned from the biased dataset. In that case, how would they affect
learning future tasks or the knowledge already learned from the past tasks? In
this work, we carefully design systematic experiments using one synthetic and
two real-world datasets to answer the question from our empirical findings.
Specifically, we first show through two-task CL experiments that standard CL
methods, which are unaware of dataset bias, can transfer biases from one task
to another, both forward and backward, and this transfer is exacerbated
depending on whether the CL methods focus on the stability or the plasticity.
We then present that the bias transfer also exists and even accumulate in
longer sequences of tasks. Finally, we propose a simple, yet strong plug-in
method for debiasing-aware continual learning, dubbed as Group-class Balanced
Greedy Sampling (BGS). As a result, we show that our BGS can always reduce the
bias of a CL model, with a slight loss of CL performance at most.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Online <span class="highlight-title">Transformer</span>s with Spiking Neurons for Fast Prosthetic Hand
  Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11860v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11860v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathan Leroux, Jan Finkbeiner, Emre Neftci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers are state-of-the-art networks for most sequence processing
tasks. However, the self-attention mechanism often used in Transformers
requires large time windows for each computation step and thus makes them less
suitable for online signal processing compared to Recurrent Neural Networks
(RNNs). In this paper, instead of the self-attention mechanism, we use a
sliding window attention mechanism. We show that this mechanism is more
efficient for continuous signals with finite-range dependencies between input
and target, and that we can use it to process sequences element-by-element,
this making it compatible with online processing. We test our model on a finger
position regression dataset (NinaproDB8) with Surface Electromyographic (sEMG)
signals measured on the forearm skin to estimate muscle activities. Our
approach sets the new state-of-the-art in terms of accuracy on this dataset
while requiring only very short time windows of 3.5 ms at each inference step.
Moreover, we increase the sparsity of the network using Leaky-Integrate and
Fire (LIF) units, a bio-inspired neuron model that activates sparsely in time
solely when crossing a threshold. We thus reduce the number of synaptic
operations up to a factor of $\times5.3$ without loss of accuracy. Our results
hold great promises for accurate and fast online processing of sEMG signals for
smooth prosthetic hand control and is a step towards Transformers and Spiking
Neural Networks (SNNs) co-integration for energy efficient temporal signal
processing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint of 9 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dens-PU: PU Learning with Density-Based Positive Labeled Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11848v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11848v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vasileios Sevetlidis, George Pavlidis, Spyridon Mouroutsos, Antonios Gasteratos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study proposes a novel approach for solving the PU learning problem
based on an anomaly-detection strategy. Latent encodings extracted from
positive-labeled data are linearly combined to acquire new samples. These new
samples are used as embeddings to increase the density of positive-labeled data
and, thus, define a boundary that approximates the positive class. The further
a sample is from the boundary the more it is considered as a negative sample.
Once a set of negative samples is obtained, the PU learning problem reduces to
binary classification. The approach, named Dens-PU due to its reliance on the
density of positive-labeled data, was evaluated using benchmark image datasets,
and state-of-the-art results were attained.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Doubly Regularized Entropic Wasserstein Barycenters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11844v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11844v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lénaïc Chizat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study a general formulation of regularized Wasserstein barycenters that
enjoys favorable regularity, approximation, stability and (grid-free)
optimization properties. This barycenter is defined as the unique probability
measure that minimizes the sum of entropic optimal transport (EOT) costs with
respect to a family of given probability measures, plus an entropy term. We
denote it $(\lambda,\tau)$-barycenter, where $\lambda$ is the inner
regularization strength and $\tau$ the outer one. This formulation recovers
several previously proposed EOT barycenters for various choices of
$\lambda,\tau \geq 0$ and generalizes them. First, in spite of -- and in fact
owing to -- being \emph{doubly} regularized, we show that our formulation is
debiased for $\tau=\lambda/2$: the suboptimality in the (unregularized)
Wasserstein barycenter objective is, for smooth densities, of the order of the
strength $\lambda^2$ of entropic regularization, instead of
$\max\{\lambda,\tau\}$ in general. We discuss this phenomenon for isotropic
Gaussians where all $(\lambda,\tau)$-barycenters have closed form. Second, we
show that for $\lambda,\tau>0$, this barycenter has a smooth density and is
strongly stable under perturbation of the marginals. In particular, it can be
estimated efficiently: given $n$ samples from each of the probability measures,
it converges in relative entropy to the population barycenter at a rate
$n^{-1/2}$. And finally, this formulation lends itself naturally to a grid-free
optimization algorithm: we propose a simple \emph{noisy particle gradient
descent} which, in the mean-field limit, converges globally at an exponential
rate to the barycenter.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Materials Discovery with Extreme Properties via AI-Driven Combinatorial
  Chemistry 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11833v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11833v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyunseung Kim, Haeyeon Choi, Dongju Kang, Won Bo Lee, Jonggeol Na
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of most materials discovery is to discover materials that are
superior to those currently known. Fundamentally, this is close to
extrapolation, which is a weak point for most machine learning models that
learn the probability distribution of data. Herein, we develop AI-driven
combinatorial chemistry, which is a rule-based inverse molecular designer that
does not rely on data. Since our model has the potential to generate all
possible molecular structures that can be obtained from combinations of
molecular fragments, unknown materials with superior properties can be
discovered. We theoretically and empirically demonstrate that our model is more
suitable for discovering better materials than probability
distribution-learning models. In an experiment aimed at discovering molecules
that hit seven target properties, our model discovered 1,315 of all
target-hitting molecules and 7,629 of five target-hitting molecules out of
100,000 trials, whereas the probability distribution-learning models failed. To
illustrate the performance in actual problems, we also demonstrate that our
models work well on two practical applications: discovering protein docking
materials and HIV inhibitors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GLADE: Gradient Loss Augmented Degradation Enhancement for Unpaired
  Super-Resolution of Anisotropic MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11831v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11831v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michele Pascale, Vivek Muthurangu, Javier Montalt Tordera, Heather E Fitzke, Gauraang Bhatnagar, Stuart Taylor, Jennifer Steeden
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel approach to synthesise high-resolution isotropic 3D
abdominal MR images, from anisotropic 3D images in an unpaired fashion. Using a
modified CycleGAN architecture with a gradient mapping loss, we leverage
disjoint patches from the high-resolution (in-plane) data of an anisotropic
volume to enforce the network generator to increase the resolution of the
low-resolution (through-plane) slices. This will enable accelerated
whole-abdomen scanning with high-resolution isotropic images within short
breath-hold times.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Addressing Class Variable Imbalance in Federated Semi-supervised
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11809v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11809v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zehui Dong, Wenjing Liu, Siyuan Liu, Xingzhi Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Semi-supervised Learning (FSSL) combines techniques from both
fields of federated and semi-supervised learning to improve the accuracy and
performance of models in a distributed environment by using a small fraction of
labeled data and a large amount of unlabeled data. Without the need to
centralize all data in one place for training, it collect updates of model
training after devices train models at local, and thus can protect the privacy
of user data. However, during the federal training process, some of the devices
fail to collect enough data for local training, while new devices will be
included to the group training. This leads to an unbalanced global data
distribution and thus affect the performance of the global model training. Most
of the current research is focusing on class imbalance with a fixed number of
classes, while little attention is paid to data imbalance with a variable
number of classes. Therefore, in this paper, we propose Federated
Semi-supervised Learning for Class Variable Imbalance (FCVI) to solve class
variable imbalance. The class-variable learning algorithm is used to mitigate
the data imbalance due to changes of the number of classes. Our scheme is
proved to be significantly better than baseline methods, while maintaining
client privacy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12th International Conference on Cloud Computing: Services and
  Architecture (CLOUD 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exact Non-Oblivious Performance of Rademacher Random Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11774v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11774v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maciej Skorski, Alessandro Temperoni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper revisits the performance of Rademacher random projections,
establishing novel statistical guarantees that are numerically sharp and
non-oblivious with respect to the input data. More specifically, the central
result is the Schur-concavity property of Rademacher random projections with
respect to the inputs. This offers a novel geometric perspective on the
performance of random projections, while improving quantitatively on bounds
from previous works. As a corollary of this broader result, we obtained the
improved performance on data which is sparse or is distributed with small
spread. This non-oblivious analysis is a novelty compared to techniques from
previous work, and bridges the frequently observed gap between theory and
practise. The main result uses an algebraic framework for proving
Schur-concavity properties, which is a contribution of independent interest and
an elegant alternative to derivative-based criteria.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reasonable Scale Machine Learning with Open-Source Metaflow 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11761v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11761v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jacopo Tagliabue, Hugo Bowne-Anderson, Ville Tuulos, Savin Goyal, Romain Cledat, David Berg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Machine Learning (ML) gains adoption across industries and new use cases,
practitioners increasingly realize the challenges around effectively developing
and iterating on ML systems: reproducibility, debugging, scalability, and
documentation are elusive goals for real-world pipelines outside tech-first
companies. In this paper, we review the nature of ML-oriented workloads and
argue that re-purposing existing tools won't solve the current productivity
issues, as ML peculiarities warrant specialized development tooling. We then
introduce Metaflow, an open-source framework for ML projects explicitly
designed to boost the productivity of data practitioners by abstracting away
the execution of ML code from the definition of the business logic. We show how
our design addresses the main challenges in ML operations (MLOps), and document
through examples, interviews and use cases its practical impact on the field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Deep Dynamics Models for Autonomous Vehicles with Multimodal
  Latent Mapping of Surfaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11756v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11756v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Johan Vertens, Nicolai Dorka, Tim Welschehold, Michael Thompson, Wolfram Burgard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The safe deployment of autonomous vehicles relies on their ability to
effectively react to environmental changes. This can require maneuvering on
varying surfaces which is still a difficult problem, especially for slippery
terrains. To address this issue we propose a new approach that learns a
surface-aware dynamics model by conditioning it on a latent variable vector
storing surface information about the current location. A latent mapper is
trained to update these latent variables during inference from multiple
modalities on every traversal of the corresponding locations and stores them in
a map. By training everything end-to-end with the loss of the dynamics model,
we enforce the latent mapper to learn an update rule for the latent map that is
useful for the subsequent dynamics model. We implement and evaluate our
approach on a real miniature electric car. The results show that the latent map
is updated to allow more accurate predictions of the dynamics model compared to
a model without this information. We further show that by using this model, the
driving performance can be improved on varying and challenging surfaces.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Projections of Model Spaces for Latent Graph Inference <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11754v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11754v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haitz Sáez de Ocáriz Borde, Álvaro Arroyo, Ingmar Posner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks leverage the connectivity structure of graphs as an
inductive bias. Latent graph inference focuses on learning an adequate graph
structure to diffuse information on and improve the downstream performance of
the model. In this work we employ stereographic projections of the hyperbolic
and spherical model spaces, as well as products of Riemannian manifolds, for
the purpose of latent graph inference. Stereographically projected model spaces
achieve comparable performance to their non-projected counterparts, while
providing theoretical guarantees that avoid divergence of the spaces when the
curvature tends to zero. We perform experiments on both homophilic and
heterophilic graphs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the ICLR 2023 Workshop on Physics for Machine Learning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Recommendation Systems in Libraries: an Application with Heterogeneous
  Data Sources <span class="chip">EDBT</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11746v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11746v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessandro Speciale, Greta Vallero, Luca Vassio, Marco Mellia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Reading&Machine project exploits the support of digitalization to
increase the attractiveness of libraries and improve the users' experience. The
project implements an application that helps the users in their decision-making
process, providing recommendation system (RecSys)-generated lists of books the
users might be interested in, and showing them through an interactive Virtual
Reality (VR)-based Graphical User Interface (GUI). In this paper, we focus on
the design and testing of the recommendation system, employing data about all
users' loans over the past 9 years from the network of libraries located in
Turin, Italy. In addition, we use data collected by the Anobii online social
community of readers, who share their feedback and additional information about
books they read. Armed with this heterogeneous data, we build and evaluate
Content Based (CB) and Collaborative Filtering (CF) approaches. Our results
show that the CF outperforms the CB approach, improving by up to 47\% the
relevant recommendations provided to a reader. However, the performance of the
CB approach is heavily dependent on the number of books the reader has already
read, and it can work even better than CF for users with a large history.
Finally, our evaluations highlight that the performances of both approaches are
significantly improved if the system integrates and leverages the information
from the Anobii dataset, which allows us to include more user readings (for CF)
and richer book metadata (for CB).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at 7th International workshop on Data Analytics solutions
  for Real-LIfe APplications - 28th March-31st March, 2023, Ioannina, Greece.
  The paper will be published in the Proceedings of EDBT/ICDT 2023 Joint
  Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beam Management Driven by Radio Environment Maps in O-RAN Architecture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11742v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11742v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marcin Hoffmann, Pawel Kryszkiewicz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Massive Multiple-Input Multiple-Output (M-MIMO) is considered as one of
the key technologies in 5G, and future 6G networks. From the perspective of,
e.g., channel estimation, especially for high-speed users it is easier to
implement an M-MIMO network exploiting a static set of beams, i.e., Grid of
Beams (GoB). While considering GoB it is important to properly assign users to
the beams, i.e., to perform Beam Management (BM). BM can be enhanced by taking
into account historical knowledge about the radio environment, e.g., to avoid
radio link failures. The aim of this paper is to propose such a BM algorithm,
that utilizes location-dependent data stored in a Radio Environment Map (REM).
It utilizes received power maps, and user mobility patterns to optimize the BM
process in terms of Reinforcement Learning (RL) by using the Policy Iteration
method under different goal functions, e.g., maximization of received power or
minimization of beam reselections while avoiding radio link failures. The
proposed solution is compliant with the Open Radio Access Network (O-RAN)
architecture, enabling its practical implementation. Simulation studies have
shown that the proposed BM algorithm can significantly reduce the number of
beam reselections or radio link failures compared to the baseline algorithm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tensor networks for quantum machine learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11735v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11735v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hans-Martin Rieser, Frank Köster, Arne Peter Raulf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Once developed for quantum theory, tensor networks have been established as a
successful machine learning paradigm. Now, they have been ported back to the
quantum realm in the emerging field of quantum machine learning to assess
problems that classical computers are unable to solve efficiently. Their nature
at the interface between physics and machine learning makes tensor networks
easily deployable on quantum computers. In this review article, we shed light
on one of the major architectures considered to be predestined for variational
quantum machine learning. In particular, we discuss how layouts like MPS, PEPS,
TTNs and MERA can be mapped to a quantum computer, how they can be used for
machine learning and data encoding and which implementation techniques improve
their performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unlocking Layer-wise Relevance Propagation for Autoencoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11734v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11734v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kenyu Kobayashi, Renata Khasanova, Arno Schneuwly, Felix Schmidt, Matteo Casserini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autoencoders are a powerful and versatile tool often used for various
problems such as anomaly detection, image processing and machine translation.
However, their reconstructions are not always trivial to explain. Therefore, we
propose a fast explainability solution by extending the Layer-wise Relevance
Propagation method with the help of Deep Taylor Decomposition framework.
Furthermore, we introduce a novel validation technique for comparing our
explainability approach with baseline methods in the case of missing
ground-truth data. Our results highlight computational as well as qualitative
advantages of the proposed explainability solution with respect to existing
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DIPPM: a Deep Learning Inference Performance Predictive Model using
  Graph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11733v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11733v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karthick Panner Selvam, Mats Brorsson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Learning (DL) has developed to become a corner-stone in many everyday
applications that we are now relying on. However, making sure that the DL model
uses the underlying hardware efficiently takes a lot of effort. Knowledge about
inference characteristics can help to find the right match so that enough
resources are given to the model, but not too much. We have developed a DL
Inference Performance Predictive Model (DIPPM) that predicts the inference
latency, energy, and memory usage of a given input DL model on the NVIDIA A100
GPU. We also devised an algorithm to suggest the appropriate A100
Multi-Instance GPU profile from the output of DIPPM. We developed a methodology
to convert DL models expressed in multiple frameworks to a generalized graph
structure that is used in DIPPM. It means DIPPM can parse input DL models from
various frameworks. Our DIPPM can be used not only helps to find suitable
hardware configurations but also helps to perform rapid design-space
exploration for the inference performance of a model. We constructed a graph
multi-regression dataset consisting of 10,508 different DL models to train and
evaluate the performance of DIPPM, and reached a resulting Mean Absolute
Percentage Error (MAPE) as low as 1.9%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Task-based Generation of Optimized Projection Sets using Differentiable
  Ranking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11724v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11724v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linda-Sophie Schneider, Mareike Thies, Christopher Syben, Richard Schielein, Mathias Unberath, Andreas Maier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a method for selecting valuable projections in computed tomography
(CT) scans to enhance image reconstruction and diagnosis. The approach
integrates two important factors, projection-based detectability and data
completeness, into a single feed-forward neural network. The network evaluates
the value of projections, processes them through a differentiable ranking
function and makes the final selection using a straight-through estimator. Data
completeness is ensured through the label provided during training. The
approach eliminates the need for heuristically enforcing data completeness,
which may exclude valuable projections. The method is evaluated on simulated
data in a non-destructive testing scenario, where the aim is to maximize the
reconstruction quality within a specified region of interest. We achieve
comparable results to previous methods, laying the foundation for using
reconstruction-based loss functions to learn the selection of projections.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lidar Line Selection with Spatially-Aware Shapley Value for
  Cost-Efficient Depth Completion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11720v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11720v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kamil Adamczewski, Christos Sakaridis, Vaishakh Patil, Luc Van Gool
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lidar is a vital sensor for estimating the depth of a scene. Typical spinning
lidars emit pulses arranged in several horizontal lines and the monetary cost
of the sensor increases with the number of these lines. In this work, we
present the new problem of optimizing the positioning of lidar lines to find
the most effective configuration for the depth completion task. We propose a
solution to reduce the number of lines while retaining the up-to-the-mark
quality of depth completion. Our method consists of two components, (1) line
selection based on the marginal contribution of a line computed via the Shapley
value and (2) incorporating line position spread to take into account its need
to arrive at image-wide depth completion. Spatially-aware Shapley values (SaS)
succeed in selecting line subsets that yield a depth accuracy comparable to the
full lidar input while using just half of the lines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Complete <span class="highlight-title">Survey</span> on Generative AI (AIGC): Is Chat<span class="highlight-title">GPT</span> from <span class="highlight-title">GPT</span>-4 to
  <span class="highlight-title">GPT</span>-5 All You Need? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11717v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11717v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaoning Zhang, Chenshuang Zhang, Sheng Zheng, Yu Qiao, Chenghao Li, Mengchun Zhang, Sumit Kumar Dam, Chu Myaet Thwal, Ye Lin Tun, Le Luang Huy, Donguk kim, Sung-Ho Bae, Lik-Hang Lee, Yang Yang, Heng Tao Shen, In So Kweon, Choong Seon Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As ChatGPT goes viral, generative AI (AIGC, a.k.a AI-generated content) has
made headlines everywhere because of its ability to analyze and create text,
images, and beyond. With such overwhelming media coverage, it is almost
impossible for us to miss the opportunity to glimpse AIGC from a certain angle.
In the era of AI transitioning from pure analysis to creation, it is worth
noting that ChatGPT, with its most recent language model GPT-4, is just a tool
out of numerous AIGC tasks. Impressed by the capability of the ChatGPT, many
people are wondering about its limits: can GPT-5 (or other future GPT variants)
help ChatGPT unify all AIGC tasks for diversified content creation? Toward
answering this question, a comprehensive review of existing AIGC tasks is
needed. As such, our work comes to fill this gap promptly by offering a first
look at AIGC, ranging from its techniques to applications. Modern generative AI
relies on various technical foundations, ranging from model architecture and
self-supervised pretraining to generative modeling methods (like GAN and
diffusion models). After introducing the fundamental techniques, this work
focuses on the technological development of various AIGC tasks based on their
output type, including text, images, videos, 3D content, etc., which depicts
the full potential of ChatGPT's future. Moreover, we summarize their
significant applications in some mainstream industries, such as education and
creativity content. Finally, we discuss the challenges currently faced and
present an outlook on how generative AI might evolve in the near future.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>56 pages, 548 citations</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Style Miner: Find Significant and Stable Explanatory Factors in Time
  Series with Constrained Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11716v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11716v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dapeng Li, Feiyang Pan, Jia He, Zhiwei Xu, Dandan Tu, Guoliang Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In high-dimensional time-series analysis, it is essential to have a set of
key factors (namely, the style factors) that explain the change of the observed
variable. For example, volatility modeling in finance relies on a set of risk
factors, and climate change studies in climatology rely on a set of causal
factors. The ideal low-dimensional style factors should balance significance
(with high explanatory power) and stability (consistent, no significant
fluctuations). However, previous supervised and unsupervised feature extraction
methods can hardly address the tradeoff. In this paper, we propose Style Miner,
a reinforcement learning method to generate style factors. We first formulate
the problem as a Constrained Markov Decision Process with explanatory power as
the return and stability as the constraint. Then, we design fine-grained
immediate rewards and costs and use a Lagrangian heuristic to balance them
adaptively. Experiments on real-world financial data sets show that Style Miner
outperforms existing learning-based methods by a large margin and achieves a
relatively 10% gain in R-squared explanatory power compared to the
industry-renowned factors proposed by human experts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Single-Step Multiclass SVM based on Quantum Annealing for Remote
  Sensing Data Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11705v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11705v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amer Delilbasic, Bertrand Le Saux, Morris Riedel, Kristel Michielsen, Gabriele Cavallaro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, the development of quantum annealers has enabled
experimental demonstrations and has increased research interest in applications
of quantum annealing, such as in quantum machine learning and in particular for
the popular quantum SVM. Several versions of the quantum SVM have been
proposed, and quantum annealing has been shown to be effective in them.
Extensions to multiclass problems have also been made, which consist of an
ensemble of multiple binary classifiers. This work proposes a novel quantum SVM
formulation for direct multiclass classification based on quantum annealing,
called Quantum Multiclass SVM (QMSVM). The multiclass classification problem is
formulated as a single Quadratic Unconstrained Binary Optimization (QUBO)
problem solved with quantum annealing. The main objective of this work is to
evaluate the feasibility, accuracy, and time performance of this approach.
Experiments have been performed on the D-Wave Advantage quantum annealer for a
classification problem on remote sensing data. The results indicate that,
despite the memory demands of the quantum annealer, QMSVM can achieve accuracy
that is comparable to standard SVM methods and, more importantly, it scales
much more efficiently with the number of training examples, resulting in nearly
constant time. This work shows an approach for bringing together classical and
quantum computation, solving practical problems in remote sensing with current
hardware.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 10 figures, 3 tables. Submitted to IEEE JSTARS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Linking generative semi-supervised learning and generative open-set
  recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11702v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11702v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emile Reyn Engelbrecht, Johan du Preez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study investigates the relationship between semi-supervised learning
(SSL) and open-set recognition (OSR) in the context of generative adversarial
networks (GANs). Although no previous study has formally linked SSL and OSR,
their respective methods share striking similarities. Specifically, SSL-GANs
and OSR-GANs require generator to produce samples in the complementary space.
Subsequently, by regularising networks with generated samples, both SSL and OSR
classifiers generalize the open space. To demonstrate the connection between
SSL and OSR, we theoretically and experimentally compare state-of-the-art
SSL-GAN methods with state-of-the-art OSR-GAN methods. Our results indicate
that the SSL optimised margin-GANs, which have a stronger foundation in
literature, set the new standard for the combined SSL-OSR task and achieves new
state-of-other art results in certain general OSR experiments. However, the OSR
optimised adversarial reciprocal point (ARP)-GANs still slightly out-performed
margin-GANs at other OSR experiments. This result indicates unique insights for
the combined optimisation task of SSL-OSR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A High-Frequency Focused Network for Lightweight Single Image
  Super-Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11701v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11701v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaotian Weng, Yi Chen, Zhichao Zheng, Yanhui Gu, Junsheng Zhou, Yudong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lightweight neural networks for single-image super-resolution (SISR) tasks
have made substantial breakthroughs in recent years. Compared to low-frequency
information, high-frequency detail is much more difficult to reconstruct. Most
SISR models allocate equal computational resources for low-frequency and
high-frequency information, which leads to redundant processing of simple
low-frequency information and inadequate recovery of more challenging
high-frequency information. We propose a novel High-Frequency Focused Network
(HFFN) through High-Frequency Focused Blocks (HFFBs) that selectively enhance
high-frequency information while minimizing redundant feature computation of
low-frequency information. The HFFB effectively allocates more computational
resources to the more challenging reconstruction of high-frequency information.
Moreover, we propose a Local Feature Fusion Block (LFFB) effectively fuses
features from multiple HFFBs in a local region, utilizing complementary
information across layers to enhance feature representativeness and reduce
artifacts in reconstructed images. We assess the efficacy of our proposed HFFN
on five benchmark datasets and show that it significantly enhances the
super-resolution performance of the network. Our experimental results
demonstrate state-of-the-art performance in reconstructing high-frequency
information while using a low number of parameters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural networks trained on synthetically generated crystals can extract
  structural information from ICSD powder X-ray diffractograms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11699v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11699v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Henrik Schopmans, Patrick Reiser, Pascal Friederich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning techniques have successfully been used to extract structural
information such as the crystal space group from powder X-ray diffractograms.
However, training directly on simulated diffractograms from databases such as
the ICSD is challenging due to its limited size, class-inhomogeneity, and bias
toward certain structure types. We propose an alternative approach of
generating synthetic crystals with random coordinates by using the symmetry
operations of each space group. Based on this approach, we demonstrate online
training of deep ResNet-like models on up to a few million unique on-the-fly
generated synthetic diffractograms per hour. For our chosen task of space group
classification, we achieved a test accuracy of 79.9% on unseen ICSD structure
types from most space groups. This surpasses the 56.1% accuracy of the current
state-of-the-art approach of training on ICSD crystals directly. Our results
demonstrate that synthetically generated crystals can be used to extract
structural information from ICSD powder diffractograms, which makes it possible
to apply very large state-of-the-art machine learning models in the area of
powder X-ray diffraction. We further show first steps toward applying our
methodology to experimental data, where automated XRD data analysis is crucial,
especially in high-throughput settings. While we focused on the prediction of
the space group, our approach has the potential to be extended to related tasks
in the future.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data Augmentation For Label Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11698v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11698v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiqiang Kou, Yuheng Jia, Jing Wang, Boyu Shi, Xin Geng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Label distribution (LD) uses the description degree to describe instances,
which provides more fine-grained supervision information when learning with
label ambiguity. Nevertheless, LD is unavailable in many real-world
applications. To obtain LD, label enhancement (LE) has emerged to recover LD
from logical label. Existing LE approach have the following problems:
(\textbf{i}) They use logical label to train mappings to LD, but the
supervision information is too loose, which can lead to inaccurate model
prediction; (\textbf{ii}) They ignore feature redundancy and use the collected
features directly. To solve (\textbf{i}), we use the topology of the feature
space to generate more accurate label-confidence. To solve (\textbf{ii}), we
proposed a novel supervised LE dimensionality reduction approach, which
projects the original data into a lower dimensional feature space. Combining
the above two, we obtain the augmented data for LE. Further, we proposed a
novel nonlinear LE model based on the label-confidence and reduced features.
Extensive experiments on 12 real-world datasets are conducted and the results
show that our method consistently outperforms the other five comparing
approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transcriptomics-based matching of drugs to diseases with deep learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11695v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11695v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yannis Papanikolaou, Francesco Tuveri, Misa Ogura, Daniel O'Donovan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work we present a deep learning approach to conduct hypothesis-free,
transcriptomics-based matching of drugs for diseases. Our proposed neural
network architecture is trained on approved drug-disease indications, taking as
input the relevant disease and drug differential gene expression profiles, and
learns to identify novel indications. We assemble an evaluation dataset of
disease-drug indications spanning 68 diseases and evaluate in silico our
approach against the most widely used transcriptomics-based matching baselines,
CMap and the Characteristic Direction. Our results show a more than 200%
improvement over both baselines in terms of standard retrieval metrics. We
further showcase our model's ability to capture different genes' expressions
interactions among drugs and diseases. We provide our trained models, data and
code to predict with them at https://github.com/healx/dgem-nn-public.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ALOFT: A Lightweight MLP-like Architecture with Dynamic Low-frequency
  Transform for Domain Generalization <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11674v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11674v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jintao Guo, Na Wang, Lei Qi, Yinghuan Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain generalization (DG) aims to learn a model that generalizes well to
unseen target domains utilizing multiple source domains without re-training.
Most existing DG works are based on convolutional neural networks (CNNs).
However, the local operation of the convolution kernel makes the model focus
too much on local representations (e.g., texture), which inherently causes the
model more prone to overfit to the source domains and hampers its
generalization ability. Recently, several MLP-based methods have achieved
promising results in supervised learning tasks by learning global interactions
among different patches of the image. Inspired by this, in this paper, we first
analyze the difference between CNN and MLP methods in DG and find that MLP
methods exhibit a better generalization ability because they can better capture
the global representations (e.g., structure) than CNN methods. Then, based on a
recent lightweight MLP method, we obtain a strong baseline that outperforms
most state-of-the-art CNN-based methods. The baseline can learn global
structure representations with a filter to suppress structure irrelevant
information in the frequency space. Moreover, we propose a dynAmic
LOw-Frequency spectrum Transform (ALOFT) that can perturb local texture
features while preserving global structure features, thus enabling the filter
to remove structure-irrelevant information sufficiently. Extensive experiments
on four benchmarks have demonstrated that our method can achieve great
performance improvement with a small number of parameters compared to SOTA
CNN-based DG methods. Our code is available at
https://github.com/lingeringlight/ALOFT/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2023. The code is available at
  https://github.com/lingeringlight/ALOFT/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> on Class Imbalance in Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11673v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11673v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Zhang, Chuanwen Li, Jianzgong Qi, Jiayuan He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning, which allows multiple client devices in a network to
jointly train a machine learning model without direct exposure of clients'
data, is an emerging distributed learning technique due to its nature of
privacy preservation. However, it has been found that models trained with
federated learning usually have worse performance than their counterparts
trained in the standard centralized learning mode, especially when the training
data is imbalanced. In the context of federated learning, data imbalance may
occur either locally one one client device, or globally across many devices.
The complexity of different types of data imbalance has posed challenges to the
development of federated learning technique, especially considering the need of
relieving data imbalance issue and preserving data privacy at the same time.
Therefore, in the literature, many attempts have been made to handle class
imbalance in federated learning. In this paper, we present a detailed review of
recent advancements along this line. We first introduce various types of class
imbalance in federated learning, after which we review existing methods for
estimating the extent of class imbalance without the need of knowing the actual
data to preserve data privacy. After that, we discuss existing methods for
handling class imbalance in FL, where the advantages and disadvantages of the
these approaches are discussed. We also summarize common evaluation metrics for
class imbalanced tasks, and point out potential future directions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Universal Smoothed Score Functions for Generative Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11669v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11669v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saeed Saremi, Rupesh Kumar Srivastava, Francis Bach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of generative modeling based on smoothing an unknown
density of interest in $\mathbb{R}^d$ using factorial kernels with $M$
independent Gaussian channels with equal noise levels introduced by Saremi and
Srivastava (2022). First, we fully characterize the time complexity of learning
the resulting smoothed density in $\mathbb{R}^{Md}$, called M-density, by
deriving a universal form for its parametrization in which the score function
is by construction permutation equivariant. Next, we study the time complexity
of sampling an M-density by analyzing its condition number for Gaussian
distributions. This spectral analysis gives a geometric insight on the "shape"
of M-densities as one increases $M$. Finally, we present results on the sample
quality in this class of generative models on the CIFAR-10 dataset where we
report Fr\'echet inception distances (14.15), notably obtained with a single
noise level on long-run fast-mixing MCMC chains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uniform Risk Bounds for Learning with Dependent Data Sequences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11650v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11650v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabien Lauer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper extends standard results from learning theory with independent
data to sequences of dependent data. Contrary to most of the literature, we do
not rely on mixing arguments or sequential measures of complexity and derive
uniform risk bounds with classical proof patterns and capacity measures. In
particular, we show that the standard classification risk bounds based on the
VC-dimension hold in the exact same form for dependent data, and further
provide Rademacher complexity-based bounds, that remain unchanged compared to
the standard results for the identically and independently distributed case.
Finally, we show how to apply these results in the context of scenario-based
optimization in order to compute the sample complexity of random programs with
dependent constraints.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Content Retrievability in Search with Controllable Query
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11648v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11648v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gustavo Penha, Enrico Palumbo, Maryam Aziz, Alice Wang, Hugues Bouchard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An important goal of online platforms is to enable content discovery, i.e.
allow users to find a catalog entity they were not familiar with. A
pre-requisite to discover an entity, e.g. a book, with a search engine is that
the entity is retrievable, i.e. there are queries for which the system will
surface such entity in the top results. However, machine-learned search engines
have a high retrievability bias, where the majority of the queries return the
same entities. This happens partly due to the predominance of narrow intent
queries, where users create queries using the title of an already known entity,
e.g. in book search 'harry potter'. The amount of broad queries where users
want to discover new entities, e.g. in music search 'chill lyrical electronica
with an atmospheric feeling to it', and have a higher tolerance to what they
might find, is small in comparison. We focus here on two factors that have a
negative impact on the retrievability of the entities (I) the training data
used for dense retrieval models and (II) the distribution of narrow and broad
intent queries issued in the system. We propose CtrlQGen, a method that
generates queries for a chosen underlying intent-narrow or broad. We can use
CtrlQGen to improve factor (I) by generating training data for dense retrieval
models comprised of diverse synthetic queries. CtrlQGen can also be used to
deal with factor (II) by suggesting queries with broader intents to users. Our
results on datasets from the domains of music, podcasts, and books reveal that
we can significantly decrease the retrievability bias of a dense retrieval
model when using CtrlQGen. First, by using the generated queries as training
data for dense models we make 9% of the entities retrievable (go from zero to
non-zero retrievability). Second, by suggesting broader queries to users, we
can make 12% of the entities retrievable in the best case.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in the International World Wide Web
  Conference 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Are uGLAD? Time will tell! 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11647v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11647v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shima Imani, Harsh Shrivastava
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We frequently encounter multiple series that are temporally correlated in our
surroundings, such as EEG data to examine alterations in brain activity or
sensors to monitor body movements. Segmentation of multivariate time series
data is a technique for identifying meaningful patterns or changes in the time
series that can signal a shift in the system's behavior. However, most
segmentation algorithms have been designed primarily for univariate time
series, and their performance on multivariate data remains largely
unsatisfactory, making this a challenging problem. In this work, we introduce a
novel approach for multivariate time series segmentation using conditional
independence (CI) graphs. CI graphs are probabilistic graphical models that
represents the partial correlations between the nodes. We propose a domain
agnostic multivariate segmentation framework `$\texttt{tGLAD}$' which draws a
parallel between the CI graph nodes and the variables of the time series.
Consider applying a graph recovery model $\texttt{uGLAD}$ to a short interval
of the time series, it will result in a CI graph that shows partial
correlations among the variables. We extend this idea to the entire time series
by utilizing a sliding window to create a batch of time intervals and then run
a single $\texttt{uGLAD}$ model in multitask learning mode to recover all the
CI graphs simultaneously. As a result, we obtain a corresponding temporal CI
graphs representation. We then designed a first-order and second-order based
trajectory tracking algorithms to study the evolution of these graphs across
distinct intervals. Finally, an `Allocation' algorithm is used to determine a
suitable segmentation of the temporal graph sequence. $\texttt{tGLAD}$ provides
a competitive time complexity of $O(N)$ for settings where number of variables
$D<<N$. We demonstrate successful empirical results on a Physical Activity
Monitoring data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Manipulating Transfer Learning for Property Inference <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11643v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11643v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yulong Tian, Fnu Suya, Anshuman Suri, Fengyuan Xu, David Evans
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transfer learning is a popular method for tuning pretrained (upstream) models
for different downstream tasks using limited data and computational resources.
We study how an adversary with control over an upstream model used in transfer
learning can conduct property inference attacks on a victim's tuned downstream
model. For example, to infer the presence of images of a specific individual in
the downstream training set. We demonstrate attacks in which an adversary can
manipulate the upstream model to conduct highly effective and specific property
inference attacks (AUC score $> 0.9$), without incurring significant
performance loss on the main task. The main idea of the manipulation is to make
the upstream model generate activations (intermediate features) with different
distributions for samples with and without a target property, thus enabling the
adversary to distinguish easily between downstream models trained with and
without training examples that have the target property. Our code is available
at https://github.com/yulongt23/Transfer-Inference.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Q-Network Based Decision Making for Autonomous Driving <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11634v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11634v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Max Peter Ronecker, Yuan Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Currently decision making is one of the biggest challenges in autonomous
driving. This paper introduces a method for safely navigating an autonomous
vehicle in highway scenarios by combining deep Q-Networks and insight from
control theory. A Deep Q-Network is trained in simulation to serve as a central
decision-making unit by proposing targets for a trajectory planner. The
generated trajectories in combination with a controller for longitudinal
movement are used to execute lane change maneuvers. In order to prove the
functionality of this approach it is evaluated on two different highway traffic
scenarios. Furthermore, the impact of different state representations on the
performance and training process is analyzed. The results show that the
proposed system can produce efficient and safe driving behavior.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at 2019 International Conference on Robotics and Automation
  Sciences (ICRAS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Embarrassingly Simple Approach for Wafer Feature Extraction and
  Defect Pattern Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11632v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11632v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nitish Shukla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Identifying defect patterns in a wafer map during manufacturing is crucial to
find the root cause of the underlying issue and provides valuable insights on
improving yield in the foundry. Currently used methods use deep neural networks
to identify the defects. These methods are generally very huge and have
significant inference time. They also require GPU support to efficiently
operate. All these issues make these models not fit for on-line prediction in
the manufacturing foundry. In this paper, we propose an extremely simple yet
effective technique to extract features from wafer images. The proposed method
is extremely fast, intuitive, and non-parametric while being explainable. The
experiment results show that the proposed pipeline outperforms conventional
deep learning models. Our feature extraction requires no training or
fine-tuning while preserving the relative shape and location of data points as
revealed by our interpretability analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Assessor-Guided Learning for Continual Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11624v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11624v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Anwar Ma'sum, Mahardhika Pratama, Edwin Lughofer, Weiping Ding, Wisnu Jatmiko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes an assessor-guided learning strategy for continual
learning where an assessor guides the learning process of a base learner by
controlling the direction and pace of the learning process thus allowing an
efficient learning of new environments while protecting against the
catastrophic interference problem. The assessor is trained in a meta-learning
manner with a meta-objective to boost the learning process of the base learner.
It performs a soft-weighting mechanism of every sample accepting positive
samples while rejecting negative samples. The training objective of a base
learner is to minimize a meta-weighted combination of the cross entropy loss
function, the dark experience replay (DER) loss function and the knowledge
distillation loss function whose interactions are controlled in such a way to
attain an improved performance. A compensated over-sampling (COS) strategy is
developed to overcome the class imbalanced problem of the episodic memory due
to limited memory budgets. Our approach, Assessor-Guided Learning Approach
(AGLA), has been evaluated in the class-incremental and task-incremental
learning problems. AGLA achieves improved performances compared to its
competitors while the theoretical analysis of the COS strategy is offered.
Source codes of AGLA, baseline algorithms and experimental logs are shared
publicly in \url{https://github.com/anwarmaxsum/AGLA} for further study.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted for publication to Information Sciences</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Blow-up Algorithm for Sum-of-Products Polynomials and Real Log Canonical
  Thresholds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11619v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11619v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joe Hirose
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When considering a real log canonical threshold (RLCT) that gives a Bayesian
generalization error, in general, papers replace a mean error function with a
relatively simple polynomial whose RLCT corresponds to that of the mean error
function, and obtain its RLCT by resolving its singularities through an
algebraic operation called blow-up. Though it is known that the singularities
of any polynomial can be resolved by a finite number of blow-up iterations, it
is not clarified whether or not it is possible to resolve singularities of a
specific polynomial by applying a specific blow-up algorithm. Therefore this
paper considers the blow-up algorithm for the polynomials called
sum-of-products (sop) polynomials and its RLCT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>47 pages, 5 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Convergence of stochastic gradient descent on parameterized sphere with
  applications to variational Monte Carlo simulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11602v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11602v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nilin Abrahamsen, Zhiyan Ding, Gil Goldshlager, Lin Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We analyze stochastic gradient descent (SGD) type algorithms on a
high-dimensional sphere which is parameterized by a neural network up to a
normalization constant. We provide a new algorithm for the setting of
supervised learning and show its convergence both theoretically and
numerically. We also provide the first proof of convergence for the
unsupervised setting, which corresponds to the widely used variational Monte
Carlo (VMC) method in quantum physics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Difficulty in learning chirality for <span class="highlight-title">Transformer</span> fed with SMILES 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11593v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11593v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yasuhiro Yoshikai, Tadahaya Mizuno, Shumpei Nemoto, Hiroyuki Kusuhara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have seen development of descriptor generation based on
representation learning of extremely diverse molecules, especially those that
apply natural language processing (NLP) models to SMILES, a literal
representation of molecular structure. However, little research has been done
on how these models understand chemical structure. To address this, we
investigated the relationship between the learning progress of SMILES and
chemical structure using a representative NLP model, the Transformer. The
results suggest that while the Transformer learns partial structures of
molecules quickly, it requires extended training to understand overall
structures. Consistently, the accuracy of molecular property predictions using
descriptors generated from models at different learning steps was similar from
the beginning to the end of training. Furthermore, we found that the
Transformer requires particularly long training to learn chirality and
sometimes stagnates with low translation accuracy due to misunderstanding of
enantiomers. These findings are expected to deepen understanding of NLP models
in chemistry.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Experimentation at Scale: Bayesian Algorithms for Flexible
  Batches 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11582v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11582v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ethan Che, Hongseok Namkoong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Standard bandit algorithms that assume continual reallocation of measurement
effort are challenging to implement due to delayed feedback and
infrastructural/organizational difficulties. Motivated by practical instances
involving a handful of reallocation epochs in which outcomes are measured in
batches, we develop a new adaptive experimentation framework that can flexibly
handle any batch size. Our main observation is that normal approximations
universal in statistical inference can also guide the design of scalable
adaptive designs. By deriving an asymptotic sequential experiment, we formulate
a dynamic program that can leverage prior information on average rewards. State
transitions of the dynamic program are differentiable with respect to the
sampling allocations, allowing the use of gradient-based methods for planning
and policy optimization. We propose a simple iterative planning method,
Residual Horizon Optimization, which selects sampling allocations by optimizing
a planning objective via stochastic gradient-based methods. Our method
significantly improves statistical power over standard adaptive policies, even
when compared to Bayesian bandit algorithms (e.g., Thompson sampling) that
require full distributional knowledge of individual rewards. Overall, we expand
the scope of adaptive experimentation to settings which are difficult for
standard adaptive policies, including problems with a small number of
reallocation epochs, low signal-to-noise ratio, and unknown reward
distributions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Multi-stage Inference on Tabular Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11580v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11580v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel S Johnson, Igor L Markov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many ML applications and products train on medium amounts of input data but
get bottlenecked in real-time inference. When implementing ML systems,
conventional wisdom favors segregating ML code into services queried by product
code via Remote Procedure Call (RPC) APIs. This approach clarifies the overall
software architecture and simplifies product code by abstracting away ML
internals. However, the separation adds network latency and entails additional
CPU overhead. Hence, we simplify inference algorithms and embed them into the
product code to reduce network communication. For public datasets and a
high-performance real-time platform that deals with tabular data, we show that
over half of the inputs are often amenable to such optimization, while the
remainder can be handled by the original model. By applying our optimization
with AutoML to both training and inference, we reduce inference latency by
1.3x, CPU resources by 30%, and network communication between application
front-end and ML back-end by about 50% for a commercial end-to-end ML platform
that serves millions of real-time decisions per second.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Feature-adjacent multi-fidelity physics-informed machine learning for
  partial differential equations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11577v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11577v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenqian Chen, Panos Stinis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Physics-informed neural networks have emerged as an alternative method for
solving partial differential equations. However, for complex problems, the
training of such networks can still require high-fidelity data which can be
expensive to generate. To reduce or even eliminate the dependency on
high-fidelity data, we propose a novel multi-fidelity architecture which is
based on a feature space shared by the low- and high-fidelity solutions. In the
feature space, the projections of the low-fidelity and high-fidelity solutions
are adjacent by constraining their relative distance. The feature space is
represented with an encoder and its mapping to the original solution space is
effected through a decoder. The proposed multi-fidelity approach is validated
on forward and inverse problems for steady and unsteady problems described by
partial differential equations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Healthcare Embeddings for Improving Patient Care 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11563v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11563v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hankyu Jang, Sulyun Lee, D. M. Hasibul Hasan, Philip M. Polgreen, Sriram V. Pemmaraju, Bijaya Adhikari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As hospitals move towards automating and integrating their computing systems,
more fine-grained hospital operations data are becoming available. These data
include hospital architectural drawings, logs of interactions between patients
and healthcare professionals, prescription data, procedures data, and data on
patient admission, discharge, and transfers. This has opened up many
fascinating avenues for healthcare-related prediction tasks for improving
patient care. However, in order to leverage off-the-shelf machine learning
software for these tasks, one needs to learn structured representations of
entities involved from heterogeneous, dynamic data streams. Here, we propose
DECENT, an auto-encoding heterogeneous co-evolving dynamic neural network, for
learning heterogeneous dynamic embeddings of patients, doctors, rooms, and
medications from diverse data streams. These embeddings capture similarities
among doctors, rooms, patients, and medications based on static attributes and
dynamic interactions. DECENT enables several applications in healthcare
prediction, such as predicting mortality risk and case severity of patients,
adverse events (e.g., transfer back into an intensive care unit), and future
healthcare-associated infections. The results of using the learned patient
embeddings in predictive modeling show that DECENT has a gain of up to 48.1% on
the mortality risk prediction task, 12.6% on the case severity prediction task,
6.4% on the medical intensive care unit transfer task, and 3.8% on the
Clostridioides difficile (C.diff) Infection (CDI) prediction task over the
state-of-the-art baselines. In addition, case studies on the learned doctor,
medication, and room embeddings show that our approach learns meaningful and
interpretable embeddings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in IEEE/ACM ASONAM 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic-Aware Loss for Learning with Label Noise 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11562v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11562v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiu-Chuan Li, Xiaobo Xia, Fei Zhu, Tongliang Liu, Xu-Yao Zhang, Cheng-Lin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Label noise poses a serious threat to deep neural networks (DNNs). Employing
robust loss function which reconciles fitting ability with robustness is a
simple but effective strategy to handle this problem. However, the widely-used
static trade-off between these two factors contradicts the dynamic nature of
DNNs learning with label noise, leading to inferior performance. Therefore, we
propose a dynamics-aware loss (DAL) to solve this problem. Considering that
DNNs tend to first learn generalized patterns, then gradually overfit label
noise, DAL strengthens the fitting ability initially, then gradually increases
the weight of robustness. Moreover, at the later stage, we let DNNs put more
emphasis on easy examples which are more likely to be correctly labeled than
hard ones and introduce a bootstrapping term to further reduce the negative
impact of label noise. Both the detailed theoretical analyses and extensive
experimental results demonstrate the superiority of our method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Vertex Replacement Grammars 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11553v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11553v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Gonzalez Cedre, Justus Isaiah Hibshman, Timothy La Fond, Grant Boquet, Tim Weninger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Context-free graph grammars have shown a remarkable ability to model
structures in real-world relational data. However, graph grammars lack the
ability to capture time-changing phenomena since the left-to-right transitions
of a production rule do not represent temporal change. In the present work, we
describe dynamic vertex-replacement grammars (DyVeRG), which generalize vertex
replacement grammars in the time domain by providing a formal framework for
updating a learned graph grammar in accordance with modifications to its
underlying data. We show that DyVeRG grammars can be learned from, and used to
generate, real-world dynamic graphs faithfully while remaining
human-interpretable. We also demonstrate their ability to forecast by computing
dyvergence scores, a novel graph similarity measurement exposed by this
framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ModEFormer: Modality-Preserving Embedding for Audio-Video
  Synchronization using <span class="highlight-title">Transformer</span>s <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11551v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11551v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akash Gupta, Rohun Tripathi, Wondong Jang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lack of audio-video synchronization is a common problem during television
broadcasts and video conferencing, leading to an unsatisfactory viewing
experience. A widely accepted paradigm is to create an error detection
mechanism that identifies the cases when audio is leading or lagging. We
propose ModEFormer, which independently extracts audio and video embeddings
using modality-specific transformers. Different from the other
transformer-based approaches, ModEFormer preserves the modality of the input
streams which allows us to use a larger batch size with more negative audio
samples for contrastive learning. Further, we propose a trade-off between the
number of negative samples and number of unique samples in a batch to
significantly exceed the performance of previous methods. Experimental results
show that ModEFormer achieves state-of-the-art performance, 94.5% for LRS2 and
90.9% for LRS3. Finally, we demonstrate how ModEFormer can be used for offset
detection for test clips.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper accepted at ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fix the Noise: Disentangling Source Feature for Controllable Domain
  Translation <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11545v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11545v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongyeun Lee, Jae Young Lee, Doyeon Kim, Jaehyun Choi, Jaejun Yoo, Junmo Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies show strong generative performance in domain translation
especially by using transfer learning techniques on the unconditional
generator. However, the control between different domain features using a
single model is still challenging. Existing methods often require additional
models, which is computationally demanding and leads to unsatisfactory visual
quality. In addition, they have restricted control steps, which prevents a
smooth transition. In this paper, we propose a new approach for high-quality
domain translation with better controllability. The key idea is to preserve
source features within a disentangled subspace of a target feature space. This
allows our method to smoothly control the degree to which it preserves source
features while generating images from an entirely new domain using only a
single model. Our extensive experiments show that the proposed method can
produce more consistent and realistic images than previous works and maintain
precise controllability over different levels of transformation. The code is
available at https://github.com/LeeDongYeun/FixNoise.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2023. The code is available at
  https://github.com/LeeDongYeun/FixNoise. Extended from arXiv:2204.14079 (AICC
  workshop at CVPR 2022)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MSTFormer: Motion Inspired Spatial-temporal <span class="highlight-title">Transformer</span> with
  Dynamic-aware Attention for long-term Vessel Trajectory Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11540v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11540v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huimin Qiang, Zhiyuan Guo, Shiyuan Xie, Xiaodong Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Incorporating the dynamics knowledge into the model is critical for achieving
accurate trajectory prediction while considering the spatial and temporal
characteristics of the vessel. However, existing methods rarely consider the
underlying dynamics knowledge and directly use machine learning algorithms to
predict the trajectories. Intuitively, the vessel's motions are following the
laws of dynamics, e.g., the speed of a vessel decreases when turning a corner.
Yet, it is challenging to combine dynamic knowledge and neural networks due to
their inherent heterogeneity. Against this background, we propose MSTFormer, a
motion inspired vessel trajectory prediction method based on Transformer. The
contribution of this work is threefold. First, we design a data augmentation
method to describe the spatial features and motion features of the trajectory.
Second, we propose a Multi-headed Dynamic-aware Self-attention mechanism to
focus on trajectory points with frequent motion transformations. Finally, we
construct a knowledge-inspired loss function to further boost the performance
of the model. Experimental results on real-world datasets show that our
strategy not only effectively improves long-term predictive capability but also
outperforms backbones on cornering data.The ablation analysis further confirms
the efficacy of the proposed method. To the best of our knowledge, MSTFormer is
the first neural network model for trajectory prediction fused with vessel
motion dynamics, providing a worthwhile direction for future research.The
source code is available at https://github.com/simple316/MSTFormer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Indeterminate Probability Neural Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11536v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11536v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Yang, Chuang Liu, Xiaofeng Ma, Weijia Lu, Ning Wu, Bingyang Li, Zhifei Yang, Peng Liu, Lin Sun, Xiaodong Zhang, Can Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new general model called IPNN - Indeterminate Probability Neural
Network, which combines neural network and probability theory together. In the
classical probability theory, the calculation of probability is based on the
occurrence of events, which is hardly used in current neural networks. In this
paper, we propose a new general probability theory, which is an extension of
classical probability theory, and makes classical probability theory a special
case to our theory. Besides, for our proposed neural network framework, the
output of neural network is defined as probability events, and based on the
statistical analysis of these events, the inference model for classification
task is deduced. IPNN shows new property: It can perform unsupervised
clustering while doing classification. Besides, IPNN is capable of making very
large classification with very small neural network, e.g. model with 100 output
nodes can classify 10 billion categories. Theoretical advantages are reflected
in experimental results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Counterfactually Fair Regression with Double Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11529v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11529v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patrick Rehill
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Counterfactual fairness is an approach to AI fairness that tries to make
decisions based on the outcomes that an individual with some kind of sensitive
status would have had without this status. This paper proposes Double Machine
Learning (DML) Fairness which analogises this problem of counterfactual
fairness in regression problems to that of estimating counterfactual outcomes
in causal inference under the Potential Outcomes framework. It uses arbitrary
machine learning methods to partial out the effect of sensitive variables on
nonsensitive variables and outcomes. Assuming that the effects of the two sets
of variables are additively separable, outcomes will be approximately equalised
and individual-level outcomes will be counterfactually fair. This paper
demonstrates the approach in a simulation study pertaining to discrimination in
workplace hiring and an application on real data estimating the GPAs of law
school students. It then discusses when it is appropriate to apply such a
method to problems of real-world discrimination where constructs are
conceptually complex and finally, whether DML Fairness can achieve justice in
these settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 16 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SIFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11525v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11525v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shreyas Saxena, Vithursan Thangarasa, Abhay Gupta, Sean Lie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works have explored the use of weight sparsity to improve the training
efficiency (test accuracy w.r.t training FLOPs) of deep neural networks (DNNs).
These works aim to reduce training FLOPs but training with sparse weights often
leads to accuracy loss or requires longer train schedules, making the resulting
training efficiency less clear. In contrast, we focus on using sparsity to
increase accuracy while using the same FLOPS as the dense model and show
training efficiency gains through higher accuracy. In this work, we introduce
SIFT, a family of Sparse Iso-FLOP Transformations which are used as drop-in
replacements for dense layers to improve their representational capacity and
FLOP efficiency. Each transformation is parameterized by a single parameter
(sparsity level) and provides a larger search space to find optimal sparse
masks. Without changing any training hyperparameters, replacing dense layers
with SIFT leads to significant improvements across computer vision (CV) and
natural language processing (NLP) tasks, including ResNet-18 on ImageNet
(+3.5%) and GPT-3 Small on WikiText-103 (-0.4 PPL), both matching larger dense
model variants with 2x or more FLOPs. To the best of our knowledge, this is the
first work to demonstrate the use of sparsity for improving accuracy of dense
models via a simple-to-use set of sparse transformations. Code is available at:
https://github.com/CerebrasResearch/SIFT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Online Learning for Equilibrium Pricing in Markets under Incomplete
  Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11522v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11522v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Devansh Jalota, Haoyuan Sun, Navid Azizan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The study of market equilibria is central to economic theory, particularly in
efficiently allocating scarce resources. However, the computation of
equilibrium prices at which the supply of goods matches their demand typically
relies on having access to complete information on private attributes of
agents, e.g., suppliers' cost functions, which are often unavailable in
practice. Motivated by this practical consideration, we consider the problem of
setting equilibrium prices in the incomplete information setting wherein a
market operator seeks to satisfy the customer demand for a commodity by
purchasing the required amount from competing suppliers with privately known
cost functions unknown to the market operator. In this incomplete information
setting, we consider the online learning problem of learning equilibrium prices
over time while jointly optimizing three performance metrics -- unmet demand,
cost regret, and payment regret -- pertinent in the context of equilibrium
pricing over a horizon of $T$ periods. We first consider the setting when
suppliers' cost functions are fixed and develop algorithms that achieve a
regret of $O(\log \log T)$ when the customer demand is constant over time, or
$O(\sqrt{T} \log \log T)$ when the demand is variable over time. Next, we
consider the setting when the suppliers' cost functions can vary over time and
illustrate that no online algorithm can achieve sublinear regret on all three
metrics when the market operator has no information about how the cost
functions change over time. Thus, we consider an augmented setting wherein the
operator has access to hints/contexts that, without revealing the complete
specification of the cost functions, reflect the variation in the cost
functions over time and propose an algorithm with sublinear regret in this
augmented setting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ STDLens: Model Hijacking-resilient Federated Learning for Object
  Detection <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11511v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11511v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ka-Ho Chow, Ling Liu, Wenqi Wei, Fatih Ilhan, Yanzhao Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) has been gaining popularity as a collaborative
learning framework to train deep learning-based object detection models over a
distributed population of clients. Despite its advantages, FL is vulnerable to
model hijacking. The attacker can control how the object detection system
should misbehave by implanting Trojaned gradients using only a small number of
compromised clients in the collaborative learning process. This paper
introduces STDLens, a principled approach to safeguarding FL against such
attacks. We first investigate existing mitigation mechanisms and analyze their
failures caused by the inherent errors in spatial clustering analysis on
gradients. Based on the insights, we introduce a three-tier forensic framework
to identify and expel Trojaned gradients and reclaim the performance over the
course of FL. We consider three types of adaptive attacks and demonstrate the
robustness of STDLens against advanced adversaries. Extensive experiments show
that STDLens can protect FL against different model hijacking attacks and
outperform existing methods in identifying and removing Trojaned gradients with
significantly higher precision and much lower false-positive rates.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023. Source Code: https://github.com/git-disl/STDLens</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI-in-the-Loop -- The impact of HMI in AI-based Application 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11508v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11508v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julius Schöning, Clemens Westerkamp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence (AI) and human-machine interaction (HMI) are two
keywords that usually do not fit embedded applications. Within the steps needed
before applying AI to solve a specific task, HMI is usually missing during the
AI architecture design and the training of an AI model. The human-in-the-loop
concept is prevalent in all other steps of developing AI, from data analysis
via data selection and cleaning to performance evaluation. During AI
architecture design, HMI can immediately highlight unproductive layers of the
architecture so that lightweight network architecture for embedded applications
can be created easily. We show that by using this HMI, users can instantly
distinguish which AI architecture should be trained and evaluated first since a
high accuracy on the task could be expected. This approach reduces the
resources needed for AI development by avoiding training and evaluating AI
architectures with unproductive layers and leads to lightweight AI
architectures. These resulting lightweight AI architectures will enable HMI
while running the AI on an edge device. By enabling HMI during an AI uses
inference, we will introduce the AI-in-the-loop concept that combines AI's and
humans' strengths. In our AI-in-the-loop approach, the AI remains the working
horse and primarily solves the task. If the AI is unsure whether its inference
solves the task correctly, it asks the user to use an appropriate HMI.
Consequently, AI will become available in many applications soon since HMI will
make AI more reliable and explainable.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages; 9 figures; 1 table;</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DG-Trans: Dual-level Graph <span class="highlight-title">Transformer</span> for Spatiotemporal Incident
  Impact Prediction on Traffic Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12238v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12238v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanshen Sun, Kaiqun Fu, Chang-Tien Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The prompt estimation of traffic incident impacts can guide commuters in
their trip planning and improve the resilience of transportation agencies'
decision-making on resilience. However, it is more challenging than node-level
and graph-level forecasting tasks, as it requires extracting the anomaly
subgraph or sub-time-series from dynamic graphs. In this paper, we propose
DG-Trans, a novel traffic incident impact prediction framework, to foresee the
impact of traffic incidents through dynamic graph learning. The proposed
framework contains a dual-level spatial transformer and an
importance-score-based temporal transformer, and the performance of this
framework is justified by two newly constructed benchmark datasets. The
dual-level spatial transformer removes unnecessary edges between nodes to
isolate the affected subgraph from the other nodes. Meanwhile, the
importance-score-based temporal transformer identifies abnormal changes in node
features, causing the predictions to rely more on measurement changes after the
incident occurs. Therefore, DG-Trans is equipped with dual abilities that
extract spatiotemporal dependency and identify anomaly nodes affected by
incidents while removing noise introduced by benign nodes. Extensive
experiments on real-world datasets verify that DG-Trans outperforms the
existing state-of-the-art methods, especially in extracting spatiotemporal
dependency patterns and predicting traffic accident impacts. It offers
promising potential for traffic incident management systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Secure Aggregation in Federated Learning is not Private: Leaking User
  Data at Large Scale through Model Modification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12233v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12233v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joshua C. Zhao, Atul Sharma, Ahmed Roushdy Elkordy, Yahya H. Ezzeldin, Salman Avestimehr, Saurabh Bagchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Security and privacy are important concerns in machine learning. End user
devices often contain a wealth of data and this information is sensitive and
should not be shared with servers or enterprises. As a result, federated
learning was introduced to enable machine learning over large decentralized
datasets while promising privacy by eliminating the need for data sharing.
However, prior work has shown that shared gradients often contain private
information and attackers can gain knowledge either through malicious
modification of the architecture and parameters or by using optimization to
approximate user data from the shared gradients. Despite this, most attacks
have so far been limited in scale of number of clients, especially failing when
client gradients are aggregated together using secure model aggregation. The
attacks that still function are strongly limited in the number of clients
attacked, amount of training samples they leak, or number of iterations they
take to be trained. In this work, we introduce MANDRAKE, an attack that
overcomes previous limitations to directly leak large amounts of client data
even under secure aggregation across large numbers of clients. Furthermore, we
break the anonymity of aggregation as the leaked data is identifiable and
directly tied back to the clients they come from. We show that by sending
clients customized convolutional parameters, the weight gradients of data
points between clients will remain separate through aggregation. With an
aggregation across many clients, prior work could only leak less than 1% of
images. With the same number of non-zero parameters, and using only a single
training iteration, MANDRAKE leaks 70-80% of data samples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Infrastructure-based End-to-End Learning and Prevention of Driver
  Failure <span class="chip">ICRA 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12224v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12224v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Noam Buckman, Shiva Sreeram, Mathias Lechner, Yutong Ban, Ramin Hasani, Sertac Karaman, Daniela Rus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intelligent intersection managers can improve safety by detecting dangerous
drivers or failure modes in autonomous vehicles, warning oncoming vehicles as
they approach an intersection. In this work, we present FailureNet, a recurrent
neural network trained end-to-end on trajectories of both nominal and reckless
drivers in a scaled miniature city. FailureNet observes the poses of vehicles
as they approach an intersection and detects whether a failure is present in
the autonomy stack, warning cross-traffic of potentially dangerous drivers.
FailureNet can accurately identify control failures, upstream perception
errors, and speeding drivers, distinguishing them from nominal driving. The
network is trained and deployed with autonomous vehicles in the MiniCity.
Compared to speed or frequency-based predictors, FailureNet's recurrent neural
network structure provides improved predictive power, yielding upwards of 84%
accuracy when deployed on hardware.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages. Accepted to ICRA 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Community detection in complex networks via node similarity, graph
  representation learning, and hierarchical clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12212v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12212v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Łukasz Brzozowski, Grzegorz Siudem, Marek Gagolewski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Community detection is a critical challenge in the analysis of real-world
graphs and complex networks, including social, transportation, citation,
cybersecurity networks, and food webs. Motivated by many similarities between
community detection and clustering in Euclidean spaces, we propose three
algorithm frameworks to apply hierarchical clustering methods for community
detection in graphs. We show that using our methods, it is possible to apply
various linkage-based (single-, complete-, average- linkage, Ward, Genie)
clustering algorithms to find communities based on vertex similarity matrices,
eigenvector matrices thereof, and Euclidean vector representations of nodes. We
convey a comprehensive analysis of choices for each framework, including
state-of-the-art graph representation learning algorithms, such as Deep Neural
Graph Representation, and a vertex proximity matrix known to yield high-quality
results in machine learning -- Positive Pointwise Mutual Information. Overall,
we test over a hundred combinations of framework components and show that some
-- including Wasserman-Faust and PPMI proximity, DNGR representation -- can
compete with algorithms such as state-of-the-art Leiden and Louvain and easily
outperform other known community detection algorithms. Notably, our algorithms
remain hierarchical and allow the user to specify any number of clusters a
priori.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Random Projection k Nearest Neighbours Ensemble for Classification via
  Extended Neighbourhood Rule 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12210v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12210v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amjad Ali, Muhammad Hamraz, Dost Muhammad Khan, Wajdan Deebani, Zardad Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensembles based on k nearest neighbours (kNN) combine a large number of base
learners, each constructed on a sample taken from a given training data.
Typical kNN based ensembles determine the k closest observations in the
training data bounded to a test sample point by a spherical region to predict
its class. In this paper, a novel random projection extended neighbourhood rule
(RPExNRule) ensemble is proposed where bootstrap samples from the given
training data are randomly projected into lower dimensions for additional
randomness in the base models and to preserve features information. It uses the
extended neighbourhood rule (ExNRule) to fit kNN as base learners on randomly
projected bootstrap samples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 8 diagrams, 69 references</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MAGVLT: Masked Generative Vision-and-Language <span class="highlight-title">Transformer</span> <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12208v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12208v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sungwoong Kim, Daejin Jo, Donghoon Lee, Jongmin Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While generative modeling on multimodal image-text data has been actively
developed with large-scale paired datasets, there have been limited attempts to
generate both image and text data by a single model rather than a generation of
one fixed modality conditioned on the other modality. In this paper, we explore
a unified generative vision-and-language (VL) model that can produce both
images and text sequences. Especially, we propose a generative VL transformer
based on the non-autoregressive mask prediction, named MAGVLT, and compare it
with an autoregressive generative VL transformer (ARGVLT). In comparison to
ARGVLT, the proposed MAGVLT enables bidirectional context encoding, fast
decoding by parallel token predictions in an iterative refinement, and extended
editing capabilities such as image and text infilling. For rigorous training of
our MAGVLT with image-text pairs from scratch, we combine the image-to-text,
text-to-image, and joint image-and-text mask prediction tasks. Moreover, we
devise two additional tasks based on the step-unrolled mask prediction and the
selective prediction on the mixture of two image-text pairs. Experimental
results on various downstream generation tasks of VL benchmarks show that our
MAGVLT outperforms ARGVLT by a large margin even with significant inference
speedup. Particularly, MAGVLT achieves competitive results on both zero-shot
image-to-text and text-to-image generation tasks from MS-COCO by one
moderate-sized model (fewer than 500M parameters) even without the use of
monomodal data and networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Policy Optimization for Personalized Interventions in Behavioral Health 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12206v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12206v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jackie Baek, Justin J. Boutilier, Vivek F. Farias, Jonas Oddur Jonasson, Erez Yoeli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Problem definition: Behavioral health interventions, delivered through
digital platforms, have the potential to significantly improve health outcomes,
through education, motivation, reminders, and outreach. We study the problem of
optimizing personalized interventions for patients to maximize some long-term
outcome, in a setting where interventions are costly and capacity-constrained.
  Methodology/results: This paper provides a model-free approach to solving
this problem. We find that generic model-free approaches from the reinforcement
learning literature are too data intensive for healthcare applications, while
simpler bandit approaches make progress at the expense of ignoring long-term
patient dynamics. We present a new algorithm we dub DecompPI that approximates
one step of policy iteration. Implementing DecompPI simply consists of a
prediction task from offline data, alleviating the need for online
experimentation. Theoretically, we show that under a natural set of structural
assumptions on patient dynamics, DecompPI surprisingly recovers at least 1/2 of
the improvement possible between a naive baseline policy and the optimal
policy. At the same time, DecompPI is both robust to estimation errors and
interpretable. Through an empirical case study on a mobile health platform for
improving treatment adherence for tuberculosis, we find that DecompPI can
provide the same efficacy as the status quo with approximately half the
capacity of interventions.
  Managerial implications: DecompPI is general and is easily implementable for
organizations aiming to improve long-term behavior through targeted
interventions. Our case study suggests that the platform's costs of deploying
interventions can potentially be cut by 50%, which facilitates the ability to
scale up the system in a cost-efficient fashion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analytical Conjugate Priors for Subclasses of Generalized Pareto
  Distributions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12199v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12199v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Masataro Asai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article is written for pedagogical purposes aiming at practitioners
trying to estimate the finite support of continuous probability distributions,
i.e., the minimum and the maximum of a distribution defined on a finite domain.
Generalized Pareto distribution GP({\theta}, {\sigma}, {\xi}) is a
three-parameter distribution which plays a key role in Peaks-Over-Threshold
framework for tail estimation in Extreme Value Theory. Estimators for GP often
lack analytical solutions and the best known Bayesian methods for GP involves
numerical methods. Moreover, existing literature focuses on estimating the
scale {\sigma} and the shape {\xi}, lacking discussion of the estimation of the
location {\theta} which is the lower support of (minimum value possible in) a
GP. To fill the gap, we analyze four two-parameter subclasses of GP whose
conjugate priors can be obtained analytically, although some of the results are
known. Namely, we prove the conjugacy for {\xi} > 0 (Pareto), {\xi} = 0
(Shifted Exponential), {\xi} < 0 (Power), and {\xi} = -1 (Two-parameter
Uniform).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Viscoelastic Constitutive Artificial Neural Networks (vCANNs) $-$ a
  framework for data-driven anisotropic nonlinear finite viscoelasticity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12164v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12164v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kian P. Abdolazizi, Kevin Linka, Christian J. Cyron
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The constitutive behavior of polymeric materials is often modeled by finite
linear viscoelastic (FLV) or quasi-linear viscoelastic (QLV) models. These
popular models are simplifications that typically cannot accurately capture the
nonlinear viscoelastic behavior of materials. For example, the success of
attempts to capture strain rate-dependent behavior has been limited so far. To
overcome this problem, we introduce viscoelastic Constitutive Artificial Neural
Networks (vCANNs), a novel physics-informed machine learning framework for
anisotropic nonlinear viscoelasticity at finite strains. vCANNs rely on the
concept of generalized Maxwell models enhanced with nonlinear strain
(rate)-dependent properties represented by neural networks. The flexibility of
vCANNs enables them to automatically identify accurate and sparse constitutive
models of a broad range of materials. To test vCANNs, we trained them on
stress-strain data from Polyvinyl Butyral, the electro-active polymers VHB 4910
and 4905, and a biological tissue, the rectus abdominis muscle. Different
loading conditions were considered, including relaxation tests, cyclic
tension-compression tests, and blast loads. We demonstrate that vCANNs can
learn to capture the behavior of all these materials accurately and
computationally efficiently without human guidance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning a Depth Covariance Function <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12157v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12157v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eric Dexheimer, Andrew J. Davison
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose learning a depth covariance function with applications to
geometric vision tasks. Given RGB images as input, the covariance function can
be flexibly used to define priors over depth functions, predictive
distributions given observations, and methods for active point selection. We
leverage these techniques for a selection of downstream tasks: depth
completion, bundle adjustment, and monocular dense visual odometry.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023. Project page: https://edexheim.github.io/depth_cov/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Pre-Processing: A Learning Framework for End-to-end Brain MRI
  Pre-processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12148v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12148v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinzi He, Alan Wang, Mert R. Sabuncu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Head MRI pre-processing involves converting raw images to an
intensity-normalized, skull-stripped brain in a standard coordinate space. In
this paper, we propose an end-to-end weakly supervised learning approach,
called Neural Pre-processing (NPP), for solving all three sub-tasks
simultaneously via a neural network, trained on a large dataset without
individual sub-task supervision. Because the overall objective is highly
under-constrained, we explicitly disentangle geometric-preserving intensity
mapping (skull-stripping and intensity normalization) and spatial
transformation (spatial normalization). Quantitative results show that our
model outperforms state-of-the-art methods which tackle only a single sub-task.
Our ablation experiments demonstrate the importance of the architecture design
we chose for NPP. Furthermore, NPP affords the user the flexibility to control
each of these tasks at inference time. The code and model are freely-available
at \url{https://github.com/Novestars/Neural-Pre-processing}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Universal Approximation Property of Hamiltonian Deep Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12147v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12147v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Zakwan, Massimiliano d'Angelo, Giancarlo Ferrari-Trecate
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the universal approximation capabilities of
Hamiltonian Deep Neural Networks (HDNNs) that arise from the discretization of
Hamiltonian Neural Ordinary Differential Equations. Recently, it has been shown
that HDNNs enjoy, by design, non-vanishing gradients, which provide numerical
stability during training. However, although HDNNs have demonstrated
state-of-the-art performance in several applications, a comprehensive study to
quantify their expressivity is missing. In this regard, we provide a universal
approximation theorem for HDNNs and prove that a portion of the flow of HDNNs
can approximate arbitrary well any continuous function over a compact domain.
This result provides a solid theoretical foundation for the practical use of
HDNNs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Fabrication Fidelity of Integrated Nanophotonic Devices Using
  Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12136v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12136v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dusan Gostimirovic, Yuri Grinberg, Dan-Xia Xu, Odile Liboiron-Ladouceur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Next-generation integrated nanophotonic device designs leverage advanced
optimization techniques such as inverse design and topology optimization which
achieve high performance and extreme miniaturization by optimizing a massively
complex design space enabled by small feature sizes. However, unless the
optimization is heavily constrained, the generated small features are not
reliably fabricated, leading to optical performance degradation. Even for
simpler, conventional designs, fabrication-induced performance degradation
still occurs. The degree of deviation from the original design not only depends
on the size and shape of its features, but also on the distribution of features
and the surrounding environment, presenting complex, proximity-dependent
behavior. Without proprietary fabrication process specifications, design
corrections can only be made after calibrating fabrication runs take place. In
this work, we introduce a general deep machine learning model that
automatically corrects photonic device design layouts prior to first
fabrication. Only a small set of scanning electron microscopy images of
engineered training features are required to create the deep learning model.
With correction, the outcome of the fabricated layout is closer to what is
intended, and thus so too is the performance of the design. Without modifying
the nanofabrication process, adding significant computation in design, or
requiring proprietary process specifications, we believe our model opens the
door to new levels of reliability and performance in next-generation photonic
circuits.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fundamentals of Generative Large Language Models and Perspectives in
  Cyber-Defense 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12132v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12132v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrei Kucharavy, Zachary Schillaci, Loïc Maréchal, Maxime Würsch, Ljiljana Dolamic, Remi Sabonnadiere, Dimitri Percia David, Alain Mermoud, Vincent Lenders
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative Language Models gained significant attention in late 2022 / early
2023, notably with the introduction of models refined to act consistently with
users' expectations of interactions with AI (conversational models). Arguably
the focal point of public attention has been such a refinement of the GPT3
model -- the ChatGPT and its subsequent integration with auxiliary
capabilities, including search as part of Microsoft Bing. Despite extensive
prior research invested in their development, their performance and
applicability to a range of daily tasks remained unclear and niche. However,
their wider utilization without a requirement for technical expertise, made in
large part possible through conversational fine-tuning, revealed the extent of
their true capabilities in a real-world environment. This has garnered both
public excitement for their potential applications and concerns about their
capabilities and potential malicious uses. This review aims to provide a brief
overview of the history, state of the art, and implications of Generative
Language Models in terms of their principles, abilities, limitations, and
future prospects -- especially in the context of cyber-defense, with a focus on
the Swiss operational environment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>41 pages (without references), 13 figures; public report of
  Cyber-Defence Campus</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Is <span class="highlight-title">BERT</span> Blind? Exploring the Effect of Vision-and-Language <span class="highlight-title">Pretrain</span>ing
  on Visual Language Understanding <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12513v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12513v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Morris Alper, Michael Fiman, Hadar Averbuch-Elor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most humans use visual imagination to understand and reason about language,
but models such as BERT reason about language using knowledge acquired during
text-only pretraining. In this work, we investigate whether vision-and-language
pretraining can improve performance on text-only tasks that involve implicit
visual reasoning, focusing primarily on zero-shot probing methods. We propose a
suite of visual language understanding (VLU) tasks for probing the visual
reasoning abilities of text encoder models, as well as various non-visual
natural language understanding (NLU) tasks for comparison. We also contribute a
novel zero-shot knowledge probing method, Stroop probing, for applying models
such as CLIP to text-only tasks without needing a prediction head such as the
masked language modelling head of models like BERT. We show that SOTA
multimodally trained text encoders outperform unimodally trained text encoders
on the VLU tasks while being underperformed by them on the NLU tasks, lending
new context to previously mixed results regarding the NLU capabilities of
multimodal models. We conclude that exposure to images during pretraining
affords inherent visual reasoning knowledge that is reflected in language-only
tasks that require implicit visual reasoning. Our findings bear importance in
the broader context of multimodal learning, providing principled guidelines for
the choice of text encoders used in such contexts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be presented in CVPR 2023. Project webpage:
  https://isbertblind.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CLSA: Contrastive Learning-based Survival Analysis for Popularity
  Prediction in MEC Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12097v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12097v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zohreh Hajiakhondi-Meybodi, Arash Mohammadi, Jamshid Abouei, Konstantinos N. Plataniotis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mobile Edge Caching (MEC) integrated with Deep Neural Networks (DNNs) is an
innovative technology with significant potential for the future generation of
wireless networks, resulting in a considerable reduction in users' latency. The
MEC network's effectiveness, however, heavily relies on its capacity to predict
and dynamically update the storage of caching nodes with the most popular
contents. To be effective, a DNN-based popularity prediction model needs to
have the ability to understand the historical request patterns of content,
including their temporal and spatial correlations. Existing state-of-the-art
time-series DNN models capture the latter by simultaneously inputting the
sequential request patterns of multiple contents to the network, considerably
increasing the size of the input sample. This motivates us to address this
challenge by proposing a DNN-based popularity prediction framework based on the
idea of contrasting input samples against each other, designed for the Unmanned
Aerial Vehicle (UAV)-aided MEC networks. Referred to as the Contrastive
Learning-based Survival Analysis (CLSA), the proposed architecture consists of
a self-supervised Contrastive Learning (CL) model, where the temporal
information of sequential requests is learned using a Long Short Term Memory
(LSTM) network as the encoder of the CL architecture. Followed by a Survival
Analysis (SA) network, the output of the proposed CLSA architecture is
probabilities for each content's future popularity, which are then sorted in
descending order to identify the Top-K popular contents. Based on the
simulation results, the proposed CLSA architecture outperforms its counterparts
across the classification accuracy and cache-hit ratio.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Physics informed machine learning with Smoothed particle hydrodynamics:
  Hierarchy of reduced Lagrangian models of turbulence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.13311v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.13311v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Woodward, Yifeng Tian, Criston Hyett, Chris Fryer, Daniel Livescu, Mikhail Stepanov, Michael Chertkov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building efficient, accurate and generalizable reduced order models of
developed turbulence remains a major challenge. This manuscript approaches this
problem by developing a hierarchy of parameterized reduced Lagrangian models
for turbulent flows, and investigates the effects of enforcing physical
structure through Smoothed Particle Hydrodynamics (SPH) versus relying on
neural networks (NN)s as universal function approximators. Starting from Neural
Network (NN) parameterizations of a Lagrangian acceleration operator, this
hierarchy of models gradually incorporates a weakly compressible and
parameterized SPH framework, which enforces physical symmetries, such as
Galilean, rotational and translational invariances. Within this hierarchy, two
new parameterized smoothing kernels are developed in order to increase the
flexibility of the learn-able SPH simulators. For each model we experiment with
different loss functions which are minimized using gradient based optimization,
where efficient computations of gradients are obtained by using Automatic
Differentiation (AD) and Sensitivity Analysis (SA). Each model within the
hierarchy is trained on two data sets associated with weekly compressible
Homogeneous Isotropic Turbulence (HIT): (1) a validation set using weakly
compressible SPH; and (2) a high fidelity set from Direct Numerical Simulations
(DNS). Numerical evidence shows that encoding more SPH structure improves
generalizability to different turbulent Mach numbers and time shifts, and that
including the novel parameterized smoothing kernels improves the accuracy of
SPH at the resolved scales.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient and Feasible Robotic Assembly Sequence Planning via Graph
  Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10135v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10135v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matan Atad, Jianxiang Feng, Ismael Rodríguez, Maximilian Durner, Rudolph Triebel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic Robotic Assembly Sequence Planning (RASP) can significantly improve
productivity and resilience in modern manufacturing along with the growing need
for greater product customization. One of the main challenges in realizing such
automation resides in efficiently finding solutions from a growing number of
potential sequences for increasingly complex assemblies. Besides, costly
feasibility checks are always required for the robotic system. To address this,
we propose a holistic graphical approach including a graph representation
called Assembly Graph for product assemblies and a policy architecture, Graph
Assembly Processing Network, dubbed GRACE for assembly sequence generation.
Secondly, we use GRACE to extract meaningful information from the graph input
and predict assembly sequences in a step-by-step manner. In experiments, we
show that our approach can predict feasible assembly sequences across product
variants of aluminum profiles based on data collected in simulation of a
dual-armed robotic system. We further demonstrate that our method is capable of
detecting infeasible assemblies, substantially alleviating the undesirable
impacts from false predictions, and hence facilitating real-world deployment
soon. Code and training data will be open-sourced.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MixMask: Revisiting Masking Strategy for Siamese ConvNets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.11456v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.11456v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kirill Vishniakov, Eric Xing, Zhiqiang Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in self-supervised learning have integrated Masked Image
Modeling (MIM) and Siamese Networks into a unified framework that leverages the
benefits of both techniques. However, several issues remain unaddressed when
applying conventional erase-based masking with Siamese ConvNets. These include
(I) the inability to drop uninformative masked regions in ConvNets as they
process data continuously, resulting in low training efficiency compared to ViT
models; and (II) the mismatch between erase-based masking and the
contrastive-based objective in Siamese ConvNets, which differs from the MIM
approach. In this paper, we propose a filling-based masking strategy called
MixMask to prevent information incompleteness caused by the randomly erased
regions in an image in the vanilla masking method. Furthermore, we introduce a
flexible loss function design that considers the semantic distance change
between two different mixed views to adapt the integrated architecture and
prevent mismatches between the transformed input and objective in Masked
Siamese ConvNets (MSCN). We conducted extensive experiments on various
datasets, including CIFAR-100, Tiny-ImageNet, and ImageNet-1K. The results
demonstrate that our proposed framework achieves superior accuracy on linear
probing, semi-supervised, and supervised finetuning, outperforming the
state-of-the-art MSCN by a significant margin. Additionally, we demonstrate the
superiority of our approach in object detection and segmentation tasks. Our
source code is available at https://github.com/LightnessOfBeing/MixMask.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical report. Code is available at
  https://github.com/LightnessOfBeing/MixMask</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comprehensive <span class="highlight-title">Review</span> of Spiking Neural Networks: Interpretation,
  Optimization, Efficiency, and Best Practices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10780v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10780v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Malcolm, Josue Casco-Rodriguez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Biological neural networks continue to inspire breakthroughs in neural
network performance. And yet, one key area of neural computation that has been
under-appreciated and under-investigated is biologically plausible,
energy-efficient spiking neural networks, whose potential is especially
attractive for low-power, mobile, or otherwise hardware-constrained settings.
We present a literature review of recent developments in the interpretation,
optimization, efficiency, and accuracy of spiking neural networks. Key
contributions include identification, discussion, and comparison of
cutting-edge methods in spiking neural network optimization, energy-efficiency,
and evaluation, starting from first principles so as to be accessible to new
practitioners.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CoReS: Compatible Representations via Stationarity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.07632v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.07632v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niccolo Biondi, Federico Pernici, Matteo Bruni, Alberto Del Bimbo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a novel method to learn internal feature
representation models that are \textit{compatible} with previously learned
ones. Compatible features enable for direct comparison of old and new learned
features, allowing them to be used interchangeably over time. This eliminates
the need for visual search systems to extract new features for all previously
seen images in the gallery-set when sequentially upgrading the representation
model. Extracting new features is typically quite expensive or infeasible in
the case of very large gallery-sets and/or real time systems (i.e.,
face-recognition systems, social networks, life-long learning systems, robotics
and surveillance systems). Our approach, called Compatible Representations via
Stationarity (CoReS), achieves compatibility by encouraging stationarity to the
learned representation model without relying on previously learned models.
Stationarity allows features' statistical properties not to change under time
shift so that the current learned features are inter-operable with the old
ones. We evaluate single and sequential multi-model upgrading in growing
large-scale training datasets and we show that our method improves the
state-of-the-art in achieving compatible features by a large margin. In
particular, upgrading ten times with training data taken from CASIA-WebFace and
evaluating in Labeled Face in the Wild (LFW), we obtain a 49\% increase in
measuring the average number of times compatibility is achieved, which is a
544\% relative improvement over previous state-of-the-art.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>in IEEE Transactions on Pattern Analysis and Machine Intelligence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Representations of Bi-level Knowledge Graphs for Reasoning
  beyond Link Prediction <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.02601v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.02601v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chanyoung Chung, Joyce Jiyoung Whang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge graphs represent known facts using triplets. While existing
knowledge graph embedding methods only consider the connections between
entities, we propose considering the relationships between triplets. For
example, let us consider two triplets $T_1$ and $T_2$ where $T_1$ is
(Academy_Awards, Nominates, Avatar) and $T_2$ is (Avatar, Wins,
Academy_Awards). Given these two base-level triplets, we see that $T_1$ is a
prerequisite for $T_2$. In this paper, we define a higher-level triplet to
represent a relationship between triplets, e.g., $\langle T_1$,
PrerequisiteFor, $T_2\rangle$ where PrerequisiteFor is a higher-level relation.
We define a bi-level knowledge graph that consists of the base-level and the
higher-level triplets. We also propose a data augmentation strategy based on
the random walks on the bi-level knowledge graph to augment plausible triplets.
Our model called BiVE learns embeddings by taking into account the structures
of the base-level and the higher-level triplets, with additional consideration
of the augmented triplets. We propose two new tasks: triplet prediction and
conditional link prediction. Given a triplet $T_1$ and a higher-level relation,
the triplet prediction predicts a triplet that is likely to be connected to
$T_1$ by the higher-level relation, e.g., $\langle T_1$, PrerequisiteFor,
?$\rangle$. The conditional link prediction predicts a missing entity in a
triplet conditioned on another triplet, e.g., $\langle T_1$, PrerequisiteFor,
(Avatar, Wins, ?)$\rangle$. Experimental results show that BiVE significantly
outperforms all other methods in the two new tasks and the typical base-level
link prediction in real-world bi-level knowledge graphs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 3 figures, 15 tables. 37th AAAI Conference on Artificial
  Intelligence (AAAI 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Policy Adaptation from Foundation Model Feedback <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07398v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07398v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuying Ge, Annabella Macaluso, Li Erran Li, Ping Luo, Xiaolong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent progress on vision-language foundation models have brought significant
advancement to building general-purpose robots. By using the pre-trained models
to encode the scene and instructions as inputs for decision making, the
instruction-conditioned policy can generalize across different objects and
tasks. While this is encouraging, the policy still fails in most cases given an
unseen task or environment. In this work, we propose Policy Adaptation from
Foundation model Feedback (PAFF). When deploying the trained policy to a new
task or a new environment, we first let the policy play with randomly generated
instructions to record the demonstrations. While the execution could be wrong,
we can use the pre-trained foundation models to provide feedback to relabel the
demonstrations. This automatically provides new pairs of
demonstration-instruction data for policy fine-tuning. We evaluate our method
on a broad range of experiments with the focus on generalization on unseen
objects, unseen tasks, unseen environments, and sim-to-real transfer. We show
PAFF improves baselines by a large margin in all cases. Our project page is
available at https://geyuying.github.io/PAFF/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2023; Project page: https://geyuying.github.io/PAFF/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Calibration Matters: Tackling Maximization Bias in Large-scale
  Advertising Recommendation Systems <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.09809v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.09809v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yewen Fan, Nian Si, Kun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Calibration is defined as the ratio of the average predicted click rate to
the true click rate. The optimization of calibration is essential to many
online advertising recommendation systems because it directly affects the
downstream bids in ads auctions and the amount of money charged to advertisers.
Despite its importance, calibration optimization often suffers from a problem
called "maximization bias". Maximization bias refers to the phenomenon that the
maximum of predicted values overestimates the true maximum. The problem is
introduced because the calibration is computed on the set selected by the
prediction model itself. It persists even if unbiased predictions can be
achieved on every datapoint and worsens when covariate shifts exist between the
training and test sets. To mitigate this problem, we theorize the
quantification of maximization bias and propose a variance-adjusting debiasing
(VAD) meta-algorithm in this paper. The algorithm is efficient, robust, and
practical as it is able to mitigate maximization bias problems under covariate
shifts, neither incurring additional online serving costs nor compromising the
ranking performance. We demonstrate the effectiveness of the proposed algorithm
using a state-of-the-art recommendation neural network model on a large-scale
real-world dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalized partitioned local depth 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10167v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10167v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kenneth S. Berenhaut, John D. Foley, Liangdongsheng Lyu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we provide a generalization of the concept of cohesion as
introduced recently by Berenhaut, Moore and Melvin [Proceedings of the National
Academy of Sciences, 119 (4) (2022)]. The formulation presented builds on the
technique of partitioned local depth by distilling two key probabilistic
concepts: local relevance and support division. Earlier results are extended
within the new context, and examples of applications to revealing communities
in data with uncertainty are included.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Improved exposition, examples in 5.1 & 5.3 expanded, 17 pages, 7
  figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GAMR: A Guided Attention Model for (visual) Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.04928v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.04928v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohit Vaishnav, Thomas Serre
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans continue to outperform modern AI systems in their ability to flexibly
parse and understand complex visual scenes. Here, we present a novel module for
visual reasoning, the Guided Attention Model for (visual) Reasoning (GAMR),
which instantiates an active vision theory -- positing that the brain solves
complex visual reasoning problems dynamically -- via sequences of attention
shifts to select and route task-relevant visual information into memory.
Experiments on an array of visual reasoning tasks and datasets demonstrate
GAMR's ability to learn visual routines in a robust and sample-efficient
manner. In addition, GAMR is shown to be capable of zero-shot generalization on
completely novel reasoning tasks. Overall, our work provides computational
support for cognitive theories that postulate the need for a critical interplay
between attention and memory to dynamically maintain and manipulate
task-relevant visual information to solve complex visual reasoning tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multiagent Reinforcement Learning for Autonomous Routing and Pickup
  Problem with Adaptation to Variable Demand <span class="chip">ICRA 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.14983v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.14983v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Garces, Sushmita Bhattacharya, Stephanie Gil, Dimitri Bertsekas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We derive a learning framework to generate routing/pickup policies for a
fleet of autonomous vehicles tasked with servicing stochastically appearing
requests on a city map. We focus on policies that 1) give rise to coordination
amongst the vehicles, thereby reducing wait times for servicing requests, 2)
are non-myopic, and consider a-priori potential future requests, 3) can adapt
to changes in the underlying demand distribution. Specifically, we are
interested in policies that are adaptive to fluctuations of actual demand
conditions in urban environments, such as on-peak vs. off-peak hours. We
achieve this through a combination of (i) an online play algorithm that
improves the performance of an offline-trained policy, and (ii) an offline
approximation scheme that allows for adapting to changes in the underlying
demand model. In particular, we achieve adaptivity of our learned policy to
different demand distributions by quantifying a region of validity using the
q-valid radius of a Wasserstein Ambiguity Set. We propose a mechanism for
switching the originally trained offline approximation when the current demand
is outside the original validity region. In this case, we propose to use an
offline architecture, trained on a historical demand model that is closer to
the current demand in terms of Wasserstein distance. We learn routing and
pickup policies over real taxicab requests in San Francisco with high
variability between on-peak and off-peak hours, demonstrating the ability of
our method to adapt to real fluctuation in demand distributions. Our numerical
results demonstrate that our method outperforms alternative rollout-based
reinforcement learning schemes, as well as other classical methods from
operations research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures, 3 tables, accepted to ICRA 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cross-Domain Evaluation of a Deep Learning-Based Type Inference System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.09189v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.09189v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bernd Gruner, Tim Sonnekalb, Thomas S. Heinze, Clemens-Alexander Brust
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optional type annotations allow for enriching dynamic programming languages
with static typing features like better Integrated Development Environment
(IDE) support, more precise program analysis, and early detection and
prevention of type-related runtime errors. Machine learning-based type
inference promises interesting results for automating this task. However, the
practical usage of such systems depends on their ability to generalize across
different domains, as they are often applied outside their training domain. In
this work, we investigate Type4Py as a representative of state-of-the-art deep
learning-based type inference systems, by conducting extensive cross-domain
experiments. Thereby, we address the following problems: class imbalances,
out-of-vocabulary words, dataset shifts, and unknown classes. To perform such
experiments, we use the datasets ManyTypes4Py and CrossDomainTypes4Py. The
latter we introduce in this paper. Our dataset enables the evaluation of type
inference systems in different domains of software projects and has over
1,000,000 type annotations mined on the platforms GitHub and Libraries. It
consists of data from the two domains web development and scientific
calculation. Through our experiments, we detect that the shifts in the dataset
and the long-tailed distribution with many rare and unknown data types decrease
the performance of the deep learning-based type inference system drastically.
In this context, we test unsupervised domain adaptation methods and fine-tuning
to overcome these issues. Moreover, we investigate the impact of
out-of-vocabulary words.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint for the MSR'23 technical track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DeepGraviLens: a Multi-Modal Architecture for Classifying Gravitational
  Lensing Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.00701v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.00701v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolò Oreste Pinciroli Vago, Piero Fraternali
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gravitational lensing is the relativistic effect generated by massive bodies,
which bend the space-time surrounding them. It is a deeply investigated topic
in astrophysics and allows validating theoretical relativistic results and
studying faint astrophysical objects that would not be visible otherwise. In
recent years Machine Learning methods have been applied to support the analysis
of the gravitational lensing phenomena by detecting lensing effects in data
sets consisting of images associated with brightness variation time series.
However, the state-of-art approaches either consider only images and neglect
time-series data or achieve relatively low accuracy on the most difficult data
sets. This paper introduces DeepGraviLens, a novel multi-modal network that
classifies spatio-temporal data belonging to one non-lensed system type and
three lensed system types. It surpasses the current state of the art accuracy
results by $\approx$ 19% to $\approx$ 43%, depending on the considered data
set. Such an improvement will enable the acceleration of the analysis of lensed
objects in upcoming astrophysical surveys, which will exploit the petabytes of
data collected, e.g., from the Vera C. Rubin Observatory.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ QAID: Question Answering Inspired Few-shot Intent Detection <span class="chip">ICLR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.01593v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.01593v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Asaf Yehudai, Matan Vetzler, Yosi Mass, Koren Lazar, Doron Cohen, Boaz Carmeli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intent detection with semantically similar fine-grained intents is a
challenging task. To address it, we reformulate intent detection as a
question-answering retrieval task by treating utterances and intent names as
questions and answers. To that end, we utilize a question-answering retrieval
architecture and adopt a two stages training schema with batch contrastive
loss. In the pre-training stage, we improve query representations through
self-supervised training. Then, in the fine-tuning stage, we increase
contextualized token-level similarity scores between queries and answers from
the same intent. Our results on three few-shot intent detection benchmarks
achieve state-of-the-art performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Explainable AI does not provide the explanations end-users are asking
  for 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.11577v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.11577v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Savio Rozario, George Čevora
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Explainable Artificial Intelligence (XAI) techniques are frequently required
by users in many AI systems with the goal of understanding complex models,
their associated predictions, and gaining trust. While suitable for some
specific tasks during development, their adoption by organisations to enhance
trust in machine learning systems has unintended consequences. In this paper we
discuss XAI's limitations in deployment and conclude that transparency
alongside with rigorous validation are better suited to gaining trust in AI
systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Privacy-Preserving Tree-Based Inference with Fully Homomorphic
  Encryption 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.01254v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.01254v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jordan Frery, Andrei Stoian, Roman Bredehoft, Luis Montero, Celia Kherfallah, Benoit Chevallier-Mames, Arthur Meyre
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Privacy enhancing technologies (PETs) have been proposed as a way to protect
the privacy of data while still allowing for data analysis. In this work, we
focus on Fully Homomorphic Encryption (FHE), a powerful tool that allows for
arbitrary computations to be performed on encrypted data. FHE has received lots
of attention in the past few years and has reached realistic execution times
and correctness.
  More precisely, we explain in this paper how we apply FHE to tree-based
models and get state-of-the-art solutions over encrypted tabular data. We show
that our method is applicable to a wide range of tree-based models, including
decision trees, random forests, and gradient boosted trees, and has been
implemented within the Concrete-ML library, which is open-source at
https://github.com/zama-ai/concrete-ml. With a selected set of use-cases, we
demonstrate that our FHE version is very close to the unprotected version in
terms of accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Threat of Adversarial Attacks on Machine Learning in Network
  Security -- A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1911.02621v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1911.02621v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olakunle Ibitoye, Rana Abou-Khamis, Mohamed el Shehaby, Ashraf Matrawy, M. Omair Shafiq
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning models have made many decision support systems to be faster,
more accurate, and more efficient. However, applications of machine learning in
network security face a more disproportionate threat of active adversarial
attacks compared to other domains. This is because machine learning
applications in network security such as malware detection, intrusion
detection, and spam filtering are by themselves adversarial in nature. In what
could be considered an arm's race between attackers and defenders, adversaries
constantly probe machine learning systems with inputs that are explicitly
designed to bypass the system and induce a wrong prediction. In this survey, we
first provide a taxonomy of machine learning techniques, tasks, and depth. We
then introduce a classification of machine learning in network security
applications. Next, we examine various adversarial attacks against machine
learning in network security and introduce two classification approaches for
adversarial attacks in network security. First, we classify adversarial attacks
in network security based on a taxonomy of network security applications.
Secondly, we categorize adversarial attacks in network security into a problem
space vs feature space dimensional classification model. We then analyze the
various defenses against adversarial attacks on machine learning-based network
security applications. We conclude by introducing an adversarial risk grid map
and evaluating several existing adversarial attacks against machine learning in
network security using the risk grid map. We also identify where each attack
classification resides within the adversarial risk grid map.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Topology optimization with physics-informed neural networks: application
  to noninvasive detection of hidden geometries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.09280v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.09280v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saviz Mowlavi, Ken Kamrin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting hidden geometrical structures from surface measurements under
electromagnetic, acoustic, or mechanical loading is the goal of noninvasive
imaging techniques in medical and industrial applications. Solving the inverse
problem can be challenging due to the unknown topology and geometry, the
sparsity of the data, and the complexity of the physical laws. Physics-informed
neural networks (PINNs) have shown promise as a simple-yet-powerful tool for
problem inversion, but they have yet to be applied to general problems with a
priori unknown topology. Here, we introduce a topology optimization framework
based on PINNs that solves geometry detection problems without prior knowledge
of the number or types of shapes. We allow for arbitrary solution topology by
representing the geometry using a material density field that approaches binary
values thanks to a novel eikonal regularization. We validate our framework by
detecting the number, locations, and shapes of hidden voids and inclusions in
linear and nonlinear elastic bodies using measurements of outer surface
displacement from a single mechanical loading experiment. Our methodology opens
a pathway for PINNs to solve various engineering problems targeting geometry
optimization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 16 figures including supplementary information. Added
  supplementary movies</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contrastive learning for regression in multi-site brain age prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.08326v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.08326v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carlo Alberto Barbano, Benoit Dufumier, Edouard Duchesnay, Marco Grangetto, Pietro Gori
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building accurate Deep Learning (DL) models for brain age prediction is a
very relevant topic in neuroimaging, as it could help better understand
neurodegenerative disorders and find new biomarkers. To estimate accurate and
generalizable models, large datasets have been collected, which are often
multi-site and multi-scanner. This large heterogeneity negatively affects the
generalization performance of DL models since they are prone to overfit
site-related noise. Recently, contrastive learning approaches have been shown
to be more robust against noise in data or labels. For this reason, we propose
a novel contrastive learning regression loss for robust brain age prediction
using MRI scans. Our method achieves state-of-the-art performance on the
OpenBHB challenge, yielding the best generalization capability and robustness
to site-related noise.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Message Passing for Objective-Based Uncertainty Quantification
  and Optimal Experimental Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.07120v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.07120v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qihua Chen, Xuejin Chen, Hyun-Myung Woo, Byung-Jun Yoon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Various real-world scientific applications involve the mathematical modeling
of complex uncertain systems with numerous unknown parameters. Accurate
parameter estimation is often practically infeasible in such systems, as the
available training data may be insufficient and the cost of acquiring
additional data may be high. In such cases, based on a Bayesian paradigm, we
can design robust operators retaining the best overall performance across all
possible models and design optimal experiments that can effectively reduce
uncertainty to enhance the performance of such operators maximally. While
objective-based uncertainty quantification (objective-UQ) based on MOCU (mean
objective cost of uncertainty) provides an effective means for quantifying
uncertainty in complex systems, the high computational cost of estimating MOCU
has been a challenge in applying it to real-world scientific/engineering
problems. In this work, we propose a novel scheme to reduce the computational
cost for objective-UQ via MOCU based on a data-driven approach. We adopt a
neural message-passing model for surrogate modeling, incorporating a novel
axiomatic constraint loss that penalizes an increase in the estimated system
uncertainty. As an illustrative example, we consider the optimal experimental
design (OED) problem for uncertain Kuramoto models, where the goal is to
predict the experiments that can most effectively enhance robust
synchronization performance through uncertainty reduction. We show that our
proposed approach can accelerate MOCU-based OED by four to five orders of
magnitude, without any visible performance loss compared to the
state-of-the-art. The proposed approach applies to general OED tasks, beyond
the Kuramoto model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 5 figures, accepted by Engineering Applications of
  Artificial Intelligence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diversity Induced Environment Design via Self-Play 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.02119v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.02119v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dexun Li, Wenjun Li, Pradeep Varakantham
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work on designing an appropriate distribution of environments has
shown promise for training effective generally capable agents. Its success is
partly because of a form of adaptive curriculum learning that generates
environment instances (or levels) at the frontier of the agent's capabilities.
However, such an environment design framework often struggles to find effective
levels in challenging design spaces and requires costly interactions with the
environment. In this paper, we aim to introduce diversity in the Unsupervised
Environment Design (UED) framework. Specifically, we propose a task-agnostic
method to identify observed/hidden states that are representative of a given
level. The outcome of this method is then utilized to characterize the
diversity between two levels, which as we show can be crucial to effective
performance. In addition, to improve sampling efficiency, we incorporate the
self-play technique that allows the environment generator to automatically
generate environments that are of great benefit to the training agent.
Quantitatively, our approach, Diversity-induced Environment Design via
Self-Play (DivSP), shows compelling performance over existing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">GPT</span>4MIA: Utilizing Generative <span class="highlight-title">Pre-train</span>ed <span class="highlight-title">Transformer</span> (<span class="highlight-title">GPT</span>-3) as A
  Plug-and-Play Transductive Model for Medical Image Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.08722v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.08722v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhe Zhang, Danny Z. Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a novel approach (called GPT4MIA) that utilizes
Generative Pre-trained Transformer (GPT) as a plug-and-play transductive
inference tool for medical image analysis (MIA). We provide theoretical
analysis on why a large pre-trained language model such as GPT-3 can be used as
a plug-and-play transductive inference model for MIA. At the methodological
level, we develop several technical treatments to improve the efficiency and
effectiveness of GPT4MIA, including better prompt structure design, sample
selection, and prompt ordering of representative samples/features. We present
two concrete use cases (with workflow) of GPT4MIA: (1) detecting prediction
errors and (2) improving prediction accuracy, working in conjecture with
well-established vision-based models for image classification (e.g., ResNet).
Experiments validate that our proposed method is effective for these two tasks.
We further discuss the opportunities and challenges in utilizing
Transformer-based large language models for broader MIA applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Version 3: Added appendix with more results and visualizations.
  Questions and suggestions are welcome</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Training Invertible Neural Networks as Autoencoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11239v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11239v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        The-Gia Leo Nguyen, Lynton Ardizzone, Ullrich Köthe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autoencoders are able to learn useful data representations in an unsupervised
matter and have been widely used in various machine learning and computer
vision tasks. In this work, we present methods to train Invertible Neural
Networks (INNs) as (variational) autoencoders which we call INN (variational)
autoencoders. Our experiments on MNIST, CIFAR and CelebA show that for low
bottleneck sizes our INN autoencoder achieves results similar to the classical
autoencoder. However, for large bottleneck sizes our INN autoencoder
outperforms its classical counterpart. Based on the empirical results, we
hypothesize that INN autoencoders might not have any intrinsic information loss
and thereby are not bounded to a maximal number of layers (depth) after which
only suboptimal results can be achieved.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Conference Paper at GCPR2019</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Simplifying Momentum-based Riemannian Submanifold Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.09738v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.09738v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wu Lin, Valentin Duruisseaux, Melvin Leok, Frank Nielsen, Mohammad Emtiyaz Khan, Mark Schmidt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Riemannian submanifold optimization with momentum is computationally
challenging because ensuring iterates remain on the submanifold often requires
solving difficult differential equations. We simplify such optimization
algorithms for the submanifold of symmetric positive-definite matrices with the
affine invariant metric. We propose a generalized version of the Riemannian
normal coordinates which dynamically trivializes the problem into a Euclidean
unconstrained problem. We use our approach to explain and simplify existing
approaches for structured covariances and develop efficient second-order
optimizers for deep learning without explicit matrix inverses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>updated the main text and added more numerical results</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SAMSON: Sharpness-Aware Minimization Scaled by Outlier Normalization for
  Improving DNN Generalization and Robustness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.11561v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.11561v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gonçalo Mordido, Sébastien Henwood, Sarath Chandar, François Leduc-Primeau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Energy-efficient deep neural network (DNN) accelerators are prone to
non-idealities that degrade DNN performance at inference time. To mitigate such
degradation, existing methods typically add perturbations to the DNN weights
during training to simulate inference on noisy hardware. However, this often
requires knowledge about the target hardware and leads to a trade-off between
DNN performance and robustness, decreasing the former to increase the latter.
In this work, we show that applying sharpness-aware training, by optimizing for
both the loss value and loss sharpness, significantly improves robustness to
noisy hardware at inference time without relying on any assumptions about the
target hardware. In particular, we propose a new adaptive sharpness-aware
method that conditions the worst-case perturbation of a given weight not only
on its magnitude but also on the range of the weight distribution. This is
achieved by performing sharpness-aware minimization scaled by outlier
minimization (SAMSON). Our approach outperforms existing sharpness-aware
training methods both in terms of model generalization performance in noiseless
regimes and robustness in noisy settings, as measured on several architectures
and datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Short-length SSVEP data extension by a novel generative adversarial
  networks based framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.05599v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.05599v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yudong Pan, Ning Li, Yangsong Zhang, Peng Xu, Dezhong Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Steady-state visual evoked potentials (SSVEPs) based brain-computer interface
(BCI) has received considerable attention due to its high information transfer
rate (ITR) and available quantity of targets. However, the performance of
frequency identification methods heavily hinges on the amount of user
calibration data and data length, which hinders the deployment in real-world
applications. Recently, generative adversarial networks (GANs)-based data
generation methods have been widely adopted to create synthetic
electroencephalography (EEG) data, holds promise to address these issues. In
this paper, we proposed a GAN-based end-to-end signal transformation network
for data length extension, termed as TEGAN. TEGAN transforms short-length SSVEP
signals into long-length artificial SSVEP signals. By incorporating a novel
U-Net generator architecture and an auxiliary classifier into the network
architecture, the TEGAN could produce conditioned features in the synthetic
data. Additionally, we introduced a two-stage training strategy and the
LeCam-divergence regularization term to regularize the training process of GAN
during the network implementation. The proposed TEGAN was evaluated on two
public SSVEP datasets (a 4-class dataset and a 12-class dataset). With the
assistance of TEGAN, the performance of traditional frequency recognition
methods and deep learning-based methods have been significantly improved under
limited calibration data. And the classification performance gap of various
frequency recognition methods has been narrowed. This study substantiates the
feasibility of the proposed method to extend the data length for short-time
SSVEP signals for developing a high-performance BCI system. The proposed
GAN-based methods have the great potential of shortening the calibration time
and cutting down the budget for various real-world BCI-based applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 10 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ask-AC: An Initiative Advisor-in-the-Loop Actor-Critic Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.01955v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.01955v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shunyu Liu, Na Yu, Jie Song, Kaixuan Chen, Zunlei Feng, Mingli Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the promising results achieved, state-of-the-art interactive
reinforcement learning schemes rely on passively receiving supervision signals
from advisor experts, in the form of either continuous monitoring or
pre-defined rules, which inevitably result in a cumbersome and expensive
learning process. In this paper, we introduce a novel initiative
advisor-in-the-loop actor-critic framework, termed as Ask-AC, that replaces the
unilateral advisor-guidance mechanism with a bidirectional learner-initiative
one, and thereby enables a customized and efficacious message exchange between
learner and advisor. At the heart of Ask-AC are two complementary components,
namely action requester and adaptive state selector, that can be readily
incorporated into various discrete actor-critic architectures. The former
component allows the agent to initiatively seek advisor intervention in the
presence of uncertain states, while the latter identifies the unstable states
potentially missed by the former especially when environment changes, and then
learns to promote the ask action on such states. Experimental results on both
stationary and non-stationary environments and across different actor-critic
backbones demonstrate that the proposed framework significantly improves the
learning efficiency of the agent, and achieves the performances on par with
those obtained by continuous advisor monitoring.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Models that Can See and Read 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07389v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07389v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roy Ganz, Oren Nuriel, Aviad Aberdam, Yair Kittenplon, Shai Mazor, Ron Litman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Question Answering (VQA) and Image Captioning (CAP), which are among
the most popular vision-language tasks, have analogous scene-text versions that
require reasoning from the text in the image. Despite their obvious
resemblance, the two are treated independently and, as we show, yield
task-specific methods that can either see or read, but not both. In this work,
we conduct an in-depth analysis of this phenomenon and propose UniTNT, a
Unified Text-Non-Text approach, which grants existing multimodal architectures
scene-text understanding capabilities. Specifically, we treat scene-text
information as an additional modality, fusing it with any pretrained
encoder-decoder-based architecture via designated modules. Thorough experiments
reveal that UniTNT leads to the first single model that successfully handles
both task types. Moreover, we show that scene-text understanding capabilities
can boost vision-language models' performance on general VQA and CAP by up to
2.69% and 0.6 CIDEr, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Whose Emotion Matters? Speaking Activity Localisation without Prior
  Knowledge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.15377v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.15377v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hugo Carneiro, Cornelius Weber, Stefan Wermter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of emotion recognition in conversations (ERC) benefits from the
availability of multiple modalities, as provided, for example, in the
video-based Multimodal EmotionLines Dataset (MELD). However, only a few
research approaches use both acoustic and visual information from the MELD
videos. There are two reasons for this: First, label-to-video alignments in
MELD are noisy, making those videos an unreliable source of emotional speech
data. Second, conversations can involve several people in the same scene, which
requires the localisation of the utterance source. In this paper, we introduce
MELD with Fixed Audiovisual Information via Realignment (MELD-FAIR) by using
recent active speaker detection and automatic speech recognition models, we are
able to realign the videos of MELD and capture the facial expressions from
speakers in 96.92% of the utterances provided in MELD. Experiments with a
self-supervised voice recognition model indicate that the realigned MELD-FAIR
videos more closely match the transcribed utterances given in the MELD dataset.
Finally, we devise a model for emotion recognition in conversations trained on
the realigned MELD-FAIR videos, which outperforms state-of-the-art models for
ERC based on vision alone. This indicates that localising the source of
speaking activities is indeed effective for extracting facial expressions from
the uttering speakers and that faces provide more informative visual cues than
the visual features state-of-the-art models have been using so far. The
MELD-FAIR realignment data, and the code of the realignment procedure and of
the emotional recognition, are available at
https://github.com/knowledgetechnologyuhh/MELD-FAIR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 8 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vid2Seq: Large-Scale <span class="highlight-title">Pretrain</span>ing of a Visual Language Model for Dense
  Video Captioning <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.14115v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.14115v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antoine Yang, Arsha Nagrani, Paul Hongsuck Seo, Antoine Miech, Jordi Pont-Tuset, Ivan Laptev, Josef Sivic, Cordelia Schmid
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we introduce Vid2Seq, a multi-modal single-stage dense event
captioning model pretrained on narrated videos which are readily-available at
scale. The Vid2Seq architecture augments a language model with special time
tokens, allowing it to seamlessly predict event boundaries and textual
descriptions in the same output sequence. Such a unified model requires
large-scale training data, which is not available in current annotated
datasets. We show that it is possible to leverage unlabeled narrated videos for
dense video captioning, by reformulating sentence boundaries of transcribed
speech as pseudo event boundaries, and using the transcribed speech sentences
as pseudo event captions. The resulting Vid2Seq model pretrained on the
YT-Temporal-1B dataset improves the state of the art on a variety of dense
video captioning benchmarks including YouCook2, ViTT and ActivityNet Captions.
Vid2Seq also generalizes well to the tasks of video paragraph captioning and
video clip captioning, and to few-shot settings. Our code is publicly available
at https://antoyang.github.io/vid2seq.html.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023 Camera-Ready; Project Webpage:
  https://antoyang.github.io/vid2seq.html ; 18 pages; 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning and controlling the source-filter representation of speech with
  a variational autoencoder 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.07075v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.07075v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samir Sadok, Simon Leglaive, Laurent Girin, Xavier Alameda-Pineda, Renaud Séguier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding and controlling latent representations in deep generative
models is a challenging yet important problem for analyzing, transforming and
generating various types of data. In speech processing, inspiring from the
anatomical mechanisms of phonation, the source-filter model considers that
speech signals are produced from a few independent and physically meaningful
continuous latent factors, among which the fundamental frequency $f_0$ and the
formants are of primary importance. In this work, we start from a variational
autoencoder (VAE) trained in an unsupervised manner on a large dataset of
unlabeled natural speech signals, and we show that the source-filter model of
speech production naturally arises as orthogonal subspaces of the VAE latent
space. Using only a few seconds of labeled speech signals generated with an
artificial speech synthesizer, we propose a method to identify the latent
subspaces encoding $f_0$ and the first three formant frequencies, we show that
these subspaces are orthogonal, and based on this orthogonality, we develop a
method to accurately and independently control the source-filter speech factors
within the latent subspaces. Without requiring additional information such as
text or human-labeled data, this results in a deep generative model of speech
spectrograms that is conditioned on $f_0$ and the formant frequencies, and
which is applied to the transformation speech signals. Finally, we also propose
a robust $f_0$ estimation method that exploits the projection of a speech
signal onto the learned latent subspace associated with $f_0$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 7 figures, companion website:
  https://samsad35.github.io/site-sfvae/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sharpness-aware Quantization for Deep Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.12273v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.12273v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Liu, Jianfei Cai, Bohan Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Network quantization is a dominant paradigm of model compression. However,
the abrupt changes in quantized weights during training often lead to severe
loss fluctuations and result in a sharp loss landscape, making the gradients
unstable and thus degrading the performance. Recently, Sharpness-Aware
Minimization (SAM) has been proposed to smooth the loss landscape and improve
the generalization performance of the models. Nevertheless, directly applying
SAM to the quantized models can lead to perturbation mismatch or diminishment
issues, resulting in suboptimal performance. In this paper, we propose a novel
method, dubbed Sharpness-Aware Quantization (SAQ), to explore the effect of SAM
in model compression, particularly quantization for the first time.
Specifically, we first provide a unified view of quantization and SAM by
treating them as introducing quantization noises and adversarial perturbations
to the model weights, respectively. According to whether the noise and
perturbation terms depend on each other, SAQ can be formulated into three
cases, which are analyzed and compared comprehensively. Furthermore, by
introducing an efficient training strategy, SAQ only incurs a little additional
training overhead compared with the default optimizer (e.g., SGD or AdamW).
Extensive experiments on both convolutional neural networks and Transformers
across various datasets (i.e., ImageNet, CIFAR-10/100, Oxford Flowers-102,
Oxford-IIIT Pets) show that SAQ improves the generalization performance of the
quantized models, yielding the SOTA results in uniform quantization. For
example, on ImageNet, SAQ outperforms AdamW by 1.2% on the Top-1 accuracy for
4-bit ViT-B/16. Our 4-bit ResNet-50 surpasses the previous SOTA method by 0.9%
on the Top-1 accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Tech report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bandits Corrupted by Nature: Lower Bounds on Regret and Robust
  Optimistic Algorithm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.03186v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.03186v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Debabrota Basu, Odalric-Ambrym Maillard, Timothée Mathieu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the corrupted bandit problem, i.e. a stochastic multi-armed bandit
problem with $k$ unknown reward distributions, which are heavy-tailed and
corrupted by a history-independent adversary or Nature. To be specific, the
reward obtained by playing an arm comes from corresponding heavy-tailed reward
distribution with probability $1-\varepsilon \in (0.5,1]$ and an arbitrary
corruption distribution of unbounded support with probability $\varepsilon \in
[0,0.5)$.
  First, we provide $\textit{a problem-dependent lower bound on the regret}$ of
any corrupted bandit algorithm. The lower bounds indicate that the corrupted
bandit problem is harder than the classical stochastic bandit problem with
sub-Gaussian or heavy-tail rewards.
  Following that, we propose a novel UCB-type algorithm for corrupted bandits,
namely HubUCB, that builds on Huber's estimator for robust mean estimation.
Leveraging a novel concentration inequality of Huber's estimator, we prove that
HubUCB achieves a near-optimal regret upper bound.
  Since computing Huber's estimator has quadratic complexity, we further
introduce a sequential version of Huber's estimator that exhibits linear
complexity. We leverage this sequential estimator to design SeqHubUCB that
enjoys similar regret guarantees while reducing the computational burden.
  Finally, we experimentally illustrate the efficiency of HubUCB and SeqHubUCB
in solving corrupted bandits for different reward distributions and different
levels of corruptions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scheduling and Aggregation Design for Asynchronous Federated Learning
  over Wireless Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07356v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07356v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chung-Hsuan Hu, Zheng Chen, Erik G. Larsson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) is a collaborative machine learning (ML) framework
that combines on-device training and server-based aggregation to train a common
ML model among distributed agents. In this work, we propose an asynchronous FL
design with periodic aggregation to tackle the straggler issue in FL systems.
Considering limited wireless communication resources, we investigate the effect
of different scheduling policies and aggregation designs on the convergence
performance. Driven by the importance of reducing the bias and variance of the
aggregated model updates, we propose a scheduling policy that jointly considers
the channel quality and training data representation of user devices. The
effectiveness of our channel-aware data-importance-based scheduling policy,
compared with state-of-the-art methods proposed for synchronous FL, is
validated through simulations. Moreover, we show that an ``age-aware''
aggregation weighting design can significantly improve the learning performance
in an asynchronous FL setting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Compute-Efficient Deep Learning: Algorithmic Trends and Opportunities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.06640v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.06640v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brian R. Bartoldson, Bhavya Kailkhura, Davis Blalock
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although deep learning has made great progress in recent years, the exploding
economic and environmental costs of training neural networks are becoming
unsustainable. To address this problem, there has been a great deal of research
on *algorithmically-efficient deep learning*, which seeks to reduce training
costs not at the hardware or implementation level, but through changes in the
semantics of the training program. In this paper, we present a structured and
comprehensive overview of the research in this field. First, we formalize the
*algorithmic speedup* problem, then we use fundamental building blocks of
algorithmically efficient training to develop a taxonomy. Our taxonomy
highlights commonalities of seemingly disparate methods and reveals current
research gaps. Next, we present evaluation best practices to enable
comprehensive, fair, and reliable comparisons of speedup techniques. To further
aid research and applications, we discuss common bottlenecks in the training
pipeline (illustrated via experiments) and offer taxonomic mitigation
strategies for them. Finally, we highlight some unsolved research challenges
and present promising future directions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>77 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PaGE-Link: Path-based Graph Neural Network Explanation for Heterogeneous
  Link Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.12465v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.12465v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shichang Zhang, Jiani Zhang, Xiang Song, Soji Adeshina, Da Zheng, Christos Faloutsos, Yizhou Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transparency and accountability have become major concerns for black-box
machine learning (ML) models. Proper explanations for the model behavior
increase model transparency and help researchers develop more accountable
models. Graph neural networks (GNN) have recently shown superior performance in
many graph ML problems than traditional methods, and explaining them has
attracted increased interest. However, GNN explanation for link prediction (LP)
is lacking in the literature. LP is an essential GNN task and corresponds to
web applications like recommendation and sponsored search on web. Given
existing GNN explanation methods only address node/graph-level tasks, we
propose Path-based GNN Explanation for heterogeneous Link prediction
(PaGE-Link) that generates explanations with connection interpretability,
enjoys model scalability, and handles graph heterogeneity. Qualitatively,
PaGE-Link can generate explanations as paths connecting a node pair, which
naturally captures connections between the two nodes and easily transfer to
human-interpretable explanations. Quantitatively, explanations generated by
PaGE-Link improve AUC for recommendation on citation and user-item graphs by 9
- 35% and are chosen as better by 78.79% of responses in human evaluation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stochastic regularized majorization-minimization with weakly convex and
  multi-convex surrogates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.01652v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.01652v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanbaek Lyu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stochastic majorization-minimization (SMM) is a class of stochastic
optimization algorithms that proceed by sampling new data points and minimizing
a recursive average of surrogate functions of an objective function. The
surrogates are required to be strongly convex and convergence rate analysis for
the general non-convex setting was not available. In this paper, we propose an
extension of SMM where surrogates are allowed to be only weakly convex or block
multi-convex, and the averaged surrogates are approximately minimized with
proximal regularization or block-minimized within diminishing radii,
respectively. For the general nonconvex constrained setting with non-i.i.d.
data samples, we show that the first-order optimality gap of the proposed
algorithm decays at the rate $O((\log n)^{1+\epsilon}/n^{1/2})$ for the
empirical loss and $O((\log n)^{1+\epsilon}/n^{1/4})$ for the expected loss,
where $n$ denotes the number of data samples processed. Under some additional
assumption, the latter convergence rate can be improved to $O((\log
n)^{1+\epsilon}/n^{1/2})$. As a corollary, we obtain the first convergence rate
bounds for various optimization methods under general nonconvex dependent data
setting: Double-averaging projected gradient descent and its generalizations,
proximal point empirical risk minimization, and online matrix/tensor
decomposition algorithms. We also provide experimental validation of our
results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>64 pages, 5 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fighting Money Laundering with Statistics and Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.04207v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.04207v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rasmus Jensen, Alexandros Iosifidis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Money laundering is a profound global problem. Nonetheless, there is little
scientific literature on statistical and machine learning methods for
anti-money laundering. In this paper, we focus on anti-money laundering in
banks and provide an introduction and review of the literature. We propose a
unifying terminology with two central elements: (i) client risk profiling and
(ii) suspicious behavior flagging. We find that client risk profiling is
characterized by diagnostics, i.e., efforts to find and explain risk factors.
On the other hand, suspicious behavior flagging is characterized by
non-disclosed features and hand-crafted risk indices. Finally, we discuss
directions for future research. One major challenge is the need for more public
data sets. This may potentially be addressed by synthetic data generation.
Other possible research directions include semi-supervised and deep learning,
interpretability, and fairness of the results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in IEEE Access, vol. 11, pp. 8889-8903,
  doi:10.1109/ACCESS.2023.3239549</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sim-to-Real Transfer for Quadrupedal Locomotion via Terrain <span class="highlight-title">Transformer</span> <span class="chip">ICRA2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07740v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07740v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hang Lai, Weinan Zhang, Xialin He, Chen Yu, Zheng Tian, Yong Yu, Jun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep reinforcement learning has recently emerged as an appealing alternative
for legged locomotion over multiple terrains by training a policy in physical
simulation and then transferring it to the real world (i.e., sim-to-real
transfer). Despite considerable progress, the capacity and scalability of
traditional neural networks are still limited, which may hinder their
applications in more complex environments. In contrast, the Transformer
architecture has shown its superiority in a wide range of large-scale sequence
modeling tasks, including natural language processing and decision-making
problems. In this paper, we propose Terrain Transformer (TERT), a high-capacity
Transformer model for quadrupedal locomotion control on various terrains.
Furthermore, to better leverage Transformer in sim-to-real scenarios, we
present a novel two-stage training framework consisting of an offline
pretraining stage and an online correction stage, which can naturally integrate
Transformer with privileged training. Extensive experiments in simulation
demonstrate that TERT outperforms state-of-the-art baselines on different
terrains in terms of return, energy consumption and control smoothness. In
further real-world validation, TERT successfully traverses nine challenging
terrains, including sand pit and stair down, which can not be accomplished by
strong baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICRA2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Merak: An Efficient Distributed DNN Training Framework with Automated 3D
  Parallelism for Giant Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.04959v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.04959v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiquan Lai, Shengwei Li, Xudong Tang, Keshi Ge, Weijie Liu, Yabo Duan, Linbo Qiao, Dongsheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models are becoming the dominant deep learning technologies.
Pretraining a foundation model is always time-consumed due to the large scale
of both the model parameter and training dataset. Besides being
computing-intensive, the training process is extremely memory-intensive and
communication-intensive. These features make it necessary to apply 3D
parallelism, which integrates data parallelism, pipeline model parallelism and
tensor model parallelism, to achieve high training efficiency.
  To achieve this goal, some custom software frameworks such as Megatron-LM and
DeepSpeed are developed. However, current 3D parallelism frameworks still meet
two issues: i) they are not transparent to model developers, which need to
manually modify the model to parallelize training. ii) their utilization of
computation, GPU memory and network bandwidth are not sufficient. We propose
Merak, an automated 3D parallelism deep learning training framework with high
resource utilization. Merak automatically deploys with an automatic model
partitioner, which uses a graph sharding algorithm on a proxy representation of
the model. Merak also presents the non-intrusive API for scaling out foundation
model training with minimal code modification. In addition, we design a
high-performance 3D parallel runtime engine in Merak. It uses several
techniques to exploit available training resources, including shifted critical
path pipeline schedule that brings a higher computation utilization,
stage-aware recomputation that makes use of idle worker memory, and
sub-pipelined tensor model parallelism that overlaps communication and
computation. Experiments on 64 GPUs show Merak can speedup the training
performance over the state-of-the-art 3D parallelism frameworks of models with
1.5, 2.5, 8.3, and 20 billion parameters by up to 1.42X, 1.39X, 1.43X, and
1.61X, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Chat<span class="highlight-title">GPT</span> Is on the Horizon: Could a Large Language Model Be All We Need
  for Intelligent Transportation? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05382v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05382v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ou Zheng, Mohamed Abdel-Aty, Dongdong Wang, Zijin Wang, Shengxuan Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ChatGPT, developed by OpenAI, is one of the milestone large language models
(LLMs) with 6 billion parameters. ChatGPT has demonstrated the impressive
language understanding capability of LLM, particularly in generating
conversational response. As LLMs start to gain more attention in various
research or engineering domains, it is time to envision how LLM may
revolutionize the way we approach intelligent transportation systems. This
paper explores the future applications of LLM in addressing key transportation
problems. By leveraging LLM with cross-modal encoder, an intelligent system can
also process traffic data from different modalities and execute transportation
operations through an LLM. We present and validate these potential
transportation applications equipped by LLM. To further demonstrate this
potential, we also provide a concrete smartphone-based crash report
auto-generation and analysis framework as a use case. Despite the potential
benefits, challenges related to data privacy, data quality, and model bias must
be considered. Overall, the use of LLM in intelligent transport systems holds
promise for more efficient, intelligent, and sustainable transportation systems
that further improve daily life around the world.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to Nature - Machine Intelligence (13 Pages, 8 Figures)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PhyGNNet: Solving spatiotemporal PDEs with Physics-informed Graph Neural
  Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.04319v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.04319v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Longxiang Jiang, Liyuan Wang, Xinkun Chu, Yonghao Xiao, Hao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Solving partial differential equations (PDEs) is an important research means
in the fields of physics, biology, and chemistry. As an approximate alternative
to numerical methods, PINN has received extensive attention and played an
important role in many fields. However, PINN uses a fully connected network as
its model, which has limited fitting ability and limited extrapolation ability
in both time and space. In this paper, we propose PhyGNNet for solving partial
differential equations on the basics of a graph neural network which consists
of encoder, processer, and decoder blocks. In particular, we divide the
computing area into regular grids, define partial differential operators on the
grids, then construct pde loss for the network to optimize to build PhyGNNet
model. What's more, we conduct comparative experiments on Burgers equation and
heat equation to validate our approach, the results show that our method has
better fit ability and extrapolation ability both in time and spatial areas
compared with PINN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>there some errors in method describtion</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ De<span class="highlight-title">BERT</span>aV3: Improving De<span class="highlight-title">BERT</span>a using ELECTRA-Style <span class="highlight-title">Pre-Train</span>ing with
  Gradient-Disentangled Embedding Sharing <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.09543v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.09543v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengcheng He, Jianfeng Gao, Weizhu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a new pre-trained language model, DeBERTaV3, which
improves the original DeBERTa model by replacing mask language modeling (MLM)
with replaced token detection (RTD), a more sample-efficient pre-training task.
Our analysis shows that vanilla embedding sharing in ELECTRA hurts training
efficiency and model performance. This is because the training losses of the
discriminator and the generator pull token embeddings in different directions,
creating the "tug-of-war" dynamics. We thus propose a new gradient-disentangled
embedding sharing method that avoids the tug-of-war dynamics, improving both
training efficiency and the quality of the pre-trained model. We have
pre-trained DeBERTaV3 using the same settings as DeBERTa to demonstrate its
exceptional performance on a wide range of downstream natural language
understanding (NLU) tasks. Taking the GLUE benchmark with eight tasks as an
example, the DeBERTaV3 Large model achieves a 91.37% average score, which is
1.37% over DeBERTa and 1.91% over ELECTRA, setting a new state-of-the-art
(SOTA) among the models with a similar structure. Furthermore, we have
pre-trained a multi-lingual model mDeBERTa and observed a larger improvement
over strong baselines compared to English models. For example, the mDeBERTa
Base achieves a 79.8% zero-shot cross-lingual accuracy on XNLI and a 3.6%
improvement over XLM-R Base, creating a new SOTA on this benchmark. We have
made our pre-trained models and inference code publicly available at
https://github.com/microsoft/DeBERTa.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 10 tables, 2 Figures. The DeBERTaV3 model significantly
  improves performance of the downstream NLU tasks over models with a similar
  structure, e.g. DeBERTaV3 large achieves 91.37% average GLUE score which is
  1.37% over DeBERTa large. XSmall has only 22M backbone parameters, but
  significantly outperforms RoBERTa/XLNet-base. Paper is published as a
  conference paper at ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Risk-Sensitive Reinforcement Learning with Exponential Criteria 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.09010v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.09010v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erfaun Noorani, Christos Mavridis, John Baras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While risk-neutral reinforcement learning has shown experimental success in a
number of applications, it is well-known to be non-robust with respect to noise
and perturbations in the parameters of the system. For this reason,
risk-sensitive reinforcement learning algorithms have been studied to introduce
robustness and sample efficiency, and lead to better real-life performance. In
this work, we introduce new model-free risk-sensitive reinforcement learning
algorithms as variations of widely-used Policy Gradient algorithms with similar
implementation properties. In particular, we study the effect of exponential
criteria on the risk-sensitivity of the policy of a reinforcement learning
agent, and develop variants of the Monte Carlo Policy Gradient algorithm and
the online (temporal-difference) Actor-Critic algorithm. Analytical results
showcase that the use of exponential criteria generalize commonly used ad-hoc
regularization approaches. The implementation, performance, and robustness
properties of the proposed methods are evaluated in simulated experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Joint Differentiable Optimization and Verification for Certified
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.12243v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.12243v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixuan Wang, Simon Zhan, Zhilu Wang, Chao Huang, Zhaoran Wang, Zhuoran Yang, Qi Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In model-based reinforcement learning for safety-critical control systems, it
is important to formally certify system properties (e.g., safety, stability)
under the learned controller. However, as existing methods typically apply
formal verification \emph{after} the controller has been learned, it is
sometimes difficult to obtain any certificate, even after many iterations
between learning and verification. To address this challenge, we propose a
framework that jointly conducts reinforcement learning and formal verification
by formulating and solving a novel bilevel optimization problem, which is
differentiable by the gradients from the value function and certificates.
Experiments on a variety of examples demonstrate the significant advantages of
our framework over the model-based stochastic value gradient (SVG) method and
the model-free proximal policy optimization (PPO) method in finding feasible
controllers with barrier functions and Lyapunov functions that ensure system
safety and stability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted to International Conference on Cyber-Physical
  Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rotating without Seeing: Towards In-hand Dexterity through Touch 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10880v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10880v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhao-Heng Yin, Binghao Huang, Yuzhe Qin, Qifeng Chen, Xiaolong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tactile information plays a critical role in human dexterity. It reveals
useful contact information that may not be inferred directly from vision. In
fact, humans can even perform in-hand dexterous manipulation without using
vision. Can we enable the same ability for the multi-finger robot hand? In this
paper, we present Touch Dexterity, a new system that can perform in-hand object
rotation using only touching without seeing the object. Instead of relying on
precise tactile sensing in a small region, we introduce a new system design
using dense binary force sensors (touch or no touch) overlaying one side of the
whole robot hand (palm, finger links, fingertips). Such a design is low-cost,
giving a larger coverage of the object, and minimizing the Sim2Real gap at the
same time. We train an in-hand rotation policy using Reinforcement Learning on
diverse objects in simulation. Relying on touch-only sensing, we can directly
deploy the policy in a real robot hand and rotate novel objects that are not
presented in training. Extensive ablations are performed on how tactile
information help in-hand manipulation.Our project is available at
https://touchdexterity.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://touchdexterity.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Resolution Online Deterministic Annealing: A Hierarchical and
  Progressive Learning Architecture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08189v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08189v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christos Mavridis, John Baras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hierarchical learning algorithms that gradually approximate a solution to a
data-driven optimization problem are essential to decision-making systems,
especially under limitations on time and computational resources. In this
study, we introduce a general-purpose hierarchical learning architecture that
is based on the progressive partitioning of a possibly multi-resolution data
space. The optimal partition is gradually approximated by solving a sequence of
optimization sub-problems that yield a sequence of partitions with increasing
number of subsets. We show that the solution of each optimization problem can
be estimated online using gradient-free stochastic approximation updates. As a
consequence, a function approximation problem can be defined within each subset
of the partition and solved using the theory of two-timescale stochastic
approximation algorithms. This simulates an annealing process and defines a
robust and interpretable heuristic method to gradually increase the complexity
of the learning architecture in a task-agnostic manner, giving emphasis to
regions of the data space that are considered more important according to a
predefined criterion. Finally, by imposing a tree structure in the progression
of the partitions, we provide a means to incorporate potential multi-resolution
structure of the data space into this approach, significantly reducing its
complexity, while introducing hierarchical variable-rate feature extraction
properties similar to certain classes of deep learning architectures.
Asymptotic convergence analysis and experimental results are provided for
supervised and unsupervised learning problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Computational Choreography using Human Motion Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.04366v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.04366v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patrick Perrine, Trevor Kirkby
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Should deep learning models be trained to analyze human performance art? To
help answer this question, we explore an application of deep neural networks to
synthesize artistic human motion. Problem tasks in human motion synthesis can
include predicting the motions of humans in-the-wild, as well as generating new
sequences of motions based on said predictions. We will discuss the potential
of a less traditional application, where learning models are applied to
predicting dance movements. There have been notable, recent efforts to analyze
dance movements in a computational light, such as the Everybody Dance Now (EDN)
learning model and a Cal Poly master's thesis, Take The Lead (TTL). We have
effectively combined these two works along with our own deep neural network to
produce a new system for dance motion prediction, image-to-image translation,
and video generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 7 figures, to be submitted to AIVR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast exploration and learning of latent graphs with aliased observations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07397v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07397v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miguel Lazaro-Gredilla, Ishan Deshpande, Sivaramakrishnan Swaminathan, Meet Dave, Dileep George
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Consider this scenario: an agent navigates a latent graph by performing
actions that take it from one node to another. The chosen action determines the
probability distribution over the next visited node. At each node, the agent
receives an observation, but this observation is not unique, so it does not
identify the node, making the problem aliased. The purpose of this work is to
provide a policy that approximately maximizes exploration efficiency (i.e., how
well the graph is recovered for a given exploration budget). In the unaliased
case, we show improved performance w.r.t. state-of-the-art reinforcement
learning baselines. For the aliased case we are not aware of suitable baselines
and instead show faster recovery w.r.t. a random policy for a wide variety of
topologies, and exponentially faster recovery than a random policy for
challenging topologies. We dub the algorithm eFeX (from eFficient eXploration).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v2: Added extra figure and fixed typos</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Policy Mirror Descent Inherently Explores Action Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04386v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04386v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Li, Guanghui Lan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Explicit exploration in the action space was assumed to be indispensable for
online policy gradient methods to avoid a drastic degradation in sample
complexity, for solving general reinforcement learning problems over finite
state and action spaces. In this paper, we establish for the first time an
$\tilde{\mathcal{O}}(1/\epsilon^2)$ sample complexity for online policy
gradient methods without incorporating any exploration strategies. The
essential development consists of two new on-policy evaluation operators and a
novel analysis of the stochastic policy mirror descent method (SPMD). SPMD with
the first evaluation operator, called value-based estimation, tailors to the
Kullback-Leibler divergence. Provided the Markov chains on the state space of
generated policies are uniformly mixing with non-diminishing minimal visitation
measure, an $\tilde{\mathcal{O}}(1/\epsilon^2)$ sample complexity is obtained
with a linear dependence on the size of the action space. SPMD with the second
evaluation operator, namely truncated on-policy Monte Carlo (TOMC), attains an
$\tilde{\mathcal{O}}(\mathcal{H}_{\mathcal{D}}/\epsilon^2)$ sample complexity,
where $\mathcal{H}_{\mathcal{D}}$ mildly depends on the effective horizon and
the size of the action space with properly chosen Bregman divergence (e.g.,
Tsallis divergence). SPMD with TOMC also exhibits stronger convergence
properties in that it controls the optimality gap with high probability rather
than in expectation. In contrast to explicit exploration, these new policy
gradient methods can prevent repeatedly committing to potentially high-risk
actions when searching for optimal policies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Maximum margin learning of t-SPNs for cell classification with filtered
  input 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.09065v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.09065v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haeyong Kang, Chang D. Yoo, Yongcheon Na
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An algorithm based on a deep probabilistic architecture referred to as a
tree-structured sum-product network (t-SPN) is considered for cell
classification. The t-SPN is constructed such that the unnormalized probability
is represented as conditional probabilities of a subset of most similar cell
classes. The constructed t-SPN architecture is learned by maximizing the
margin, which is the difference in the conditional probability between the true
and the most competitive false label. To enhance the generalization ability of
the architecture, L2-regularization (REG) is considered along with the maximum
margin (MM) criterion in the learning process. To highlight cell features, this
paper investigates the effectiveness of two generic high-pass filters: ideal
high-pass filtering and the Laplacian of Gaussian (LOG) filtering. On both
HEp-2 and Feulgen benchmark datasets, the t-SPN architecture learned based on
the max-margin criterion with regularization produced the highest accuracy rate
compared to other state-of-the-art algorithms that include convolutional neural
network (CNN) based algorithms. The ideal high-pass filter was more effective
on the HEp-2 dataset, which is based on immunofluorescence staining, while the
LOG was more effective on the Feulgen dataset, which is based on Feulgen
staining.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Semi-supervised Semantics-guided Adversarial Training for Trajectory
  Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.14230v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.14230v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruochen Jiao, Xiangguo Liu, Takami Sato, Qi Alfred Chen, Qi Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predicting the trajectories of surrounding objects is a critical task for
self-driving vehicles and many other autonomous systems. Recent works
demonstrate that adversarial attacks on trajectory prediction, where small
crafted perturbations are introduced to history trajectories, may significantly
mislead the prediction of future trajectories and induce unsafe planning.
However, few works have addressed enhancing the robustness of this important
safety-critical task.In this paper, we present a novel adversarial training
method for trajectory prediction. Compared with typical adversarial training on
image tasks, our work is challenged by more random input with rich context and
a lack of class labels. To address these challenges, we propose a method based
on a semi-supervised adversarial autoencoder, which models disentangled
semantic features with domain knowledge and provides additional latent labels
for the adversarial training. Extensive experiments with different types of
attacks demonstrate that our Semisupervised Semantics-guided Adversarial
Training (SSAT) method can effectively mitigate the impact of adversarial
attacks by up to 73% and outperform other popular defense methods. In addition,
experiments show that our method can significantly improve the system's robust
generalization to unseen patterns of attacks. We believe that such
semantics-guided architecture and advancement on robust generalization is an
important step for developing robust prediction models and enabling safe
decision-making.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, adversarial training for trajectory prediction</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Statistical Analysis of Karcher Means for Random Restricted PSD Matrices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.12426v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.12426v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hengchao Chen, Xiang Li, Qiang Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Non-asymptotic statistical analysis is often missing for modern
geometry-aware machine learning algorithms due to the possibly intricate
non-linear manifold structure. This paper studies an intrinsic mean model on
the manifold of restricted positive semi-definite matrices and provides a
non-asymptotic statistical analysis of the Karcher mean. We also consider a
general extrinsic signal-plus-noise model, under which a deterministic error
bound of the Karcher mean is provided. As an application, we show that the
distributed principal component analysis algorithm, LRC-dPCA, achieves the same
performance as the full sample PCA algorithm. Numerical experiments lend strong
support to our theories.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contextual Linear Bandits under Noisy Features: Towards Bayesian Oracles <span class="chip">AISTATS2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1703.01347v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1703.01347v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jung-hun Kim, Se-Young Yun, Minchan Jeong, Jun Hyun Nam, Jinwoo Shin, Richard Combes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study contextual linear bandit problems under feature uncertainty; they
are noisy with missing entries. To address the challenges of the noise, we
analyze Bayesian oracles given observed noisy features. Our Bayesian analysis
finds that the optimal hypothesis can be far from the underlying realizability
function, depending on the noise characteristics, which are highly
non-intuitive and do not occur for classical noiseless setups. This implies
that classical approaches cannot guarantee a non-trivial regret bound.
Therefore, we propose an algorithm that aims at the Bayesian oracle from
observed information under this model, achieving $\tilde{O}(d\sqrt{T})$ regret
bound when there is a large number of arms. We demonstrate the proposed
algorithm using synthetic and real-world datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages; accepted at AISTATS2023; minor corrections to Bayesian
  features</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Environmental Sensor Placement with Convolutional Gaussian Neural
  Processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.10381v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.10381v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tom R. Andersson, Wessel P. Bruinsma, Stratis Markou, James Requeima, Alejandro Coca-Castro, Anna Vaughan, Anna-Louise Ellis, Matthew Lazzara, Daniel C. Jones, J. Scott Hosking, Richard E. Turner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Environmental sensors are crucial for monitoring weather conditions and the
impacts of climate change. However, it is challenging to maximise measurement
informativeness and place sensors efficiently, particularly in remote regions
like Antarctica. Probabilistic machine learning models can evaluate placement
informativeness by predicting the uncertainty reduction provided by a new
sensor. Gaussian process (GP) models are widely used for this purpose, but they
struggle with capturing complex non-stationary behaviour and scaling to large
datasets. This paper proposes using a convolutional Gaussian neural process
(ConvGNP) to address these issues. A ConvGNP uses neural networks to
parameterise a joint Gaussian distribution at arbitrary target locations,
enabling flexibility and scalability. Using simulated surface air temperature
anomaly over Antarctica as ground truth, the ConvGNP learns spatial and
seasonal non-stationarities, outperforming a non-stationary GP baseline. In a
simulated sensor placement experiment, the ConvGNP better predicts the
performance boost obtained from new observations than GP baselines, leading to
more informative sensor placements. We connect our work with similar machine
learning and physics-based approaches and discuss steps towards an operational
sensor placement recommendation system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In review for the Climate Informatics 2023 special issue of
  Environmental Data Science</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Long-tailed Classification from a Bayesian-decision-theory Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06075v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06075v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bolian Li, Ruqi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long-tailed classification poses a challenge due to its heavy imbalance in
class probabilities and tail-sensitivity risks with asymmetric misprediction
costs. Recent attempts have used re-balancing loss and ensemble methods, but
they are largely heuristic and depend heavily on empirical results, lacking
theoretical explanation. Furthermore, existing methods overlook the decision
loss, which characterizes different costs associated with tailed classes. This
paper presents a general and principled framework from a
Bayesian-decision-theory perspective, which unifies existing techniques
including re-balancing and ensemble methods, and provides theoretical
justifications for their effectiveness. From this perspective, we derive a
novel objective based on the integrated risk and a Bayesian deep-ensemble
approach to improve the accuracy of all classes, especially the "tail".
Besides, our framework allows for task-adaptive decision loss which provides
provably optimal decisions in varying task scenarios, along with the capability
to quantify uncertainty. Finally, We conduct comprehensive experiments,
including standard classification, tail-sensitive classification with a new
False Head Rate metric, calibration, and ablation studies. Our framework
significantly improves the current SOTA even on large-scale real-world datasets
like ImageNet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Valid Inference after Causal Discovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.05949v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.05949v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paula Gradu, Tijana Zrnic, Yixin Wang, Michael I. Jordan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Causal discovery and causal effect estimation are two fundamental tasks in
causal inference. While many methods have been developed for each task
individually, statistical challenges arise when applying these methods jointly:
estimating causal effects after running causal discovery algorithms on the same
data leads to "double dipping," invalidating the coverage guarantees of
classical confidence intervals. To this end, we develop tools for valid
post-causal-discovery inference. Across empirical studies, we show that a naive
combination of causal discovery and subsequent inference algorithms leads to
highly inflated miscoverage rates; on the other hand, applying our method
provides reliable coverage while achieving more accurate causal discovery than
data splitting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Context-Aware Timewise VAEs for Real-Time Vehicle Trajectory Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.10873v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.10873v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pei Xu, Jean-Bernard Hayet, Ioannis Karamouzas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-time, accurate prediction of human steering behaviors has wide
applications, from developing intelligent traffic systems to deploying
autonomous driving systems in both real and simulated worlds. In this paper, we
present ContextVAE, a context-aware approach for multi-modal vehicle trajectory
prediction. Built upon the backbone architecture of a timewise variational
autoencoder, ContextVAE employs a dual attention mechanism for observation
encoding that accounts for the environmental context information and the
dynamic agents' states in a unified way. By utilizing features extracted from
semantic maps during agent state encoding, our approach takes into account both
the social features exhibited by agents on the scene and the physical
environment constraints to generate map-compliant and socially-aware
trajectories. We perform extensive testing on the nuScenes prediction
challenge, Lyft Level 5 dataset and Waymo Open Motion Dataset to show the
effectiveness of our approach and its state-of-the-art performance. In all
tested datasets, ContextVAE models are fast to train and provide high-quality
multi-modal predictions in real-time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Task-Oriented Communications for NextG: End-to-End Deep Learning and AI
  Security Aspects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.09668v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.09668v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yalin E. Sagduyu, Sennur Ulukus, Aylin Yener
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Communications systems to date are primarily designed with the goal of
reliable transfer of digital sequences (bits). Next generation (NextG)
communication systems are beginning to explore shifting this design paradigm to
reliably executing a given task such as in task-oriented communications. In
this paper, wireless signal classification is considered as the task for the
NextG Radio Access Network (RAN), where edge devices collect wireless signals
for spectrum awareness and communicate with the NextG base station (gNodeB)
that needs to identify the signal label. Edge devices may not have sufficient
processing power and may not be trusted to perform the signal classification
task, whereas the transfer of signals to the gNodeB may not be feasible due to
stringent delay, rate, and energy restrictions. Task-oriented communications is
considered by jointly training the transmitter, receiver and classifier
functionalities as an encoder-decoder pair for the edge device and the gNodeB.
This approach improves the accuracy compared to the separated case of signal
transfer followed by classification. Adversarial machine learning poses a major
security threat to the use of deep learning for task-oriented communications. A
major performance loss is shown when backdoor (Trojan) and adversarial
(evasion) attacks target the training and test processes of task-oriented
communications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding the Diffusion Objective as a Weighted Integral of ELBOs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.00848v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.00848v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Diederik P. Kingma, Ruiqi Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models in the literature are optimized with various objectives that
are special cases of a weighted loss, where the weighting function specifies
the weight per noise level. Uniform weighting corresponds to maximizing the
ELBO, a principled approximation of maximum likelihood. In current practice
diffusion models are optimized with non-uniform weighting due to better results
in terms of sample quality. In this work we expose a direct relationship
between the weighted loss (with any weighting) and the ELBO objective.
  We show that the weighted loss can be written as a weighted integral of
ELBOs, with one ELBO per noise level. If the weighting function is monotonic,
then the weighted loss is a likelihood-based objective: it maximizes the ELBO
under simple data augmentation, namely Gaussian noise perturbation. Our main
contribution is a deeper theoretical understanding of the diffusion objective,
but we also performed some experiments comparing monotonic with non-monotonic
weightings, finding that monotonic weighting performs competitively with the
best published results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Al<span class="highlight-title">bert</span>a Plan for AI Research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.11173v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.11173v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Richard S. Sutton, Michael Bowling, Patrick M. Pilarski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Herein we describe our approach to artificial intelligence research, which we
call the Alberta Plan. The Alberta Plan is pursued within our research groups
in Alberta and by others who are like minded throughout the world. We welcome
all who would join us in this pursuit.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unconstrained Dynamic Regret via Sparse Coding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.13349v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.13349v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyu Zhang, Ashok Cutkosky, Ioannis Ch. Paschalidis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motivated by time series forecasting, we study Online Linear Optimization
(OLO) under the coupling of two problem structures: the domain is unbounded,
and the performance of an algorithm is measured by its dynamic regret. Handling
either of them requires the regret bound to depend on certain complexity
measure of the comparator sequence -- specifically, the comparator norm in
unconstrained OLO, and the path length in dynamic regret. In contrast to a
recent work (Jacobsen & Cutkosky, 2022) that adapts to the combination of these
two complexity measures, we propose an alternative complexity measure by
recasting the problem into sparse coding. Adaptivity can be achieved by a
simple modular framework, which naturally exploits more intricate prior
knowledge of the environment. Along the way, we also present a new gradient
adaptive algorithm for static unconstrained OLO, designed using novel
continuous time machinery. This could be of independent interest.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Added experiments</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MobileNeRF: Exploiting the Polygon Rasterization Pipeline for Efficient
  Neural Field Rendering on Mobile Architectures <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.00277v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.00277v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiqin Chen, Thomas Funkhouser, Peter Hedman, Andrea Tagliasacchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Radiance Fields (NeRFs) have demonstrated amazing ability to
synthesize images of 3D scenes from novel views. However, they rely upon
specialized volumetric rendering algorithms based on ray marching that are
mismatched to the capabilities of widely deployed graphics hardware. This paper
introduces a new NeRF representation based on textured polygons that can
synthesize novel images efficiently with standard rendering pipelines. The NeRF
is represented as a set of polygons with textures representing binary opacities
and feature vectors. Traditional rendering of the polygons with a z-buffer
yields an image with features at every pixel, which are interpreted by a small,
view-dependent MLP running in a fragment shader to produce a final pixel color.
This approach enables NeRFs to be rendered with the traditional polygon
rasterization pipeline, which provides massive pixel-level parallelism,
achieving interactive frame rates on a wide range of compute platforms,
including mobile phones.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023. Project page: https://mobile-nerf.github.io, code:
  https://github.com/google-research/jax3d/tree/main/jax3d/projects/mobilenerf</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AIREPAIR: A Repair Platform for Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.15387v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.15387v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xidan Song, Youcheng Sun, Mustafa A. Mustafa, Lucas Cordeiro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present AIREPAIR, a platform for repairing neural networks. It features
the integration of existing network repair tools. Based on AIREPAIR, one can
run different repair methods on the same model, thus enabling the fair
comparison of different repair techniques. We evaluate AIREPAIR with three
state-of-the-art repair tools on popular deep-learning datasets and models. Our
evaluation confirms the utility of AIREPAIR, by comparing and analyzing the
results from different repair techniques. A demonstration is available at
https://youtu.be/UkKw5neeWhw.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Auto-Encoder Neural Network Incorporating X-Ray Fluorescence Fundamental
  Parameters with Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.12239v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.12239v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew Dirks, David Poole
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider energy-dispersive X-ray Fluorescence (EDXRF) applications where
the fundamental parameters method is impractical such as when instrument
parameters are unavailable. For example, on a mining shovel or conveyor belt,
rocks are constantly moving (leading to varying angles of incidence and
distances) and there may be other factors not accounted for (like dust). Neural
networks do not require instrument and fundamental parameters but training
neural networks requires XRF spectra labelled with elemental composition, which
is often limited because of its expense. We develop a neural network model that
learns from limited labelled data and also benefits from domain knowledge by
learning to invert a forward model. The forward model uses transition energies
and probabilities of all elements and parameterized distributions to
approximate other fundamental and instrument parameters. We evaluate the model
and baseline models on a rock dataset from a lithium mineral exploration
project. Our model works particularly well for some low-Z elements (Li, Mg, Al,
and K) as well as some high-Z elements (Sn and Pb) despite these elements being
outside the suitable range for common spectrometers to directly measure, likely
owing to the ability of neural networks to learn correlations and non-linear
relationships.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>X-Ray Spectrometry 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Penalty-based Bilevel Gradient Descent Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.05185v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.05185v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Shen, Quan Xiao, Tianyi Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bilevel optimization enjoys a wide range of applications in hyper-parameter
optimization, meta-learning and reinforcement learning. However, bilevel
optimization problems are difficult to solve. Recent progress on scalable
bilevel algorithms mainly focuses on bilevel optimization problems where the
lower-level objective is either strongly convex or unconstrained. In this work,
we tackle the bilevel problem through the lens of the penalty method. We show
that under certain conditions, the penalty reformulation recovers the solutions
of the original bilevel problem. Further, we propose the penalty-based bilevel
gradient descent (PBGD) algorithm and establish its finite-time convergence for
the constrained bilevel problem without lower-level strong convexity.
Experiments showcase the efficiency of the proposed PBGD algorithm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Improved Section 4 by removing a critical assumption; Added citations</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MFBE: Leveraging Multi-Field Information of FAQs for Efficient Dense
  Retrieval <span class="chip">PAKDD</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.11953v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.11953v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Debopriyo Banerjee, Mausam Jain, Ashish Kulkarni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the domain of question-answering in NLP, the retrieval of Frequently Asked
Questions (FAQ) is an important sub-area which is well researched and has been
worked upon for many languages. Here, in response to a user query, a retrieval
system typically returns the relevant FAQs from a knowledge-base. The efficacy
of such a system depends on its ability to establish semantic match between the
query and the FAQs in real-time. The task becomes challenging due to the
inherent lexical gap between queries and FAQs, lack of sufficient context in
FAQ titles, scarcity of labeled data and high retrieval latency. In this work,
we propose a bi-encoder-based query-FAQ matching model that leverages multiple
combinations of FAQ fields (like, question, answer, and category) both during
model training and inference. Our proposed Multi-Field Bi-Encoder (MFBE) model
benefits from the additional context resulting from multiple FAQ fields and
performs well even with minimal labeled data. We empirically support this claim
through experiments on proprietary as well as open-source public datasets in
both unsupervised and supervised settings. Our model achieves around 27% and
20% better top-1 accuracy for the FAQ retrieval task on internal and open
datasets, respectively over the best performing baseline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors contributed equally to this work. 12 pages, 3
  figures, 5 tables. Accepted at the 2023 Pacific-Asia Conference On Knowledge
  Discovery And Data Mining (PAKDD)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Safe Exploration Incurs Nearly No Additional Sample Complexity for
  Reward-free RL <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.14057v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.14057v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruiquan Huang, Jing Yang, Yingbin Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reward-free reinforcement learning (RF-RL), a recently introduced RL
paradigm, relies on random action-taking to explore the unknown environment
without any reward feedback information. While the primary goal of the
exploration phase in RF-RL is to reduce the uncertainty in the estimated model
with minimum number of trajectories, in practice, the agent often needs to
abide by certain safety constraint at the same time. It remains unclear how
such safe exploration requirement would affect the corresponding sample
complexity in order to achieve the desired optimality of the obtained policy in
planning. In this work, we make a first attempt to answer this question. In
particular, we consider the scenario where a safe baseline policy is known
beforehand, and propose a unified Safe reWard-frEe ExploraTion (SWEET)
framework. We then particularize the SWEET framework to the tabular and the
low-rank MDP settings, and develop algorithms coined Tabular-SWEET and
Low-rank-SWEET, respectively. Both algorithms leverage the concavity and
continuity of the newly introduced truncated value functions, and are
guaranteed to achieve zero constraint violation during exploration with high
probability. Furthermore, both algorithms can provably find a near-optimal
policy subject to any constraint in the planning phase. Remarkably, the sample
complexities under both algorithms match or even outperform the state of the
art in their constraint-free counterparts up to some constant factors, proving
that safety constraint hardly increases the sample complexity for RF-RL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Near-optimal inference in adaptive linear regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2107.02266v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2107.02266v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Koulik Khamaru, Yash Deshpande, Tor Lattimore, Lester Mackey, Martin J. Wainwright
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When data is collected in an adaptive manner, even simple methods like
ordinary least squares can exhibit non-normal asymptotic behavior. As an
undesirable consequence, hypothesis tests and confidence intervals based on
asymptotic normality can lead to erroneous results. We propose a family of
online debiasing estimators to correct these distributional anomalies in least
squares estimation. Our proposed methods take advantage of the covariance
structure present in the dataset and provide sharper estimates in directions
for which more information has accrued. We establish an asymptotic normality
property for our proposed online debiasing estimators under mild conditions on
the data collection process and provide asymptotically exact confidence
intervals. We additionally prove a minimax lower bound for the adaptive linear
regression problem, thereby providing a baseline by which to compare
estimators. There are various conditions under which our proposed estimators
achieve the minimax lower bound. We demonstrate the usefulness of our theory
via applications to multi-armed bandit, autoregressive time series estimation,
and active learning with exploration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>51 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">6</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal <span class="highlight-title">Pre-train</span>ing Framework for Sequential Recommendation via
  Contrastive Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11879v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11879v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingzi Zhang, Xin Zhou, Zhiqi Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential recommendation systems utilize the sequential interactions of
users with items as their main supervision signals in learning users'
preferences. However, existing methods usually generate unsatisfactory results
due to the sparsity of user behavior data. To address this issue, we propose a
novel pre-training framework, named Multimodal Sequence Mixup for Sequential
Recommendation (MSM4SR), which leverages both users' sequential behaviors and
items' multimodal content (\ie text and images) for effectively recommendation.
Specifically, MSM4SR tokenizes each item image into multiple textual keywords
and uses the pre-trained BERT model to obtain initial textual and visual
features of items, for eliminating the discrepancy between the text and image
modalities. A novel backbone network, \ie Multimodal Mixup Sequence Encoder
(M$^2$SE), is proposed to bridge the gap between the item multimodal content
and the user behavior, using a complementary sequence mixup strategy. In
addition, two contrastive learning tasks are developed to assist M$^2$SE in
learning generalized multimodal representations of the user behavior sequence.
Extensive experiments on real-world datasets demonstrate that MSM4SR
outperforms state-of-the-art recommendation methods. Moreover, we further
verify the effectiveness of MSM4SR on other challenging tasks including
cold-start and cross-domain recommendation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Complete <span class="highlight-title">Survey</span> on Generative AI (AIGC): Is Chat<span class="highlight-title">GPT</span> from <span class="highlight-title">GPT</span>-4 to
  <span class="highlight-title">GPT</span>-5 All You Need? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11717v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11717v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaoning Zhang, Chenshuang Zhang, Sheng Zheng, Yu Qiao, Chenghao Li, Mengchun Zhang, Sumit Kumar Dam, Chu Myaet Thwal, Ye Lin Tun, Le Luang Huy, Donguk kim, Sung-Ho Bae, Lik-Hang Lee, Yang Yang, Heng Tao Shen, In So Kweon, Choong Seon Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As ChatGPT goes viral, generative AI (AIGC, a.k.a AI-generated content) has
made headlines everywhere because of its ability to analyze and create text,
images, and beyond. With such overwhelming media coverage, it is almost
impossible for us to miss the opportunity to glimpse AIGC from a certain angle.
In the era of AI transitioning from pure analysis to creation, it is worth
noting that ChatGPT, with its most recent language model GPT-4, is just a tool
out of numerous AIGC tasks. Impressed by the capability of the ChatGPT, many
people are wondering about its limits: can GPT-5 (or other future GPT variants)
help ChatGPT unify all AIGC tasks for diversified content creation? Toward
answering this question, a comprehensive review of existing AIGC tasks is
needed. As such, our work comes to fill this gap promptly by offering a first
look at AIGC, ranging from its techniques to applications. Modern generative AI
relies on various technical foundations, ranging from model architecture and
self-supervised pretraining to generative modeling methods (like GAN and
diffusion models). After introducing the fundamental techniques, this work
focuses on the technological development of various AIGC tasks based on their
output type, including text, images, videos, 3D content, etc., which depicts
the full potential of ChatGPT's future. Moreover, we summarize their
significant applications in some mainstream industries, such as education and
creativity content. Finally, we discuss the challenges currently faced and
present an outlook on how generative AI might evolve in the near future.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>56 pages, 548 citations</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Low-complexity Deep Video Compression with A Distributed Coding
  Architecture <span class="chip">ICME 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11599v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11599v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinjie Zhang, Jiawei Shao, Jun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prevalent predictive coding-based video compression methods rely on a heavy
encoder to reduce the temporal redundancy, which makes it challenging to deploy
them on resource-constrained devices. Meanwhile, as early as the 1970s,
distributed source coding theory has indicated that independent encoding and
joint decoding with side information (SI) can achieve high-efficient
compression of correlated sources. This has inspired a distributed coding
architecture aiming at reducing the encoding complexity. However, traditional
distributed coding methods suffer from a substantial performance gap to
predictive coding ones. Inspired by the great success of learning-based
compression, we propose the first end-to-end distributed deep video compression
framework to improve the rate-distortion performance. A key ingredient is an
effective SI generation module at the decoder, which helps to effectively
exploit inter-frame correlations without computation-intensive encoder-side
motion estimation and compensation. Experiments show that our method
significantly outperforms conventional distributed video coding and H.264.
Meanwhile, it enjoys 6-7x encoding speedup against DVC [1] with comparable
compression performance. Code is released at
https://github.com/Xinjie-Q/Distributed-DVC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICME 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SVCNet: Scribble-based Video Colorization Network with Temporal
  Aggregation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11591v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11591v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuzhi Zhao, Lai-Man Po, Kangcheng Liu, Xuehui Wang, Wing-Yin Yu, Pengfei Xian, Yujia Zhang, Mengyang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a scribble-based video colorization network with
temporal aggregation called SVCNet. It can colorize monochrome videos based on
different user-given color scribbles. It addresses three common issues in the
scribble-based video colorization area: colorization vividness, temporal
consistency, and color bleeding. To improve the colorization quality and
strengthen the temporal consistency, we adopt two sequential sub-networks in
SVCNet for precise colorization and temporal smoothing, respectively. The first
stage includes a pyramid feature encoder to incorporate color scribbles with a
grayscale frame, and a semantic feature encoder to extract semantics. The
second stage finetunes the output from the first stage by aggregating the
information of neighboring colorized frames (as short-range connections) and
the first colorized frame (as a long-range connection). To alleviate the color
bleeding artifacts, we learn video colorization and segmentation
simultaneously. Furthermore, we set the majority of operations on a fixed small
image resolution and use a Super-resolution Module at the tail of SVCNet to
recover original sizes. It allows the SVCNet to fit different image resolutions
at the inference. Finally, we evaluate the proposed SVCNet on DAVIS and Videvo
benchmarks. The experimental results demonstrate that SVCNet produces both
higher-quality and more temporally consistent videos than other well-known
video colorization approaches. The codes and models can be found at
https://github.com/zhaoyuzhi/SVCNet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under revision of IEEE Transactions on Image Processing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Positive-Augmented Constrastive Learning for Image and Video Captioning
  Evaluation <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12112v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12112v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sara Sarto, Manuele Barraco, Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The CLIP model has been recently proven to be very effective for a variety of
cross-modal tasks, including the evaluation of captions generated from
vision-and-language architectures. In this paper, we propose a new recipe for a
contrastive-based evaluation metric for image captioning, namely
Positive-Augmented Contrastive learning Score (PAC-S), that in a novel way
unifies the learning of a contrastive visual-semantic space with the addition
of generated images and text on curated data. Experiments spanning several
datasets demonstrate that our new metric achieves the highest correlation with
human judgments on both images and videos, outperforming existing
reference-based metrics like CIDEr and SPICE and reference-free metrics like
CLIP-Score. Finally, we test the system-level correlation of the proposed
metric when considering popular image captioning approaches, and assess the
impact of employing different cross-modal features. Our source code and trained
models are publicly available at: https://github.com/aimagelab/pacscore.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023 (highlight paper)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">GPT</span>4MIA: Utilizing Generative <span class="highlight-title">Pre-train</span>ed <span class="highlight-title">Transformer</span> (<span class="highlight-title">GPT</span>-3) as A
  Plug-and-Play Transductive Model for Medical Image Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.08722v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.08722v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhe Zhang, Danny Z. Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a novel approach (called GPT4MIA) that utilizes
Generative Pre-trained Transformer (GPT) as a plug-and-play transductive
inference tool for medical image analysis (MIA). We provide theoretical
analysis on why a large pre-trained language model such as GPT-3 can be used as
a plug-and-play transductive inference model for MIA. At the methodological
level, we develop several technical treatments to improve the efficiency and
effectiveness of GPT4MIA, including better prompt structure design, sample
selection, and prompt ordering of representative samples/features. We present
two concrete use cases (with workflow) of GPT4MIA: (1) detecting prediction
errors and (2) improving prediction accuracy, working in conjecture with
well-established vision-based models for image classification (e.g., ResNet).
Experiments validate that our proposed method is effective for these two tasks.
We further discuss the opportunities and challenges in utilizing
Transformer-based large language models for broader MIA applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Version 3: Added appendix with more results and visualizations.
  Questions and suggestions are welcome</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-03-20T00:00:00Z">2023-03-20</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">43</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EVA-02: A Visual Representation for Neon Genesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11331v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11331v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Fang, Quan Sun, Xinggang Wang, Tiejun Huang, Xinlong Wang, Yue Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We launch EVA-02, a next-generation Transformer-based visual representation
pre-trained to reconstruct strong and robust language-aligned vision features
via masked image modeling. With an updated plain Transformer architecture as
well as extensive pre-training from an open & accessible giant CLIP vision
encoder, EVA-02 demonstrates superior performance compared to prior
state-of-the-art approaches across various representative vision tasks, while
utilizing significantly fewer parameters and compute budgets. Notably, using
exclusively publicly accessible training data, EVA-02 with only 304M parameters
achieves a phenomenal 90.0 fine-tuning top-1 accuracy on ImageNet-1K val set.
Additionally, our EVA-02-CLIP can reach up to 80.4 zero-shot top-1 on
ImageNet-1K, outperforming the previous largest & best open-sourced CLIP with
only ~1/6 parameters and ~1/6 image-text training data. We offer four EVA-02
variants in various model sizes, ranging from 6M to 304M parameters, all with
impressive performance. To facilitate open access and open research, we release
the complete suite of EVA-02 to the community at
https://github.com/baaivision/EVA/tree/master/EVA-02.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To Asuka. Code & Models:
  https://github.com/baaivision/EVA/tree/master/EVA-02</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Context-faithful <span class="highlight-title">Prompt</span>ing for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11315v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11315v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxuan Zhou, Sheng Zhang, Hoifung Poon, Muhao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) encode parametric knowledge about world facts
and have shown remarkable performance in knowledge-driven NLP tasks. However,
their reliance on parametric knowledge may cause them to overlook contextual
cues, leading to incorrect predictions in context-sensitive NLP tasks (e.g.,
knowledge acquisition tasks). In this paper, we seek to assess and enhance
LLMs' contextual faithfulness in two aspects: knowledge conflict and prediction
with abstention. We demonstrate that LLMs' faithfulness can be significantly
improved using carefully designed prompting strategies. In particular, we
identify opinion-based prompts and counterfactual demonstrations as the most
effective methods. Opinion-based prompts reframe the context as a narrator's
statement and inquire about the narrator's opinions, while counterfactual
demonstrations use instances containing false facts to improve faithfulness in
knowledge conflict situations. Neither technique requires additional training.
We conduct experiments on three datasets of two standard NLP tasks, machine
reading comprehension and relation extraction, and the results demonstrate
significant improvement in faithfulness to contexts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and data will be released at
  https://github.com/wzhouad/context-faithful-llm</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Shannon Game with Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11192v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11192v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vilém Zouhar, Sunit Bhattacharya, Ondřej Bojar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Shannon game has long been used as a thought experiment in linguistics
and NLP, asking participants to guess the next letter in a sentence based on
its preceding context. We extend the game by introducing an optional extra
modality in the form of image information. To investigate the impact of
multimodal information in this game, we use human participants and a language
model (LM, GPT-2). We show that the addition of image information improves both
self-reported confidence and accuracy for both humans and LM. Certain word
classes, such as nouns and determiners, benefit more from the additional
modality information. The priming effect in both humans and the LM becomes more
apparent as the context size (extra modality information + sentence context)
increases. These findings highlight the potential of multimodal information in
improving language understanding and modeling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Conversation Modeling to Predict Derailment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11184v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11184v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaqing Yuan, Munindar P. Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversations among online users sometimes derail, i.e., break down into
personal attacks. Such derailment has a negative impact on the healthy growth
of cyberspace communities. The ability to predict whether ongoing conversations
are likely to derail could provide valuable real-time insight to interlocutors
and moderators. Prior approaches predict conversation derailment
retrospectively without the ability to forestall the derailment proactively.
Some works attempt to make dynamic prediction as the conversation develops, but
fail to incorporate multisource information, such as conversation structure and
distance to derailment.
  We propose a hierarchical transformer-based framework that combines
utterance-level and conversation-level information to capture fine-grained
contextual semantics. We propose a domain-adaptive pretraining objective to
integrate conversational structure information and a multitask learning scheme
to leverage the distance from each utterance to derailment. An evaluation of
our framework on two conversation derailment datasets yields improvement over
F1 score for the prediction of derailment. These results demonstrate the
effectiveness of incorporating multisource information.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DocRED-FE: A Document-Level Fine-Grained Entity And Relation Extraction
  <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11141v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11141v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongbo Wang, Weimin Xiong, Yifan Song, Dawei Zhu, Yu Xia, Sujian Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Joint entity and relation extraction (JERE) is one of the most important
tasks in information extraction. However, most existing works focus on
sentence-level coarse-grained JERE, which have limitations in real-world
scenarios. In this paper, we construct a large-scale document-level
fine-grained JERE dataset DocRED-FE, which improves DocRED with Fine-Grained
Entity Type. Specifically, we redesign a hierarchical entity type schema
including 11 coarse-grained types and 119 fine-grained types, and then
re-annotate DocRED manually according to this schema. Through comprehensive
experiments we find that: (1) DocRED-FE is challenging to existing JERE models;
(2) Our fine-grained entity types promote relation classification. We make
DocRED-FE with instruction and the code for our baselines publicly available at
https://github.com/PKU-TANGENT/DOCRED-FE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cocktail Hu<span class="highlight-title">BERT</span>: Generalized <span class="highlight-title">Self-Supervised</span> <span class="highlight-title">Pre-train</span>ing for Mixture
  and Single-Source Speech <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11131v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11131v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maryam Fazel-Zarandi, Wei-Ning Hsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning leverages unlabeled data effectively, improving
label efficiency and generalization to domains without labeled data. While
recent work has studied generalization to more acoustic/linguistic domains,
languages, and modalities, these investigations are limited to single-source
speech with one primary speaker in the recording. This paper presents Cocktail
HuBERT, a self-supervised learning framework that generalizes to mixture speech
using a masked pseudo source separation objective. This objective encourages
the model to identify the number of sources, separate and understand the
context, and infer the content of masked regions represented as discovered
units. Cocktail HuBERT outperforms state-of-the-art results with 69% lower WER
on multi-speaker ASR, 31% lower DER on diarization, and is competitive on
single- and multi-speaker tasks from SUPERB.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EmotionIC: Emotional Inertia and Contagion-driven Dependency Modelling
  for Emotion Recognition in Conversation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11117v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11117v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liu Yingjian, Li Jiang, Wang Xiaoping, Zeng Zhigang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emotion Recognition in Conversation (ERC) has attracted growing attention in
recent years as a result of the advancement and implementation of
human-computer interface technologies. However, previous approaches to modeling
global and local context dependencies lost the diversity of dependency
information and do not take the context dependency into account at the
classification level. In this paper, we propose a novel approach to dependency
modeling driven by Emotional Inertia and Contagion (EmotionIC) for
conversational emotion recognition at the feature extraction and classification
levels. At the feature extraction level, our designed Identity Masked
Multi-head Attention (IM-MHA) captures the identity-based long-distant context
in the dialogue to contain the diverse influence of different participants and
construct the global emotional atmosphere, while the devised Dialogue-based
Gate Recurrent Unit (DialogGRU) that aggregates the emotional tendencies of
dyadic dialogue is applied to refine the contextual features with inter- and
intra-speaker dependencies. At the classification level, by introducing skip
connections in Conditional Random Field (CRF), we elaborate the Skip-chain CRF
(SkipCRF) to capture the high-order dependencies within and between speakers,
and to emulate the emotional flow of distant participants. Experimental results
show that our method can significantly outperform the state-of-the-art models
on four benchmark datasets. The ablation studies confirm that our modules can
effectively model emotional inertia and contagion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Language Models for Knowledge Base Completion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11082v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11082v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Blerta Veseli, Sneha Singhania, Simon Razniewski, Gerhard Weikum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Structured knowledge bases (KBs) are a foundation of many intelligent
applications, yet are notoriously incomplete. Language models (LMs) have
recently been proposed for unsupervised knowledge base completion (KBC), yet,
despite encouraging initial results, questions regarding their suitability
remain open. Existing evaluations often fall short because they only evaluate
on popular subjects, or sample already existing facts from KBs. In this work,
we introduce a novel, more challenging benchmark dataset, and a methodology
tailored for a realistic assessment of the KBC potential of LMs. For automated
assessment, we curate a dataset called WD-KNOWN, which provides an unbiased
random sample of Wikidata, containing over 3.9 million facts. In a second step,
we perform a human evaluation on predictions that are not yet in the KB, as
only this provides real insights into the added value over existing KBs. Our
key finding is that biases in dataset conception of previous benchmarks lead to
a systematic overestimate of LM performance for KBC. However, our results also
reveal strong areas of LMs. We could, for example, perform a significant
completion of Wikidata on the relations nativeLanguage, by a factor of ~21
(from 260k to 5.8M) at 82% precision, usedLanguage, by a factor of ~2.1 (from
2.1M to 6.6M) at 82% precision, and citizenOf by a factor of ~0.3 (from 4.2M to
5.3M) at 90% precision. Moreover, we find that LMs possess surprisingly strong
generalization capabilities: even on relations where most facts were not
directly observed in LM training, prediction quality can be high.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Data and code available at https://github.com/bveseli/LMsForKBC</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeID-<span class="highlight-title">GPT</span>: Zero-shot Medical Text De-Identification by <span class="highlight-title">GPT</span>-4 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11032v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11032v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengliang Liu, Xiaowei Yu, Lu Zhang, Zihao Wu, Chao Cao, Haixing Dai, Lin Zhao, Wei Liu, Dinggang Shen, Quanzheng Li, Tianming Liu, Dajiang Zhu, Xiang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The digitization of healthcare has facilitated the sharing and re-using of
medical data but has also raised concerns about confidentiality and privacy.
HIPAA (Health Insurance Portability and Accountability Act) mandates removing
re-identifying information before the dissemination of medical records. Thus,
effective and efficient solutions for de-identifying medical data, especially
those in free-text forms, are highly needed. While various computer-assisted
de-identification methods, including both rule-based and learning-based, have
been developed and used in prior practice, such solutions still lack
generalizability or need to be fine-tuned according to different scenarios,
significantly imposing restrictions in wider use. The advancement of large
language models (LLM), such as ChatGPT and GPT-4, have shown great potential in
processing text data in the medical domain with zero-shot in-context learning,
especially in the task of privacy protection, as these models can identify
confidential information by their powerful named entity recognition (NER)
capability. In this work, we developed a novel GPT4-enabled de-identification
framework ("DeID-GPT") to automatically identify and remove the identifying
information. Compared to existing commonly used medical text data
de-identification methods, our developed DeID-GPT showed the highest accuracy
and remarkable reliability in masking private information from the unstructured
medical text while preserving the original structure and meaning of the text.
This study is one of the earliest to utilize ChatGPT and GPT-4 for medical text
data processing and de-identification, which provides insights for further
research and solution development on the use of LLMs such as ChatGPT/GPT-4 in
healthcare. Codes and benchmarking data information are available at
https://github.com/yhydhx/ChatGPT-API.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Controllable Ancient Chinese Lyrics Generation Based on Phrase Prototype
  Retrieving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11005v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11005v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Yi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating lyrics and poems is one of the essential downstream tasks in the
Natural Language Processing (NLP) field. Current methods have performed well in
some lyrics generation scenarios but need further improvements in tasks
requiring fine control. We propose a novel method for generating ancient
Chinese lyrics (Song Ci), a type of ancient lyrics that involves precise
control of song structure. The system is equipped with a phrase retriever and a
phrase connector. Based on an input prompt, the phrase retriever picks phrases
from a database to construct a phrase pool. The phrase connector then selects a
series of phrases from the phrase pool that minimizes a multi-term loss
function that considers rhyme, song structure, and fluency. Experimental
results show that our method can generate high-quality ancient Chinese lyrics
while performing well on topic and song structure control. We also expect our
approach to be generalized to other lyrics-generating tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Translate your gibberish: black-box adversarial attack on machine
  translation systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10974v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10974v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrei Chertkov, Olga Tsymboi, Mikhail Pautov, Ivan Oseledets
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural networks are deployed widely in natural language processing tasks on
the industrial scale, and perhaps the most often they are used as compounds of
automatic machine translation systems. In this work, we present a simple
approach to fool state-of-the-art machine translation tools in the task of
translation from Russian to English and vice versa. Using a novel black-box
gradient-free tensor-based optimizer, we show that many online translation
tools, such as Google, DeepL, and Yandex, may both produce wrong or offensive
translations for nonsensical adversarial input queries and refuse to translate
seemingly benign input phrases. This vulnerability may interfere with
understanding a new language and simply worsen the user's experience while
using machine translation systems, and, hence, additional improvements of these
tools are required to establish better translation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Reliable Neural Machine Translation with Consistency-Aware
  Meta-Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10966v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10966v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rongxiang Weng, Qiang Wang, Wensen Cheng, Changfeng Zhu, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural machine translation (NMT) has achieved remarkable success in producing
high-quality translations. However, current NMT systems suffer from a lack of
reliability, as their outputs that are often affected by lexical or syntactic
changes in inputs, resulting in large variations in quality. This limitation
hinders the practicality and trustworthiness of NMT. A contributing factor to
this problem is that NMT models trained with the one-to-one paradigm struggle
to handle the source diversity phenomenon, where inputs with the same meaning
can be expressed differently. In this work, we treat this problem as a bilevel
optimization problem and present a consistency-aware meta-learning (CAML)
framework derived from the model-agnostic meta-learning (MAML) algorithm to
address it. Specifically, the NMT model with CAML (named CoNMT) first learns a
consistent meta representation of semantically equivalent sentences in the
outer loop. Subsequently, a mapping from the meta representation to the output
sentence is learned in the inner loop, allowing the NMT model to translate
semantically equivalent sentences to the same target sentence. We conduct
experiments on the NIST Chinese to English task, three WMT translation tasks,
and the TED M2O task. The results demonstrate that CoNMT effectively improves
overall translation quality and reliably handles diverse inputs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Code-Switching Text Generation and Injection in Mandarin-English ASR <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10949v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10949v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haibin Yu, Yuxuan Hu, Yao Qian, Ma Jin, Linquan Liu, Shujie Liu, Yu Shi, Yanmin Qian, Edward Lin, Michael Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Code-switching speech refers to a means of expression by mixing two or more
languages within a single utterance. Automatic Speech Recognition (ASR) with
End-to-End (E2E) modeling for such speech can be a challenging task due to the
lack of data. In this study, we investigate text generation and injection for
improving the performance of an industry commonly-used streaming model,
Transformer-Transducer (T-T), in Mandarin-English code-switching speech
recognition. We first propose a strategy to generate code-switching text data
and then investigate injecting generated text into T-T model explicitly by
Text-To-Speech (TTS) conversion or implicitly by tying speech and text latent
spaces. Experimental results on the T-T model trained with a dataset containing
1,800 hours of real Mandarin-English code-switched speech show that our
approaches to inject generated code-switching text significantly boost the
performance of T-T models, i.e., 16% relative Token-based Error Rate (TER)
reduction averaged on three evaluation sets, and the approach of tying speech
and text latent spaces is superior to that of TTS conversion on the evaluation
set which contains more homogeneous data with the training set.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On-the-fly Text Retrieval for End-to-End ASR Adaptation <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10942v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10942v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bolaji Yusuf, Aditya Gourav, Ankur Gandhe, Ivan Bulyko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  End-to-end speech recognition models are improved by incorporating external
text sources, typically by fusion with an external language model. Such
language models have to be retrained whenever the corpus of interest changes.
Furthermore, since they store the entire corpus in their parameters, rare words
can be challenging to recall. In this work, we propose augmenting a
transducer-based ASR model with a retrieval language model, which directly
retrieves from an external text corpus plausible completions for a partial ASR
hypothesis. These completions are then integrated into subsequent predictions
by an adapter, which is trained once, so that the corpus of interest can be
switched without incurring the computational overhead of retraining. Our
experiments show that the proposed model significantly improves the performance
of a transducer baseline on a pair of question-answering datasets. Further, it
outperforms shallow fusion on recognition of named entities by about 7
relative; when the two are combined, the relative improvement increases to 13%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICASSP 2023; Appendix added to include ablations that
  could not fit into the conference 4-page limit</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Representation Learning for Small-Footprint Keyword Spotting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10912v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10912v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Cui, Liyong Guo, Quandong Wang, Peng Gao, Yujun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we investigate representation learning for low-resource
keyword spotting (KWS). The main challenges of KWS are limited labeled data and
limited available device resources. To address those challenges, we explore
representation learning for KWS by self-supervised contrastive learning and
self-training with pretrained model. First, local-global contrastive siamese
networks (LGCSiam) are designed to learn similar utterance-level
representations for similar audio samplers by proposed local-global contrastive
loss without requiring ground-truth. Second, a self-supervised pretrained
Wav2Vec 2.0 model is applied as a constraint module (WVC) to force the KWS
model to learn frame-level acoustic representations. By the LGCSiam and WVC
modules, the proposed small-footprint KWS model can be pretrained with
unlabeled data. Experiments on speech commands dataset show that the
self-training WVC module and the self-supervised LGCSiam module significantly
improve accuracy, especially in the case of training on a small labeled
dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Relate auditory speech to EEG by shallow-deep attention-based network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10897v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10897v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Cui, Liyong Guo, Lang He, Jiyao Liu, ErCheng Pei, Yujun Wang, Dongmei Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electroencephalography (EEG) plays a vital role in detecting how brain
responses to different stimulus. In this paper, we propose a novel Shallow-Deep
Attention-based Network (SDANet) to classify the correct auditory stimulus
evoking the EEG signal. It adopts the Attention-based Correlation Module (ACM)
to discover the connection between auditory speech and EEG from global aspect,
and the Shallow-Deep Similarity Classification Module (SDSCM) to decide the
classification result via the embeddings learned from the shallow and deep
layers. Moreover, various training strategies and data augmentation are used to
boost the model robustness. Experiments are conducted on the dataset provided
by Auditory EEG challenge (ICASSP Signal Processing Grand Challenge 2023).
Results show that the proposed model has a significant gain over the baseline
on the match-mismatch track.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Character, Word, or Both? Revisiting the Segmentation Granularity for
  Chinese <span class="highlight-title">Pre-train</span>ed Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10893v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10893v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinnian Liang, Zefan Zhou, Hui Huang, Shuangzhi Wu, Tong Xiao, Muyun Yang, Zhoujun Li, Chao Bian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pretrained language models (PLMs) have shown marvelous improvements across
various NLP tasks. Most Chinese PLMs simply treat an input text as a sequence
of characters, and completely ignore word information. Although Whole Word
Masking can alleviate this, the semantics in words is still not well
represented. In this paper, we revisit the segmentation granularity of Chinese
PLMs. We propose a mixed-granularity Chinese BERT (MigBERT) by considering both
characters and words. To achieve this, we design objective functions for
learning both character and word-level representations. We conduct extensive
experiments on various Chinese NLP tasks to evaluate existing PLMs as well as
the proposed MigBERT. Experimental results show that MigBERT achieves new SOTA
performance on all these tasks. Further analysis demonstrates that words are
semantically richer than characters. More interestingly, we show that MigBERT
also works with Japanese. Our code and model have been released
here~\footnote{https://github.com/xnliang98/MigBERT}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Improving-Leaderboard(SIL): A Call for Real-World Centric Natural
  Language Processing Leaderboards 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10888v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10888v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chanjun Park, Hyeonseok Moon, Seolhwa Lee, Jaehyung Seo, Sugyeong Eo, Heuiseok Lim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Leaderboard systems allow researchers to objectively evaluate Natural
Language Processing (NLP) models and are typically used to identify models that
exhibit superior performance on a given task in a predetermined setting.
However, we argue that evaluation on a given test dataset is just one of many
performance indications of the model. In this paper, we claim leaderboard
competitions should also aim to identify models that exhibit the best
performance in a real-world setting. We highlight three issues with current
leaderboard systems: (1) the use of a single, static test set, (2) discrepancy
between testing and real-world application (3) the tendency for
leaderboard-centric competition to be biased towards the test set. As a
solution, we propose a new paradigm of leaderboard systems that addresses these
issues of current leaderboard system. Through this study, we hope to induce a
paradigm shift towards more real -world-centric leaderboard competitions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-task <span class="highlight-title">Transformer</span> with Relation-attention and Type-attention for
  Named Entity Recognition <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10870v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10870v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ying Mo, Hongyin Tang, Jiahao Liu, Qifan Wang, Zenglin Xu, Jingang Wang, Wei Wu, Zhoujun Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Named entity recognition (NER) is an important research problem in natural
language processing. There are three types of NER tasks, including flat, nested
and discontinuous entity recognition. Most previous sequential labeling models
are task-specific, while recent years have witnessed the rising of generative
models due to the advantage of unifying all NER tasks into the seq2seq model
framework. Although achieving promising performance, our pilot studies
demonstrate that existing generative models are ineffective at detecting entity
boundaries and estimating entity types. This paper proposes a multi-task
Transformer, which incorporates an entity boundary detection task into the
named entity recognition task. More concretely, we achieve entity boundary
detection by classifying the relations between tokens within the sentence. To
improve the accuracy of entity-type mapping during decoding, we adopt an
external knowledge base to calculate the prior entity-type distributions and
then incorporate the information into the model via the self and
cross-attention mechanisms. We perform experiments on an extensive set of NER
benchmarks, including two flat, three nested, and three discontinuous NER
datasets. Experimental results show that our approach considerably improves the
generative NER model's performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages,accepted ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Retrieving Multimodal Information for Augmented Generation: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10868v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10868v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruochen Zhao, Hailin Chen, Weishi Wang, Fangkai Jiao, Xuan Long Do, Chengwei Qin, Bosheng Ding, Xiaobao Guo, Minzhi Li, Xingxuan Li, Shafiq Joty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this survey, we review methods that retrieve multimodal knowledge to
assist and augment generative models. This group of works focuses on retrieving
grounding contexts from external sources, including images, codes, tables,
graphs, and audio. As multimodal learning and generative AI have become more
and more impactful, such retrieval augmentation offers a promising solution to
important concerns such as factuality, reasoning, interpretability, and
robustness. We provide an in-depth review of retrieval-augmented generation in
different modalities and discuss potential future directions. As this is an
emerging field, we continue to add new papers and methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PanGu-Σ: Towards Trillion Parameter Language Model with Sparse
  Heterogeneous Computing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10845v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10845v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaozhe Ren, Pingyi Zhou, Xinfan Meng, Xinjing Huang, Yadao Wang, Weichao Wang, Pengfei Li, Xiaoda Zhang, Alexander Podolskiy, Grigory Arshinov, Andrey Bout, Irina Piontkovskaya, Jiansheng Wei, Xin Jiang, Teng Su, Qun Liu, Jun Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The scaling of large language models has greatly improved natural language
understanding, generation, and reasoning. In this work, we develop a system
that trained a trillion-parameter language model on a cluster of Ascend 910 AI
processors and MindSpore framework, and present the language model with 1.085T
parameters named PanGu-{\Sigma}. With parameter inherent from PanGu-{\alpha},
we extend the dense Transformer model to sparse one with Random Routed Experts
(RRE), and efficiently train the model over 329B tokens by using Expert
Computation and Storage Separation(ECSS). This resulted in a 6.3x increase in
training throughput through heterogeneous computing. Our experimental findings
show that PanGu-{\Sigma} provides state-of-the-art performance in zero-shot
learning of various Chinese NLP downstream tasks. Moreover, it demonstrates
strong abilities when fine-tuned in application data of open-domain dialogue,
question answering, machine translation and code generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language Model Behavior: A Comprehensive <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11504v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11504v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tyler A. Chang, Benjamin K. Bergen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer language models have received widespread public attention, yet
their generated text is often surprising even to NLP researchers. In this
survey, we discuss over 250 recent studies of English language model behavior
before task-specific fine-tuning. Language models possess basic capabilities in
syntax, semantics, pragmatics, world knowledge, and reasoning, but these
capabilities are sensitive to specific inputs and surface features. Despite
dramatic increases in generated text quality as models scale to hundreds of
billions of parameters, the models are still prone to unfactual responses,
commonsense errors, memorized text, and social biases. Many of these weaknesses
can be framed as over-generalizations or under-generalizations of learned
patterns in text. We synthesize recent results to highlight what is currently
known about what large language models can and cannot do.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models and Simple, Stupid Bugs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11455v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11455v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Jesse, Toufique Ahmed, Premkumar T. Devanbu, Emily Morgan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the advent of powerful neural language models, AI-based systems to
assist developers in coding tasks are becoming widely available; Copilot is one
such system. Copilot uses Codex, a large language model (LLM), to complete code
conditioned on a preceding "prompt". Codex, however, is trained on public
GitHub repositories, viz., on code that may include bugs and vulnerabilities.
Previous studies [1], [2] show Codex reproduces vulnerabilities seen in
training. In this study, we examine how prone Codex is to generate an
interesting bug category, single statement bugs, commonly referred to as
simple, stupid bugs or SStuBs in the MSR community. We find that Codex and
similar LLMs do help avoid some SStuBs, but do produce known, verbatim SStuBs
as much as 2x as likely than known, verbatim correct code. We explore the
consequences of the Codex generated SStuBs and propose avoidance strategies
that suggest the possibility of reducing the production of known, verbatim
SStubs, and increase the possibility of producing known, verbatim fixes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at International Conference on Mining Software Repositories
  (MSR-2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mind meets machine: Unravelling <span class="highlight-title">GPT</span>-4's cognitive psychology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11436v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11436v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         Sifatkaur, Manmeet Singh, Vaisakh SB, Neetiraj Malviya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Commonsense reasoning is a basic ingredient of intelligence in humans,
empowering the ability to deduce conclusions based on the observations of
surroundings. Large language models (LLMs) are emerging as potent tools
increasingly capable of performing human-level tasks. The recent development in
the form of GPT-4 and its demonstrated success in tasks complex to humans such
as medical exam, bar exam and others has led to an increased confidence in the
LLMs to become perfect instruments of intelligence. Though, the GPT-4 paper has
shown performance on some common sense reasoning tasks, a comprehensive
assessment of GPT-4 on common sense reasoning tasks, particularly on the
existing well-established datasets is missing. In this study, we focus on the
evaluation of GPT-4's performance on a set of common sense reasoning questions
from the widely used CommonsenseQA dataset along with tools from cognitive
psychology. In doing so, we understand how GPT-4 processes and integrates
common sense knowledge with contextual information, providing insight into the
underlying cognitive processes that enable its ability to generate common sense
responses. We show that GPT-4 exhibits a high level of accuracy in answering
common sense questions, outperforming its predecessor, GPT-3 and GPT-3.5. We
show that the accuracy of GPT-4 on CommonSenseQA is 83 % and it has been shown
in the original study that human accuracy over the same data was 89 %.
Although, GPT-4 falls short of the human performance, it is a substantial
improvement from the original 56.5 % in the original language model used by the
CommonSenseQA study. Our results strengthen the already available assessments
and confidence on GPT-4's common sense reasoning abilities which have
significant potential to revolutionize the field of AI, by enabling machines to
bridge the gap between human and machine reasoning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ eP-ALM: Efficient Perceptual Augmentation of Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11403v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11403v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mustafa Shukor, Corentin Dancette, Matthieu Cord
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have so far impressed the world, with
unprecedented capabilities that emerge in models at large scales. On the vision
side, transformer models (i.e., ViT) are following the same trend, achieving
the best performance on challenging benchmarks. With the abundance of such
unimodal models, a natural question arises; do we need also to follow this
trend to tackle multimodal tasks? In this work, we propose to rather direct
effort to efficient adaptations of existing models, and propose to augment
Language Models with perception. Existing approaches for adapting pretrained
models for vision-language tasks still rely on several key components that
hinder their efficiency. In particular, they still train a large number of
parameters, rely on large multimodal pretraining, use encoders (e.g., CLIP)
trained on huge image-text datasets, and add significant inference overhead. In
addition, most of these approaches have focused on Zero-Shot and In Context
Learning, with little to no effort on direct finetuning. We investigate the
minimal computational effort needed to adapt unimodal models for multimodal
tasks and propose a new challenging setup, alongside different approaches, that
efficiently adapts unimodal pretrained models. We show that by freezing more
than 99\% of total parameters, training only one linear projection layer, and
prepending only one trainable token, our approach (dubbed eP-ALM) significantly
outperforms other baselines on VQA and Captioning across Image, Video, and
Audio modalities, following the proposed setup. The code will be available
here: https://github.com/mshukor/eP-ALM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/mshukor/eP-ALM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MM-REACT: <span class="highlight-title">Prompt</span>ing Chat<span class="highlight-title">GPT</span> for Multimodal Reasoning and Action 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11381v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11381v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, Lijuan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose MM-REACT, a system paradigm that integrates ChatGPT with a pool of
vision experts to achieve multimodal reasoning and action. In this paper, we
define and explore a comprehensive list of advanced vision tasks that are
intriguing to solve, but may exceed the capabilities of existing vision and
vision-language models. To achieve such advanced visual intelligence, MM-REACT
introduces a textual prompt design that can represent text descriptions,
textualized spatial coordinates, and aligned file names for dense visual
signals such as images and videos. MM-REACT's prompt design allows language
models to accept, associate, and process multimodal information, thereby
facilitating the synergetic combination of ChatGPT and various vision experts.
Zero-shot experiments demonstrate MM-REACT's effectiveness in addressing the
specified capabilities of interests and its wide application in different
scenarios that require advanced visual understanding. Furthermore, we discuss
and compare MM-REACT's system paradigm with an alternative approach that
extends language models for multimodal scenarios through joint finetuning.
Code, demo, video, and visualization are available at
https://multimodal-react.github.io/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reflexion: an autonomous agent with dynamic memory and self-reflection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11366v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11366v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Noah Shinn, Beck Labash, Ashwin Gopinath
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in decision-making large language model (LLM) agents have
demonstrated impressive performance across various benchmarks. However, these
state-of-the-art approaches typically necessitate internal model fine-tuning,
external model fine-tuning, or policy optimization over a defined state space.
Implementing these methods can prove challenging due to the scarcity of
high-quality training data or the lack of well-defined state space. Moreover,
these agents do not possess certain qualities inherent to human decision-making
processes, specifically the ability to learn from mistakes. Self-reflection
allows humans to efficiently solve novel problems through a process of trial
and error. Building on recent research, we propose Reflexion, an approach that
endows an agent with dynamic memory and self-reflection capabilities to enhance
its existing reasoning trace and task-specific action choice abilities. To
achieve full automation, we introduce a straightforward yet effective heuristic
that enables the agent to pinpoint hallucination instances, avoid repetition in
action sequences, and, in some environments, construct an internal memory map
of the given environment. To assess our approach, we evaluate the agent's
ability to complete decision-making tasks in AlfWorld environments and
knowledge-intensive, search-based question-and-answer tasks in HotPotQA
environments. We observe success rates of 97% and 51%, respectively, and
provide a discussion on the emergent property of self-reflection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised Cross-Domain Rumor Detection with Contrastive Learning and
  Cross-Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11945v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11945v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyan Ran, Caiyan Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Massive rumors usually appear along with breaking news or trending topics,
seriously hindering the truth. Existing rumor detection methods are mostly
focused on the same domain, and thus have poor performance in cross-domain
scenarios due to domain shift. In this work, we propose an end-to-end
instance-wise and prototype-wise contrastive learning model with a
cross-attention mechanism for cross-domain rumor detection. The model not only
performs cross-domain feature alignment but also enforces target samples to
align with the corresponding prototypes of a given source domain. Since target
labels in a target domain are unavailable, we use a clustering-based approach
with carefully initialized centers by a batch of source domain samples to
produce pseudo labels. Moreover, we use a cross-attention mechanism on a pair
of source data and target data with the same labels to learn domain-invariant
representations. Because the samples in a domain pair tend to express similar
semantic patterns, especially on the people's attitudes (e.g., supporting or
denying) towards the same category of rumors, the discrepancy between a pair of
the source domain and target domain will be decreased. We conduct experiments
on four groups of cross-domain datasets and show that our proposed model
achieves state-of-the-art performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Foundation Models for Clinical Text Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13314v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13314v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaina Raza, Syed Raza Bashir
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Infectious diseases are a significant public health concern globally, and
extracting relevant information from scientific literature can facilitate the
development of effective prevention and treatment strategies. However, the
large amount of clinical data available presents a challenge for information
extraction. To address this challenge, this study proposes a natural language
processing (NLP) framework that uses a pre-trained transformer model fine-tuned
on task-specific data to extract key information related to infectious diseases
from free-text clinical data. The proposed framework includes three components:
a data layer for preparing datasets from clinical texts, a foundation model
layer for entity extraction, and an assessment layer for performance analysis.
The results of the evaluation indicate that the proposed method outperforms
standard methods, and leveraging prior knowledge through the pre-trained
transformer model makes it useful for investigating other infectious diseases
in the future.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Semantic Text Similarity to rank Hypernyms of Financial Terms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13475v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13475v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sohom Ghosh, Ankush Chopra, Sudip Kumar Naskar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over the years, there has been a paradigm shift in how users access financial
services. With the advancement of digitalization more users have been
preferring the online mode of performing financial activities. This has led to
the generation of a huge volume of financial content. Most investors prefer to
go through these contents before making decisions. Every industry has terms
that are specific to the domain it operates in. Banking and Financial Services
are not an exception to this. In order to fully comprehend these contents, one
needs to have a thorough understanding of the financial terms. Getting a basic
idea about a term becomes easy when it is explained with the help of the broad
category to which it belongs. This broad category is referred to as hypernym.
For example, "bond" is a hypernym of the financial term "alternative
debenture". In this paper, we propose a system capable of extracting and
ranking hypernyms for a given financial term. The system has been trained with
financial text corpora obtained from various sources like DBpedia [4],
Investopedia, Financial Industry Business Ontology (FIBO), prospectus and so
on. Embeddings of these terms have been extracted using FinBERT [3], FinISH [1]
and fine-tuned using SentenceBERT [54]. A novel approach has been used to
augment the training set with negative samples. It uses the hierarchy present
in FIBO. Finally, we benchmark the system performance with that of the existing
ones. We establish that it performs better than the existing ones and is also
scalable.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our code base:
  https://github.com/sohomghosh/FinSim_Financial_Hypernym_detection</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Capabilities of <span class="highlight-title">GPT</span>-4 on Medical Challenge Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13375v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13375v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Harsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, Eric Horvitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated remarkable capabilities in
natural language understanding and generation across various domains, including
medicine. We present a comprehensive evaluation of GPT-4, a state-of-the-art
LLM, on medical competency examinations and benchmark datasets. GPT-4 is a
general-purpose model that is not specialized for medical problems through
training or engineered to solve clinical tasks. Our analysis covers two sets of
official practice materials for the USMLE, a three-step examination program
used to assess clinical competency and grant licensure in the United States. We
also evaluate performance on the MultiMedQA suite of benchmark datasets. Beyond
measuring model performance, experiments were conducted to investigate the
influence of test questions containing both text and images on model
performance, probe for memorization of content during training, and study
probability calibration, which is of critical importance in high-stakes
applications like medicine. Our results show that GPT-4, without any
specialized prompt crafting, exceeds the passing score on USMLE by over 20
points and outperforms earlier general-purpose models (GPT-3.5) as well as
models specifically fine-tuned on medical knowledge (Med-PaLM, a prompt-tuned
version of Flan-PaLM 540B). In addition, GPT-4 is significantly better
calibrated than GPT-3.5, demonstrating a much-improved ability to predict the
likelihood that its answers are correct. We also explore the behavior of the
model qualitatively through a case study that shows the ability of GPT-4 to
explain medical reasoning, personalize explanations to students, and
interactively craft new counterfactual scenarios around a medical case.
Implications of the findings are discussed for potential uses of GPT-4 in
medical education, assessment, and clinical practice, with appropriate
attention to challenges of accuracy and safety.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>33 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GraphCFC: A Directed Graph based Cross-modal Feature Complementation
  Approach for Multimodal Conversational Emotion Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.12261v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.12261v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiang Li, Xiaoping Wang, Guoqing Lv, Zhigang Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emotion Recognition in Conversation (ERC) plays a significant part in
Human-Computer Interaction (HCI) systems since it can provide empathetic
services. Multimodal ERC can mitigate the drawbacks of uni-modal approaches.
Recently, Graph Neural Networks (GNNs) have been widely used in a variety of
fields due to their superior performance in relation modeling. In multimodal
ERC, GNNs are capable of extracting both long-distance contextual information
and inter-modal interactive information. Unfortunately, since existing methods
such as MMGCN directly fuse multiple modalities, redundant information may be
generated and diverse information may be lost. In this work, we present a
directed Graph based Cross-modal Feature Complementation (GraphCFC) module that
can efficiently model contextual and interactive information. GraphCFC
alleviates the problem of heterogeneity gap in multimodal fusion by utilizing
multiple subspace extractors and Pair-wise Cross-modal Complementary (PairCC)
strategy. We extract various types of edges from the constructed graph for
encoding, thus enabling GNNs to extract crucial contextual and interactive
information more accurately when performing message passing. Furthermore, we
design a GNN structure called GAT-MLP, which can provide a new unified network
framework for multimodal learning. The experimental results on two benchmark
datasets show that our GraphCFC outperforms the state-of-the-art (SOTA)
approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ What Makes Sentences Semantically Related: A Textual Relatedness <span class="highlight-title">Dataset</span>
  and Empirical Study <span class="chip">EACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.04845v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.04845v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Abdalla, Krishnapriya Vishnubhotla, Saif M. Mohammad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The degree of semantic relatedness of two units of language has long been
considered fundamental to understanding meaning. Additionally, automatically
determining relatedness has many applications such as question answering and
summarization. However, prior NLP work has largely focused on semantic
similarity, a subset of relatedness, because of a lack of relatedness datasets.
In this paper, we introduce a dataset for Semantic Textual Relatedness,
STR-2022, that has 5,500 English sentence pairs manually annotated using a
comparative annotation framework, resulting in fine-grained scores. We show
that human intuition regarding relatedness of sentence pairs is highly
reliable, with a repeat annotation correlation of 0.84. We use the dataset to
explore questions on what makes sentences semantically related. We also show
the utility of STR-2022 for evaluating automatic methods of sentence
representation and for various downstream NLP tasks.
  Our dataset, data statement, and annotation questionnaire can be found at:
https://doi.org/10.5281/zenodo.7599667
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EACL 2023; Our dataset, data statement, and annotation
  questionnaire can be found at: https://doi.org/10.5281/zenodo.7599667</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Aug<span class="highlight-title">GPT</span>: Leveraging Chat<span class="highlight-title">GPT</span> for Text Data Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.13007v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.13007v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haixing Dai, Zhengliang Liu, Wenxiong Liao, Xiaoke Huang, Yihan Cao, Zihao Wu, Lin Zhao, Shaochen Xu, Wei Liu, Ninghao Liu, Sheng Li, Dajiang Zhu, Hongmin Cai, Lichao Sun, Quanzheng Li, Dinggang Shen, Tianming Liu, Xiang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text data augmentation is an effective strategy for overcoming the challenge
of limited sample sizes in many natural language processing (NLP) tasks. This
challenge is especially prominent in the few-shot learning scenario, where the
data in the target domain is generally much scarcer and of lowered quality. A
natural and widely-used strategy to mitigate such challenges is to perform data
augmentation to better capture the data invariance and increase the sample
size. However, current text data augmentation methods either can't ensure the
correct labeling of the generated data (lacking faithfulness) or can't ensure
sufficient diversity in the generated data (lacking compactness), or both.
Inspired by the recent success of large language models, especially the
development of ChatGPT, which demonstrated improved language comprehension
abilities, in this work, we propose a text data augmentation approach based on
ChatGPT (named AugGPT). AugGPT rephrases each sentence in the training samples
into multiple conceptually similar but semantically different samples. The
augmented samples can then be used in downstream model training. Experiment
results on few-shot learning text classification tasks show the superior
performance of the proposed AugGPT approach over state-of-the-art text data
augmentation methods in terms of testing accuracy and distribution of the
augmented samples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Toward a realistic model of speech processing in the brain with
  <span class="highlight-title">self-supervised</span> learning <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.01685v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.01685v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juliette Millet, Charlotte Caucheteux, Pierre Orhan, Yves Boubenec, Alexandre Gramfort, Ewan Dunbar, Christophe Pallier, Jean-Remi King
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Several deep neural networks have recently been shown to generate activations
similar to those of the brain in response to the same input. These algorithms,
however, remain largely implausible: they require (1) extraordinarily large
amounts of data, (2) unobtainable supervised labels, (3) textual rather than
raw sensory input, and / or (4) implausibly large memory (e.g. thousands of
contextual words). These elements highlight the need to identify algorithms
that, under these limitations, would suffice to account for both behavioral and
brain responses. Focusing on the issue of speech processing, we here
hypothesize that self-supervised algorithms trained on the raw waveform
constitute a promising candidate. Specifically, we compare a recent
self-supervised architecture, Wav2Vec 2.0, to the brain activity of 412
English, French, and Mandarin individuals recorded with functional Magnetic
Resonance Imaging (fMRI), while they listened to ~1h of audio books. Our
results are four-fold. First, we show that this algorithm learns brain-like
representations with as little as 600 hours of unlabelled speech -- a quantity
comparable to what infants can be exposed to during language acquisition.
Second, its functional hierarchy aligns with the cortical hierarchy of speech
processing. Third, different training regimes reveal a functional
specialization akin to the cortex: Wav2Vec 2.0 learns sound-generic,
speech-specific and language-specific representations similar to those of the
prefrontal and temporal cortices. Fourth, we confirm the similarity of this
specialization with the behavior of 386 additional participants. These
elements, resulting from the largest neuroimaging benchmark to date, show how
self-supervised learning can account for a rich organization of speech
processing in the brain, and thus delineate a path to identify the laws of
language acquisition which shape the human brain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Science of Detecting LLM-Generated Texts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07205v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07205v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruixiang Tang, Yu-Neng Chuang, Xia Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emergence of large language models (LLMs) has resulted in the production
of LLM-generated texts that is highly sophisticated and almost
indistinguishable from texts written by humans. However, this has also sparked
concerns about the potential misuse of such texts, such as spreading
misinformation and causing disruptions in the education system. Although many
detection approaches have been proposed, a comprehensive understanding of the
achievements and challenges is still lacking. This survey aims to provide an
overview of existing LLM-generated text detection techniques and enhance the
control and regulation of language generation models. Furthermore, we emphasize
crucial considerations for future research, including the development of
comprehensive evaluation metrics and the threat posed by open-source LLMs, to
drive progress in the area of LLM-generated text detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Error-Guided Correction Model for Chinese Spelling Error Correction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06323v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06323v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Sun, Xiuyu Wu, Yunfang Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although existing neural network approaches have achieved great success on
Chinese spelling correction, there is still room to improve. The model is
required to avoid over-correction and to distinguish a correct token from its
phonological and visually similar ones. In this paper, we propose an
error-guided correction model (EGCM) to improve Chinese spelling correction. By
borrowing the powerful ability of BERT, we propose a novel zero-shot error
detection method to do a preliminary detection, which guides our model to
attend more on the probably wrong tokens in encoding and to avoid modifying the
correct tokens in generating. Furthermore, we introduce a new loss function to
integrate the error confusion set, which enables our model to distinguish
easily misused tokens. Moreover, our model supports highly parallel decoding to
meet real application requirements. Experiments are conducted on widely used
benchmarks. Our model achieves superior performance against state-of-the-art
approaches by a remarkable margin, on both the correction quality and
computation speed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Maximum Linear Arrangement Problem for trees under projectivity and
  planarity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.06924v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.06924v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lluís Alemany-Puig, Juan Luis Esteban, Ramon Ferrer-i-Cancho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A linear arrangement is a mapping $\pi$ from the $n$ vertices of a graph $G$
to $n$ distinct consecutive integers. Linear arrangements can be represented by
drawing the vertices along a horizontal line and drawing the edges as
semicircles above said line. In this setting, the length of an edge is defined
as the absolute value of the difference between the positions of its two
vertices in the arrangement, and the cost of an arrangement as the sum of all
edge lengths. Here we study two variants of the Maximum Linear Arrangement
problem (MaxLA), which consists of finding an arrangement that maximizes the
cost. In the planar variant for free trees, vertices have to be arranged in
such a way that there are no edge crossings. In the projective variant for
rooted trees, arrangements have to be planar and the root of the tree cannot be
covered by any edge. In this paper we present algorithms that are linear in
time and space to solve planar and projective MaxLA for trees. We also prove
several properties of maximum projective and planar arrangements, and show that
caterpillar trees maximize planar MaxLA over all trees of a fixed size thereby
generalizing a previous extremal result on trees.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VieCap4H-VLSP 2021: ObjectAoA-Enhancing performance of Object Relation
  <span class="highlight-title">Transformer</span> with Attention on Attention for Vietnamese image captioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.05405v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.05405v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nghia Hieu Nguyen, Duong T. D. Vo, Minh-Quan Ha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image captioning is currently a challenging task that requires the ability to
both understand visual information and use human language to describe this
visual information in the image. In this paper, we propose an efficient way to
improve the image understanding ability of transformer-based method by
extending Object Relation Transformer architecture with Attention on Attention
mechanism. Experiments on the VieCap4H dataset show that our proposed method
significantly outperforms its original structure on both the public test and
private test of the Image Captioning shared task held by VLSP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publishing at the VNU Journal of Science: Computer
  Science and Communication Engineering</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EcoFormer: Energy-Saving Attention with Linear Complexity <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.09004v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.09004v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Liu, Zizheng Pan, Haoyu He, Jianfei Cai, Bohan Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer is a transformative framework that models sequential data and has
achieved remarkable performance on a wide range of tasks, but with high
computational and energy cost. To improve its efficiency, a popular choice is
to compress the models via binarization which constrains the floating-point
values into binary ones to save resource consumption owing to cheap bitwise
operations significantly. However, existing binarization methods only aim at
minimizing the information loss for the input distribution statistically, while
ignoring the pairwise similarity modeling at the core of the attention. To this
end, we propose a new binarization paradigm customized to high-dimensional
softmax attention via kernelized hashing, called EcoFormer, to map the original
queries and keys into low-dimensional binary codes in Hamming space. The
kernelized hash functions are learned to match the ground-truth similarity
relations extracted from the attention map in a self-supervised way. Based on
the equivalence between the inner product of binary codes and the Hamming
distance as well as the associative property of matrix multiplication, we can
approximate the attention in linear complexity by expressing it as a
dot-product of binary codes. Moreover, the compact binary representations of
queries and keys enable us to replace most of the expensive multiply-accumulate
operations in attention with simple accumulations to save considerable on-chip
energy footprint on edge devices. Extensive experiments on both vision and
language tasks show that EcoFormer consistently achieves comparable performance
with standard attentions while consuming much fewer resources. For example,
based on PVTv2-B0 and ImageNet-1K, Ecoformer achieves a 73% on-chip energy
footprint reduction with only a 0.33% performance drop compared to the standard
attention. Code is available at https://github.com/ziplab/EcoFormer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2022 camera ready; First two authors contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Privately Fine-Tuning Large Language Models with Differential Privacy <span class="chip">ICDM</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.15042v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.15042v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rouzbeh Behnia, Mohamamdreza Ebrahimi, Jason Pacheco, Balaji Padmanabhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained Large Language Models (LLMs) are an integral part of modern AI
that have led to breakthrough performances in complex AI tasks. Major AI
companies with expensive infrastructures are able to develop and train these
large models with billions and millions of parameters from scratch. Third
parties, researchers, and practitioners are increasingly adopting these
pre-trained models and fine-tuning them on their private data to accomplish
their downstream AI tasks. However, it has been shown that an adversary can
extract/reconstruct the exact training samples from these LLMs, which can lead
to revealing personally identifiable information. The issue has raised deep
concerns about the privacy of LLMs. Differential privacy (DP) provides a
rigorous framework that allows adding noise in the process of training or
fine-tuning LLMs such that extracting the training data becomes infeasible
(i.e., with a cryptographically small success probability). While the
theoretical privacy guarantees offered in most extant studies assume learning
models from scratch through many training iterations in an asymptotic setting,
this assumption does not hold in fine-tuning scenarios in which the number of
training iterations is significantly smaller. To address the gap, we present
\ewtune, a DP framework for fine-tuning LLMs based on Edgeworth accountant with
finite-sample privacy guarantees. Our results across four well-established
natural language understanding (NLU) tasks show that while \ewtune~adds privacy
guarantees to LLM fine-tuning process, it directly contributes to decreasing
the induced noise to up to 5.6\% and improves the state-of-the-art LLMs
performance by up to 1.1\% across all NLU tasks. We have open-sourced our
implementations for wide adoption and public testing purposes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Publised at IEEE ICDM Workshop on Machine Learning for Cybersecurity
  (MLC) 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mitigating Covertly Unsafe Text within Natural Language Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.09306v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.09306v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex Mei, Anisha Kabir, Sharon Levy, Melanie Subbiah, Emily Allaway, John Judge, Desmond Patton, Bruce Bimber, Kathleen McKeown, William Yang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An increasingly prevalent problem for intelligent technologies is text
safety, as uncontrolled systems may generate recommendations to their users
that lead to injury or life-threatening consequences. However, the degree of
explicitness of a generated statement that can cause physical harm varies. In
this paper, we distinguish types of text that can lead to physical harm and
establish one particularly underexplored category: covertly unsafe text. Then,
we further break down this category with respect to the system's information
and discuss solutions to mitigate the generation of text in each of these
subcategories. Ultimately, our work defines the problem of covertly unsafe
language that causes physical harm and argues that this subtle yet dangerous
issue needs to be prioritized by stakeholders and regulators. We highlight
mitigation strategies to inspire future researchers to tackle this challenging
problem and help improve safety within smart systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Findings of the 2022 Conference on Empirical Methods in Natural
  Language Processing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GLEN: General-Purpose Event Detection for Thousands of Types 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.09093v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.09093v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiusi Zhan, Sha Li, Kathryn Conger, Martha Palmer, Heng Ji, Jiawei Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of event extraction systems has been hindered by the absence
of wide-coverage, large-scale datasets. To make event extraction systems more
accessible, we build a general-purpose event detection dataset GLEN, which
covers 3,465 different event types, making it over 20x larger in ontology than
any current dataset. GLEN is created by utilizing the DWD Overlay, which
provides a mapping between Wikidata Qnodes and PropBank rolesets. This enables
us to use the abundant existing annotation for PropBank as distant supervision.
In addition, we also propose a new multi-stage event detection model
specifically designed to handle the large ontology size and partial labels in
GLEN. We show that our model exhibits superior performance (~10% F1 gain)
compared to both conventional classification baselines and newer
definition-based models. Finally, we perform error analysis and show that label
noise is still the largest challenge for improving performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors contributed equally. (15 pages, 11 figures)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">150</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EVA-02: A Visual Representation for Neon Genesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11331v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11331v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Fang, Quan Sun, Xinggang Wang, Tiejun Huang, Xinlong Wang, Yue Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We launch EVA-02, a next-generation Transformer-based visual representation
pre-trained to reconstruct strong and robust language-aligned vision features
via masked image modeling. With an updated plain Transformer architecture as
well as extensive pre-training from an open & accessible giant CLIP vision
encoder, EVA-02 demonstrates superior performance compared to prior
state-of-the-art approaches across various representative vision tasks, while
utilizing significantly fewer parameters and compute budgets. Notably, using
exclusively publicly accessible training data, EVA-02 with only 304M parameters
achieves a phenomenal 90.0 fine-tuning top-1 accuracy on ImageNet-1K val set.
Additionally, our EVA-02-CLIP can reach up to 80.4 zero-shot top-1 on
ImageNet-1K, outperforming the previous largest & best open-sourced CLIP with
only ~1/6 parameters and ~1/6 image-text training data. We offer four EVA-02
variants in various model sizes, ranging from 6M to 304M parameters, all with
impressive performance. To facilitate open access and open research, we release
the complete suite of EVA-02 to the community at
https://github.com/baaivision/EVA/tree/master/EVA-02.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To Asuka. Code & Models:
  https://github.com/baaivision/EVA/tree/master/EVA-02</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Legs as Manipulator: Pushing Quadrupedal Agility Beyond Locomotion <span class="chip">ICRA 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11330v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11330v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuxin Cheng, Ashish Kumar, Deepak Pathak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Locomotion has seen dramatic progress for walking or running across
challenging terrains. However, robotic quadrupeds are still far behind their
biological counterparts, such as dogs, which display a variety of agile skills
and can use the legs beyond locomotion to perform several basic manipulation
tasks like interacting with objects and climbing. In this paper, we take a step
towards bridging this gap by training quadruped robots not only to walk but
also to use the front legs to climb walls, press buttons, and perform object
interaction in the real world. To handle this challenging optimization, we
decouple the skill learning broadly into locomotion, which involves anything
that involves movement whether via walking or climbing a wall, and
manipulation, which involves using one leg to interact while balancing on the
other three legs. These skills are trained in simulation using curriculum and
transferred to the real world using our proposed sim2real variant that builds
upon recent locomotion success. Finally, we combine these skills into a robust
long-term plan by learning a behavior tree that encodes a high-level task
hierarchy from one clean expert demonstration. We evaluate our method in both
simulation and real-world showing successful executions of both short as well
as long-range tasks and how robustness helps confront external perturbations.
Videos at https://robot-skills.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICRA 2023. Videos at https://robot-skills.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sound Localization from Motion: Jointly Learning Sound Direction and
  Camera Rotation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11329v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11329v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyang Chen, Shengyi Qian, Andrew Owens
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The images and sounds that we perceive undergo subtle but geometrically
consistent changes as we rotate our heads. In this paper, we use these cues to
solve a problem we call Sound Localization from Motion (SLfM): jointly
estimating camera rotation and localizing sound sources. We learn to solve
these tasks solely through self-supervision. A visual model predicts camera
rotation from a pair of images, while an audio model predicts the direction of
sound sources from binaural sounds. We train these models to generate
predictions that agree with one another. At test time, the models can be
deployed independently. To obtain a feature representation that is well-suited
to solving this challenging problem, we also propose a method for learning an
audio-visual representation through cross-view binauralization: estimating
binaural sound from one view, given images and sound from another. Our model
can successfully estimate accurate rotations on both real and synthetic scenes,
and localize sound sources with accuracy competitive with state-of-the-art
self-supervised approaches. Project site: https://ificl.github.io/SLfM/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project site: https://ificl.github.io/SLfM/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zero-1-to-3: Zero-shot One Image to 3D Object 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11328v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11328v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, Carl Vondrick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Zero-1-to-3, a framework for changing the camera viewpoint of an
object given just a single RGB image. To perform novel view synthesis in this
under-constrained setting, we capitalize on the geometric priors that
large-scale diffusion models learn about natural images. Our conditional
diffusion model uses a synthetic dataset to learn controls of the relative
camera viewpoint, which allow new images to be generated of the same object
under a specified camera transformation. Even though it is trained on a
synthetic dataset, our model retains a strong zero-shot generalization ability
to out-of-distribution datasets as well as in-the-wild images, including
impressionist paintings. Our viewpoint-conditioned diffusion approach can
further be used for the task of 3D reconstruction from a single image.
Qualitative and quantitative experiments show that our method significantly
outperforms state-of-the-art single-view 3D reconstruction and novel view
synthesis models by leveraging Internet-scale pre-training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Website: https://zero123.cs.columbia.edu/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3D Concept Learning and Reasoning from Multi-View Images <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11327v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11327v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yining Hong, Chunru Lin, Yilun Du, Zhenfang Chen, Joshua B. Tenenbaum, Chuang Gan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans are able to accurately reason in 3D by gathering multi-view
observations of the surrounding world. Inspired by this insight, we introduce a
new large-scale benchmark for 3D multi-view visual question answering
(3DMV-VQA). This dataset is collected by an embodied agent actively moving and
capturing RGB images in an environment using the Habitat simulator. In total,
it consists of approximately 5k scenes, 600k images, paired with 50k questions.
We evaluate various state-of-the-art models for visual reasoning on our
benchmark and find that they all perform poorly. We suggest that a principled
approach for 3D reasoning from multi-view images should be to infer a compact
3D representation of the world from the multi-view images, which is further
grounded on open-vocabulary semantic concepts, and then to execute reasoning on
these 3D representations. As the first step towards this approach, we propose a
novel 3D concept learning and reasoning (3D-CLR) framework that seamlessly
combines these components via neural fields, 2D pre-trained vision-language
models, and neural reasoning operators. Experimental results suggest that our
framework outperforms baseline models by a large margin, but the challenge
remains largely unsolved. We further perform an in-depth analysis of the
challenges and highlight potential future directions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023. Project page: https://vis-www.cs.umass.edu/3d-clr/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Better 3D Knowledge Transfer via Masked Image Modeling for
  Multi-view 3D Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11325v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11325v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jihao Liu, Tai Wang, Boxiao Liu, Qihang Zhang, Yu Liu, Hongsheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-view camera-based 3D detection is a challenging problem in computer
vision. Recent works leverage a pretrained LiDAR detection model to transfer
knowledge to a camera-based student network. However, we argue that there is a
major domain gap between the LiDAR BEV features and the camera-based BEV
features, as they have different characteristics and are derived from different
sources. In this paper, we propose Geometry Enhanced Masked Image Modeling
(GeoMIM) to transfer the knowledge of the LiDAR model in a pretrain-finetune
paradigm for improving the multi-view camera-based 3D detection. GeoMIM is a
multi-camera vision transformer with Cross-View Attention (CVA) blocks that
uses LiDAR BEV features encoded by the pretrained BEV model as learning
targets. During pretraining, GeoMIM's decoder has a semantic branch completing
dense perspective-view features and the other geometry branch reconstructing
dense perspective-view depth maps. The depth branch is designed to be
camera-aware by inputting the camera's parameters for better transfer
capability. Extensive results demonstrate that GeoMIM outperforms existing
methods on nuScenes benchmark, achieving state-of-the-art performance for
camera-based 3D object detection and 3D segmentation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Open-vocabulary Panoptic Segmentation with Embedding Modulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11324v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11324v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xi Chen, Shuang Li, Ser-Nam Lim, Antonio Torralba, Hengshuang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-vocabulary image segmentation is attracting increasing attention due to
its critical applications in the real world. Traditional closed-vocabulary
segmentation methods are not able to characterize novel objects, whereas
several recent open-vocabulary attempts obtain unsatisfactory results, i.e.,
notable performance reduction on the closed vocabulary and massive demand for
extra data. To this end, we propose OPSNet, an omnipotent and data-efficient
framework for Open-vocabulary Panoptic Segmentation. Specifically, the
exquisitely designed Embedding Modulation module, together with several
meticulous components, enables adequate embedding enhancement and information
exchange between the segmentation model and the visual-linguistic well-aligned
CLIP encoder, resulting in superior segmentation performance under both open-
and closed-vocabulary settings with much fewer need of additional data.
Extensive experimental evaluations are conducted across multiple datasets
(e.g., COCO, ADE20K, Cityscapes, and PascalContext) under various
circumstances, where the proposed OPSNet achieves state-of-the-art results,
which demonstrates the effectiveness and generality of the proposed approach.
The code and trained models will be made publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ScribbleSeg: Scribble-based Interactive Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11320v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11320v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xi Chen, Yau Shing Jonathan Cheung, Ser-Nam Lim, Hengshuang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interactive segmentation enables users to extract masks by providing simple
annotations to indicate the target, such as boxes, clicks, or scribbles. Among
these interaction formats, scribbles are the most flexible as they can be of
arbitrary shapes and sizes. This enables scribbles to provide more indications
of the target object. However, previous works mainly focus on click-based
configuration, and the scribble-based setting is rarely explored. In this work,
we attempt to formulate a standard protocol for scribble-based interactive
segmentation. Basically, we design diversified strategies to simulate scribbles
for training, propose a deterministic scribble generator for evaluation, and
construct a challenging benchmark. Besides, we build a strong framework
ScribbleSeg, consisting of a Prototype Adaption Module(PAM) and a Corrective
Refine Module (CRM), for the task. Extensive experiments show that ScribbleSeg
performs notably better than previous click-based methods. We hope this could
serve as a more powerful and general solution for interactive segmentation. Our
code will be made available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative Semantic Segmentation <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11316v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11316v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaqi Chen, Jiachen Lu, Xiatian Zhu, Li Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Generative Semantic Segmentation (GSS), a generative learning
approach for semantic segmentation. Uniquely, we cast semantic segmentation as
an image-conditioned mask generation problem. This is achieved by replacing the
conventional per-pixel discriminative learning with a latent prior learning
process. Specifically, we model the variational posterior distribution of
latent variables given the segmentation mask. To that end, the segmentation
mask is expressed with a special type of image (dubbed as maskige). This
posterior distribution allows to generate segmentation masks unconditionally.
To achieve semantic segmentation on a given image, we further introduce a
conditioning network. It is optimized by minimizing the divergence between the
posterior distribution of maskige (i.e., segmentation masks) and the latent
prior distribution of input training images. Extensive experiments on standard
benchmarks show that our GSS can perform competitively to prior art
alternatives in the standard semantic segmentation setting, whilst achieving a
new state of the art in the more challenging cross-domain setting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at CVPR2023, code at http://github.com/fudan-zvg/GSS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CLIP goes 3D: Leveraging <span class="highlight-title">Prompt</span> Tuning for Language Grounded 3D
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11313v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11313v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deepti Hegde, Jeya Maria Jose Valanarasu, Vishal M. Patel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language models like CLIP have been widely adopted for various tasks
due to their impressive zero-shot capabilities. However, CLIP is not suitable
for extracting 3D geometric features as it was trained on only images and text
by natural language supervision. We work on addressing this limitation and
propose a new framework termed CG3D (CLIP Goes 3D) where a 3D encoder is
learned to exhibit zero-shot capabilities. CG3D is trained using triplets of
pointclouds, corresponding rendered 2D images, and texts using natural language
supervision. To align the features in a multimodal embedding space, we utilize
contrastive loss on 3D features obtained from the 3D encoder, as well as visual
and text features extracted from CLIP. We note that the natural images used to
train CLIP and the rendered 2D images in CG3D have a distribution shift.
Attempting to train the visual and text encoder to account for this shift
results in catastrophic forgetting and a notable decrease in performance. To
solve this, we employ prompt tuning and introduce trainable parameters in the
input space to shift CLIP towards the 3D pre-training dataset utilized in CG3D.
We extensively test our pre-trained CG3D framework and demonstrate its
impressive capabilities in zero-shot, open scene understanding, and retrieval
tasks. Further, it also serves as strong starting weights for fine-tuning in
downstream 3D recognition tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Website: https://jeya-maria-jose.github.io/cg3d-web/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DIME-Net: Neural Network-Based Dynamic Intrinsic Parameter Rectification
  for Cameras with Optical Image Stabilization System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11307v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11307v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shu-Hao Yeh, Shuangyu Xie, Di Wang, Wei Yan, Dezhen Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optical Image Stabilization (OIS) system in mobile devices reduces image
blurring by steering lens to compensate for hand jitters. However, OIS changes
intrinsic camera parameters (i.e. $\mathrm{K}$ matrix) dynamically which
hinders accurate camera pose estimation or 3D reconstruction. Here we propose a
novel neural network-based approach that estimates $\mathrm{K}$ matrix in
real-time so that pose estimation or scene reconstruction can be run at camera
native resolution for the highest accuracy on mobile devices. Our network
design takes gratified projection model discrepancy feature and 3D point
positions as inputs and employs a Multi-Layer Perceptron (MLP) to approximate
$f_{\mathrm{K}}$ manifold. We also design a unique training scheme for this
network by introducing a Back propagated PnP (BPnP) layer so that reprojection
error can be adopted as the loss function. The training process utilizes
precise calibration patterns for capturing accurate $f_{\mathrm{K}}$ manifold
but the trained network can be used anywhere. We name the proposed Dynamic
Intrinsic Manifold Estimation network as DIME-Net and have it implemented and
tested on three different mobile devices. In all cases, DIME-Net can reduce
reprojection error by at least $64\%$ indicating that our design is successful.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Localizing Object-level Shape Variations with Text-to-Image Diffusion
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11306v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11306v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Or Patashnik, Daniel Garibi, Idan Azuri, Hadar Averbuch-Elor, Daniel Cohen-Or
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image models give rise to workflows which often begin with an
exploration step, where users sift through a large collection of generated
images. The global nature of the text-to-image generation process prevents
users from narrowing their exploration to a particular object in the image. In
this paper, we present a technique to generate a collection of images that
depicts variations in the shape of a specific object, enabling an object-level
shape exploration process. Creating plausible variations is challenging as it
requires control over the shape of the generated object while respecting its
semantics. A particular challenge when generating object variations is
accurately localizing the manipulation applied over the object's shape. We
introduce a prompt-mixing technique that switches between prompts along the
denoising process to attain a variety of shape choices. To localize the
image-space operation, we present two techniques that use the self-attention
layers in conjunction with the cross-attention layers. Moreover, we show that
these localization techniques are general and effective beyond the scope of
generating object variations. Extensive results and comparisons demonstrate the
effectiveness of our method in generating object variations, and the competence
of our localization techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page at https://orpatashnik.github.io/local-prompt-mixing/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SVDiff: Compact Parameter Space for Diffusion Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11305v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11305v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ligong Han, Yinxiao Li, Han Zhang, Peyman Milanfar, Dimitris Metaxas, Feng Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have achieved remarkable success in text-to-image
generation, enabling the creation of high-quality images from text prompts or
other modalities. However, existing methods for customizing these models are
limited by handling multiple personalized subjects and the risk of overfitting.
Moreover, their large number of parameters is inefficient for model storage. In
this paper, we propose a novel approach to address these limitations in
existing text-to-image diffusion models for personalization. Our method
involves fine-tuning the singular values of the weight matrices, leading to a
compact and efficient parameter space that reduces the risk of overfitting and
language-drifting. We also propose a Cut-Mix-Unmix data-augmentation technique
to enhance the quality of multi-subject image generation and a simple
text-based image editing framework. Our proposed SVDiff method has a
significantly smaller model size (1.7MB for StableDiffusion) compared to
existing methods (vanilla DreamBooth 3.66GB, Custom Diffusion 73MB), making it
more practical for real-world applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 21 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Audio-Visual Source Localization via False Negative Aware
  Contrastive Learning <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11302v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11302v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weixuan Sun, Jiayi Zhang, Jianyuan Wang, Zheyuan Liu, Yiran Zhong, Tianpeng Feng, Yandong Guo, Yanhao Zhang, Nick Barnes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised audio-visual source localization aims to locate sound-source
objects in video frames without extra annotations. Recent methods often
approach this goal with the help of contrastive learning, which assumes only
the audio and visual contents from the same video are positive samples for each
other. However, this assumption would suffer from false negative samples in
real-world training. For example, for an audio sample, treating the frames from
the same audio class as negative samples may mislead the model and therefore
harm the learned representations e.g., the audio of a siren wailing may
reasonably correspond to the ambulances in multiple images). Based on this
observation, we propose a new learning strategy named False Negative Aware
Contrastive (FNAC) to mitigate the problem of misleading the training with such
false negative samples. Specifically, we utilize the intra-modal similarities
to identify potentially similar samples and construct corresponding adjacency
matrices to guide contrastive learning. Further, we propose to strengthen the
role of true negative samples by explicitly leveraging the visual features of
sound sources to facilitate the differentiation of authentic sounding source
regions. FNAC achieves state-of-the-art performances on Flickr-SoundNet,
VGG-Sound, and AVSBench, which demonstrates the effectiveness of our method in
mitigating the false negative issue. The code is available at
\url{https://github.com/weixuansun/FNAC-AVL}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VoxelNeXt: Fully Sparse VoxelNet for 3D Object Detection and Tracking <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11301v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11301v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yukang Chen, Jianhui Liu, Xiangyu Zhang, Xiaojuan Qi, Jiaya Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D object detectors usually rely on hand-crafted proxies, e.g., anchors or
centers, and translate well-studied 2D frameworks to 3D. Thus, sparse voxel
features need to be densified and processed by dense prediction heads, which
inevitably costs extra computation. In this paper, we instead propose VoxelNext
for fully sparse 3D object detection. Our core insight is to predict objects
directly based on sparse voxel features, without relying on hand-crafted
proxies. Our strong sparse convolutional network VoxelNeXt detects and tracks
3D objects through voxel features entirely. It is an elegant and efficient
framework, with no need for sparse-to-dense conversion or NMS post-processing.
Our method achieves a better speed-accuracy trade-off than other mainframe
detectors on the nuScenes dataset. For the first time, we show that a fully
sparse voxel-based representation works decently for LIDAR 3D object detection
and tracking. Extensive experiments on nuScenes, Waymo, and Argoverse2
benchmarks validate the effectiveness of our approach. Without bells and
whistles, our model outperforms all existing LIDAR methods on the nuScenes
tracking test benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In CVPR 2023, Code and models are available at
  https://github.com/dvlab-research/VoxelNeXt</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reliability in Semantic Segmentation: Are We on the Right Track? <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11298v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11298v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pau de Jorge, Riccardo Volpi, Philip Torr, Gregory Rogez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motivated by the increasing popularity of transformers in computer vision, in
recent times there has been a rapid development of novel architectures. While
in-domain performance follows a constant, upward trend, properties like
robustness or uncertainty estimation are less explored -leaving doubts about
advances in model reliability. Studies along these axes exist, but they are
mainly limited to classification models. In contrast, we carry out a study on
semantic segmentation, a relevant task for many real-world applications where
model reliability is paramount. We analyze a broad variety of models, spanning
from older ResNet-based architectures to novel transformers and assess their
reliability based on four metrics: robustness, calibration, misclassification
detection and out-of-distribution (OOD) detection. We find that while recent
models are significantly more robust, they are not overall more reliable in
terms of uncertainty estimation. We further explore methods that can come to
the rescue and show that improving calibration can also help with other
uncertainty metrics such as misclassification or OOD detection. This is the
first study on modern segmentation models focused on both robustness and
uncertainty estimation and we hope it will help practitioners and researchers
interested in this fundamental vision task. Code available at
https://github.com/naver/relis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Attribute-preserving Face <span class="highlight-title">Dataset</span> Anonymization via Latent Code
  Optimization <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11296v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11296v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simone Barattin, Christos Tzelepis, Ioannis Patras, Nicu Sebe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work addresses the problem of anonymizing the identity of faces in a
dataset of images, such that the privacy of those depicted is not violated,
while at the same time the dataset is useful for downstream task such as for
training machine learning models. To the best of our knowledge, we are the
first to explicitly address this issue and deal with two major drawbacks of the
existing state-of-the-art approaches, namely that they (i) require the costly
training of additional, purpose-trained neural networks, and/or (ii) fail to
retain the facial attributes of the original images in the anonymized
counterparts, the preservation of which is of paramount importance for their
use in downstream tasks. We accordingly present a task-agnostic anonymization
procedure that directly optimizes the images' latent representation in the
latent space of a pre-trained GAN. By optimizing the latent codes directly, we
ensure both that the identity is of a desired distance away from the original
(with an identity obfuscation loss), whilst preserving the facial attributes
(using a novel feature-matching loss in FaRL's deep feature space). We
demonstrate through a series of both qualitative and quantitative experiments
that our method is capable of anonymizing the identity of the images whilst --
crucially -- better-preserving the facial attributes. We make the code and the
pre-trained models publicly available at: https://github.com/chi0tzp/FALCO.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cascading Hierarchical Networks with Multi-task Balanced Loss for
  Fine-grained hashing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11274v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11274v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xianxian Zeng, Yanjun Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the explosive growth in the number of fine-grained images in the
Internet era, it has become a challenging problem to perform fast and efficient
retrieval from large-scale fine-grained images. Among the many retrieval
methods, hashing methods are widely used due to their high efficiency and small
storage space occupation. Fine-grained hashing is more challenging than
traditional hashing problems due to the difficulties such as low inter-class
variances and high intra-class variances caused by the characteristics of
fine-grained images. To improve the retrieval accuracy of fine-grained hashing,
we propose a cascaded network to learn compact and highly semantic hash codes,
and introduce an attention-guided data augmentation method. We refer to this
network as a cascaded hierarchical data augmentation network. We also propose a
novel approach to coordinately balance the loss of multi-task learning. We do
extensive experiments on some common fine-grained visual classification
datasets. The experimental results demonstrate that our proposed method
outperforms several state-of-art hashing methods and can effectively improve
the accuracy of fine-grained retrieval. The source code is publicly available:
https://github.com/kaiba007/FG-CNET.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking the backbone architecture for tiny object detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11267v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11267v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinlai Ning, Haoyan Guan, Michael Spratling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tiny object detection has become an active area of research because images
with tiny targets are common in several important real-world scenarios.
However, existing tiny object detection methods use standard deep neural
networks as their backbone architecture. We argue that such backbones are
inappropriate for detecting tiny objects as they are designed for the
classification of larger objects, and do not have the spatial resolution to
identify small targets. Specifically, such backbones use max-pooling or a large
stride at early stages in the architecture. This produces lower resolution
feature-maps that can be efficiently processed by subsequent layers. However,
such low-resolution feature-maps do not contain information that can reliably
discriminate tiny objects. To solve this problem we design 'bottom-heavy'
versions of backbones that allocate more resources to processing
higher-resolution features without introducing any additional computational
burden overall. We also investigate if pre-training these backbones on images
of appropriate size, using CIFAR100 and ImageNet32, can further improve
performance on tiny object detection. Results on TinyPerson and WiderFace show
that detectors with our proposed backbones achieve better results than the
current state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zero-Shot Noise2Noise: Efficient Image Denoising without any Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11253v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11253v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youssef Mansour, Reinhard Heckel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, self-supervised neural networks have shown excellent image
denoising performance. However, current dataset free methods are either
computationally expensive, require a noise model, or have inadequate image
quality. In this work we show that a simple 2-layer network, without any
training data or knowledge of the noise distribution, can enable high-quality
image denoising at low computational cost. Our approach is motivated by
Noise2Noise and Neighbor2Neighbor and works well for denoising pixel-wise
independent noise. Our experiments on artificial, real-world camera, and
microscope noise show that our method termed ZS-N2N (Zero Shot Noise2Noise)
often outperforms existing dataset-free methods at a reduced cost, making it
suitable for use cases with scarce data availability and limited compute
resources. A demo of our implementation including our code and hyperparameters
can be found in the following colab notebook:
https://colab.research.google.com/drive/1i82nyizTdszyHkaHBuKPbWnTzao8HF9b
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards End-to-End Generative Modeling of Long Videos with
  Memory-Efficient Bidirectional <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11251v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11251v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaehoon Yoo, Semin Kim, Doyup Lee, Chiheon Kim, Seunghoon Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autoregressive transformers have shown remarkable success in video
generation. However, the transformers are prohibited from directly learning the
long-term dependency in videos due to the quadratic complexity of
self-attention, and inherently suffering from slow inference time and error
propagation due to the autoregressive process. In this paper, we propose
Memory-efficient Bidirectional Transformer (MeBT) for end-to-end learning of
long-term dependency in videos and fast inference. Based on recent advances in
bidirectional transformers, our method learns to decode the entire
spatio-temporal volume of a video in parallel from partially observed patches.
The proposed transformer achieves a linear time complexity in both encoding and
decoding, by projecting observable context tokens into a fixed number of latent
tokens and conditioning them to decode the masked tokens through the
cross-attention. Empowered by linear complexity and bidirectional modeling, our
method demonstrates significant improvement over the autoregressive
Transformers for generating moderately long videos in both quality and speed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Augment and Criticize: Exploring Informative Samples for Semi-Supervised
  Monocular 3D Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11243v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11243v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenyu Li, Zhipeng Zhang, Heng Fan, Yuan He, Ke Wang, Xianming Liu, Junjun Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we improve the challenging monocular 3D object detection
problem with a general semi-supervised framework. Specifically, having observed
that the bottleneck of this task lies in lacking reliable and informative
samples to train the detector, we introduce a novel, simple, yet effective
`Augment and Criticize' framework that explores abundant informative samples
from unlabeled data for learning more robust detection models. In the `Augment'
stage, we present the Augmentation-based Prediction aGgregation (APG), which
aggregates detections from various automatically learned augmented views to
improve the robustness of pseudo label generation. Since not all pseudo labels
from APG are beneficially informative, the subsequent `Criticize' phase is
presented. In particular, we introduce the Critical Retraining Strategy (CRS)
that, unlike simply filtering pseudo labels using a fixed threshold (e.g.,
classification score) as in 2D semi-supervised tasks, leverages a learnable
network to evaluate the contribution of unlabeled images at different training
timestamps. This way, the noisy samples prohibitive to model evolution could be
effectively suppressed. To validate our framework, we apply it to MonoDLE and
MonoFlex. The two new detectors, dubbed 3DSeMo_DLE and 3DSeMo_FLEX, achieve
state-of-the-art results with remarkable improvements for over 3.5% AP_3D/BEV
(Easy) on KITTI, showing its effectiveness and generality. Code and models will
be released.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Make Landscape Flatter in Differentially Private Federated Learning <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11242v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11242v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Shi, Yingqi Liu, Kang Wei, Li Shen, Xueqian Wang, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To defend the inference attacks and mitigate the sensitive information
leakages in Federated Learning (FL), client-level Differentially Private FL
(DPFL) is the de-facto standard for privacy protection by clipping local
updates and adding random noise. However, existing DPFL methods tend to make a
sharper loss landscape and have poorer weight perturbation robustness,
resulting in severe performance degradation. To alleviate these issues, we
propose a novel DPFL algorithm named DP-FedSAM, which leverages gradient
perturbation to mitigate the negative impact of DP. Specifically, DP-FedSAM
integrates Sharpness Aware Minimization (SAM) optimizer to generate local
flatness models with better stability and weight perturbation robustness, which
results in the small norm of local updates and robustness to DP noise, thereby
improving the performance. From the theoretical perspective, we analyze in
detail how DP-FedSAM mitigates the performance degradation induced by DP.
Meanwhile, we give rigorous privacy guarantees with R\'enyi DP and present the
sensitivity analysis of local updates. At last, we empirically confirm that our
algorithm achieves state-of-the-art (SOTA) performance compared with existing
SOTA baselines in DPFL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR2023, 18 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Training Invertible Neural Networks as Autoencoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11239v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11239v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        The-Gia Leo Nguyen, Lynton Ardizzone, Ullrich Koethe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autoencoders are able to learn useful data representations in an unsupervised
matter and have been widely used in various machine learning and computer
vision tasks. In this work, we present methods to train Invertible Neural
Networks (INNs) as (variational) autoencoders which we call INN (variational)
autoencoders. Our experiments on MNIST, CIFAR and CelebA show that for low
bottleneck sizes our INN autoencoder achieves results similar to the classical
autoencoder. However, for large bottleneck sizes our INN autoencoder
outperforms its classical counterpart. Based on the empirical results, we
hypothesize that INN autoencoders might not have any intrinsic information loss
and thereby are not bounded to a maximal number of layers (depth) after which
only suboptimal results can be achieved.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Conference Paper at GCPR2019</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FullFormer: Generating Shapes Inside Shapes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11235v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11235v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tejaswini Medi, Jawad Tayyub, Muhammad Sarmad, Frank Lindseth, Margret Keuper
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Implicit generative models have been widely employed to model 3D data and
have recently proven to be successful in encoding and generating high-quality
3D shapes. This work builds upon these models and alleviates current
limitations by presenting the first implicit generative model that facilitates
the generation of complex 3D shapes with rich internal geometric details. To
achieve this, our model uses unsigned distance fields to represent nested 3D
surfaces allowing learning from non-watertight mesh data. We propose a
transformer-based autoregressive model for 3D shape generation that leverages
context-rich tokens from vector quantized shape embeddings. The generated
tokens are decoded into an unsigned distance field which is rendered into a
novel 3D shape exhibiting a rich internal structure. We demonstrate that our
model achieves state-of-the-art point cloud generation results on popular
classes of 'Cars', 'Planes', and 'Chairs' of the ShapeNet dataset.
Additionally, we curate a dataset that exclusively comprises shapes with
realistic internal details from the `Cars' class of ShapeNet and demonstrate
our method's efficacy in generating these shapes with internal geometry.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bimodal SegNet: Instance Segmentation Fusing Events and RGB Frames for
  Robotic Grasping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11228v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11228v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanket Kachole, Xiaoqian Huang, Fariborz Baghaei Naeini, Rajkumar Muthusamy, Dimitrios Makris, Yahya Zweiri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object segmentation for robotic grasping under dynamic conditions often faces
challenges such as occlusion, low light conditions, motion blur and object size
variance. To address these challenges, we propose a Deep Learning network that
fuses two types of visual signals, event-based data and RGB frame data. The
proposed Bimodal SegNet network has two distinct encoders, one for each signal
input and a spatial pyramidal pooling with atrous convolutions. Encoders
capture rich contextual information by pooling the concatenated features at
different resolutions while the decoder obtains sharp object boundaries. The
evaluation of the proposed method undertakes five unique image degradation
challenges including occlusion, blur, brightness, trajectory and scale variance
on the Event-based Segmentation (ESD) Dataset. The evaluation results show a
6-10\% segmentation accuracy improvement over state-of-the-art methods in terms
of mean intersection over the union and pixel accuracy. The model code is
available at https://github.com/sanket0707/Bimodal-SegNet.git
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 Pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HiFace: High-Fidelity 3D Face Reconstruction by Learning Static and
  Dynamic Details 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11225v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11225v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zenghao Chai, Tianke Zhang, Tianyu He, Xu Tan, Tadas Baltrusaitis, HsiangTao Wu, Runnan Li, Sheng Zhao, Chun Yuan, Jiang Bian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Morphable Models (3DMMs) demonstrate great potential for reconstructing
faithful and animatable 3D facial surfaces from a single image. The facial
surface is influenced by the coarse shape, as well as the static detail (e,g.,
person-specific appearance) and dynamic detail (e.g., expression-driven
wrinkles). Previous work struggles to decouple the static and dynamic details
through image-level supervision, leading to reconstructions that are not
realistic. In this paper, we aim at high-fidelity 3D face reconstruction and
propose HiFace to explicitly model the static and dynamic details.
Specifically, the static detail is modeled as the linear combination of a
displacement basis, while the dynamic detail is modeled as the linear
interpolation of two displacement maps with polarized expressions. We exploit
several loss functions to jointly learn the coarse shape and fine details with
both synthetic and real-world datasets, which enable HiFace to reconstruct
high-fidelity 3D shapes with animatable details. Extensive quantitative and
qualitative experiments demonstrate that HiFace presents state-of-the-art
reconstruction quality and faithfully recovers both the static and dynamic
details. Our project page can be found at https://project-hiface.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Tech report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cascaded Latent Diffusion Models for High-Resolution Chest X-ray
  Synthesis <span class="chip">PAKDD 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11224v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11224v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tobias Weber, Michael Ingrisch, Bernd Bischl, David Rügamer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While recent advances in large-scale foundational models show promising
results, their application to the medical domain has not yet been explored in
detail. In this paper, we progress into the realms of large-scale modeling in
medical synthesis by proposing Cheff - a foundational cascaded latent diffusion
model, which generates highly-realistic chest radiographs providing
state-of-the-art quality on a 1-megapixel scale. We further propose MaCheX,
which is a unified interface for public chest datasets and forms the largest
open collection of chest X-rays up to date. With Cheff conditioned on
radiological reports, we further guide the synthesis process over text prompts
and unveil the research area of report-to-chest-X-ray generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at PAKDD 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NeTO:Neural Reconstruction of Transparent Objects with Self-Occlusion
  Aware Refraction-Tracing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11219v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11219v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zongcheng Li, Xiaoxiao Long, Yusen Wang, Tuo Cao, Wenping Wang, Fei Luo, Chunxia Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel method, called NeTO, for capturing 3D geometry of solid
transparent objects from 2D images via volume rendering. Reconstructing
transparent objects is a very challenging task, which is ill-suited for
general-purpose reconstruction techniques due to the specular light transport
phenomena. Although existing refraction-tracing based methods, designed
specially for this task, achieve impressive results, they still suffer from
unstable optimization and loss of fine details, since the explicit surface
representation they adopted is difficult to be optimized, and the
self-occlusion problem is ignored for refraction-tracing. In this paper, we
propose to leverage implicit Signed Distance Function (SDF) as surface
representation, and optimize the SDF field via volume rendering with a
self-occlusion aware refractive ray tracing. The implicit representation
enables our method to be capable of reconstructing high-quality reconstruction
even with a limited set of images, and the self-occlusion aware strategy makes
it possible for our method to accurately reconstruct the self-occluded regions.
Experiments show that our method achieves faithful reconstruction results and
outperforms prior works by a large margin. Visit our project page at
\url{https://www.xxlong.site/NeTO/}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>www.xxlong.site/NeTO/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Inverse problem regularization with hierarchical variational
  autoencoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11217v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11217v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jean Prost, Antoine Houdard, Andrés Almansa, Nicolas Papadakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose to regularize ill-posed inverse problems using a
deep hierarchical variational autoencoder (HVAE) as an image prior. The
proposed method synthesizes the advantages of i) denoiser-based Plug \& Play
approaches and ii) generative model based approaches to inverse problems.
First, we exploit VAE properties to design an efficient algorithm that benefits
from convergence guarantees of Plug-and-Play (PnP) methods. Second, our
approach is not restricted to specialized datasets and the proposed PnP-HVAE
model is able to solve image restoration problems on natural images of any
size. Our experiments show that the proposed PnP-HVAE method is competitive
with both SOTA denoiser-based PnP approaches, and other SOTA restoration
methods based on generative models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Generate 3D Representations of Building Roofs Using
  Single-View Aerial Imagery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11215v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11215v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maxim Khomiakov, Alejandro Valverde Mahou, Alba Reinders Sánchez, Jes Frellsen, Michael Riis Andersen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel pipeline for learning the conditional distribution of a
building roof mesh given pixels from an aerial image, under the assumption that
roof geometry follows a set of regular patterns. Unlike alternative methods
that require multiple images of the same object, our approach enables
estimating 3D roof meshes using only a single image for predictions. The
approach employs the PolyGen, a deep generative transformer architecture for 3D
meshes. We apply this model in a new domain and investigate the sensitivity of
the image resolution. We propose a novel metric to evaluate the performance of
the inferred meshes, and our results show that the model is robust even at
lower resolutions, while qualitatively producing realistic representations for
out-of-distribution samples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Copyright 2023 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accurate Detection of Mediastinal Lesions with nnDetection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11214v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11214v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Baumgartner, Peter M. Full, Klaus H. Maier-Hein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The accurate detection of mediastinal lesions is one of the rarely explored
medical object detection problems. In this work, we applied a modified version
of the self-configuring method nnDetection to the Mediastinal Lesion Analysis
(MELA) Challenge 2022. By incorporating automatically generated pseudo masks,
training high capacity models with large patch sizes in a multi GPU setup and
an adapted augmentation scheme to reduce localization errors caused by
rotations, our method achieved an excellent FROC score of 0.9922 at IoU 0.10
and 0.9880 at IoU 0.3 in our cross-validation experiments. The submitted
ensemble ranked third in the competition with a FROC score of 0.9897 on the
MELA challenge leaderboard.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in "Lesion Segmentation in Surgical and Diagnostic
  Applications"</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Less is More: Reducing Task and Model Complexity for 3D Point Cloud
  Semantic Segmentation <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11203v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11203v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Li, Hubert P. H. Shum, Toby P. Breckon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Whilst the availability of 3D LiDAR point cloud data has significantly grown
in recent years, annotation remains expensive and time-consuming, leading to a
demand for semi-supervised semantic segmentation methods with application
domains such as autonomous driving. Existing work very often employs relatively
large segmentation backbone networks to improve segmentation accuracy, at the
expense of computational costs. In addition, many use uniform sampling to
reduce ground truth data requirements for learning needed, often resulting in
sub-optimal performance. To address these issues, we propose a new pipeline
that employs a smaller architecture, requiring fewer ground-truth annotations
to achieve superior segmentation accuracy compared to contemporary approaches.
This is facilitated via a novel Sparse Depthwise Separable Convolution module
that significantly reduces the network parameter count while retaining overall
task performance. To effectively sub-sample our training data, we propose a new
Spatio-Temporal Redundant Frame Downsampling (ST-RFD) method that leverages
knowledge of sensor motion within the environment to extract a more diverse
subset of training data frame samples. To leverage the use of limited annotated
data samples, we further propose a soft pseudo-label method informed by LiDAR
reflectivity. Our method outperforms contemporary semi-supervised work in terms
of mIoU, using less labeled data, on the SemanticKITTI (59.5@5%) and
ScribbleKITTI (58.1@5%) benchmark datasets, based on a 2.3x reduction in model
parameters and 641x fewer multiply-add operations whilst also demonstrating
significant performance improvement on limited training data (i.e., Less is
More).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2023; Code at https://github.com/l1997i/lim3d; 11
  pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Architecture, <span class="highlight-title">Dataset</span> and Model-Scale Agnostic Data-free Meta-Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11183v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11183v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixuan Hu, Li Shen, Zhenyi Wang, Tongliang Liu, Chun Yuan, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of data-free meta-learning is to learn useful prior knowledge from a
collection of pre-trained models without accessing their training data.
However, existing works only solve the problem in parameter space, which (i)
ignore the fruitful data knowledge contained in the pre-trained models; (ii)
can not scale to large-scale pre-trained models; (iii) can only meta-learn
pre-trained models with the same network architecture. To address those issues,
we propose a unified framework, dubbed PURER, which contains: (1) ePisode
cUrriculum inveRsion (ECI) during data-free meta training; and (2) invErsion
calibRation following inner loop (ICFIL) during meta testing. During meta
training, we propose ECI to perform pseudo episode training for learning to
adapt fast to new unseen tasks. Specifically, we progressively synthesize a
sequence of pseudo episodes by distilling the training data from each
pre-trained model. The ECI adaptively increases the difficulty level of pseudo
episodes according to the real-time feedback of the meta model. We formulate
the optimization process of meta training with ECI as an adversarial form in an
end-to-end manner. During meta testing, we further propose a simple
plug-and-play supplement-ICFIL-only used during meta testing to narrow the gap
between meta training and meta testing task distribution. Extensive experiments
in various real-world scenarios show the superior performance of ours.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Correctable and Adaptable Inference for Generalizable Human Pose
  Estimation <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11180v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11180v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhehan Kan, Shuoshuo Chen, Ce Zhang, Yushun Tang, Zhihai He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A central challenge in human pose estimation, as well as in many other
machine learning and prediction tasks, is the generalization problem. The
learned network does not have the capability to characterize the prediction
error, generate feedback information from the test sample, and correct the
prediction error on the fly for each individual test sample, which results in
degraded performance in generalization. In this work, we introduce a
self-correctable and adaptable inference (SCAI) method to address the
generalization challenge of network prediction and use human pose estimation as
an example to demonstrate its effectiveness and performance. We learn a
correction network to correct the prediction result conditioned by a fitness
feedback error. This feedback error is generated by a learned fitness feedback
network which maps the prediction result to the original input domain and
compares it against the original input. Interestingly, we find that this
self-referential feedback error is highly correlated with the actual prediction
error. This strong correlation suggests that we can use this error as feedback
to guide the correction process. It can be also used as a loss function to
quickly adapt and optimize the correction network during the inference process.
Our extensive experimental results on human pose estimation demonstrate that
the proposed SCAI method is able to significantly improve the generalization
capability and performance of human pose estimation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Integration of Radiomics and Tumor Biomarkers in Interpretable Machine
  Learning Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11177v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11177v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lennart Brocki, Neo Christopher Chung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the unprecedented performance of deep neural networks (DNNs) in
computer vision, their practical application in the diagnosis and prognosis of
cancer using medical imaging has been limited. One of the critical challenges
for integrating diagnostic DNNs into radiological and oncological applications
is their lack of interpretability, preventing clinicians from understanding the
model predictions. Therefore, we study and propose the integration of
expert-derived radiomics and DNN-predicted biomarkers in interpretable
classifiers which we call ConRad, for computerized tomography (CT) scans of
lung cancer. Importantly, the tumor biomarkers are predicted from a concept
bottleneck model (CBM) such that once trained, our ConRad models do not require
labor-intensive and time-consuming biomarkers. In our evaluation and practical
application, the only input to ConRad is a segmented CT scan. The proposed
model is compared to convolutional neural networks (CNNs) which act as a black
box classifier. We further investigated and evaluated all combinations of
radiomics, predicted biomarkers and CNN features in five different classifiers.
We found the ConRad models using non-linear SVM and the logistic regression
with the Lasso outperform others in five-fold cross-validation, although we
highlight that interpretability of ConRad is its primary advantage. The Lasso
is used for feature selection, which substantially reduces the number of
non-zero weights while increasing the accuracy. Overall, the proposed ConRad
model combines CBM-derived biomarkers and radiomics features in an
interpretable ML model which perform excellently for the lung nodule malignancy
classification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Computationally Budgeted Continual Learning: What Does Matter? <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11165v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11165v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ameya Prabhu, Hasan Abed Al Kader Hammoud, Puneet Dokania, Philip H. S. Torr, Ser-Nam Lim, Bernard Ghanem, Adel Bibi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual Learning (CL) aims to sequentially train models on streams of
incoming data that vary in distribution by preserving previous knowledge while
adapting to new data. Current CL literature focuses on restricted access to
previously seen data, while imposing no constraints on the computational budget
for training. This is unreasonable for applications in-the-wild, where systems
are primarily constrained by computational and time budgets, not storage. We
revisit this problem with a large-scale benchmark and analyze the performance
of traditional CL approaches in a compute-constrained setting, where effective
memory samples used in training can be implicitly restricted as a consequence
of limited computation. We conduct experiments evaluating various CL sampling
strategies, distillation losses, and partial fine-tuning on two large-scale
datasets, namely ImageNet2K and Continual Google Landmarks V2 in data
incremental, class incremental, and time incremental settings. Through
extensive experiments amounting to a total of over 1500 GPU-hours, we find
that, under compute-constrained setting, traditional CL approaches, with no
exception, fail to outperform a simple minimal baseline that samples uniformly
from memory. Our conclusions are consistent in a different number of stream
time steps, e.g., 20 to 200, and under several computational budgets. This
suggests that most existing CL methods are particularly too computationally
expensive for realistic budgeted deployment. Code for this project is available
at: https://github.com/drimpossible/BudgetCL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Appearing in CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Picture that Sketch: Photorealistic Image Generation from Abstract
  Sketches <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11162v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11162v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Subhadeep Koley, Ayan Kumar Bhunia, Aneeshan Sain, Pinaki Nath Chowdhury, Tao Xiang, Yi-Zhe song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given an abstract, deformed, ordinary sketch from untrained amateurs like you
and me, this paper turns it into a photorealistic image - just like those shown
in Fig. 1(a), all non-cherry-picked. We differ significantly from prior art in
that we do not dictate an edgemap-like sketch to start with, but aim to work
with abstract free-hand human sketches. In doing so, we essentially democratise
the sketch-to-photo pipeline, "picturing" a sketch regardless of how good you
sketch. Our contribution at the outset is a decoupled encoder-decoder training
paradigm, where the decoder is a StyleGAN trained on photos only. This
importantly ensures that generated results are always photorealistic. The rest
is then all centred around how best to deal with the abstraction gap between
sketch and photo. For that, we propose an autoregressive sketch mapper trained
on sketch-photo pairs that maps a sketch to the StyleGAN latent space. We
further introduce specific designs to tackle the abstract nature of human
sketches, including a fine-grained discriminative loss on the back of a trained
sketch-photo retrieval model, and a partial-aware sketch augmentation strategy.
Finally, we showcase a few downstream tasks our generation model enables,
amongst them is showing how fine-grained sketch-based image retrieval, a
well-studied problem in the sketch community, can be reduced to an image
(generated) to image retrieval task, surpassing state-of-the-arts. We put
forward generated results in the supplementary for everyone to scrutinise.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in CVPR 2023. Project page available at
  https://subhadeepkoley.github.io/PictureThatSketch</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AnimeDiffusion: Anime Face Line Drawing Colorization via Diffusion
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11137v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11137v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Cao, Xiangqiao Meng, P. Y. Mok, Xueting Liu, Tong-Yee Lee, Ping Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is a time-consuming and tedious work for manually colorizing anime line
drawing images, which is an essential stage in cartoon animation creation
pipeline. Reference-based line drawing colorization is a challenging task that
relies on the precise cross-domain long-range dependency modelling between the
line drawing and reference image. Existing learning methods still utilize
generative adversarial networks (GANs) as one key module of their model
architecture. In this paper, we propose a novel method called AnimeDiffusion
using diffusion models that performs anime face line drawing colorization
automatically. To the best of our knowledge, this is the first diffusion model
tailored for anime content creation. In order to solve the huge training
consumption problem of diffusion models, we design a hybrid training strategy,
first pre-training a diffusion model with classifier-free guidance and then
fine-tuning it with image reconstruction guidance. We find that with a few
iterations of fine-tuning, the model shows wonderful colorization performance,
as illustrated in Fig. 1. For training AnimeDiffusion, we conduct an anime face
line drawing colorization benchmark dataset, which contains 31696 training data
and 579 testing data. We hope this dataset can fill the gap of no available
high resolution anime face dataset for colorization method evaluation. Through
multiple quantitative metrics evaluated on our dataset and a user study, we
demonstrate AnimeDiffusion outperforms state-of-the-art GANs-based models for
anime face line drawing colorization. We also collaborate with professional
artists to test and apply our AnimeDiffusion for their creation work. We
release our code on https://github.com/xq-meng/AnimeDiffusion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TWINS: A Fine-Tuning Framework for Improved Transferability of
  Adversarial Robustness and Generalization <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11135v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11135v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziquan Liu, Yi Xu, Xiangyang Ji, Antoni B. Chan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have seen the ever-increasing importance of pre-trained models
and their downstream training in deep learning research and applications. At
the same time, the defense for adversarial examples has been mainly
investigated in the context of training from random initialization on simple
classification tasks. To better exploit the potential of pre-trained models in
adversarial robustness, this paper focuses on the fine-tuning of an
adversarially pre-trained model in various classification tasks. Existing
research has shown that since the robust pre-trained model has already learned
a robust feature extractor, the crucial question is how to maintain the
robustness in the pre-trained model when learning the downstream task. We study
the model-based and data-based approaches for this goal and find that the two
common approaches cannot achieve the objective of improving both generalization
and adversarial robustness. Thus, we propose a novel statistics-based approach,
Two-WIng NormliSation (TWINS) fine-tuning framework, which consists of two
neural networks where one of them keeps the population means and variances of
pre-training data in the batch normalization layers. Besides the robust
information transfer, TWINS increases the effective learning rate without
hurting the training stability since the relationship between a weight norm and
its gradient norm in standard batch normalization layer is broken, resulting in
a faster escape from the sub-optimal initialization and alleviating the robust
overfitting. Finally, TWINS is shown to be effective on a wide range of image
classification datasets in terms of both generalization and robustness. Our
code is available at https://github.com/ziquanliu/CVPR2023-TWINS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep learning automated quantification of lung disease in pulmonary
  hypertension on CT pulmonary angiography: A preliminary clinical study with
  external validation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11130v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11130v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael J. Sharkey, Krit Dwivedi, Samer Alabed, Andrew J. Swift
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Purpose: Lung disease assessment in precapillary pulmonary hypertension (PH)
is essential for appropriate patient management. This study aims to develop an
artificial intelligence (AI) deep learning model for lung texture
classification in CT Pulmonary Angiography (CTPA), and evaluate its correlation
with clinical assessment methods.
  Materials and Methods: In this retrospective study with external validation,
122 patients with pre-capillary PH were used to train (n=83), validate (n=17)
and test (n=10 internal test, n=12 external test) a patch based DenseNet-121
classification model. "Normal", "Ground glass", "Ground glass with
reticulation", "Honeycombing", and "Emphysema" were classified as per the
Fleishner Society glossary of terms. Ground truth classes were segmented by two
radiologists with patches extracted from the labelled regions. Proportion of
lung volume for each texture was calculated by classifying patches throughout
the entire lung volume to generate a coarse texture classification mapping
throughout the lung parenchyma. AI output was assessed against diffusing
capacity of carbon monoxide (DLCO) and specialist radiologist reported disease
severity.
  Results: Micro-average AUCs for the validation, internal test, and external
test were 0.92, 0.95, and 0.94, respectively. The model had consistent
performance across parenchymal textures, demonstrated strong correlation with
diffusing capacity of carbon monoxide (DLCO), and showed good correspondence
with disease severity reported by specialist radiologists.
  Conclusion: The classification model demonstrates excellent performance on
external validation. The clinical utility of its output has been demonstrated.
This objective, repeatable measure of disease severity can aid in patient
management in adjunct to radiological reporting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 3 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MT-SNN: Enhance Spiking Neural Network with Multiple Thresholds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11127v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11127v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoting Wang, Yanxiang Zhang, Yongzhe Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spiking neural networks (SNNs), as a biology-inspired method mimicking the
spiking nature of brain neurons, is a promising energy-efficient alternative to
the traditional artificial neural networks (ANNs). The energy saving of SNNs is
mainly from multiplication free property brought by binarized intermediate
activations. In this paper, we proposed a Multiple Threshold (MT) approach to
alleviate the precision loss brought by the binarized activations, such that
SNNs can reach higher accuracy at fewer steps. We evaluate the approach on
CIFAR10, CIFAR100 and DVS-CIFAR10, and demonstrate that MT can promote SNNs
extensively, especially at early steps. For example, With MT,
Parametric-Leaky-Integrate-Fire(PLIF) based VGG net can even outperform the ANN
counterpart with 1 step.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robustifying Token Attention for Vision <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11126v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11126v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yong Guo, David Stutz, Bernt Schiele
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the success of vision transformers (ViTs), they still suffer from
significant drops in accuracy in the presence of common corruptions, such as
noise or blur. Interestingly, we observe that the attention mechanism of ViTs
tends to rely on few important tokens, a phenomenon we call token overfocusing.
More critically, these tokens are not robust to corruptions, often leading to
highly diverging attention patterns. In this paper, we intend to alleviate this
overfocusing issue and make attention more stable through two general
techniques: First, our Token-aware Average Pooling (TAP) module encourages the
local neighborhood of each token to take part in the attention mechanism.
Specifically, TAP learns average pooling schemes for each token such that the
information of potentially important tokens in the neighborhood can adaptively
be taken into account. Second, we force the output tokens to aggregate
information from a diverse set of input tokens rather than focusing on just a
few by using our Attention Diversification Loss (ADL). We achieve this by
penalizing high cosine similarity between the attention vectors of different
tokens. In experiments, we apply our methods to a wide range of transformer
architectures and improve robustness significantly. For example, we improve
corruption robustness on ImageNet-C by 2.4% while simultaneously improving
accuracy by 0.4% based on state-of-the-art robust architecture FAN. Also, when
finetuning on semantic segmentation tasks, we improve robustness on
CityScapes-C by 2.4% and ACDC by 3.1%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 figures and 5 tables in the main paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Positional Diffusion: Ordering Unordered Sets with Diffusion
  Probabilistic Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11120v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11120v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Giuliari, Gianluca Scarpellini, Stuart James, Yiming Wang, Alessio Del Bue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Positional reasoning is the process of ordering unsorted parts contained in a
set into a consistent structure. We present Positional Diffusion, a
plug-and-play graph formulation with Diffusion Probabilistic Models to address
positional reasoning. We use the forward process to map elements' positions in
a set to random positions in a continuous space. Positional Diffusion learns to
reverse the noising process and recover the original positions through an
Attention-based Graph Neural Network. We conduct extensive experiments with
benchmark datasets including two puzzle datasets, three sentence ordering
datasets, and one visual storytelling dataset, demonstrating that our method
outperforms long-lasting research on puzzle solving with up to +18% compared to
the second-best deep learning method, and performs on par against the
state-of-the-art methods on sentence ordering and visual storytelling. Our work
highlights the suitability of diffusion models for ordering problems and
proposes a novel formulation and method for solving various ordering tasks.
Project website at https://iit-pavis.github.io/Positional_Diffusion/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SeiT: Storage-Efficient Vision Training with Tokens Using 1% of Pixel
  Storage 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11114v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11114v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Song Park, Sanghyuk Chun, Byeongho Heo, Wonjae Kim, Sangdoo Yun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We need billion-scale images to achieve more generalizable and
ground-breaking vision models, as well as massive dataset storage to ship the
images (e.g., the LAION-4B dataset needs 240TB storage space). However, it has
become challenging to deal with unlimited dataset storage with limited storage
infrastructure. A number of storage-efficient training methods have been
proposed to tackle the problem, but they are rarely scalable or suffer from
severe damage to performance. In this paper, we propose a storage-efficient
training strategy for vision classifiers for large-scale datasets (e.g.,
ImageNet) that only uses 1024 tokens per instance without using the raw level
pixels; our token storage only needs <1% of the original JPEG-compressed raw
pixels. We also propose token augmentations and a Stem-adaptor module to make
our approach able to use the same architecture as pixel-based approaches with
only minimal modifications on the stem layer and the carefully tuned
optimization settings. Our experimental results on ImageNet-1k show that our
method significantly outperforms other storage-efficient training methods with
a large gap. We further show the effectiveness of our method in other practical
scenarios, storage-efficient pre-training, and continual learning. Code is
available at https://github.com/naver-ai/seit
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>First two authors contributed equally; 15 pages, 1.1MB</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ I2Edit: Towards Multi-turn Interactive Image Editing via Dialogue 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11108v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11108v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xing Cui, Zekun Li, Peipei Li, Yibo Hu, Hailin Shi, Zhaofeng He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although there have been considerable research efforts on controllable facial
image editing, the desirable interactive setting where the users can interact
with the system to adjust their requirements dynamically hasn't been well
explored. This paper focuses on facial image editing via dialogue and
introduces a new benchmark dataset, Multi-turn Interactive Image Editing
(I2Edit), for evaluating image editing quality and interaction ability in
real-world interactive facial editing scenarios. The dataset is constructed
upon the CelebA-HQ dataset with images annotated with a multi-turn dialogue
that corresponds to the user editing requirements. I2Edit is challenging, as it
needs to 1) track the dynamically updated user requirements and edit the images
accordingly, as well as 2) generate the appropriate natural language response
to communicate with the user. To address these challenges, we propose a
framework consisting of a dialogue module and an image editing module. The
former is for user edit requirements tracking and generating the corresponding
indicative responses, while the latter edits the images conditioned on the
tracked user edit requirements. In contrast to previous works that simply treat
multi-turn interaction as a sequence of single-turn interactions, we extract
the user edit requirements from the whole dialogue history instead of the
current single turn. The extracted global user edit requirements enable us to
directly edit the input raw image to avoid error accumulation and attribute
forgetting issues. Extensive quantitative and qualitative experiments on the
I2Edit dataset demonstrate the advantage of our proposed framework over the
previous single-turn methods. We believe our new dataset could serve as a
valuable resource to push forward the exploration of real-world, complex
interactive image editing. Code and data will be made public.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Coreset Sampling from Open-Set for Fine-Grained <span class="highlight-title">Self-Supervised</span> Learning <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11101v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11101v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sungnyun Kim, Sangmin Bae, Se-Young Yun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning in general domains has constantly been extended to
domain-specific tasks requiring the recognition of fine-grained
characteristics. However, real-world applications for fine-grained tasks suffer
from two challenges: a high reliance on expert knowledge for annotation and
necessity of a versatile model for various downstream tasks in a specific
domain (e.g., prediction of categories, bounding boxes, or pixel-wise
annotations). Fortunately, the recent self-supervised learning (SSL) is a
promising approach to pretrain a model without annotations, serving as an
effective initialization for any downstream tasks. Since SSL does not rely on
the presence of annotation, in general, it utilizes the large-scale unlabeled
dataset, referred to as an open-set. In this sense, we introduce a novel
Open-Set Self-Supervised Learning problem under the assumption that a
large-scale unlabeled open-set is available, as well as the fine-grained target
dataset, during a pretraining phase. In our problem setup, it is crucial to
consider the distribution mismatch between the open-set and target dataset.
Hence, we propose SimCore algorithm to sample a coreset, the subset of an
open-set that has a minimum distance to the target dataset in the latent space.
We demonstrate that SimCore significantly improves representation learning
performance through extensive experimental settings, including eleven
fine-grained datasets and seven open-sets in various downstream tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Multi-Task Deep Learning Approach for Sensor-based Human Activity
  Recognition and Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11100v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11100v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Furong Duan, Tao Zhu, Jinqiang Wang, Liming Chen, Huansheng Ning, Yaping Wan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sensor-based human activity segmentation and recognition are two important
and challenging problems in many real-world applications and they have drawn
increasing attention from the deep learning community in recent years. Most of
the existing deep learning works were designed based on pre-segmented sensor
streams and they have treated activity segmentation and recognition as two
separate tasks. In practice, performing data stream segmentation is very
challenging. We believe that both activity segmentation and recognition may
convey unique information which can complement each other to improve the
performance of the two tasks. In this paper, we firstly proposes a new
multitask deep neural network to solve the two tasks simultaneously. The
proposed neural network adopts selective convolution and features multiscale
windows to segment activities of long or short time durations. First, multiple
windows of different scales are generated to center on each unit of the feature
sequence. Then, the model is trained to predict, for each window, the activity
class and the offset to the true activity boundaries. Finally, overlapping
windows are filtered out by non-maximum suppression, and adjacent windows of
the same activity are concatenated to complete the segmentation task. Extensive
experiments were conducted on eight popular benchmarking datasets, and the
results show that our proposed method outperforms the state-of-the-art methods
both for activity recognition and segmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A closer look at the training dynamics of knowledge distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11098v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11098v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roy Miles, Krystian Mikolajczyk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we revisit the efficacy of knowledge distillation as a function
matching and metric learning problem. In doing so we verify three important
design decisions, namely the normalisation, soft maximum function, and
projection layers as key ingredients. We theoretically show that the projector
implicitly encodes information on past examples, enabling relational gradients
for the student. We then show that the normalisation of representations is
tightly coupled with the training dynamics of this projector, which can have a
large impact on the students performance. Finally, we show that a simple soft
maximum function can be used to address any significant capacity gap problems.
Experimental results on various benchmark datasets demonstrate that using these
insights can lead to superior or comparable performance to state-of-the-art
knowledge distillation techniques, despite being much more computationally
efficient. In particular, we obtain these results across image classification
(CIFAR100 and ImageNet), object detection (COCO2017), and on more difficult
distillation objectives, such as training data efficient transformers, whereby
we attain a 77.2% top-1 accuracy with DeiT-Ti on ImageNet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scene Graph Based Fusion Network For Image-Text Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11090v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11090v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guoliang Wang, Yanlei Shang, Yong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A critical challenge to image-text retrieval is how to learn accurate
correspondences between images and texts. Most existing methods mainly focus on
coarse-grained correspondences based on co-occurrences of semantic objects,
while failing to distinguish the fine-grained local correspondences. In this
paper, we propose a novel Scene Graph based Fusion Network (dubbed SGFN), which
enhances the images'/texts' features through intra- and cross-modal fusion for
image-text retrieval. To be specific, we design an intra-modal hierarchical
attention fusion to incorporate semantic contexts, such as objects, attributes,
and relationships, into images'/texts' feature vectors via scene graphs, and a
cross-modal attention fusion to combine the contextual semantics and local
fusion via contextual vectors. Extensive experiments on public datasets
Flickr30K and MSCOCO show that our SGFN performs better than quite a few SOTA
image-text retrieval methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EmoTalk: Speech-driven emotional disentanglement for 3D face animation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11089v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11089v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqiao Peng, Haoyu Wu, Zhenbo Song, Hao Xu, Xiangyu Zhu, Hongyan Liu, Jun He, Zhaoxin Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech-driven 3D face animation aims to generate realistic facial expressions
that match the speech content and emotion. However, existing methods often
neglect emotional facial expressions or fail to disentangle them from speech
content. To address this issue, this paper proposes an end-to-end neural
network to disentangle different emotions in speech so as to generate rich 3D
facial expressions. Specifically, we introduce the emotion disentangling
encoder (EDE) to disentangle the emotion and content in the speech by
cross-reconstructed speech signals with different emotion labels. Then an
emotion-guided feature fusion decoder is employed to generate a 3D talking face
with enhanced emotion. The decoder is driven by the disentangled identity,
emotional, and content embeddings so as to generate controllable personal and
emotional styles. Finally, considering the scarcity of the 3D emotional talking
face data, we resort to the supervision of facial blendshapes, which enables
the reconstruction of plausible 3D faces from 2D emotional data, and contribute
a large-scale 3D emotional talking face dataset (3D-ETF) to train the network.
Our experiments and user studies demonstrate that our approach outperforms
state-of-the-art methods and exhibits more diverse facial movements. We
recommend watching the supplementary video:
https://ziqiaopeng.github.io/emotalk
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pluralistic Aging Diffusion Autoencoder 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11086v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11086v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peipei Li, Rui Wang, Huaibo Huang, Ran He, Zhaofeng He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Face aging is an ill-posed problem because multiple plausible aging patterns
may correspond to a given input. Most existing methods often produce one
deterministic estimation. This paper proposes a novel CLIP-driven Pluralistic
Aging Diffusion Autoencoder (PADA) to enhance the diversity of aging patterns.
First, we employ diffusion models to generate diverse low-level aging details
via a sequential denoising reverse process. Second, we present Probabilistic
Aging Embedding (PAE) to capture diverse high-level aging patterns, which
represents age information as probabilistic distributions in the common CLIP
latent space. A text-guided KL-divergence loss is designed to guide this
learning. Our method can achieve pluralistic face aging conditioned on
open-world aging texts and arbitrary unseen face images. Qualitative and
quantitative experiments demonstrate that our method can generate more diverse
and high-quality plausible aging results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Discovering Interpretable Directions in the Semantic Latent Space of
  Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11073v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11073v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        René Haas, Inbar Huberman-Spiegelglas, Rotem Mulayoff, Tomer Michaeli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Denoising Diffusion Models (DDMs) have emerged as a strong competitor to
Generative Adversarial Networks (GANs). However, despite their widespread use
in image synthesis and editing applications, their latent space is still not as
well understood. Recently, a semantic latent space for DDMs, coined
`$h$-space', was shown to facilitate semantic image editing in a way
reminiscent of GANs. The $h$-space is comprised of the bottleneck activations
in the DDM's denoiser across all timesteps of the diffusion process. In this
paper, we explore the properties of h-space and propose several novel methods
for finding meaningful semantic directions within it. We start by studying
unsupervised methods for revealing interpretable semantic directions in
pretrained DDMs. Specifically, we show that global latent directions emerge as
the principal components in the latent space. Additionally, we provide a novel
method for discovering image-specific semantic directions by spectral analysis
of the Jacobian of the denoiser w.r.t. the latent code. Next, we extend the
analysis by finding directions in a supervised fashion in unconditional DDMs.
We demonstrate how such directions can be found by relying on either a labeled
data set of real images or by annotating generated samples with a
domain-specific attribute classifier. We further show how to semantically
disentangle the found direction by simple linear projection. Our approaches are
applicable without requiring any architectural modifications, text-based
guidance, CLIP-based optimization, or model fine-tuning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boosting Semi-Supervised Learning by Exploiting All Unlabeled Data <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11066v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11066v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhao Chen, Xin Tan, Borui Zhao, Zhaowei Chen, Renjie Song, Jiajun Liang, Xuequan Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semi-supervised learning (SSL) has attracted enormous attention due to its
vast potential of mitigating the dependence on large labeled datasets. The
latest methods (e.g., FixMatch) use a combination of consistency regularization
and pseudo-labeling to achieve remarkable successes. However, these methods all
suffer from the waste of complicated examples since all pseudo-labels have to
be selected by a high threshold to filter out noisy ones. Hence, the examples
with ambiguous predictions will not contribute to the training phase. For
better leveraging all unlabeled examples, we propose two novel techniques:
Entropy Meaning Loss (EML) and Adaptive Negative Learning (ANL). EML
incorporates the prediction distribution of non-target classes into the
optimization objective to avoid competition with target class, and thus
generating more high-confidence predictions for selecting pseudo-label. ANL
introduces the additional negative pseudo-label for all unlabeled data to
leverage low-confidence examples. It adaptively allocates this label by
dynamically evaluating the top-k performance of the model. EML and ANL do not
introduce any additional parameter and hyperparameter. We integrate these
techniques with FixMatch, and develop a simple yet powerful framework called
FullMatch. Extensive experiments on several common SSL benchmarks
(CIFAR-10/100, SVHN, STL-10 and ImageNet) demonstrate that FullMatch exceeds
FixMatch by a large margin. Integrated with FlexMatch (an advanced
FixMatch-based framework), we achieve state-of-the-art performance. Source code
is at https://github.com/megvii-research/FullMatch.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Foresightful Dense Visual Affordance for Deformable Object
  Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11057v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11057v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruihai Wu, Chuanruo Ning, Hao Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding and manipulating deformable objects (e.g., ropes and fabrics)
is an essential yet challenging task with broad applications. Difficulties come
from complex states and dynamics, diverse configurations and high-dimensional
action space of deformable objects. Besides, the manipulation tasks usually
require multiple steps to accomplish, and greedy policies may easily lead to
local optimal states. Existing studies usually tackle this problem using
reinforcement learning or imitating expert demonstrations, with limitations in
modeling complex states or requiring hand-crafted expert policies. In this
paper, we study deformable object manipulation using dense visual affordance,
with generalization towards diverse states, and propose a novel kind of
foresightful dense affordance, which avoids local optima by estimating states'
values for long-term manipulation. We propose a framework for learning this
representation, with novel designs such as multi-stage stable learning and
efficient self-supervised data collection without experts. Experiments
demonstrate the superiority of our proposed foresightful dense affordance.
Project page: https://hyperplane-lab.github.io/DeformableAffordance
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parameter-Free Channel Attention for Image Classification and
  Super-Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11055v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11055v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Shi, Lingxiao Yang, Wangpeng An, Xiantong Zhen, Liuqing Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The channel attention mechanism is a useful technique widely employed in deep
convolutional neural networks to boost the performance for image processing
tasks, eg, image classification and image super-resolution. It is usually
designed as a parameterized sub-network and embedded into the convolutional
layers of the network to learn more powerful feature representations. However,
current channel attention induces more parameters and therefore leads to higher
computational costs. To deal with this issue, in this work, we propose a
Parameter-Free Channel Attention (PFCA) module to boost the performance of
popular image classification and image super-resolution networks, but
completely sweep out the parameter growth of channel attention. Experiments on
CIFAR-100, ImageNet, and DIV2K validate that our PFCA module improves the
performance of ResNet on image classification and improves the performance of
MSRResNet on image super-resolution tasks, respectively, while bringing little
growth of parameters and FLOPs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ContraNeRF: Generalizable Neural Radiance Fields for Synthetic-to-real
  Novel View Synthesis via Contrastive Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11052v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11052v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Yang, Lanqing Hong, Aoxue Li, Tianyang Hu, Zhenguo Li, Gim Hee Lee, Liwei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although many recent works have investigated generalizable NeRF-based novel
view synthesis for unseen scenes, they seldom consider the synthetic-to-real
generalization, which is desired in many practical applications. In this work,
we first investigate the effects of synthetic data in synthetic-to-real novel
view synthesis and surprisingly observe that models trained with synthetic data
tend to produce sharper but less accurate volume densities. For pixels where
the volume densities are correct, fine-grained details will be obtained.
Otherwise, severe artifacts will be produced. To maintain the advantages of
using synthetic data while avoiding its negative effects, we propose to
introduce geometry-aware contrastive learning to learn multi-view consistent
features with geometric constraints. Meanwhile, we adopt cross-view attention
to further enhance the geometry perception of features by querying features
across input views. Experiments demonstrate that under the synthetic-to-real
setting, our method can render images with higher quality and better
fine-grained details, outperforming existing generalizable novel view synthesis
methods in terms of PSNR, SSIM, and LPIPS. When trained on real data, our
method also achieves state-of-the-art results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting <span class="highlight-title">Transformer</span> for Point Cloud-based 3D Scene Graph Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11048v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11048v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changsheng Lv, Mengshi Qi, Xia Li, Zhengyuan Yang, Huadong Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose the semantic graph Transformer (SGT) for the 3D
scene graph generation. The task aims to parse a cloud point-based scene into a
semantic structural graph, with the core challenge of modeling the complex
global structure. Existing methods based on graph convolutional networks (GCNs)
suffer from the over-smoothing dilemma and could only propagate information
from limited neighboring nodes. In contrast, our SGT uses Transformer layers as
the base building block to allow global information passing, with two types of
proposed Transformer layers tailored for the 3D scene graph generation task.
Specifically, we introduce the graph embedding layer to best utilize the global
information in graph edges while maintaining comparable computation costs.
Additionally, we propose the semantic injection layer to leverage categorical
text labels and visual object knowledge. We benchmark our SGT on the
established 3DSSG benchmark and achieve a 35.9% absolute improvement in
relationship prediction's R@50 and an 80.40% boost on the subset with complex
scenes over the state-of-the-art. Our analyses further show SGT's superiority
in the long-tailed and zero-shot scenarios. We will release the code and model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Sparse to Precise: A Practical Editing Approach for Intracardiac
  Echocardiography Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11041v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11041v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed H. Shahin, Yan Zhuang, Noha El-Zehiry
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate and safe catheter ablation procedures for patients with atrial
fibrillation require precise segmentation of cardiac structures in Intracardiac
Echocardiography (ICE) imaging. Prior studies have suggested methods that
employ 3D geometry information from the ICE transducer to create a sparse ICE
volume by placing 2D frames in a 3D grid, enabling training of 3D segmentation
models. However, the resulting 3D masks from these models can be inaccurate and
may lead to serious clinical complications due to the sparse sampling in ICE
data, frames misalignment, and cardiac motion. To address this issue, we
propose an interactive editing framework that allows users to edit segmentation
output by drawing scribbles on a 2D frame. The user interaction is mapped to
the 3D grid and utilized to execute an editing step that modifies the
segmentation in the vicinity of the interaction while preserving the previous
segmentation away from the interaction. Furthermore, our framework accommodates
multiple edits to the segmentation output in a sequential manner without
compromising previous edits. This paper presents a novel loss function and a
novel evaluation metric specifically designed for editing. Results from
cross-validation and testing indicate that our proposed loss function
outperforms standard losses and training strategies in terms of segmentation
quality and following user input. Additionally, we show quantitatively and
qualitatively that subsequent edits do not compromise previous edits when using
our method, as opposed to standard segmentation losses. Overall, our approach
enhances the accuracy of the segmentation while avoiding undesired changes away
from user interactions and without compromising the quality of previously
edited regions, leading to better patient outcomes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking Robustness of 3D Object Detection to Common Corruptions in
  Autonomous Driving <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11040v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11040v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinpeng Dong, Caixin Kang, Jinlai Zhang, Zijian Zhu, Yikai Wang, Xiao Yang, Hang Su, Xingxing Wei, Jun Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D object detection is an important task in autonomous driving to perceive
the surroundings. Despite the excellent performance, the existing 3D detectors
lack the robustness to real-world corruptions caused by adverse weathers,
sensor noises, etc., provoking concerns about the safety and reliability of
autonomous driving systems. To comprehensively and rigorously benchmark the
corruption robustness of 3D detectors, in this paper we design 27 types of
common corruptions for both LiDAR and camera inputs considering real-world
driving scenarios. By synthesizing these corruptions on public datasets, we
establish three corruption robustness benchmarks -- KITTI-C, nuScenes-C, and
Waymo-C. Then, we conduct large-scale experiments on 24 diverse 3D object
detection models to evaluate their corruption robustness. Based on the
evaluation results, we draw several important findings, including: 1)
motion-level corruptions are the most threatening ones that lead to significant
performance drop of all models; 2) LiDAR-camera fusion models demonstrate
better robustness; 3) camera-only models are extremely vulnerable to image
corruptions, showing the indispensability of LiDAR point clouds. We release the
benchmarks and codes at https://github.com/kkkcx/3D_Corruptions_AD. We hope
that our benchmarks and findings can provide insights for future research on
developing robust 3D object detection models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Internal Structure Attention Network for Fingerprint Presentation Attack
  Detection from Optical Coherence Tomography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11034v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11034v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haohao Sun, Yilong Zhang, Peng Chen, Haixia Wang, Ronghua Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As a non-invasive optical imaging technique, optical coherence tomography
(OCT) has proven promising for automatic fingerprint recognition system (AFRS)
applications. Diverse approaches have been proposed for OCT-based fingerprint
presentation attack detection (PAD). However, considering the complexity and
variety of PA samples, it is extremely challenging to increase the
generalization ability with the limited PA dataset. To solve the challenge,
this paper presents a novel supervised learning-based PAD method, denoted as
ISAPAD, which applies prior knowledge to guide network training and enhance the
generalization ability. The proposed dual-branch architecture can not only
learns global features from the OCT image, but also concentrate on layered
structure feature which comes from the internal structure attention module
(ISAM). The simple yet effective ISAM enables the proposed network to obtain
layered segmentation features belonging only to Bonafide from noisy OCT volume
data directly. Combined with effective training strategies and PAD score
generation rules, ISAPAD obtains optimal PAD performance in limited training
data. Domain generalization experiments and visualization analysis validate the
effectiveness of the proposed method for OCT PAD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Dual-branch <span class="highlight-title">Self-supervised</span> Representation Learning Framework for
  Tumour Segmentation in Whole Slide Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11019v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11019v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Wang, Euijoon Ahn, Jinman Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Supervised deep learning methods have achieved considerable success in
medical image analysis, owing to the availability of large-scale and
well-annotated datasets. However, creating such datasets for whole slide images
(WSIs) in histopathology is a challenging task due to their gigapixel size. In
recent years, self-supervised learning (SSL) has emerged as an alternative
solution to reduce the annotation overheads in WSIs, as it does not require
labels for training. These SSL approaches, however, are not designed for
handling multi-resolution WSIs, which limits their performance in learning
discriminative image features. In this paper, we propose a Dual-branch SSL
Framework for WSI tumour segmentation (DSF-WSI) that can effectively learn
image features from multi-resolution WSIs. Our DSF-WSI connected two branches
and jointly learnt low and high resolution WSIs in a self-supervised manner.
Moreover, we introduced a novel Context-Target Fusion Module (CTFM) and a
masked jigsaw pretext task to align the learnt multi-resolution features.
Furthermore, we designed a Dense SimSiam Learning (DSL) strategy to maximise
the similarity of different views of WSIs, enabling the learnt representations
to be more efficient and discriminative. We evaluated our method using two
public datasets on breast and liver cancer segmentation tasks. The experiment
results demonstrated that our DSF-WSI can effectively extract robust and
efficient representations, which we validated through subsequent fine-tuning
and semi-supervised settings. Our proposed method achieved better accuracy than
other state-of-the-art approaches. Code is available at
https://github.com/Dylan-H-Wang/dsf-wsi.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Optical Flow from Event Camera with Rendered <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11011v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11011v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinglong Luo, Kunming Luo, Ao Luo, Zhengning Wang, Ping Tan, Shuaicheng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of estimating optical flow from event cameras. One
important issue is how to build a high-quality event-flow dataset with accurate
event values and flow labels. Previous datasets are created by either capturing
real scenes by event cameras or synthesizing from images with pasted foreground
objects. The former case can produce real event values but with calculated flow
labels, which are sparse and inaccurate. The later case can generate dense flow
labels but the interpolated events are prone to errors. In this work, we
propose to render a physically correct event-flow dataset using computer
graphics models. In particular, we first create indoor and outdoor 3D scenes by
Blender with rich scene content variations. Second, diverse camera motions are
included for the virtual capturing, producing images and accurate flow labels.
Third, we render high-framerate videos between images for accurate events. The
rendered dataset can adjust the density of events, based on which we further
introduce an adaptive density module (ADM). Experiments show that our proposed
dataset can facilitate event-flow learning, whereas previous approaches when
trained on our dataset can improve their performances constantly by a
relatively large margin. In addition, event-flow pipelines when equipped with
our ADM can further improve performances.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tubelet-Contrastive Self-Supervision for Video-Efficient Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11003v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11003v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fida Mohammad Thoker, Hazel Doughty, Cees Snoek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a self-supervised method for learning motion-focused video
representations. Existing approaches minimize distances between temporally
augmented videos, which maintain high spatial similarity. We instead propose to
learn similarities between videos with identical local motion dynamics but an
otherwise different appearance. We do so by adding synthetic motion
trajectories to videos which we refer to as tubelets. By simulating different
tubelet motions and applying transformations, such as scaling and rotation, we
introduce motion patterns beyond what is present in the pretraining data. This
allows us to learn a video representation that is remarkably data-efficient:
our approach maintains performance when using only 25% of the pretraining
videos. Experiments on 10 diverse downstream settings demonstrate our
competitive performance and generalizability to new domains and fine-grained
actions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Induced Feature Selection by Structured Pruning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10999v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10999v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathan Hubens, Victor Delvigne, Matei Mancas, Bernard Gosselin, Marius Preda, Titus Zaharia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of sparsity inducing techniques in neural networks has been of a
great help in the last few years. Indeed, those methods allowed to find lighter
and faster networks, able to perform more efficiently in resource-constrained
environment such as mobile devices or highly requested servers. Such a sparsity
is generally imposed on the weights of neural networks, reducing the footprint
of the architecture. In this work, we go one step further by imposing sparsity
jointly on the weights and on the input data. This can be achieved following a
three-step process: 1) impose a certain structured sparsity on the weights of
the network; 2) track back input features corresponding to zeroed blocks of
weight; 3) remove useless weights and input features and retrain the network.
Performing pruning both on the network and on input data not only allows for
extreme reduction in terms of parameters and operations but can also serve as
an interpretation process. Indeed, with the help of data pruning, we now have
information about which input feature is useful for the network to keep its
performance. Experiments conducted on a variety of architectures and datasets:
MLP validated on MNIST, CIFAR10/100 and ConvNets (VGG16 and ResNet18),
validated on CIFAR10/100 and CALTECH101 respectively, show that it is possible
to achieve additional gains in terms of total parameters and in FLOPs by
performing pruning on input data, while also increasing accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Versatile Depth Estimator Based on Common Relative Depth Estimation and
  Camera-Specific Relative-to-Metric Depth Conversion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10991v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10991v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinyoung Jun, Jae-Han Lee, Chang-Su Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A typical monocular depth estimator is trained for a single camera, so its
performance drops severely on images taken with different cameras. To address
this issue, we propose a versatile depth estimator (VDE), composed of a common
relative depth estimator (CRDE) and multiple relative-to-metric converters
(R2MCs). The CRDE extracts relative depth information, and each R2MC converts
the relative information to predict metric depths for a specific camera. The
proposed VDE can cope with diverse scenes, including both indoor and outdoor
scenes, with only a 1.12\% parameter increase per camera. Experimental results
demonstrate that VDE supports multiple cameras effectively and efficiently and
also achieves state-of-the-art performance in the conventional single-camera
scenario.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Attention Disturbance and Dual-Path Constraint Network for Occluded
  Person Re-Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10976v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10976v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaer Xia, Lei Tan, Pingyang Dai, Mingbo Zhao, Yongjian Wu, Rongrong Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Occluded person re-identification (Re-ID) aims to address the potential
occlusion problem when matching occluded or holistic pedestrians from different
camera views. Many methods use the background as artificial occlusion and rely
on attention networks to exclude noisy interference. However, the significant
discrepancy between simple background occlusion and realistic occlusion can
negatively impact the generalization of the network.To address this issue, we
propose a novel transformer-based Attention Disturbance and Dual-Path
Constraint Network (ADP) to enhance the generalization of attention networks.
Firstly, to imitate real-world obstacles, we introduce an Attention Disturbance
Mask (ADM) module that generates an offensive noise, which can distract
attention like a realistic occluder, as a more complex form of
occlusion.Secondly, to fully exploit these complex occluded images, we develop
a Dual-Path Constraint Module (DPC) that can obtain preferable supervision
information from holistic images through dual-path interaction. With our
proposed method, the network can effectively circumvent a wide variety of
occlusions using the basic ViT baseline. Comprehensive experimental evaluations
conducted on person re-ID benchmarks demonstrate the superiority of ADP over
state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VIMI: Vehicle-Infrastructure Multi-view Intermediate Fusion for
  Camera-based 3D Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10975v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10975v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhe Wang, Siqi Fan, Xiaoliang Huo, Tongda Xu, Yan Wang, Jingjing Liu, Yilun Chen, Ya-Qin Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In autonomous driving, Vehicle-Infrastructure Cooperative 3D Object Detection
(VIC3D) makes use of multi-view cameras from both vehicles and traffic
infrastructure, providing a global vantage point with rich semantic context of
road conditions beyond a single vehicle viewpoint. Two major challenges prevail
in VIC3D: 1) inherent calibration noise when fusing multi-view images, caused
by time asynchrony across cameras; 2) information loss when projecting 2D
features into 3D space. To address these issues, We propose a novel 3D object
detection framework, Vehicles-Infrastructure Multi-view Intermediate fusion
(VIMI). First, to fully exploit the holistic perspectives from both vehicles
and infrastructure, we propose a Multi-scale Cross Attention (MCA) module that
fuses infrastructure and vehicle features on selective multi-scales to correct
the calibration noise introduced by camera asynchrony. Then, we design a
Camera-aware Channel Masking (CCM) module that uses camera parameters as priors
to augment the fused features. We further introduce a Feature Compression (FC)
module with channel and spatial compression blocks to reduce the size of
transmitted features for enhanced efficiency. Experiments show that VIMI
achieves 15.61% overall AP_3D and 21.44% AP_BEV on the new VIC3D dataset,
DAIR-V2X-C, significantly outperforming state-of-the-art early fusion and late
fusion methods with comparable transmission cost.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semantic segmentation of surgical hyperspectral images under geometric
  domain shifts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10972v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10972v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan Sellner, Silvia Seidlitz, Alexander Studier-Fischer, Alessandro Motta, Berkin Özdemir, Beat Peter Müller-Stich, Felix Nickel, Lena Maier-Hein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robust semantic segmentation of intraoperative image data could pave the way
for automatic surgical scene understanding and autonomous robotic surgery.
Geometric domain shifts, however, although common in real-world open surgeries
due to variations in surgical procedures or situs occlusions, remain a topic
largely unaddressed in the field. To address this gap in the literature, we (1)
present the first analysis of state-of-the-art (SOA) semantic segmentation
networks in the presence of geometric out-of-distribution (OOD) data, and (2)
address generalizability with a dedicated augmentation technique termed "Organ
Transplantation" that we adapted from the general computer vision community.
According to a comprehensive validation on six different OOD data sets
comprising 600 RGB and hyperspectral imaging (HSI) cubes from 33 pigs
semantically annotated with 19 classes, we demonstrate a large performance drop
of SOA organ segmentation networks applied to geometric OOD data. Surprisingly,
this holds true not only for conventional RGB data (drop of Dice similarity
coefficient (DSC) by 46 %) but also for HSI data (drop by 45 %), despite the
latter's rich information content per pixel. Using our augmentation scheme
improves on the SOA DSC by up to 67 % (RGB) and 90 % (HSI) and renders
performance on par with in-distribution performance on real OOD test data. The
simplicity and effectiveness of our augmentation scheme makes it a valuable
network-independent tool for addressing geometric domain shifts in semantic
scene segmentation of intraoperative data. Our code and pre-trained models will
be made publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors (Jan Sellner and Silvia Seidlitz) contributed
  equally to this paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Self-Supervised</span> Learning for Multimodal Non-Rigid 3D Shape Matching <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10971v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10971v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongliang Cao, Florian Bernard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The matching of 3D shapes has been extensively studied for shapes represented
as surface meshes, as well as for shapes represented as point clouds. While
point clouds are a common representation of raw real-world 3D data (e.g. from
laser scanners), meshes encode rich and expressive topological information, but
their creation typically requires some form of (often manual) curation. In
turn, methods that purely rely on point clouds are unable to meet the matching
quality of mesh-based methods that utilise the additional topological
structure. In this work we close this gap by introducing a self-supervised
multimodal learning strategy that combines mesh-based functional map
regularisation with a contrastive loss that couples mesh and point cloud data.
Our shape matching approach allows to obtain intramodal correspondences for
triangle meshes, complete point clouds, and partially observed point clouds, as
well as correspondences across these data modalities. We demonstrate that our
method achieves state-of-the-art results on several challenging benchmark
datasets even in comparison to recent supervised methods, and that our method
reaches previously unseen cross-dataset generalisation ability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-time Semantic Scene Completion Via Feature Aggregation and
  Conditioned Prediction <span class="chip">ICIP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10967v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10967v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaokang Chen, Yajie Xing, Gang Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic Scene Completion (SSC) aims to simultaneously predict the volumetric
occupancy and semantic category of a 3D scene. In this paper, we propose a
real-time semantic scene completion method with a feature aggregation strategy
and conditioned prediction module. Feature aggregation fuses feature with
different receptive fields and gathers context to improve scene completion
performance. And the conditioned prediction module adopts a two-step prediction
scheme that takes volumetric occupancy as a condition to enhance semantic
completion prediction. We conduct experiments on three recognized benchmarks
NYU, NYUCAD, and SUNCG. Our method achieves competitive performance at a speed
of 110 FPS on one GTX 1080 Ti GPU.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICIP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Implicit Vision-Language Feature Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10962v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10962v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kenneth Blomqvist, Francesco Milano, Jen Jen Chung, Lionel Ott, Roland Siegwart
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, groundbreaking results have been presented on open-vocabulary
semantic image segmentation. Such methods segment each pixel in an image into
arbitrary categories provided at run-time in the form of text prompts, as
opposed to a fixed set of classes defined at training time. In this work, we
present a zero-shot volumetric open-vocabulary semantic scene segmentation
method. Our method builds on the insight that we can fuse image features from a
vision-language model into a neural implicit representation. We show that the
resulting feature field can be segmented into different classes by assigning
points to natural language text prompts. The implicit volumetric representation
enables us to segment the scene both in 3D and 2D by rendering feature maps
from any given viewpoint of the scene. We show that our method works on noisy
real-world data and can run in real-time on live sensor data dynamically
adjusting to text prompts. We also present quantitative comparisons on the
ScanNet dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LFACon: Introducing Anglewise Attention to No-Reference Quality
  Assessment in Light Field Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10961v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10961v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiang Qu, Xiaoming Chen, Yuk Ying Chung, Weidong Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Light field imaging can capture both the intensity information and the
direction information of light rays. It naturally enables a
six-degrees-of-freedom viewing experience and deep user engagement in virtual
reality. Compared to 2D image assessment, light field image quality assessment
(LFIQA) needs to consider not only the image quality in the spatial domain but
also the quality consistency in the angular domain. However, there is a lack of
metrics to effectively reflect the angular consistency and thus the angular
quality of a light field image (LFI). Furthermore, the existing LFIQA metrics
suffer from high computational costs due to the excessive data volume of LFIs.
In this paper, we propose a novel concept of "anglewise attention" by
introducing a multihead self-attention mechanism to the angular domain of an
LFI. This mechanism better reflects the LFI quality. In particular, we propose
three new attention kernels, including anglewise self-attention, anglewise grid
attention, and anglewise central attention. These attention kernels can realize
angular self-attention, extract multiangled features globally or selectively,
and reduce the computational cost of feature extraction. By effectively
incorporating the proposed kernels, we further propose our light field
attentional convolutional neural network (LFACon) as an LFIQA metric. Our
experimental results show that the proposed LFACon metric significantly
outperforms the state-of-the-art LFIQA metrics. For the majority of distortion
types, LFACon attains the best performance with lower complexity and less
computational time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for IEEE VR 2023 (TVCG Special Issues) (Early Access)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improved Benthic Classification using Resolution Scaling and SymmNet
  Unsupervised Domain Adaptation <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10960v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10960v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heather Doig, Oscar Pizarro, Stefan B. Williams
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous Underwater Vehicles (AUVs) conduct regular visual surveys of
marine environments to characterise and monitor the composition and diversity
of the benthos. The use of machine learning classifiers for this task is
limited by the low numbers of annotations available and the many fine-grained
classes involved. In addition to these challenges, there are domain shifts
between image sets acquired during different AUV surveys due to changes in
camera systems, imaging altitude, illumination and water column properties
leading to a drop in classification performance for images from a different
survey where some or all these elements may have changed. This paper proposes a
framework to improve the performance of a benthic morphospecies classifier when
used to classify images from a different survey compared to the training data.
We adapt the SymmNet state-of-the-art Unsupervised Domain Adaptation method
with an efficient bilinear pooling layer and image scaling to normalise spatial
resolution, and show improved classification accuracy. We test our approach on
two datasets with images from AUV surveys with different imaging payloads and
locations. The results show that generic domain adaptation can be enhanced to
produce a significant increase in accuracy for images from an AUV survey that
differs from the training images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 6 figures. Accepted to IEEE International Conference on
  Robotics and Automation (ICRA) 2023, London UK</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Long-Term Indoor Localization with Metric-Semantic Mapping using a Floor
  Plan Prior <span class="chip">IROS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10959v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10959v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicky Zimmerman, Matteo Sodano, Elias Marks, Jens Behley, Cyrill Stachniss
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object-based maps are relevant for scene understanding since they integrate
geometric and semantic information of the environment, allowing autonomous
robots to robustly localize and interact with on objects. In this paper, we
address the task of constructing a metric-semantic map for the purpose of
long-term object-based localization. We exploit 3D object detections from
monocular RGB frames for both, the object-based map construction, and for
globally localizing in the constructed map. To tailor the approach to a target
environment, we propose an efficient way of generating 3D annotations to
finetune the 3D object detection model. We evaluate our map construction in an
office building, and test our long-term localization approach on challenging
sequences recorded in the same environment over nine months. The experiments
suggest that our approach is suitable for constructing metric-semantic maps,
and that our localization approach is robust to long-term changes. Both, the
mapping algorithm and the localization pipeline can run online on an onboard
computer. We will release an open-source C++/ROS implementation of our
approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, submitted to IROS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tracker Meets Night: A <span class="highlight-title">Transformer</span> Enhancer for UAV Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10951v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10951v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjie Ye, Changhong Fu, Ziang Cao, Shan An, Guangze Zheng, Bowen Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most previous progress in object tracking is realized in daytime scenes with
favorable illumination. State-of-the-arts can hardly carry on their superiority
at night so far, thereby considerably blocking the broadening of visual
tracking-related unmanned aerial vehicle (UAV) applications. To realize
reliable UAV tracking at night, a spatial-channel Transformer-based low-light
enhancer (namely SCT), which is trained in a novel task-inspired manner, is
proposed and plugged prior to tracking approaches. To achieve semantic-level
low-light enhancement targeting the high-level task, the novel spatial-channel
attention module is proposed to model global information while preserving local
context. In the enhancement process, SCT denoises and illuminates nighttime
images simultaneously through a robust non-linear curve projection. Moreover,
to provide a comprehensive evaluation, we construct a challenging nighttime
tracking benchmark, namely DarkTrack2021, which contains 110 challenging
sequences with over 100 K frames in total. Evaluations on both the public
UAVDark135 benchmark and the newly constructed DarkTrack2021 benchmark show
that the task-inspired design enables SCT with significant performance gains
for nighttime UAV tracking compared with other top-ranked low-light enhancers.
Real-world tests on a typical UAV platform further verify the practicability of
the proposed approach. The DarkTrack2021 benchmark and the code of the proposed
approach are publicly available at https://github.com/vision4robotics/SCT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Open-World Pose Transfer via Sequential Test-Time Adaption 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10945v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10945v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyang Chen, Xiaoyu Xian, Zhijing Yang, Tianshui Chen, Yongyi Lu, Yukai Shi, Jinshan Pan, Liang Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pose transfer aims to transfer a given person into a specified posture, has
recently attracted considerable attention. A typical pose transfer framework
usually employs representative datasets to train a discriminative model, which
is often violated by out-of-distribution (OOD) instances. Recently, test-time
adaption (TTA) offers a feasible solution for OOD data by using a pre-trained
model that learns essential features with self-supervision. However, those
methods implicitly make an assumption that all test distributions have a
unified signal that can be learned directly. In open-world conditions, the pose
transfer task raises various independent signals: OOD appearance and skeleton,
which need to be extracted and distributed in speciality. To address this
point, we develop a SEquential Test-time Adaption (SETA). In the test-time
phrase, SETA extracts and distributes external appearance texture by augmenting
OOD data for self-supervised training. To make non-Euclidean similarity among
different postures explicit, SETA uses the image representations derived from a
person re-identification (Re-ID) model for similarity computation. By
addressing implicit posture representation in the test-time sequentially, SETA
greatly improves the generalization performance of current pose transfer
models. In our experiment, we first show that pose transfer can be applied to
open-world applications, including Tiktok reenactment and celebrity motion
synthesis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We call for a solid pose transfer model that can handle open-world
  instances beyond a specific dataset</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Location-Free Scene Graph Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10944v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10944v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ege Özsoy, Felix Holm, Tobias Czempiel, Nassir Navab, Benjamin Busam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scene Graph Generation (SGG) is a challenging visual understanding task. It
combines the detection of entities and relationships between them in a scene.
Both previous works and existing evaluation metrics rely on bounding box
labels, even though many downstream scene graph applications do not need
location information. The need for localization labels significantly increases
the annotation cost and hampers the creation of more and larger scene graph
datasets. We suggest breaking the dependency of scene graphs on bounding box
labels by proposing location-free scene graph generation (LF-SGG). This new
task aims at predicting instances of entities, as well as their relationships,
without spatial localization. To objectively evaluate the task, the predicted
and ground truth scene graphs need to be compared. We solve this NP-hard
problem through an efficient algorithm using branching. Additionally, we design
the first LF-SGG method, Pix2SG, using autoregressive sequence modeling. Our
proposed method is evaluated on Visual Genome and 4D-OR. Although using
significantly fewer labels during training, we achieve 74.12\% of the
location-supervised SOTA performance on Visual Genome and even outperform the
best method on 4D-OR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HMC: Hierarchical Mesh Coarsening for Skeleton-free Motion Retargeting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10941v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10941v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyu Wang, Shaoli Huang, Fang Zhao, Chun Yuan, Ying Shan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a simple yet effective method for skeleton-free motion
retargeting. Previous methods transfer motion between high-resolution meshes,
failing to preserve the inherent local-part motions in the mesh. Addressing
this issue, our proposed method learns the correspondence in a coarse-to-fine
fashion by integrating the retargeting process with a mesh-coarsening pipeline.
First, we propose a mesh-coarsening module that coarsens the mesh
representations for better motion transfer. This module improves the ability to
handle small-part motion and preserves the local motion interdependence between
neighboring mesh vertices. Furthermore, we leverage a hierarchical refinement
procedure to complement missing mesh details by gradually improving the
low-resolution mesh output with a higher-resolution one. We evaluate our method
on several well-known 3D character datasets, and it yields an average
improvement of 25% on point-wise mesh euclidean distance (PMD) against the
start-of-art method. Moreover, our qualitative results show that our method is
significantly helpful in preserving the moving consistency of different body
parts on the target character due to disentangling body-part structures and
mesh details in a hierarchical way.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boosting Weakly Supervised Object Detection using Fusion and Priors from
  Hallucinated Depth 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10937v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10937v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cagri Gungor, Adriana Kovashka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent attention and exploration of depth for various tasks, it is
still an unexplored modality for weakly-supervised object detection (WSOD). We
propose an amplifier method for enhancing the performance of WSOD by
integrating depth information. Our approach can be applied to any WSOD method
based on multiple-instance learning, without necessitating additional
annotations or inducing large computational expenses. Our proposed method
employs a monocular depth estimation technique to obtain hallucinated depth
information, which is then incorporated into a Siamese WSOD network using
contrastive loss and fusion. By analyzing the relationship between language
context and depth, we calculate depth priors to identify the bounding box
proposals that may contain an object of interest. These depth priors are then
utilized to update the list of pseudo ground-truth boxes, or adjust the
confidence of per-box predictions. Our proposed method is evaluated on six
datasets (COCO, PASCAL VOC, Conceptual Captions, Clipart1k, Watercolor2k, and
Comic2k) by implementing it on top of two state-of-the-art WSOD methods, and we
demonstrate a substantial enhancement in performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Explore Informative Trajectories and Samples for Embodied
  Perception <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10936v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10936v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ya Jing, Tao Kong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We are witnessing significant progress on perception models, specifically
those trained on large-scale internet images. However, efficiently generalizing
these perception models to unseen embodied tasks is insufficiently studied,
which will help various relevant applications (e.g., home robots). Unlike
static perception methods trained on pre-collected images, the embodied agent
can move around in the environment and obtain images of objects from any
viewpoints. Therefore, efficiently learning the exploration policy and
collection method to gather informative training samples is the key to this
task. To do this, we first build a 3D semantic distribution map to train the
exploration policy self-supervised by introducing the semantic distribution
disagreement and the semantic distribution uncertainty rewards. Note that the
map is generated from multi-view observations and can weaken the impact of
misidentification from an unfamiliar viewpoint. Our agent is then encouraged to
explore the objects with different semantic distributions across viewpoints, or
uncertain semantic distributions. With the explored informative trajectories,
we propose to select hard samples on trajectories based on the semantic
distribution uncertainty to reduce unnecessary observations that can be
correctly identified. Experiments show that the perception model fine-tuned
with our method outperforms the baselines trained with other exploration
policies. Further, we demonstrate the robustness of our method in real-robot
experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in IEEE International Conference on Robotics and
  Automation (ICRA), 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Object Removal for Effective Slam 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10923v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10923v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Phani Krishna Uppala, Abhishek Bamotra, Raj Kolamuri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research paper focuses on the problem of dynamic objects and their
impact on effective motion planning and localization. The paper proposes a
two-step process to address this challenge, which involves finding the dynamic
objects in the scene using a Flow-based method and then using a deep Video
inpainting algorithm to remove them. The study aims to test the validity of
this approach by comparing it with baseline results using two state-of-the-art
SLAM algorithms, ORB-SLAM2 and LSD, and understanding the impact of dynamic
objects and the corresponding trade-offs. The proposed approach does not
require any significant modifications to the baseline SLAM algorithms, and
therefore, the computational effort required remains unchanged. The paper
presents a detailed analysis of the results obtained and concludes that the
proposed method is effective in removing dynamic objects from the scene,
leading to improved SLAM performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Behavior Recognition in Smart Classroom with Multiple Students
  Based on YOLOv5 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10916v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10916v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhifeng Wang, Jialong Yao, Chunyan Zeng, Wanxuan Wu, Hongmin Xu, Yang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning-based computer vision technology has grown stronger in recent
years, and cross-fertilization using computer vision technology has been a
popular direction in recent years. The use of computer vision technology to
identify students' learning behavior in the classroom can reduce the workload
of traditional teachers in supervising students in the classroom, and ensure
greater accuracy and comprehensiveness. However, existing student learning
behavior detection systems are unable to track and detect multiple targets
precisely, and the accuracy of learning behavior recognition is not high enough
to meet the existing needs for the accurate recognition of student behavior in
the classroom. To solve this problem, we propose a YOLOv5s network structure
based on you only look once (YOLO) algorithm to recognize and analyze students'
classroom behavior in this paper. Firstly, the input images taken in the smart
classroom are pre-processed. Then, the pre-processed image is fed into the
designed YOLOv5 networks to extract deep features through convolutional layers,
and the Squeeze-and-Excitation (SE) attention detection mechanism is applied to
reduce the weight of background information in the recognition process.
Finally, the extracted features are classified by the Feature Pyramid Networks
(FPN) and Path Aggregation Network (PAN) structures. Multiple groups of
experiments were performed to compare with traditional learning behavior
recognition methods to validate the effectiveness of the proposed method. When
compared with YOLOv4, the proposed method is able to improve the mAP
performance by 11%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Actionlet-Dependent Contrastive Learning for Unsupervised Skeleton-Based
  Action Recognition <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10904v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10904v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lilang Lin, Jiahang Zhang, Jiaying Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The self-supervised pretraining paradigm has achieved great success in
skeleton-based action recognition. However, these methods treat the motion and
static parts equally, and lack an adaptive design for different parts, which
has a negative impact on the accuracy of action recognition. To realize the
adaptive action modeling of both parts, we propose an Actionlet-Dependent
Contrastive Learning method (ActCLR). The actionlet, defined as the
discriminative subset of the human skeleton, effectively decomposes motion
regions for better action modeling. In detail, by contrasting with the static
anchor without motion, we extract the motion region of the skeleton data, which
serves as the actionlet, in an unsupervised manner. Then, centering on
actionlet, a motion-adaptive data transformation method is built. Different
data transformations are applied to actionlet and non-actionlet regions to
introduce more diversity while maintaining their own characteristics.
Meanwhile, we propose a semantic-aware feature pooling method to build feature
representations among motion and static regions in a distinguished manner.
Extensive experiments on NTU RGB+D and PKUMMD show that the proposed method
achieves remarkable action recognition performance. More visualization and
quantitative experiments demonstrate the effectiveness of our method. Our
project website is available at https://langlandslin.github.io/projects/ActCLR/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR2023. The project page is at
  https://langlandslin.github.io/projects/ActCLR/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Feature Alignment and Uniformity for Test Time Adaptation <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10902v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10902v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuai Wang, Daoan Zhang, Zipei Yan, Jianguo Zhang, Rui Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Test time adaptation (TTA) aims to adapt deep neural networks when receiving
out of distribution test domain samples. In this setting, the model can only
access online unlabeled test samples and pre-trained models on the training
domains. We first address TTA as a feature revision problem due to the domain
gap between source domains and target domains. After that, we follow the two
measurements alignment and uniformity to discuss the test time feature
revision. For test time feature uniformity, we propose a test time
self-distillation strategy to guarantee the consistency of uniformity between
representations of the current batch and all the previous batches. For test
time feature alignment, we propose a memorized spatial local clustering
strategy to align the representations among the neighborhood samples for the
upcoming batch. To deal with the common noisy label problem, we propound the
entropy and consistency filters to select and drop the possible noisy labels.
To prove the scalability and efficacy of our method, we conduct experiments on
four domain generalization benchmarks and four medical image segmentation tasks
with various backbones. Experiment results show that our method not only
improves baseline stably but also outperforms existing state-of-the-art test
time adaptation methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Tiny Machine Learning Model for Point Cloud Object Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10898v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10898v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Min Zhang, Jintang Xue, Pranav Kadam, Hardik Prajapati, Shan Liu, C. -C. Jay Kuo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The design of a tiny machine learning model, which can be deployed in mobile
and edge devices, for point cloud object classification is investigated in this
work. To achieve this objective, we replace the multi-scale representation of a
point cloud object with a single-scale representation for complexity reduction,
and exploit rich 3D geometric information of a point cloud object for
performance improvement. The proposed solution is named Green-PointHop due to
its low computational complexity. We evaluate the performance of Green-PointHop
on ModelNet40 and ScanObjectNN two datasets. Green-PointHop has a model size of
64K parameters. It demands 2.3M floating-point operations (FLOPs) to classify a
ModelNet40 object of 1024 down-sampled points. Its classification performance
gaps against the state-of-the-art DGCNN method are 3% and 7% for ModelNet40 and
ScanObjectNN, respectively. On the other hand, the model size and inference
complexity of DGCNN are 42X and 1203X of those of Green-PointHop, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graphics Capsule: Learning Hierarchical 3D Face Representations from 2D
  Images <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10896v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10896v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chang Yu, Xiangyu Zhu, Xiaomei Zhang, Zhaoxiang Zhang, Zhen Lei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The function of constructing the hierarchy of objects is important to the
visual process of the human brain. Previous studies have successfully adopted
capsule networks to decompose the digits and faces into parts in an
unsupervised manner to investigate the similar perception mechanism of neural
networks. However, their descriptions are restricted to the 2D space, limiting
their capacities to imitate the intrinsic 3D perception ability of humans. In
this paper, we propose an Inverse Graphics Capsule Network (IGC-Net) to learn
the hierarchical 3D face representations from large-scale unlabeled images. The
core of IGC-Net is a new type of capsule, named graphics capsule, which
represents 3D primitives with interpretable parameters in computer graphics
(CG), including depth, albedo, and 3D pose. Specifically, IGC-Net first
decomposes the objects into a set of semantic-consistent part-level
descriptions and then assembles them into object-level descriptions to build
the hierarchy. The learned graphics capsules reveal how the neural networks,
oriented at visual perception, understand faces as a hierarchy of 3D models.
Besides, the discovered parts can be deployed to the unsupervised face
segmentation task to evaluate the semantic consistency of our method. Moreover,
the part-level descriptions with explicit physical meanings provide insight
into the face analysis that originally runs in a black box, such as the
importance of shape and texture for face recognition. Experiments on CelebA,
BP4D, and Multi-PIE demonstrate the characteristics of our IGC-Net.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leapfrog Diffusion Model for Stochastic Trajectory Prediction <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10895v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10895v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weibo Mao, Chenxin Xu, Qi Zhu, Siheng Chen, Yanfeng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To model the indeterminacy of human behaviors, stochastic trajectory
prediction requires a sophisticated multi-modal distribution of future
trajectories. Emerging diffusion models have revealed their tremendous
representation capacities in numerous generation tasks, showing potential for
stochastic trajectory prediction. However, expensive time consumption prevents
diffusion models from real-time prediction, since a large number of denoising
steps are required to assure sufficient representation ability. To resolve the
dilemma, we present LEapfrog Diffusion model (LED), a novel diffusion-based
trajectory prediction model, which provides real-time, precise, and diverse
predictions. The core of the proposed LED is to leverage a trainable leapfrog
initializer to directly learn an expressive multi-modal distribution of future
trajectories, which skips a large number of denoising steps, significantly
accelerating inference speed. Moreover, the leapfrog initializer is trained to
appropriately allocate correlated samples to provide a diversity of predicted
future trajectories, significantly improving prediction performances. Extensive
experiments on four real-world datasets, including NBA/NFL/SDD/ETH-UCY, show
that LED consistently improves performance and achieves 23.7%/21.9% ADE/FDE
improvement on NFL. The proposed LED also speeds up the inference
19.3/30.8/24.3/25.1 times compared to the standard diffusion model on
NBA/NFL/SDD/ETH-UCY, satisfying real-time inference needs. Code is available at
https://github.com/MediaBrain-SJTU/LED.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ M$^{2}$SNet: Multi-scale in Multi-scale Subtraction Network for Medical
  Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10894v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10894v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoqi Zhao, Hongpeng Jia, Youwei Pang, Long Lv, Feng Tian, Lihe Zhang, Weibing Sun, Huchuan Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate medical image segmentation is critical for early medical diagnosis.
Most existing methods are based on U-shape structure and use element-wise
addition or concatenation to fuse different level features progressively in
decoder. However, both the two operations easily generate plenty of redundant
information, which will weaken the complementarity between different level
features, resulting in inaccurate localization and blurred edges of lesions. To
address this challenge, we propose a general multi-scale in multi-scale
subtraction network (M$^{2}$SNet) to finish diverse segmentation from medical
image. Specifically, we first design a basic subtraction unit (SU) to produce
the difference features between adjacent levels in encoder. Next, we expand the
single-scale SU to the intra-layer multi-scale SU, which can provide the
decoder with both pixel-level and structure-level difference information. Then,
we pyramidally equip the multi-scale SUs at different levels with varying
receptive fields, thereby achieving the inter-layer multi-scale feature
aggregation and obtaining rich multi-scale difference information. In addition,
we build a training-free network ``LossNet'' to comprehensively supervise the
task-aware features from bottom layer to top layer, which drives our
multi-scale subtraction network to capture the detailed and structural cues
simultaneously. Without bells and whistles, our method performs favorably
against most state-of-the-art methods under different evaluation metrics on
eleven datasets of four different medical image segmentation tasks of diverse
image modalities, including color colonoscopy imaging, ultrasound imaging,
computed tomography (CT), and optical coherence tomography (OCT). The source
code can be available at \url{https://github.com/Xiaoqi-Zhao-DLUT/MSNet}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE TMI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Offline-Online Class-incremental Continual Learning via Dual-prototype
  Self-augment and Refinement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10891v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10891v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fushuo Huo, Wenchao Xu, Jingcai Guo, Haozhao Wang, Yunfeng Fan, Song Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates a new, practical, but challenging problem named
Offline-Online Class-incremental Continual Learning (O$^2$CL), which aims to
preserve the discernibility of pre-trained (i.e., offline) base classes without
buffering data examples, and efficiently learn novel classes continuously in a
single-pass (i.e., online) data stream. The challenges of this task are mainly
two-fold: 1) Both base and novel classes suffer from severe catastrophic
forgetting as no previous samples are available for replay. 2) As the online
data can only be observed once, there is no way to fully re-train the whole
model, e.g., re-calibrate the decision boundaries via prototype alignment or
feature distillation. In this paper, we propose a novel Dual-prototype
Self-augment and Refinement method (DSR) for O$^2$CL problem, which consists of
two strategies: 1) Dual class prototypes: Inner and hyper-dimensional
prototypes are exploited to utilize the pre-trained information and obtain
robust quasi-orthogonal representations rather than example buffers for both
privacy preservation and memory reduction. 2) Self-augment and refinement:
Instead of updating the whole network, we jointly optimize the extra projection
module with the self-augment inner prototypes from base and novel classes,
gradually refining the hyper-dimensional prototypes to obtain accurate decision
boundaries for learned classes. Extensive experiments demonstrate the
effectiveness and superiority of the proposed DSR in O$^2$CL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Fushuo Huo, Wenchao Xu, Jingcai Guo, Haozhao Wang, and Yunfeng Fan,
  Song Guo</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explicit Visual <span class="highlight-title">Prompt</span>ing for Low-Level Structure Segmentations <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10883v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10883v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weihuang Liu, Xi Shen, Chi-Man Pun, Xiaodong Cun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the generic problem of detecting low-level structures in images,
which includes segmenting the manipulated parts, identifying out-of-focus
pixels, separating shadow regions, and detecting concealed objects. Whereas
each such topic has been typically addressed with a domain-specific solution,
we show that a unified approach performs well across all of them. We take
inspiration from the widely-used pre-training and then prompt tuning protocols
in NLP and propose a new visual prompting model, named Explicit Visual
Prompting (EVP). Different from the previous visual prompting which is
typically a dataset-level implicit embedding, our key insight is to enforce the
tunable parameters focusing on the explicit visual content from each individual
image, i.e., the features from frozen patch embeddings and the input's
high-frequency components. The proposed EVP significantly outperforms other
parameter-efficient tuning protocols under the same amount of tunable
parameters (5.7% extra trainable parameters of each task). EVP also achieves
state-of-the-art performances on diverse low-level structure segmentation tasks
compared to task-specific solutions. Our code is available at:
https://github.com/NiFangBaAGe/Explict-Visual-Prompt.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Map Sparsification Based on 2D and 3D Discretized Grids <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10882v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10882v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyu Zhang, Yun-Hui Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Localization in a pre-built map is a basic technique for robot autonomous
navigation. Existing mapping and localization methods commonly work well in
small-scale environments. As a map grows larger, however, more memory is
required and localization becomes inefficient. To solve these problems, map
sparsification becomes a practical necessity to acquire a subset of the
original map for localization. Previous map sparsification methods add a
quadratic term in mixed-integer programming to enforce a uniform distribution
of selected landmarks, which requires high memory capacity and heavy
computation. In this paper, we formulate map sparsification in an efficient
linear form and select uniformly distributed landmarks based on 2D discretized
grids. Furthermore, to reduce the influence of different spatial distributions
between the mapping and query sequences, which is not considered in previous
methods, we also introduce a space constraint term based on 3D discretized
grids. The exhaustive experiments in different datasets demonstrate the
superiority of the proposed methods in both efficiency and localization
performance. The relevant codes will be released at
https://github.com/fishmarch/SLAM_Map_Compression.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EqMotion: Equivariant Multi-agent Motion Prediction with Invariant
  Interaction Reasoning <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10876v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10876v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenxin Xu, Robby T. Tan, Yuhong Tan, Siheng Chen, Yu Guang Wang, Xinchao Wang, Yanfeng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning to predict agent motions with relationship reasoning is important
for many applications. In motion prediction tasks, maintaining motion
equivariance under Euclidean geometric transformations and invariance of agent
interaction is a critical and fundamental principle. However, such equivariance
and invariance properties are overlooked by most existing methods. To fill this
gap, we propose EqMotion, an efficient equivariant motion prediction model with
invariant interaction reasoning. To achieve motion equivariance, we propose an
equivariant geometric feature learning module to learn a Euclidean
transformable feature through dedicated designs of equivariant operations. To
reason agent's interactions, we propose an invariant interaction reasoning
module to achieve a more stable interaction modeling. To further promote more
comprehensive motion features, we propose an invariant pattern feature learning
module to learn an invariant pattern feature, which cooperates with the
equivariant geometric feature to enhance network expressiveness. We conduct
experiments for the proposed model on four distinct scenarios: particle
dynamics, molecule dynamics, human skeleton motion prediction and pedestrian
trajectory prediction. Experimental results show that our method is not only
generally applicable, but also achieves state-of-the-art prediction
performances on all the four tasks, improving by 24.0/30.1/8.6/9.2%. Code is
available at https://github.com/MediaBrain-SJTU/EqMotion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decomposed Prototype Learning for Few-Shot Scene Graph Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10863v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10863v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingchen Li, Long Chen, Guikun Chen, Yinfu Feng, Yi Yang, Jun Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Today's scene graph generation (SGG) models typically require abundant manual
annotations to learn new predicate types. Thus, it is difficult to apply them
to real-world applications with a long-tailed distribution of predicates. In
this paper, we focus on a new promising task of SGG: few-shot SGG (FSSGG).
FSSGG encourages models to be able to quickly transfer previous knowledge and
recognize novel predicates well with only a few examples. Although many
advanced approaches have achieved great success on few-shot learning (FSL)
tasks, straightforwardly extending them into FSSGG is not applicable due to two
intrinsic characteristics of predicate concepts: 1) Each predicate category
commonly has multiple semantic meanings under different contexts. 2) The visual
appearance of relation triplets with the same predicate differs greatly under
different subject-object pairs. Both issues make it hard to model conventional
latent representations for predicate categories with state-of-the-art FSL
methods. To this end, we propose a novel Decomposed Prototype Learning (DPL).
Specifically, we first construct a decomposable prototype space to capture
intrinsic visual patterns of subjects and objects for predicates, and enhance
their feature representations with these decomposed prototypes. Then, we devise
an intelligent metric learner to assign adaptive weights to each support sample
by considering the relevance of their subject-object pairs. We further re-split
the VG dataset and compare DPL with various FSL methods to benchmark this task.
Extensive results show that DPL achieves excellent performance in both base and
novel categories.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting Realistic Test-Time Training: Sequential Inference and
  Adaptation by Anchored Clustering Regularized Self-Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10856v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10856v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongyi Su, Xun Xu, Tianrui Li, Kui Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deploying models on target domain data subject to distribution shift requires
adaptation. Test-time training (TTT) emerges as a solution to this adaptation
under a realistic scenario where access to full source domain data is not
available, and instant inference on the target domain is required. Despite many
efforts into TTT, there is a confusion over the experimental settings, thus
leading to unfair comparisons. In this work, we first revisit TTT assumptions
and categorize TTT protocols by two key factors. Among the multiple protocols,
we adopt a realistic sequential test-time training (sTTT) protocol, under which
we develop a test-time anchored clustering (TTAC) approach to enable stronger
test-time feature learning. TTAC discovers clusters in both source and target
domains and matches the target clusters to the source ones to improve
adaptation. When source domain information is strictly absent (i.e.
source-free) we further develop an efficient method to infer source domain
distributions for anchored clustering. Finally, self-training~(ST) has
demonstrated great success in learning from unlabeled data and we empirically
figure out that applying ST alone to TTT is prone to confirmation bias.
Therefore, a more effective TTT approach is introduced by regularizing
self-training with anchored clustering, and the improved model is referred to
as TTAC++. We demonstrate that, under all TTT protocols, TTAC++ consistently
outperforms the state-of-the-art methods on five TTT datasets, including
corrupted target domain, selected hard samples, synthetic-to-real adaptation
and adversarially attacked target domain. We hope this work will provide a fair
benchmarking of TTT methods, and future research should be compared within
respective protocols.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Test-time training, Self-training. arXiv admin note: substantial text
  overlap with arXiv:2206.02721</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Facial Affective Analysis based on MAE and Multi-modal Information for
  5th ABAW Competition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10849v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10849v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Zhang, Bowen Ma, Feng Qiu, Yu Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human affective behavior analysis focuses on analyzing human expressions or
other behaviors, which helps improve the understanding of human psychology.
CVPR 2023 Competition on Affective Behavior Analysis in-the-wild (ABAW) makes
great efforts to provide the diversity data for the recognition of the commonly
used emotion representations, including Action Units~(AU), basic expression
categories and Valence-Arousal~(VA). In this paper, we introduce our submission
to the CVPR 2023: ABAW5 for AU detection, expression classification, VA
estimation and emotional reaction intensity (ERI) estimation. First of all, we
introduce the vision information from an MAE model, which has been pre-trained
on a large-scale face image dataset in a self-supervised manner. Then the MAE
encoder part is finetuned on the ABAW challenges on the single frame of
Aff-wild2 dataset. We also exploit the multi-modal and temporal information
from the videos and design a transformer-based framework to fusion the
multi-modal features. Moreover, we construct a novel two-branch collaboration
training strategy to further enhance the model generalization by randomly
interpolating the logits space. The extensive quantitative experiments, as well
as ablation studies on the Aff-Wild2 dataset and Hume-Reaction dataset prove
the effectiveness of our proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond mAP: Towards better evaluation of instance segmentation <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.01614v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.01614v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rohit Jena, Lukas Zhornyak, Nehal Doiphode, Pratik Chaudhari, Vivek Buch, James Gee, Jianbo Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Correctness of instance segmentation constitutes counting the number of
objects, correctly localizing all predictions and classifying each localized
prediction. Average Precision is the de-facto metric used to measure all these
constituents of segmentation. However, this metric does not penalize duplicate
predictions in the high-recall range, and cannot distinguish instances that are
localized correctly but categorized incorrectly. This weakness has
inadvertently led to network designs that achieve significant gains in AP but
also introduce a large number of false positives. We therefore cannot rely on
AP to choose a model that provides an optimal tradeoff between false positives
and high recall. To resolve this dilemma, we review alternative metrics in the
literature and propose two new measures to explicitly measure the amount of
both spatial and categorical duplicate predictions. We also propose a Semantic
Sorting and NMS module to remove these duplicates based on a pixel occupancy
matching scheme. Experiments show that modern segmentation networks have
significant gains in AP, but also contain a considerable amount of duplicates.
Our Semantic Sorting and NMS can be added as a plug-and-play module to mitigate
hedged predictions and preserve AP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Latent Video Diffusion Models for High-Fidelity Long Video Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.13221v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.13221v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, Qifeng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI-generated content has attracted lots of attention recently, but
photo-realistic video synthesis is still challenging. Although many attempts
using GANs and autoregressive models have been made in this area, the visual
quality and length of generated videos are far from satisfactory. Diffusion
models have shown remarkable results recently but require significant
computational resources. To address this, we introduce lightweight video
diffusion models by leveraging a low-dimensional 3D latent space, significantly
outperforming previous pixel-space video diffusion models under a limited
computational budget. In addition, we propose hierarchical diffusion in the
latent space such that longer videos with more than one thousand frames can be
produced. To further overcome the performance degradation issue for long video
generation, we propose conditional latent perturbation and unconditional
guidance that effectively mitigate the accumulated errors during the extension
of video length. Extensive experiments on small domain datasets of different
categories suggest that our framework generates more realistic and longer
videos than previous strong baselines. We additionally provide an extension to
large-scale text-to-video generation to demonstrate the superiority of our
work. Our code and models will be made publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://yingqinghe.github.io/LVDM/ Github:
  https://github.com/YingqingHe/LVDM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FAQ: Feature Aggregated Queries for <span class="highlight-title">Transformer</span>-based Video Object
  Detectors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08319v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08319v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Cui, Linjie Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video object detection needs to solve feature degradation situations that
rarely happen in the image domain. One solution is to use the temporal
information and fuse the features from the neighboring frames. With
Transformerbased object detectors getting a better performance on the image
domain tasks, recent works began to extend those methods to video object
detection. However, those existing Transformer-based video object detectors
still follow the same pipeline as those used for classical object detectors,
like enhancing the object feature representations by aggregation. In this work,
we take a different perspective on video object detection. In detail, we
improve the qualities of queries for the Transformer-based models by
aggregation. To achieve this goal, we first propose a vanilla query aggregation
module that weighted averages the queries according to the features of the
neighboring frames. Then, we extend the vanilla module to a more practical
version, which generates and aggregates queries according to the features of
the input frames. Extensive experimental results validate the effectiveness of
our proposed methods: On the challenging ImageNet VID benchmark, when
integrated with our proposed modules, the current state-of-the-art
Transformer-based object detectors can be improved by more than 2.4% on mAP and
4.2% on AP50.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DSVT: Dynamic Sparse Voxel <span class="highlight-title">Transformer</span> with Rotated Sets <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06051v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06051v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haiyang Wang, Chen Shi, Shaoshuai Shi, Meng Lei, Sen Wang, Di He, Bernt Schiele, Liwei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Designing an efficient yet deployment-friendly 3D backbone to handle sparse
point clouds is a fundamental problem in 3D perception. Compared with the
customized sparse convolution, the attention mechanism in Transformers is more
appropriate for flexibly modeling long-range relationships and is easier to be
deployed in real-world applications. However, due to the sparse characteristics
of point clouds, it is non-trivial to apply a standard transformer on sparse
points. In this paper, we present Dynamic Sparse Voxel Transformer (DSVT), a
single-stride window-based voxel Transformer backbone for outdoor 3D
perception. In order to efficiently process sparse points in parallel, we
propose Dynamic Sparse Window Attention, which partitions a series of local
regions in each window according to its sparsity and then computes the features
of all regions in a fully parallel manner. To allow the cross-set connection,
we design a rotated set partitioning strategy that alternates between two
partitioning configurations in consecutive self-attention layers. To support
effective downsampling and better encode geometric information, we also propose
an attention-style 3D pooling module on sparse points, which is powerful and
deployment-friendly without utilizing any customized CUDA operations. Our model
achieves state-of-the-art performance with a broad range of 3D perception
tasks. More importantly, DSVT can be easily deployed by TensorRT with real-time
inference speed (27Hz). Code will be available at
\url{https://github.com/Haiyang-W/DSVT}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Image comparison and scaling via nonlinear elasticity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10103v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10103v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John M. Ball, Christopher L. Horner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A nonlinear elasticity model for comparing images is formulated and analyzed,
in which optimal transformations between images are sought as minimizers of an
integral functional. The existence of minimizers in a suitable class of
homeomorphisms between image domains is established under natural hypotheses.
We investigate whether for linearly related images the minimization algorithm
delivers the linear transformation as the unique minimizer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SSVM2023 Proceedings to appear. New references added plus related
  minor changes</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bilevel Imaging Learning Problems as Mathematical Programs with
  Complementarity Constraints: Reformulation and Theory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.02273v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.02273v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juan Carlos De los Reyes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate a family of bilevel imaging learning problems where the
lower-level instance corresponds to a convex variational model involving first-
and second-order nonsmooth sparsity-based regularizers. By using geometric
properties of the primal-dual reformulation of the lower-level problem and
introducing suitable auxiliar variables, we are able to reformulate the
original bilevel problems as Mathematical Programs with Complementarity
Constraints (MPCC). For the latter, we prove tight constraint qualification
conditions (MPCC-RCPLD and partial MPCC-LICQ) and derive Mordukhovich (M-) and
Strong (S-) stationarity conditions. The stationarity systems for the MPCC turn
also into stationarity conditions for the original formulation. Second-order
sufficient optimality conditions are derived as well, together with a local
uniqueness result for stationary points. The proposed reformulation may be
extended to problems in function spaces, leading to MPCC's with constraints on
the gradient of the state. The MPCC reformulation also leads to the efficient
use of available large-scale nonlinear programming solvers, as shown in a
companion paper, where different imaging applications are studied.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Imagic: Text-Based Real Image Editing with Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.09276v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.09276v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, Michal Irani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-conditioned image editing has recently attracted considerable interest.
However, most methods are currently either limited to specific editing types
(e.g., object overlay, style transfer), or apply to synthetically generated
images, or require multiple input images of a common object. In this paper we
demonstrate, for the very first time, the ability to apply complex (e.g.,
non-rigid) text-guided semantic edits to a single real image. For example, we
can change the posture and composition of one or multiple objects inside an
image, while preserving its original characteristics. Our method can make a
standing dog sit down or jump, cause a bird to spread its wings, etc. -- each
within its single high-resolution natural image provided by the user. Contrary
to previous work, our proposed method requires only a single input image and a
target text (the desired edit). It operates on real images, and does not
require any additional inputs (such as image masks or additional views of the
object). Our method, which we call "Imagic", leverages a pre-trained
text-to-image diffusion model for this task. It produces a text embedding that
aligns with both the input image and the target text, while fine-tuning the
diffusion model to capture the image-specific appearance. We demonstrate the
quality and versatility of our method on numerous inputs from various domains,
showcasing a plethora of high quality complex semantic image edits, all within
a single unified framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://imagic-editing.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Development of Real-time Rendering Technology for High-Precision Models
  in Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.00291v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.00291v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhang Wencheng, Wang Chengyi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Our autonomous driving simulation lab produces a high-precision 3D model
simulating the parking lot. However, the current model still has poor rendering
quality in some aspects. In this work, we develop a system to improve the
rendering of the model and evaluate the quality of the rendered model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>3 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ABAW: Valence-Arousal Estimation, Expression Recognition, Action Unit
  Detection & Emotional Reaction Intensity Estimation Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.01498v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.01498v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dimitrios Kollias, Panagiotis Tzirakis, Alice Baird, Alan Cowen, Stefanos Zafeiriou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The fifth Affective Behavior Analysis in-the-wild (ABAW) Competition is part
of the respective ABAW Workshop which will be held in conjunction with IEEE
Computer Vision and Pattern Recognition Conference (CVPR), 2023. The 5th ABAW
Competition is a continuation of the Competitions held at ECCV 2022, IEEE CVPR
2022, ICCV 2021, IEEE FG 2020 and CVPR 2017 Conferences, and is dedicated at
automatically analyzing affect. For this year's Competition, we feature two
corpora: i) an extended version of the Aff-Wild2 database and ii) the
Hume-Reaction dataset. The former database is an audiovisual one of around 600
videos of around 3M frames and is annotated with respect to:a) two continuous
affect dimensions -valence (how positive/negative a person is) and arousal (how
active/passive a person is)-; b) basic expressions (e.g. happiness, sadness,
neutral state); and c) atomic facial muscle actions (i.e., action units). The
latter dataset is an audiovisual one in which reactions of individuals to
emotional stimuli have been annotated with respect to seven emotional
expression intensities. Thus the 5th ABAW Competition encompasses four
Challenges: i) uni-task Valence-Arousal Estimation, ii) uni-task Expression
Classification, iii) uni-task Action Unit Detection, and iv) Emotional Reaction
Intensity Estimation. In this paper, we present these Challenges, along with
their corpora, we outline the evaluation metrics, we present the baseline
systems and illustrate their obtained performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2202.10659</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Calibrating Semantic Segmentation Models: Analyses and An Algorithm <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.12053v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.12053v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongdong Wang, Boqing Gong, Liqiang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of semantic segmentation calibration. Lots of solutions
have been proposed to approach model miscalibration of confidence in image
classification. However, to date, confidence calibration research on semantic
segmentation is still limited. We provide a systematic study on the calibration
of semantic segmentation models and propose a simple yet effective approach.
First, we find that model capacity, crop size, multi-scale testing, and
prediction correctness have impact on calibration. Among them, prediction
correctness, especially misprediction, is more important to miscalibration due
to over-confidence. Next, we propose a simple, unifying, and effective
approach, namely selective scaling, by separating correct/incorrect prediction
for scaling and more focusing on misprediction logit smoothing. Then, we study
popular existing calibration methods and compare them with selective scaling on
semantic segmentation calibration. We conduct extensive experiments with a
variety of benchmarks on both in-domain and domain-shift calibration, and show
that selective scaling consistently outperforms other methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR2023 (8 pages, 4 figures)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GeCoNeRF: Few-shot Neural Radiance Fields via Geometric Consistency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10941v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10941v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minseop Kwak, Jiuhn Song, Seungryong Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel framework to regularize Neural Radiance Field (NeRF) in a
few-shot setting with a geometry-aware consistency regularization. The proposed
approach leverages a rendered depth map at unobserved viewpoint to warp sparse
input images to the unobserved viewpoint and impose them as pseudo ground
truths to facilitate learning of NeRF. By encouraging such geometry-aware
consistency at a feature-level instead of using pixel-level reconstruction
loss, we regularize the NeRF at semantic and structural levels while allowing
for modeling view dependent radiance to account for color variations across
viewpoints. We also propose an effective method to filter out erroneous warped
solutions, along with training strategies to stabilize training during
optimization. We show that our model achieves competitive results compared to
state-of-the-art few-shot NeRF models. Project page is available at
https://ku-cvlab.github.io/GeCoNeRF/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Self-Distillation Embedded Supervised Affinity Attention Model for
  Few-Shot Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2108.06600v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2108.06600v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Zhao, Binghao Liu, Shuchang Lyu, Huojin Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-shot segmentation focuses on the generalization of models to segment
unseen object with limited annotated samples. However, existing approaches
still face two main challenges. First, huge feature distinction between support
and query images causes knowledge transferring barrier, which harms the
segmentation performance. Second, limited support prototypes cannot adequately
represent features of support objects, hard to guide high-quality query
segmentation. To deal with the above two issues, we propose self-distillation
embedded supervised affinity attention model to improve the performance of
few-shot segmentation task. Specifically, the self-distillation guided
prototype module uses self-distillation to align the features of support and
query. The supervised affinity attention module generates high-quality query
attention map to provide sufficient object information. Extensive experiments
prove that our model significantly improves the performance compared to
existing methods. Comprehensive ablation experiments and visualization studies
also show the significant effect of our method on few-shot segmentation task.
On COCO-20i dataset, we achieve new state-of-the-art results. Training code and
pretrained models are available at https://github.com/cv516Buaa/SD-AANet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vision <span class="highlight-title">Transformer</span> for Action Units Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.09917v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.09917v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tu Vu, Van Thong Huynh, Soo Hyung Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Facial Action Units detection (FAUs) represents a fine-grained classification
problem that involves identifying different units on the human face, as defined
by the Facial Action Coding System. In this paper, we present a simple yet
efficient Vision Transformer-based approach for addressing the task of Action
Units (AU) detection in the context of Affective Behavior Analysis in-the-wild
(ABAW) competition. We employ the Video Vision Transformer(ViViT) Network to
capture the temporal facial change in the video. Besides, to reduce massive
size of the Vision Transformers model, we replace the ViViT feature extraction
layers with the CNN backbone (Regnet). Our model outperform the baseline model
of ABAW 2023 challenge, with a notable 14% difference in result. Furthermore,
the achieved results are comparable to those of the top three teams in the
previous ABAW 2022 challenge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Will be updated</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Inversion-Based Style Transfer with Diffusion Models <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.13203v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.13203v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Zhang, Nisha Huang, Fan Tang, Haibin Huang, Chongyang Ma, Weiming Dong, Changsheng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The artistic style within a painting is the means of expression, which
includes not only the painting material, colors, and brushstrokes, but also the
high-level attributes including semantic elements, object shapes, etc. Previous
arbitrary example-guided artistic image generation methods often fail to
control shape changes or convey elements. The pre-trained text-to-image
synthesis diffusion probabilistic models have achieved remarkable quality, but
it often requires extensive textual descriptions to accurately portray
attributes of a particular painting. We believe that the uniqueness of an
artwork lies precisely in the fact that it cannot be adequately explained with
normal language. Our key idea is to learn artistic style directly from a single
painting and then guide the synthesis without providing complex textual
descriptions. Specifically, we assume style as a learnable textual description
of a painting. We propose an inversion-based style transfer method (InST),
which can efficiently and accurately learn the key information of an image,
thus capturing and transferring the artistic style of a painting. We
demonstrate the quality and efficiency of our method on numerous paintings of
various artists and styles. Code and models are available at
https://github.com/zyxElsa/InST.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unsupervised Cumulative Domain Adaptation for Foggy Scene Optical Flow 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07564v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07564v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanyu Zhou, Yi Chang, Wending Yan, Luxin Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optical flow has achieved great success under clean scenes, but suffers from
restricted performance under foggy scenes. To bridge the clean-to-foggy domain
gap, the existing methods typically adopt the domain adaptation to transfer the
motion knowledge from clean to synthetic foggy domain. However, these methods
unexpectedly neglect the synthetic-to-real domain gap, and thus are erroneous
when applied to real-world scenes. To handle the practical optical flow under
real foggy scenes, in this work, we propose a novel unsupervised cumulative
domain adaptation optical flow (UCDA-Flow) framework: depth-association motion
adaptation and correlation-alignment motion adaptation. Specifically, we
discover that depth is a key ingredient to influence the optical flow: the
deeper depth, the inferior optical flow, which motivates us to design a
depth-association motion adaptation module to bridge the clean-to-foggy domain
gap. Moreover, we figure out that the cost volume correlation shares similar
distribution of the synthetic and real foggy images, which enlightens us to
devise a correlation-alignment motion adaptation module to distill motion
knowledge of the synthetic foggy domain to the real foggy domain. Note that
synthetic fog is designed as the intermediate domain. Under this unified
framework, the proposed cumulative adaptation progressively transfers knowledge
from clean scenes to real foggy scenes. Extensive experiments have been
performed to verify the superiority of the proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MedLocker: A Transferable Adversarial Watermarking for Preventing
  Unauthorized Analysis of Medical Image <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.09858v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.09858v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bangzheng Pu, Xingxing Wei, Shiji Zhao, Huazhu Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The collection of medical image datasets is a demanding and laborious process
that requires significant resources. Furthermore, these medical datasets may
contain personally identifiable information, necessitating measures to ensure
that unauthorized access is prevented. Failure to do so could violate the
intellectual property rights of the dataset owner and potentially compromise
the privacy of patients. As a result, safeguarding medical datasets and
preventing unauthorized usage by AI diagnostic models is a pressing challenge.
To address this challenge, we propose a novel visible adversarial watermarking
method for medical image copyright protection, called MedLocker. Our approach
involves continuously optimizing the position and transparency of a watermark
logo, which reduces the performance of the target model, leading to incorrect
predictions. Importantly, we ensure that our method minimizes the impact on
clinical visualization by constraining watermark positions using semantical
masks (WSM), which are bounding boxes of lesion regions based on semantic
segmentation. To ensure the transferability of the watermark across different
models, we verify the cross-model transferability of the watermark generated on
a single model. Additionally, we generate a unique watermark parameter list
each time, which can be used as a certification to verify the authorization. We
evaluate the performance of MedLocker on various mainstream backbones and
validate the feasibility of adversarial watermarking for copyright protection
on two widely-used diabetic retinopathy detection datasets. Our results
demonstrate that MedLocker can effectively protect the copyright of medical
datasets and prevent unauthorized users from analyzing medical images with AI
diagnostic models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ N-Gram in Swin <span class="highlight-title">Transformer</span>s for Efficient Lightweight Image
  Super-Resolution <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.11436v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.11436v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haram Choi, Jeongmin Lee, Jihoon Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While some studies have proven that Swin Transformer (Swin) with window
self-attention (WSA) is suitable for single image super-resolution (SR), the
plain WSA ignores the broad regions when reconstructing high-resolution images
due to a limited receptive field. In addition, many deep learning SR methods
suffer from intensive computations. To address these problems, we introduce the
N-Gram context to the low-level vision with Transformers for the first time. We
define N-Gram as neighboring local windows in Swin, which differs from text
analysis that views N-Gram as consecutive characters or words. N-Grams interact
with each other by sliding-WSA, expanding the regions seen to restore degraded
pixels. Using the N-Gram context, we propose NGswin, an efficient SR network
with SCDP bottleneck taking multi-scale outputs of the hierarchical encoder.
Experimental results show that NGswin achieves competitive performance while
maintaining an efficient structure when compared with previous leading methods.
Moreover, we also improve other Swin-based SR methods with the N-Gram context,
thereby building an enhanced model: SwinIR-NG. Our improved SwinIR-NG
outperforms the current best lightweight SR approaches and establishes
state-of-the-art results. Codes are available at
https://github.com/rami0205/NGramSwin.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023 camera-ready. Codes are available at
  https://github.com/rami0205/NGramSwin</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Not All Voxels Are Equal: Semantic Scene Completion from the Point-Voxel
  Perspective <span class="chip">AAAI 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.12925v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.12925v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaokang Chen, Jiaxiang Tang, Jingbo Wang, Gang Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We revisit Semantic Scene Completion (SSC), a useful task to predict the
semantic and occupancy representation of 3D scenes, in this paper. A number of
methods for this task are always based on voxelized scene representations for
keeping local scene structure. However, due to the existence of visible empty
voxels, these methods always suffer from heavy computation redundancy when the
network goes deeper, and thus limit the completion quality. To address this
dilemma, we propose our novel point-voxel aggregation network for this task.
Firstly, we transfer the voxelized scenes to point clouds by removing these
visible empty voxels and adopt a deep point stream to capture semantic
information from the scene efficiently. Meanwhile, a light-weight voxel stream
containing only two 3D convolution layers preserves local structures of the
voxelized scenes. Furthermore, we design an anisotropic voxel aggregation
operator to fuse the structure details from the voxel stream into the point
stream, and a semantic-aware propagation module to enhance the up-sampling
process in the point stream by semantic labels. We demonstrate that our model
surpasses state-of-the-arts on two benchmarks by a large margin, with only
depth images as the input.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Data-Free Quantization <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06869v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06869v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Biao Qian, Yang Wang, Richang Hong, Meng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data-free quantization (DFQ) recovers the performance of quantized network
(Q) without the original data, but generates the fake sample via a generator
(G) by learning from full-precision network (P), which, however, is totally
independent of Q, overlooking the adaptability of the knowledge from generated
samples, i.e., informative or not to the learning process of Q, resulting into
the overflow of generalization error. Building on this, several critical
questions -- how to measure the sample adaptability to Q under varied bit-width
scenarios? whether the largest adaptability is the best? how to generate the
samples with adaptive adaptability to improve Q's generalization? To answer the
above questions, in this paper, we propose an Adaptive Data-Free Quantization
(AdaDFQ) method, which revisits DFQ from a zero-sum game perspective upon the
sample adaptability between two players -- a generator and a quantized network.
Following this viewpoint, we further define the disagreement and agreement
samples to form two boundaries, where the margin is optimized to adaptively
regulate the adaptability of generated samples to Q, so as to address the
over-and-under fitting issues. Our AdaDFQ reveals: 1) the largest adaptability
is NOT the best for sample generation to benefit Q's generalization; 2) the
knowledge of the generated sample should not be informative to Q only, but also
related to the category and distribution information of the training data for
P. The theoretical and empirical analysis validate the advantages of AdaDFQ
over the state-of-the-arts. Our code is available at
https://github.com/hfutqian/AdaDFQ.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 6 figures, Refined camera ready version for CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Facial Affect Recognition based on <span class="highlight-title">Transformer</span> Encoder and Audiovisual
  Fusion for the ABAW5 Challenge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.09158v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.09158v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyang Zhang, Liuwei An, Zishun Cui, Ao xu, Tengteng Dong, Yueqi Jiang, Jingyi Shi, Xin Liu, Xiao Sun, Meng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present our solutions for the 5th Workshop and Competition
on Affective Behavior Analysis in-the-wild (ABAW), which includes four
sub-challenges of Valence-Arousal (VA) Estimation, Expression (Expr)
Classification, Action Unit (AU) Detection and Emotional Reaction Intensity
(ERI) Estimation. The 5th ABAW competition focuses on facial affect recognition
utilizing different modalities and datasets. In our work, we extract powerful
audio and visual features using a large number of sota models. These features
are fused by Transformer Encoder and TEMMA. Besides, to avoid the possible
impact of large dimensional differences between various features, we design an
Affine Module to align different features to the same dimension. Extensive
experiments demonstrate that the superiority of the proposed method. For the VA
Estimation sub-challenge, our method obtains the mean Concordance Correlation
Coefficient (CCC) of 0.6066. For the Expression Classification sub-challenge,
the average F1 Score is 0.4055. For the AU Detection sub-challenge, the average
F1 Score is 0.5296. For the Emotional Reaction Intensity Estimation
sub-challenge, the average pearson's correlations coefficient on the validation
set is 0.3968. All of the results of four sub-challenges outperform the
baseline with a large margin.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Shape, Pose, and Appearance from a Single Image via Bootstrapped
  Radiance Field Inversion <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.11674v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.11674v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dario Pavllo, David Joseph Tan, Marie-Julie Rakotosaona, Federico Tombari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Radiance Fields (NeRF) coupled with GANs represent a promising
direction in the area of 3D reconstruction from a single view, owing to their
ability to efficiently model arbitrary topologies. Recent work in this area,
however, has mostly focused on synthetic datasets where exact ground-truth
poses are known, and has overlooked pose estimation, which is important for
certain downstream applications such as augmented reality (AR) and robotics. We
introduce a principled end-to-end reconstruction framework for natural images,
where accurate ground-truth poses are not available. Our approach recovers an
SDF-parameterized 3D shape, pose, and appearance from a single image of an
object, without exploiting multiple views during training. More specifically,
we leverage an unconditional 3D-aware generator, to which we apply a hybrid
inversion scheme where a model produces a first guess of the solution which is
then refined via optimization. Our framework can de-render an image in as few
as 10 steps, enabling its use in practical scenarios. We demonstrate
state-of-the-art results on a variety of real and synthetic benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023. Code and models are available at
  https://github.com/google-research/nerf-from-image</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SeaTurtleID: A novel long-span <span class="highlight-title">dataset</span> highlighting the importance of
  timestamps in wildlife re-identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.10307v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.10307v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kostas Papafitsoros, Lukáš Adam, Vojtěch Čermák, Lukáš Picek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces SeaTurtleID, the first public large-scale, long-span
dataset with sea turtle photographs captured in the wild. The dataset is
suitable for benchmarking re-identification methods and evaluating several
other computer vision tasks. The dataset consists of 7774 high-resolution
photographs of 400 unique individuals collected within 12 years in 1081
encounters. Each photograph is accompanied by rich metadata, e.g., identity
label, head segmentation mask, and encounter timestamp. The 12-year span of the
dataset makes it the longest-spanned public wild animal dataset with
timestamps. By exploiting this unique property, we show that timestamps are
necessary for an unbiased evaluation of animal re-identification methods
because they allow time-aware splits of the dataset into reference and query
sets. We show that time-unaware (random) splits can lead to performance
overestimation of more than 100% compared to the time-aware splits for both
feature- and CNN-based re-identification methods. We also argue that time-aware
splits correspond to more realistic re-identification pipelines than the
time-unaware ones. We recommend that animal re-identification methods should
only be tested on datasets with timestamps using time-aware splits, and we
encourage dataset curators to include such information in the associated
metadata.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TKN: <span class="highlight-title">Transformer</span>-based Keypoint Prediction Network For Real-time Video
  Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.09807v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.09807v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Li, Pengyuan Zhou, Yihang Lin, Yanbin Hao, Haiyong Xie, Yong Liao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video prediction is a complex time-series forecasting task with great
potential in many use cases. However, conventional methods overemphasize
accuracy while ignoring the slow prediction speed caused by complicated model
structures that learn too much redundant information with excessive GPU memory
consumption. Furthermore, conventional methods mostly predict frames
sequentially (frame-by-frame) and thus are hard to accelerate. Consequently,
valuable use cases such as real-time danger prediction and warning cannot
achieve fast enough inference speed to be applicable in reality. Therefore, we
propose a transformer-based keypoint prediction neural network (TKN), an
unsupervised learning method that boost the prediction process via constrained
information extraction and parallel prediction scheme. TKN is the first
real-time video prediction solution to our best knowledge, while significantly
reducing computation costs and maintaining other performance. Extensive
experiments on KTH and Human3.6 datasets demonstrate that TKN predicts 11 times
faster than existing methods while reducing memory consumption by 17.4% and
achieving state-of-the-art prediction performance on average.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Simple Framework for Open-Vocabulary Segmentation and Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08131v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08131v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Zhang, Feng Li, Xueyan Zou, Shilong Liu, Chunyuan Li, Jianfeng Gao, Jianwei Yang, Lei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present OpenSeeD, a simple Open-vocabulary Segmentation and Detection
framework that jointly learns from different segmentation and detection
datasets. To bridge the gap of vocabulary and annotation granularity, we first
introduce a pre-trained text encoder to encode all the visual concepts in two
tasks and learn a common semantic space for them. This gives us reasonably good
results compared with the counterparts trained on segmentation task only. To
further reconcile them, we locate two discrepancies: $i$) task discrepancy --
segmentation requires extracting masks for both foreground objects and
background stuff, while detection merely cares about the former; $ii$) data
discrepancy -- box and mask annotations are with different spatial granularity,
and thus not directly interchangeable. To address these issues, we propose a
decoupled decoding to reduce the interference between foreground/background and
a conditioned mask decoding to assist in generating masks for given boxes. To
this end, we develop a simple encoder-decoder model encompassing all three
techniques and train it jointly on COCO and Objects365. After pre-training, our
model exhibits competitive or stronger zero-shot transferability for both
segmentation and detection. Specifically, OpenSeeD beats the state-of-the-art
method for open-vocabulary instance and panoptic segmentation across 5
datasets, and outperforms previous work for open-vocabulary detection on LVIS
and ODinW under similar settings. When transferred to specific tasks, our model
achieves new SoTA for panoptic segmentation on COCO and ADE20K, and instance
segmentation on ADE20K and Cityscapes.
  Finally, we note that OpenSeeD is the first to explore the potential of joint
training on segmentation and detection, and hope it can be received as a strong
baseline for developing a single model for both tasks in open world.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A Simple Framework for Open-Vocabulary Segmentation and Detection</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for
  Text-to-Image Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.08453v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.08453v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, Ying Shan, Xiaohu Qie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The incredible generative ability of large-scale text-to-image (T2I) models
has demonstrated strong power of learning complex structures and meaningful
semantics. However, relying solely on text prompts cannot fully take advantage
of the knowledge learned by the model, especially when flexible and accurate
controlling (e.g., color and structure) is needed. In this paper, we aim to
``dig out" the capabilities that T2I models have implicitly learned, and then
explicitly use them to control the generation more granularly. Specifically, we
propose to learn simple and lightweight T2I-Adapters to align internal
knowledge in T2I models with external control signals, while freezing the
original large T2I models. In this way, we can train various adapters according
to different conditions, achieving rich control and editing effects in the
color and structure of the generation results. Further, the proposed
T2I-Adapters have attractive properties of practical value, such as
composability and generalization ability. Extensive experiments demonstrate
that our T2I-Adapter has promising generation quality and a wide range of
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Tech Report. GitHub: https://github.com/TencentARC/T2I-Adapter</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SRPCN: Structure Retrieval based Point Completion Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.02669v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.02669v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaiyi Zhang, Ximing Yang, Yuan Wu, Cheng Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given partial objects and some complete ones as references, point cloud
completion aims to recover authentic shapes. However, existing methods pay
little attention to general shapes, which leads to the poor authenticity of
completion results. Besides, the missing patterns are diverse in reality, but
existing methods can only handle fixed ones, which means a poor generalization
ability. Considering that a partial point cloud is a subset of the
corresponding complete one, we regard them as different samples of the same
distribution and propose Structure Retrieval based Point Completion Network
(SRPCN). It first uses k-means clustering to extract structure points and
disperses them into distributions, and then KL Divergence is used as a metric
to find the complete structure point cloud that best matches the input in a
database. Finally, a PCN-like decoder network is adopted to generate the final
results based on the retrieved structure point clouds. As structure plays an
important role in describing the general shape of an object and the proposed
structure retrieval method is robust to missing patterns, experiments show that
our method can generate more authentic results and has a stronger
generalization ability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>I think the proposed method has some defects</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Clustering disease trajectories in contrastive feature space for
  biomarker discovery in age-related macular degeneration <span class="chip">MICCAI2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.04525v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.04525v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robbie Holland, Oliver Leingang, Christopher Holmes, Philipp Anders, Rebecca Kaye, Sophie Riedl, Johannes C. Paetzold, Ivan Ezhov, Hrvoje Bogunović, Ursula Schmidt-Erfurth, Lars Fritsche, Hendrik P. N. Scholl, Sobha Sivaprasad, Andrew J. Lotery, Daniel Rueckert, Martin J. Menten
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Age-related macular degeneration (AMD) is the leading cause of blindness in
the elderly. Current grading systems based on imaging biomarkers only coarsely
group disease stages into broad categories and are unable to predict future
disease progression. It is widely believed that this is due to their focus on a
single point in time, disregarding the dynamic nature of the disease. In this
work, we present the first method to automatically discover biomarkers that
capture temporal dynamics of disease progression. Our method represents patient
time series as trajectories in a latent feature space built with contrastive
learning. Then, individual trajectories are partitioned into atomic
sub-sequences that encode transitions between disease states. These are
clustered using a newly introduced distance metric. In quantitative experiments
we found our method yields temporal biomarkers that are predictive of
conversion to late AMD. Furthermore, these clusters were highly interpretable
to ophthalmologists who confirmed that many of the clusters represent dynamics
that have previously been linked to the progression of AMD, even though they
are currently not included in any clinical grading system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to MICCAI2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MATE: Masked Autoencoders are Online 3D Test-Time Learners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.11432v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.11432v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        M. Jehanzeb Mirza, Inkyu Shin, Wei Lin, Andreas Schriebl, Kunyang Sun, Jaesung Choe, Horst Possegger, Mateusz Kozinski, In So Kweon, Kun-Jin Yoon, Horst Bischof
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Our MATE is the first Test-Time-Training (TTT) method designed for 3D data,
which makes deep networks trained for point cloud classification robust to
distribution shifts occurring in test data. Like existing TTT methods from the
2D image domain, MATE also leverages test data for adaptation. Its test-time
objective is that of a Masked Autoencoder: a large portion of each test point
cloud is removed before it is fed to the network, tasked with reconstructing
the full point cloud. Once the network is updated, it is used to classify the
point cloud. We test MATE on several 3D object classification datasets and show
that it significantly improves robustness of deep networks to several types of
corruptions commonly occurring in 3D point clouds. We show that MATE is very
efficient in terms of the fraction of points it needs for the adaptation. It
can effectively adapt given as few as 5% of tokens of each test sample, making
it extremely lightweight. Our experiments show that MATE also achieves
competitive performance by adapting sparsely on the test data, which further
reduces its computational overhead, making it ideal for real-time applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at this repository:
  https://github.com/jmiemirza/MATE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Domain generalization of 3D semantic segmentation in autonomous driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.04245v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.04245v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jules Sanchez, Jean-Emmanuel Deschaud, Francois Goulette
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Using deep learning, 3D autonomous driving semantic segmentation has become a
well-studied subject, with methods that can reach very high performance.
Nonetheless, because of the limited size of the training datasets, these models
cannot see every type of object and scene found in real-world applications. The
ability to be reliable in these various unknown environments is called domain
generalization.
  Despite its importance, domain generalization is relatively unexplored in the
case of 3D autonomous driving semantic segmentation. To fill this gap, this
paper presents the first benchmark for this application by testing
state-of-the-art methods and discussing the difficulty of tackling Laser
Imaging Detection and Ranging (LiDAR) domain shifts.
  We also propose the first method designed to address this domain
generalization, which we call 3DLabelProp. This method relies on leveraging the
geometry and sequentiality of the LiDAR data to enhance its generalization
performances by working on partially accumulated point clouds. It reaches a
mean Intersection over Union (mIoU) of 50.4% on SemanticPOSS and of 55.2% on
PandaSet solid-state LiDAR while being trained only on SemanticKITTI, making it
the state-of-the-art method for generalization (+5% and +33% better,
respectively, than the second best method).
  The code for this method will be available on GitHub.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Self-supervised</span> Character-to-Character Distillation for Text Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.00288v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.00288v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tongkun Guan, Wei Shen, Xue Yang, Qi Feng, Zekun Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When handling complicated text images (e.g., irregular structures, low
resolution, heavy occlusion, and uneven illumination), existing supervised text
recognition methods are data-hungry. Although these methods employ large-scale
synthetic text images to reduce the dependence on annotated real images, the
domain gap still limits the recognition performance. Therefore, exploring the
robust text feature representations on unlabeled real images by self-supervised
learning is a good solution. However, existing self-supervised text recognition
methods conduct sequence-to-sequence representation learning by roughly
splitting the visual features along the horizontal axis, which limits the
flexibility of the augmentations, as large geometric-based augmentations may
lead to sequence-to-sequence feature inconsistency. Motivated by this, we
propose a novel self-supervised Character-to-Character Distillation method,
CCD, which enables versatile augmentations to facilitate general text
representation learning. Specifically, we delineate the character structures of
unlabeled real images by designing a self-supervised character segmentation
module. Following this, CCD easily enriches the diversity of local characters
while keeping their pairwise alignment under flexible augmentations, using the
transformation matrix between two augmented views from images. Experiments
demonstrate that CCD achieves state-of-the-art results, with average
performance gains of 1.38% in text recognition, 1.7% in text segmentation, 0.24
dB (PSNR) and 0.0321 (SSIM) in text super-resolution. Code will be released
soon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dyna-DepthFormer: Multi-frame <span class="highlight-title">Transformer</span> for <span class="highlight-title">Self-Supervised</span> Depth
  Estimation in Dynamic Scenes <span class="chip">ICRA 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.05871v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.05871v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Songchun Zhang, Chunhui Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised methods have showed promising results on depth estimation
task. However, previous methods estimate the target depth map and camera
ego-motion simultaneously, underusing multi-frame correlation information and
ignoring the motion of dynamic objects. In this paper, we propose a novel
Dyna-Depthformer framework, which predicts scene depth and 3D motion field
jointly and aggregates multi-frame information with transformer. Our
contributions are two-fold. First, we leverage multi-view correlation through a
series of self- and cross-attention layers in order to obtain enhanced depth
feature representation. Specifically, we use the perspective transformation to
acquire the initial reference point, and use deformable attention to reduce the
computational cost. Second, we propose a warping-based Motion Network to
estimate the motion field of dynamic objects without using semantic prior. To
improve the motion field predictions, we propose an iterative optimization
strategy, together with a sparsity-regularized loss. The entire pipeline
achieves end-to-end self-supervised training by constructing a minimum
reprojection loss. Extensive experiments on the KITTI and Cityscapes benchmarks
demonstrate the effectiveness of our method and show that our method
outperforms state-of-the-art algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICRA 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DETRs with Collaborative Hybrid Assignments Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.12860v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.12860v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuofan Zong, Guanglu Song, Yu Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we provide the observation that too few queries assigned as
positive samples in DETR with one-to-one set matching leads to sparse
supervisions on the encoder's output which considerably hurt the discriminative
feature learning of the encoder and vice visa for attention learning in the
decoder. To alleviate this, we present a novel collaborative hybrid assignments
training scheme, namely Co-DETR, to learn more efficient and effective
DETR-based detectors from versatile label assignment manners. This new training
scheme can easily enhance the encoder's learning ability in end-to-end
detectors by training the multiple parallel auxiliary heads supervised by
one-to-many label assignments such as ATSS, FCOS, and Faster RCNN. In addition,
we conduct extra customized positive queries by extracting the positive
coordinates from these auxiliary heads to improve the training efficiency of
positive samples in the decoder. In inference, these auxiliary heads are
discarded and thus our method introduces no additional parameters and
computational cost to the original detector while requiring no hand-crafted
non-maximum suppression (NMS). We conduct extensive experiments to evaluate the
effectiveness of the proposed approach on DETR variants, including DAB-DETR,
Deformable-DETR, and DINO-Deformable-DETR. Specifically, we improve the basic
Deformable-DETR by 5.8% in 12-epoch training and 3.2% in 36-epoch training. The
state-of-the-art DINO-Deformable-DETR with Swin-L can still be improved from
58.5% to 59.5%. Surprisingly, incorporated with the large-scale backbone
MixMIM-g with 1-Billion parameters, we achieve the 64.5% mAP on MS COCO
test-dev, achieving superior performance with much fewer extra data sizes.
Codes will be available at https://github.com/Sense-X/Co-DETR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Tech report. Codes will be available at
  https://github.com/Sense-X/Co-DETR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leaping Into Memories: Space-Time Deep Feature Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.09941v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.09941v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandros Stergiou, Nikos Deligiannis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The success of deep learning models has led to their adaptation and adoption
by prominent video understanding methods. The majority of these approaches
encode features in a joint space-time modality for which the inner workings and
learned representations are difficult to visually interpret. We propose LEArned
Preconscious Synthesis (LEAPS), an architecture-agnostic method for
synthesizing videos from the internal spatiotemporal representations of models.
Using a stimulus video and a target class, we prime a fixed space-time model
and iteratively optimize a video initialized with random noise. We incorporate
additional regularizers to improve the feature diversity of the synthesized
videos as well as the cross-frame temporal coherence of motions. We
quantitatively and qualitatively evaluate the applicability of LEAPS by
inverting a range of spatiotemporal convolutional and attention-based
architectures trained on Kinetics-400, which to the best of our knowledge has
not been previously accomplished.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Detailed Radiance Manifolds for High-Fidelity and 3D-Consistent
  Portrait Synthesis from Monocular Image <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.13901v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.13901v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Deng, Baoyuan Wang, Heung-Yeung Shum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A key challenge for novel view synthesis of monocular portrait images is 3D
consistency under continuous pose variations. Most existing methods rely on 2D
generative models which often leads to obvious 3D inconsistency artifacts. We
present a 3D-consistent novel view synthesis approach for monocular portrait
images based on a recent proposed 3D-aware GAN, namely Generative Radiance
Manifolds (GRAM), which has shown strong 3D consistency at multiview image
generation of virtual subjects via the radiance manifolds representation.
However, simply learning an encoder to map a real image into the latent space
of GRAM can only reconstruct coarse radiance manifolds without faithful fine
details, while improving the reconstruction fidelity via instance-specific
optimization is time-consuming. We introduce a novel detail manifolds
reconstructor to learn 3D-consistent fine details on the radiance manifolds
from monocular images, and combine them with the coarse radiance manifolds for
high-fidelity reconstruction. The 3D priors derived from the coarse radiance
manifolds are used to regulate the learned details to ensure reasonable
synthesized results at novel views. Trained on in-the-wild 2D images, our
method achieves high-fidelity and 3D-consistent portrait synthesis largely
outperforming the prior art.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023 camera-ready version. Project page:
  https://yudeng.github.io/GRAMInverter/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cross-Modality Sub-Image Retrieval using Contrastive Multimodal Image
  Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.03597v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.03597v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eva Breznik, Elisabeth Wetzer, Joakim Lindblad, Nataša Sladoje
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In tissue characterization and cancer diagnostics, multimodal imaging has
emerged as a powerful technique. Thanks to computational advances, large
datasets can be exploited to discover patterns in pathologies and improve
diagnosis. However, this requires efficient and scalable image retrieval
methods. Cross-modality image retrieval is particularly challenging, since
images of similar (or even the same) content captured by different modalities
might share few common structures. We propose a new application-independent
content-based image retrieval (CBIR) system for reverse (sub-)image search
across modalities, which combines deep learning to generate representations
(embedding the different modalities in a common space) with classical feature
extraction and bag-of-words models for efficient and reliable retrieval. We
illustrate its advantages through a replacement study, exploring a number of
feature extractors and learned representations, as well as through comparison
to recent (cross-modality) CBIR methods. For the task of (sub-)image retrieval
on a (publicly available) dataset of brightfield and second harmonic generation
microscopy images, the results show that our approach is superior to all tested
alternatives. We discuss the shortcomings of the compared methods and observe
the importance of equivariance and invariance properties of the learned
representations and feature extractors in the CBIR pipeline. Code is available
at: \url{https://github.com/MIDA-group/CrossModal_ImgRetrieval}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Uncertainty-informed Mutual Learning for Joint Medical Image
  Classification and Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10049v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10049v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Ren, Ke Zou, Xianjie Liu, Yidi Chen, Xuedong Yuan, Xiaojing Shen, Meng Wang, Huazhu Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classification and segmentation are crucial in medical image analysis as they
enable accurate diagnosis and disease monitoring. However, current methods
often prioritize the mutual learning features and shared model parameters,
while neglecting the reliability of features and performances. In this paper,
we propose a novel Uncertainty-informed Mutual Learning (UML) framework for
reliable and interpretable medical image analysis. Our UML introduces
reliability to joint classification and segmentation tasks, leveraging mutual
learning with uncertainty to improve performance. To achieve this, we first use
evidential deep learning to provide image-level and pixel-wise confidences.
Then, an Uncertainty Navigator Decoder is constructed for better using mutual
features and generating segmentation results. Besides, an Uncertainty
Instructor is proposed to screen reliable masks for classification. Overall,
UML could produce confidence estimation in features and performance for each
link (classification and segmentation). The experiments on the public datasets
demonstrate that our UML outperforms existing methods in terms of both accuracy
and robustness. Our UML has the potential to explore the development of more
reliable and explainable medical image analysis models. We will release the
codes for reproduction after acceptance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 3 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VieCap4H-VLSP 2021: ObjectAoA-Enhancing performance of Object Relation
  <span class="highlight-title">Transformer</span> with Attention on Attention for Vietnamese image captioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.05405v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.05405v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nghia Hieu Nguyen, Duong T. D. Vo, Minh-Quan Ha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image captioning is currently a challenging task that requires the ability to
both understand visual information and use human language to describe this
visual information in the image. In this paper, we propose an efficient way to
improve the image understanding ability of transformer-based method by
extending Object Relation Transformer architecture with Attention on Attention
mechanism. Experiments on the VieCap4H dataset show that our proposed method
significantly outperforms its original structure on both the public test and
private test of the Image Captioning shared task held by VLSP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publishing at the VNU Journal of Science: Computer
  Science and Communication Engineering</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contracting Skeletal Kinematic Embeddings for Anomaly Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09489v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09489v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessandro Flaborea, Guido D'Amely, Stefano D'Arrigo, Marco Aurelio Sterpa, Alessio Sampieri, Fabio Galasso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting the anomaly of human behavior is paramount to timely recognizing
endangering situations, such as street fights or elderly falls. However,
anomaly detection is complex, since anomalous events are rare and because it is
an open set recognition task, i.e., what is anomalous at inference has not been
observed at training. We propose COSKAD, a novel model which encodes skeletal
human motion by an efficient graph convolutional network and learns to COntract
SKeletal kinematic embeddings onto a latent hypersphere of minimum volume for
Anomaly Detection. We propose and analyze three latent space designs for
COSKAD: the commonly-adopted Euclidean, and the new spherical-radial and
hyperbolic volumes. All three variants outperform the state-of-the-art,
including video-based techniques, on the ShangaiTechCampus, the Avenue, and on
the most recent UBnormal dataset, for which we contribute novel skeleton
annotations and the selection of human-related videos. The source code and
dataset will be released upon acceptance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to Pattern Recognition Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative Adversarial Network for Personalized Art Therapy in Melanoma
  Disease Management 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.09232v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.09232v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lennart Jütte, Ning Wang, Bernhard Roth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Melanoma is the most lethal type of skin cancer. Patients are vulnerable to
mental health illnesses which can reduce the effectiveness of the cancer
treatment and the patients adherence to drug plans. It is crucial to preserve
the mental health of patients while they are receiving treatment. However,
current art therapy approaches are not personal and unique to the patient. We
aim to provide a well-trained image style transfer model that can quickly
generate unique art from personal dermoscopic melanoma images as an additional
tool for art therapy in disease management of melanoma. Visual art appreciation
as a common form of art therapy in disease management that measurably reduces
the degree of psychological distress. We developed a network based on the
cycle-consistent generative adversarial network for style transfer that
generates personalized and unique artworks from dermoscopic melanoma images. We
developed a model that converts melanoma images into unique flower-themed
artworks that relate to the shape of the lesion and are therefore personal to
the patient. Further, we altered the initial framework and made comparisons and
evaluations of the results. With this, we increased the options in the toolbox
for art therapy in disease management of melanoma. The development of an
easy-to-use user interface ensures the availability of the approach to
stakeholders. The transformation of melanoma into flower-themed artworks is
achieved by the proposed model and the graphical user interface. This
contribution opens a new field of GANs in art therapy and could lead to more
personalized disease management.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Longitudinal thermal imaging for scalable non-residential HVAC and
  occupant behaviour characterization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.09288v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.09288v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vasantha Ramani, Miguel Martin, Pandarasamy Arjunan, Adrian Chong, Kameshwar Poolla, Clayton Miller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work presents a study on the characterization of the air-conditioning
(AC) usage pattern of non-residential buildings from thermal images collected
from an urban-scale infrared (IR) observatory. To achieve this first, an image
processing scheme, for cleaning and extraction of the temperature time series
from the thermal images is implemented. To test the accuracy of the thermal
measurements using IR camera, the extracted temperature is compared against the
ground truth surface temperature measurements. It is observed that the
detrended thermal measurements match well with the ground truth surface
temperature measurements. Subsequently, the operational pattern of the
water-cooled systems and window AC units are extracted from the analysis of the
thermal signature. It is observed that for the water-cooled system, the
difference between the rate of change of the window and wall can be used to
extract the operational pattern. While, in the case of the window AC units,
wavelet transform of the AC unit temperature is used to extract the frequency
and time domain information of the AC unit operation. The results of the
analysis are compared against the indoor temperature sensors installed in the
office spaces of the building. It is realized that the accuracy in the
prediction of the operational pattern is highest between 8 pm to 10 am, and it
reduces during the day because of solar radiation and high daytime temperature.
Subsequently, a characterization study is conducted for eight window/split AC
units from the thermal image collected during the nighttime. This forms one of
the first studies on the operational behavior of HVAC systems for
non-residential buildings using the longitudinal thermal imaging technique. The
output from this study can be used to better understand the operational and
occupant behavior, without requiring to deploy a large array of sensors in the
building space.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Divide and Conquer: 3D Point Cloud Instance Segmentation With Point-Wise
  Binarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.11209v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.11209v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiguang Zhao, Yuyao Yan, Chaolong Yang, Jianan Ye, Xi Yang, Kaizhu Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instance segmentation on point clouds is crucially important for 3D scene
understanding. Most SOTAs adopt distance clustering, which is typically
effective but does not perform well in segmenting adjacent objects with the
same semantic label (especially when they share neighboring points). Due to the
uneven distribution of offset points, these existing methods can hardly cluster
all instance points. To this end, we design a novel divide-and-conquer strategy
named PBNet that binarizes each point and clusters them separately to segment
instances. Our binary clustering divides offset instance points into two
categories: high and low density points (HPs vs. LPs). Adjacent objects can be
clearly separated by removing LPs, and then be completed and refined by
assigning LPs via a neighbor voting method. To suppress potential
over-segmentation, we propose to construct local scenes with the weight mask
for each instance. As a plug-in, the proposed binary clustering can replace the
traditional distance clustering and lead to consistent performance gains on
many mainstream baselines. A series of experiments on ScanNetV2 and S3DIS
datasets indicate the superiority of our model. In particular, PBNet ranks
first on the ScanNetV2 official benchmark challenge, achieving the highest mAP.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Message Passing Neural PDE Solvers <span class="chip">ICLR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.03376v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.03376v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Johannes Brandstetter, Daniel Worrall, Max Welling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The numerical solution of partial differential equations (PDEs) is difficult,
having led to a century of research so far. Recently, there have been pushes to
build neural--numerical hybrid solvers, which piggy-backs the modern trend
towards fully end-to-end learned systems. Most works so far can only generalize
over a subset of properties to which a generic solver would be faced,
including: resolution, topology, geometry, boundary conditions, domain
discretization regularity, dimensionality, etc. In this work, we build a
solver, satisfying these properties, where all the components are based on
neural message passing, replacing all heuristically designed components in the
computation graph with backprop-optimized neural function approximators. We
show that neural message passing solvers representationally contain some
classical methods, such as finite differences, finite volumes, and WENO
schemes. In order to encourage stability in training autoregressive models, we
put forward a method that is based on the principle of zero-stability, posing
stability as a domain adaptation problem. We validate our method on various
fluid-like flow problems, demonstrating fast, stable, and accurate performance
across different domain topologies, equation parameters, discretizations, etc.,
in 1D and 2D.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at ICLR 2022 (Spotlight paper), Github:
  https://github.com/brandstetter-johannes/MP-Neural-PDE-Solvers</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Nighttime Driving-Scene Segmentation via Dual Image-adaptive
  Learnable Filters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.01331v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.01331v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenyu Liu, Wentong Li, Jianke Zhu, Miaomiao Cui, Xuansong Xie, Lei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic segmentation on driving-scene images is vital for autonomous
driving. Although encouraging performance has been achieved on daytime images,
the performance on nighttime images are less satisfactory due to the
insufficient exposure and the lack of labeled data. To address these issues, we
present an add-on module called dual image-adaptive learnable filters
(DIAL-Filters) to improve the semantic segmentation in nighttime driving
conditions, aiming at exploiting the intrinsic features of driving-scene images
under different illuminations. DIAL-Filters consist of two parts, including an
image-adaptive processing module (IAPM) and a learnable guided filter (LGF).
With DIAL-Filters, we design both unsupervised and supervised frameworks for
nighttime driving-scene segmentation, which can be trained in an end-to-end
manner. Specifically, the IAPM module consists of a small convolutional neural
network with a set of differentiable image filters, where each image can be
adaptively enhanced for better segmentation with respect to the different
illuminations. The LGF is employed to enhance the output of segmentation
network to get the final segmentation result. The DIAL-Filters are light-weight
and efficient and they can be readily applied for both daytime and nighttime
images. Our experiments show that DAIL-Filters can significantly improve the
supervised segmentation performance on ACDC_Night and NightCity datasets, while
it demonstrates the state-of-the-art performance on unsupervised nighttime
semantic segmentation on Dark Zurich and Nighttime Driving testbeds.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE TCSVT(2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Self-supervised</span> Contrastive Learning for Audio-Visual Action Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.13386v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.13386v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Liu, Ying Tan, Haoyuan Lan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The underlying correlation between audio and visual modalities can be
utilized to learn supervised information for unlabeled videos. In this paper,
we propose an end-to-end self-supervised framework named Audio-Visual
Contrastive Learning (AVCL), to learn discriminative audio-visual
representations for action recognition. Specifically, we design an attention
based multi-modal fusion module (AMFM) to fuse audio and visual modalities. To
align heterogeneous audio-visual modalities, we construct a novel
co-correlation guided representation alignment module (CGRA). To learn
supervised information from unlabeled videos, we propose a novel
self-supervised contrastive learning module (SelfCL). Furthermore, we build a
new audio-visual action recognition dataset named Kinetics-Sounds100.
Experimental results on Kinetics-Sounds32 and Kinetics-Sounds100 datasets
demonstrate the superiority of our AVCL over the state-of-the-art methods on
large-scale action recognition benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PartNeRF: Generating Part-Aware Editable 3D Shapes without 3D
  Supervision <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.09554v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.09554v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Konstantinos Tertikas, Paschalidou Despoina, Boxiao Pan, Jeong Joon Park, Mikaela Angelina Uy, Ioannis Emiris, Yannis Avrithis, Leonidas Guibas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Impressive progress in generative models and implicit representations gave
rise to methods that can generate 3D shapes of high quality. However, being
able to locally control and edit shapes is another essential property that can
unlock several content creation applications. Local control can be achieved
with part-aware models, but existing methods require 3D supervision and cannot
produce textures. In this work, we devise PartNeRF, a novel part-aware
generative model for editable 3D shape synthesis that does not require any
explicit 3D supervision. Our model generates objects as a set of locally
defined NeRFs, augmented with an affine transformation. This enables several
editing operations such as applying transformations on parts, mixing parts from
different objects etc. To ensure distinct, manipulable parts we enforce a hard
assignment of rays to parts that makes sure that the color of each ray is only
determined by a single NeRF. As a result, altering one part does not affect the
appearance of the others. Evaluations on various ShapeNet categories
demonstrate the ability of our model to generate editable 3D objects of
improved fidelity, compared to previous part-based generative approaches that
require 3D supervision or models relying on NeRFs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in CVPR 2023, Project Page:
  https://ktertikas.github.io/part_nerf</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Watch or Listen: Robust Audio-Visual Speech Recognition with Visual
  Corruption Modeling and Reliability Scoring <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08536v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08536v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joanna Hong, Minsu Kim, Jeongsoo Choi, Yong Man Ro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper deals with Audio-Visual Speech Recognition (AVSR) under multimodal
input corruption situations where audio inputs and visual inputs are both
corrupted, which is not well addressed in previous research directions.
Previous studies have focused on how to complement the corrupted audio inputs
with the clean visual inputs with the assumption of the availability of clean
visual inputs. However, in real life, clean visual inputs are not always
accessible and can even be corrupted by occluded lip regions or noises. Thus,
we firstly analyze that the previous AVSR models are not indeed robust to the
corruption of multimodal input streams, the audio and the visual inputs,
compared to uni-modal models. Then, we design multimodal input corruption
modeling to develop robust AVSR models. Lastly, we propose a novel AVSR
framework, namely Audio-Visual Reliability Scoring module (AV-RelScore), that
is robust to the corrupted multimodal inputs. The AV-RelScore can determine
which input modal stream is reliable or not for the prediction and also can
exploit the more reliable streams in prediction. The effectiveness of the
proposed method is evaluated with comprehensive experiments on popular
benchmark databases, LRS2 and LRS3. We also show that the reliability scores
obtained by AV-RelScore well reflect the degree of corruption and make the
proposed model focus on the reliable multimodal representations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2023. Implementation available:
  https://github.com/joannahong/AV-RelScore</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Grounding DINO: Marrying DINO with Grounded <span class="highlight-title">Pre-Train</span>ing for Open-Set
  Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05499v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05499v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, Lei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present an open-set object detector, called Grounding DINO,
by marrying Transformer-based detector DINO with grounded pre-training, which
can detect arbitrary objects with human inputs such as category names or
referring expressions. The key solution of open-set object detection is
introducing language to a closed-set detector for open-set concept
generalization. To effectively fuse language and vision modalities, we
conceptually divide a closed-set detector into three phases and propose a tight
fusion solution, which includes a feature enhancer, a language-guided query
selection, and a cross-modality decoder for cross-modality fusion. While
previous works mainly evaluate open-set object detection on novel categories,
we propose to also perform evaluations on referring expression comprehension
for objects specified with attributes. Grounding DINO performs remarkably well
on all three settings, including benchmarks on COCO, LVIS, ODinW, and
RefCOCO/+/g. Grounding DINO achieves a $52.5$ AP on the COCO detection
zero-shot transfer benchmark, i.e., without any training data from COCO. It
sets a new record on the ODinW zero-shot benchmark with a mean $26.1$ AP. Code
will be available at \url{https://github.com/IDEA-Research/GroundingDINO}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code will be available at
  https://github.com/IDEA-Research/GroundingDINO</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Fake Evolutionary Generative Adversarial Networks for Imbalance
  Hyperspectral Image Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.04019v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.04019v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tanmoy Dam, Nidhi Swami, Sreenatha G. Anavatti, Hussein A. Abbass
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel multi-fake evolutionary generative adversarial
network(MFEGAN) for handling imbalance hyperspectral image classification. It
is an end-to-end approach in which different generative objective losses are
considered in the generator network to improve the classification performance
of the discriminator network. Thus, the same discriminator network has been
used as a standard classifier by embedding the classifier network on top of the
discriminating function. The effectiveness of the proposed method has been
validated through two hyperspectral spatial-spectral data sets. The same
generative and discriminator architectures have been utilized with two
different GAN objectives for a fair performance comparison with the proposed
method. It is observed from the experimental validations that the proposed
method outperforms the state-of-the-art methods with better classification
performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Learning for Inertial Positioning: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03757v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03757v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changhao Chen, Xianfei Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inertial sensors are widely utilized in smartphones, drones, robots, and IoT
devices, playing a crucial role in enabling ubiquitous and reliable
localization. Inertial sensor-based positioning is essential in various
applications, including personal navigation, location-based security, and
human-device interaction. However, low-cost MEMS inertial sensors' measurements
are inevitably corrupted by various error sources, leading to unbounded drifts
when integrated doubly in traditional inertial navigation algorithms,
subjecting inertial positioning to the problem of error drifts. In recent
years, with the rapid increase in sensor data and computational power, deep
learning techniques have been developed, sparking significant research into
addressing the problem of inertial positioning. Relevant literature in this
field spans across mobile computing, robotics, and machine learning. In this
article, we provide a comprehensive review of deep learning-based inertial
positioning and its applications in tracking pedestrians, drones, vehicles, and
robots. We connect efforts from different fields and discuss how deep learning
can be applied to address issues such as sensor calibration, positioning error
drift reduction, and multi-sensor fusion. This article aims to attract readers
from various backgrounds, including researchers and practitioners interested in
the potential of deep learning-based techniques to solve inertial positioning
problems. Our review demonstrates the exciting possibilities that deep learning
brings to the table and provides a roadmap for future research in this field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contrastive Semi-supervised Learning for Underwater Image Restoration
  via Reliable Bank <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.09101v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.09101v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shirui Huang, Keyan Wang, Huan Liu, Jun Chen, Yunsong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the remarkable achievement of recent underwater image restoration
techniques, the lack of labeled data has become a major hurdle for further
progress. In this work, we propose a mean-teacher based Semi-supervised
Underwater Image Restoration (Semi-UIR) framework to incorporate the unlabeled
data into network training. However, the naive mean-teacher method suffers from
two main problems: (1) The consistency loss used in training might become
ineffective when the teacher's prediction is wrong. (2) Using L1 distance may
cause the network to overfit wrong labels, resulting in confirmation bias. To
address the above problems, we first introduce a reliable bank to store the
"best-ever" outputs as pseudo ground truth. To assess the quality of outputs,
we conduct an empirical analysis based on the monotonicity property to select
the most trustworthy NR-IQA method. Besides, in view of the confirmation bias
problem, we incorporate contrastive regularization to prevent the overfitting
on wrong labels. Experimental results on both full-reference and non-reference
underwater benchmarks demonstrate that our algorithm has obvious improvement
over SOTA methods quantitatively and qualitatively. Code has been released at
https://github.com/Huang-ShiRui/Semi-UIR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PCT-CycleGAN: Paired Complementary Temporal Cycle-Consistent Adversarial
  Networks for Radar-Based Precipitation Nowcasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.15046v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.15046v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaeho Choi, Yura Kim, Kwang-Ho Kim, Sung-Hwa Jung, Ikhyun Cho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The precipitation nowcasting methods have been elaborated over the centuries
because rain has a crucial impact on human life. Not only quantitative
precipitation forecast (QPF) models and convolutional long short-term memory
(ConvLSTM), but also various sophisticated methods such as the latest MetNet-2
are emerging. In this paper, we propose a paired complementary temporal
cycle-consistent adversarial networks (PCT-CycleGAN) for radar-based
precipitation nowcasting, inspired by cycle-consistent adversarial networks
(CycleGAN), which shows strong performance in image-to-image translation.
PCT-CycleGAN generates temporal causality using two generator networks with
forward and backward temporal dynamics in paired complementary cycles. Each
generator network learns a huge number of one-to-one mappings about
time-dependent radar-based precipitation data to approximate a mapping function
representing the temporal dynamics in each direction. To create robust temporal
causality between paired complementary cycles, novel connection loss is
proposed. The generator network learning forward temporal dynamics in
PCT-CycleGAN generates radar-based precipitation data 10 minutes from the
current time. Also, it provides a reliable prediction of up to 2 hours with
iterative forecasting. The superiority of PCT-CycleGAN is demonstrated through
qualitative and quantitative comparisons with several previous methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EcoFormer: Energy-Saving Attention with Linear Complexity <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.09004v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.09004v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Liu, Zizheng Pan, Haoyu He, Jianfei Cai, Bohan Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer is a transformative framework that models sequential data and has
achieved remarkable performance on a wide range of tasks, but with high
computational and energy cost. To improve its efficiency, a popular choice is
to compress the models via binarization which constrains the floating-point
values into binary ones to save resource consumption owing to cheap bitwise
operations significantly. However, existing binarization methods only aim at
minimizing the information loss for the input distribution statistically, while
ignoring the pairwise similarity modeling at the core of the attention. To this
end, we propose a new binarization paradigm customized to high-dimensional
softmax attention via kernelized hashing, called EcoFormer, to map the original
queries and keys into low-dimensional binary codes in Hamming space. The
kernelized hash functions are learned to match the ground-truth similarity
relations extracted from the attention map in a self-supervised way. Based on
the equivalence between the inner product of binary codes and the Hamming
distance as well as the associative property of matrix multiplication, we can
approximate the attention in linear complexity by expressing it as a
dot-product of binary codes. Moreover, the compact binary representations of
queries and keys enable us to replace most of the expensive multiply-accumulate
operations in attention with simple accumulations to save considerable on-chip
energy footprint on edge devices. Extensive experiments on both vision and
language tasks show that EcoFormer consistently achieves comparable performance
with standard attentions while consuming much fewer resources. For example,
based on PVTv2-B0 and ImageNet-1K, Ecoformer achieves a 73% on-chip energy
footprint reduction with only a 0.33% performance drop compared to the standard
attention. Code is available at https://github.com/ziplab/EcoFormer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2022 camera ready; First two authors contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Being Comes from Not-being: Open-vocabulary Text-to-Motion Generation
  with Wordless Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.15929v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.15929v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junfan Lin, Jianlong Chang, Lingbo Liu, Guanbin Li, Liang Lin, Qi Tian, Chang-wen Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-motion generation is an emerging and challenging problem, which aims
to synthesize motion with the same semantics as the input text. However, due to
the lack of diverse labeled training data, most approaches either limit to
specific types of text annotations or require online optimizations to cater to
the texts during inference at the cost of efficiency and stability. In this
paper, we investigate offline open-vocabulary text-to-motion generation in a
zero-shot learning manner that neither requires paired training data nor extra
online optimization to adapt for unseen texts. Inspired by the prompt learning
in NLP, we pretrain a motion generator that learns to reconstruct the full
motion from the masked motion. During inference, instead of changing the motion
generator, our method reformulates the input text into a masked motion as the
prompt for the motion generator to ``reconstruct'' the motion. In constructing
the prompt, the unmasked poses of the prompt are synthesized by a text-to-pose
generator. To supervise the optimization of the text-to-pose generator, we
propose the first text-pose alignment model for measuring the alignment between
texts and 3D poses. And to prevent the pose generator from overfitting to
limited training texts, we further propose a novel wordless training mechanism
that optimizes the text-to-pose generator without any training texts. The
comprehensive experimental results show that our method obtains a significant
improvement against the baseline methods. The code is available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Consistent 3D Hand Reconstruction in Video via <span class="highlight-title">self-supervised</span> Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.09548v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.09548v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhigang Tu, Zhisheng Huang, Yujin Chen, Di Kang, Linchao Bao, Bisheng Yang, Junsong Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a method for reconstructing accurate and consistent 3D hands from
a monocular video. We observe that detected 2D hand keypoints and the image
texture provide important cues about the geometry and texture of the 3D hand,
which can reduce or even eliminate the requirement on 3D hand annotation. Thus
we propose ${\rm {S}^{2}HAND}$, a self-supervised 3D hand reconstruction model,
that can jointly estimate pose, shape, texture, and the camera viewpoint from a
single RGB input through the supervision of easily accessible 2D detected
keypoints. We leverage the continuous hand motion information contained in the
unlabeled video data and propose ${\rm {S}^{2}HAND(V)}$, which uses a set of
weights shared ${\rm {S}^{2}HAND}$ to process each frame and exploits
additional motion, texture, and shape consistency constrains to promote more
accurate hand poses and more consistent shapes and textures. Experiments on
benchmark datasets demonstrate that our self-supervised approach produces
comparable hand reconstruction performance compared with the recent
full-supervised methods in single-frame as input setup, and notably improves
the reconstruction accuracy and consistency when using video training data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2103.11703</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">6</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Multi-Stage Multi-Grained Semantic Embeddings for E-Commerce
  Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11009v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11009v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Binbin Wang, Mingming Li, Zhixiong Zeng, Jingwei Zhuo, Songlin Wang, Sulong Xu, Bo Long, Weipeng Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieving relevant items that match users' queries from billion-scale corpus
forms the core of industrial e-commerce search systems, in which
embedding-based retrieval (EBR) methods are prevailing. These methods adopt a
two-tower framework to learn embedding vectors for query and item separately
and thus leverage efficient approximate nearest neighbor (ANN) search to
retrieve relevant items. However, existing EBR methods usually ignore
inconsistent user behaviors in industrial multi-stage search systems, resulting
in insufficient retrieval efficiency with a low commercial return. To tackle
this challenge, we propose to improve EBR methods by learning Multi-level
Multi-Grained Semantic Embeddings(MMSE). We propose the multi-stage information
mining to exploit the ordered, clicked, unclicked and random sampled items in
practical user behavior data, and then capture query-item similarity via a
post-fusion strategy. We then propose multi-grained learning objectives that
integrate the retrieval loss with global comparison ability and the ranking
loss with local comparison ability to generate semantic embeddings. Both
experiments on a real-world billion-scale dataset and online A/B tests verify
the effectiveness of MMSE in achieving significant performance improvements on
metrics such as offline recall and online conversion rate (CVR).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Controllable Ancient Chinese Lyrics Generation Based on Phrase Prototype
  Retrieving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11005v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11005v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Yi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating lyrics and poems is one of the essential downstream tasks in the
Natural Language Processing (NLP) field. Current methods have performed well in
some lyrics generation scenarios but need further improvements in tasks
requiring fine control. We propose a novel method for generating ancient
Chinese lyrics (Song Ci), a type of ancient lyrics that involves precise
control of song structure. The system is equipped with a phrase retriever and a
phrase connector. Based on an input prompt, the phrase retriever picks phrases
from a database to construct a phrase pool. The phrase connector then selects a
series of phrases from the phrase pool that minimizes a multi-term loss
function that considers rhyme, song structure, and fluency. Experimental
results show that our method can generate high-quality ancient Chinese lyrics
while performing well on topic and song structure control. We also expect our
approach to be generalized to other lyrics-generating tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NASA Science Mission Directorate Knowledge Graph Discovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10871v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10871v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roelien C. Timmer, Fech Scen Khoo, Megan Mark, Marcella Scoczynski Ribeiro Martins, Anamaria Berea, Gregory Renard, Kaylin Bugbee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The size of the National Aeronautics and Space Administration (NASA) Science
Mission Directorate (SMD) is growing exponentially, allowing researchers to
make discoveries. However, making discoveries is challenging and time-consuming
due to the size of the data catalogs, and as many concepts and data are
indirectly connected. This paper proposes a pipeline to generate knowledge
graphs (KGs) representing different NASA SMD domains. These KGs can be used as
the basis for dataset search engines, saving researchers time and supporting
them in finding new connections. We collected textual data and used several
modern natural language processing (NLP) methods to create the nodes and the
edges of the KGs. We explore the cross-domain connections, discuss our
challenges, and provide future directions to inspire researchers working on
similar challenges.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IMF: Interactive Multimodal Fusion Model for Link Prediction <span class="chip">WWW'2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10816v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10816v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinhang Li, Xiangyu Zhao, Jiaxing Xu, Yong Zhang, Chunxiao Xing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Link prediction aims to identify potential missing triples in knowledge
graphs. To get better results, some recent studies have introduced multimodal
information to link prediction. However, these methods utilize multimodal
information separately and neglect the complicated interaction between
different modalities. In this paper, we aim at better modeling the
inter-modality information and thus introduce a novel Interactive Multimodal
Fusion (IMF) model to integrate knowledge from different modalities. To this
end, we propose a two-stage multimodal fusion framework to preserve
modality-specific knowledge as well as take advantage of the complementarity
between different modalities. Instead of directly projecting different
modalities into a unified space, our multimodal fusion module limits the
representations of different modalities independent while leverages bilinear
pooling for fusion and incorporates contrastive learning as additional
constraints. Furthermore, the decision fusion module delivers the learned
weighted average over the predictions of all modalities to better incorporate
the complementarity of different modalities. Our approach has been demonstrated
to be effective through empirical evaluations on several real-world datasets.
The implementation code is available online at
https://github.com/HestiaSky/IMF-Pytorch.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 7 figures, 4 tables, WWW'2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Query-as-context <span class="highlight-title">Pre-train</span>ing for Dense Passage Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.09598v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.09598v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xing Wu, Guangyuan Ma, Wanhui Qian, Zijia Lin, Fuzheng Zhang, Songlin Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, methods have been developed to improve the performance of dense
passage retrieval by using context-supervised pre-training. These methods
simply consider two passages from the same document to be relevant, without
taking into account the possibility of weakly correlated pairs. Thus, this
paper proposes query-as-context pre-training, a simple yet effective
pre-training technique to alleviate the issue. Query-as-context pre-training
assumes that the query derived from a passage is more likely to be relevant to
that passage and forms a passage-query pair. These passage-query pairs are then
used in contrastive or generative context-supervised pre-training. The
pre-trained models are evaluated on large-scale passage retrieval benchmarks
and out-of-domain zero-shot benchmarks. Experimental results show that
query-as-context pre-training brings considerable gains and meanwhile speeds up
training, demonstrating its effectiveness and efficiency. Our code will be
available at https://github.com/caskcsg/ir/tree/main/cotmae-qc .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TaDaa: real time Ticket Assignment Deep learning Auto Advisor for
  customer support, help desk, and issue ticketing systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.11187v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.11187v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leon Feng, Jnana Senapati, Bill Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes TaDaa: Ticket Assignment Deep learning Auto Advisor,
which leverages the latest Transformers models and machine learning techniques
quickly assign issues within an organization, like customer support, help desk
and alike issue ticketing systems. The project provides functionality to 1)
assign an issue to the correct group, 2) assign an issue to the best resolver,
and 3) provide the most relevant previously solved tickets to resolvers. We
leverage one ticketing system sample dataset, with over 3k+ groups and over
10k+ resolvers to obtain a 95.2% top 3 accuracy on group suggestions and a
79.0% top 5 accuracy on resolver suggestions. We hope this research will
greatly improve average issue resolution time on customer support, help desk,
and issue ticketing systems.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">126</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Legs as Manipulator: Pushing Quadrupedal Agility Beyond Locomotion <span class="chip">ICRA 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11330v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11330v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuxin Cheng, Ashish Kumar, Deepak Pathak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Locomotion has seen dramatic progress for walking or running across
challenging terrains. However, robotic quadrupeds are still far behind their
biological counterparts, such as dogs, which display a variety of agile skills
and can use the legs beyond locomotion to perform several basic manipulation
tasks like interacting with objects and climbing. In this paper, we take a step
towards bridging this gap by training quadruped robots not only to walk but
also to use the front legs to climb walls, press buttons, and perform object
interaction in the real world. To handle this challenging optimization, we
decouple the skill learning broadly into locomotion, which involves anything
that involves movement whether via walking or climbing a wall, and
manipulation, which involves using one leg to interact while balancing on the
other three legs. These skills are trained in simulation using curriculum and
transferred to the real world using our proposed sim2real variant that builds
upon recent locomotion success. Finally, we combine these skills into a robust
long-term plan by learning a behavior tree that encodes a high-level task
hierarchy from one clean expert demonstration. We evaluate our method in both
simulation and real-world showing successful executions of both short as well
as long-range tasks and how robustness helps confront external perturbations.
Videos at https://robot-skills.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICRA 2023. Videos at https://robot-skills.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3D Concept Learning and Reasoning from Multi-View Images <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11327v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11327v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yining Hong, Chunru Lin, Yilun Du, Zhenfang Chen, Joshua B. Tenenbaum, Chuang Gan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans are able to accurately reason in 3D by gathering multi-view
observations of the surrounding world. Inspired by this insight, we introduce a
new large-scale benchmark for 3D multi-view visual question answering
(3DMV-VQA). This dataset is collected by an embodied agent actively moving and
capturing RGB images in an environment using the Habitat simulator. In total,
it consists of approximately 5k scenes, 600k images, paired with 50k questions.
We evaluate various state-of-the-art models for visual reasoning on our
benchmark and find that they all perform poorly. We suggest that a principled
approach for 3D reasoning from multi-view images should be to infer a compact
3D representation of the world from the multi-view images, which is further
grounded on open-vocabulary semantic concepts, and then to execute reasoning on
these 3D representations. As the first step towards this approach, we propose a
novel 3D concept learning and reasoning (3D-CLR) framework that seamlessly
combines these components via neural fields, 2D pre-trained vision-language
models, and neural reasoning operators. Experimental results suggest that our
framework outperforms baseline models by a large margin, but the challenge
remains largely unsolved. We further perform an in-depth analysis of the
challenges and highlight potential future directions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023. Project page: https://vis-www.cs.umass.edu/3d-clr/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tangent Bundle Convolutional Learning: from Manifolds to Cellular
  Sheaves and Back 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11323v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11323v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Claudio Battiloro, Zhiyang Wang, Hans Riess, Paolo Di Lorenzo, Alejandro Ribeiro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work we introduce a convolution operation over the tangent bundle of
Riemann manifolds in terms of exponentials of the Connection Laplacian
operator. We define tangent bundle filters and tangent bundle neural networks
(TNNs) based on this convolution operation, which are novel continuous
architectures operating on tangent bundle signals, i.e. vector fields over the
manifolds. Tangent bundle filters admit a spectral representation that
generalizes the ones of scalar manifold filters, graph filters and standard
convolutional filters in continuous time. We then introduce a discretization
procedure, both in the space and time domains, to make TNNs implementable,
showing that their discrete counterpart is a novel principled variant of the
very recently introduced sheaf neural networks. We formally prove that this
discretized architecture converges to the underlying continuous TNN. Finally,
we numerically evaluate the effectiveness of the proposed architecture on
various learning tasks, both on synthetic and real data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2210.15058</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Localizing Object-level Shape Variations with Text-to-Image Diffusion
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11306v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11306v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Or Patashnik, Daniel Garibi, Idan Azuri, Hadar Averbuch-Elor, Daniel Cohen-Or
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image models give rise to workflows which often begin with an
exploration step, where users sift through a large collection of generated
images. The global nature of the text-to-image generation process prevents
users from narrowing their exploration to a particular object in the image. In
this paper, we present a technique to generate a collection of images that
depicts variations in the shape of a specific object, enabling an object-level
shape exploration process. Creating plausible variations is challenging as it
requires control over the shape of the generated object while respecting its
semantics. A particular challenge when generating object variations is
accurately localizing the manipulation applied over the object's shape. We
introduce a prompt-mixing technique that switches between prompts along the
denoising process to attain a variety of shape choices. To localize the
image-space operation, we present two techniques that use the self-attention
layers in conjunction with the cross-attention layers. Moreover, we show that
these localization techniques are general and effective beyond the scope of
generating object variations. Extensive results and comparisons demonstrate the
effectiveness of our method in generating object variations, and the competence
of our localization techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page at https://orpatashnik.github.io/local-prompt-mixing/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking SO(3)-equivariance with Bilinear Tensor Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11288v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11288v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chase Shimmin, Zhelun Li, Ema Smith
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many datasets in scientific and engineering applications are comprised of
objects which have specific geometric structure. A common example is data which
inhabits a representation of the group SO$(3)$ of 3D rotations: scalars,
vectors, tensors, \textit{etc}. One way for a neural network to exploit prior
knowledge of this structure is to enforce SO$(3)$-equivariance throughout its
layers, and several such architectures have been proposed. While general
methods for handling arbitrary SO$(3)$ representations exist, they
computationally intensive and complicated to implement. We show that by
judicious symmetry breaking, we can efficiently increase the expressiveness of
a network operating only on vector and order-2 tensor representations of
SO$(2)$. We demonstrate the method on an important problem from High Energy
Physics known as \textit{b-tagging}, where particle jets originating from
b-meson decays must be discriminated from an overwhelming QCD background. In
this task, we find that augmenting a standard architecture with our method
results in a \ensuremath{2.3\times} improvement in rejection score.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Constructing Bayesian Pseudo-Coresets using Contrastive Divergence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11278v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11278v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Piyush Tiwary, Kumar Shubham, Vivek Kashyap, Prathosh A. P
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bayesian Pseudo-Coreset (BPC) and Dataset Condensation are two parallel
streams of work that construct a synthetic set such that, a model trained
independently on this synthetic set, yields the same performance as training on
the original training set. While dataset condensation methods use non-bayesian,
heuristic ways to construct such a synthetic set, BPC methods take a bayesian
approach and formulate the problem as divergence minimization between
posteriors associated with original data and synthetic data. However, BPC
methods generally rely on distributional assumptions on these posteriors which
makes them less flexible and hinders their performance. In this work, we
propose to solve these issues by modeling the posterior associated with
synthetic data by an energy-based distribution. We derive a
contrastive-divergence-like loss function to learn the synthetic set and show a
simple and efficient way to estimate this loss. Further, we perform rigorous
experiments pertaining to the proposed method. Our experiments on multiple
datasets show that the proposed method not only outperforms previous BPC
methods but also gives performance comparable to dataset condensation
counterparts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Model Stitching: Looking For Functional Similarity Between
  Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11277v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11277v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adriano Hernandez, Rumen Dangovski, Peter Y. Lu, Marin Soljacic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model stitching (Lenc & Vedaldi 2015) is a compelling methodology to compare
different neural network representations, because it allows us to measure to
what degree they may be interchanged. We expand on a previous work from Bansal,
Nakkiran & Barak which used model stitching to compare representations of the
same shapes learned by differently seeded and/or trained neural networks of the
same architecture. Our contribution enables us to compare the representations
learned by layers with different shapes from neural networks with different
architectures. We subsequently reveal unexpected behavior of model stitching.
Namely, we find that stitching, based on convolutions, for small ResNets, can
reach high accuracy if those layers come later in the first (sender) network
than in the second (receiver), even if those layers are far apart.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Agent-based Simulation for Online Mental Health Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11272v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11272v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhan Liu, Anna Fang, Glen Moriarty, Robert Kraut, Haiyi Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online mental health communities (OMHCs) are an effective and accessible
channel to give and receive social support for individuals with mental and
emotional issues. However, a key challenge on these platforms is finding
suitable partners to interact with given that mechanisms to match users are
currently underdeveloped. In this paper, we collaborate with one of the world's
largest OMHC to develop an agent-based simulation framework and explore the
trade-offs in different matching algorithms. The simulation framework allows us
to compare current mechanisms and new algorithmic matching policies on the
platform, and observe their differing effects on a variety of outcome metrics.
Our findings include that usage of the deferred-acceptance algorithm can
significantly better the experiences of support-seekers in one-on-one chats
while maintaining low waiting time. We note key design considerations that
agent-based modeling reveals in the OMHC context, including the potential
benefits of algorithmic matching on marginalized communities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Convergence Guarantees of Overparametrized Wide Deep Inverse Prior 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11265v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11265v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathan Buskulic, Yvain Quéau, Jalal Fadili
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural networks have become a prominent approach to solve inverse problems in
recent years. Amongst the different existing methods, the Deep Image/Inverse
Priors (DIPs) technique is an unsupervised approach that optimizes a highly
overparametrized neural network to transform a random input into an object
whose image under the forward model matches the observation. However, the level
of overparametrization necessary for such methods remains an open problem. In
this work, we aim to investigate this question for a two-layers neural network
with a smooth activation function. We provide overparametrization bounds under
which such network trained via continuous-time gradient descent will converge
exponentially fast with high probability which allows to derive recovery
prediction bounds. This work is thus a first step towards a theoretical
understanding of overparametrized DIP networks, and more broadly it
participates to the theoretical understanding of neural networks in inverse
problem settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unit Scaling: Out-of-the-Box Low-Precision Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11257v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11257v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Charlie Blake, Douglas Orr, Carlo Luschi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present unit scaling, a paradigm for designing deep learning models that
simplifies the use of low-precision number formats. Training in FP16 or the
recently proposed FP8 formats offers substantial efficiency gains, but can lack
sufficient range for out-of-the-box training. Unit scaling addresses this by
introducing a principled approach to model numerics: seeking unit variance of
all weights, activations and gradients at initialisation. Unlike alternative
methods, this approach neither requires multiple training runs to find a
suitable scale nor has significant computational overhead. We demonstrate the
efficacy of unit scaling across a range of models and optimisers. We further
show that existing models can be adapted to be unit-scaled, training BERT-Large
in FP16 and then FP8 with no degradation in accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What Makes Data Suitable for a Locally Connected Neural Network? A
  Necessary and Sufficient Condition Based on Quantum Entanglement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11249v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11249v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yotam Alexander, Nimrod De La Vega, Noam Razin, Nadav Cohen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The question of what makes a data distribution suitable for deep learning is
a fundamental open problem. Focusing on locally connected neural networks (a
prevalent family of architectures that includes convolutional and recurrent
neural networks as well as local self-attention models), we address this
problem by adopting theoretical tools from quantum physics. Our main
theoretical result states that a certain locally connected neural network is
capable of accurate prediction over a data distribution if and only if the data
distribution admits low quantum entanglement under certain canonical partitions
of features. As a practical application of this result, we derive a
preprocessing method for enhancing the suitability of a data distribution to
locally connected neural networks. Experiments with widespread models over
various datasets demonstrate our findings. We hope that our use of quantum
entanglement will encourage further adoption of tools from physics for formally
reasoning about the relation between deep learning and real-world data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Memorization Capacity of Neural Networks with Conditional Computation <span class="chip">ICLR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11247v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11247v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erdem Koyuncu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many empirical studies have demonstrated the performance benefits of
conditional computation in neural networks, including reduced inference time
and power consumption. We study the fundamental limits of neural conditional
computation from the perspective of memorization capacity. For Rectified Linear
Unit (ReLU) networks without conditional computation, it is known that
memorizing a collection of $n$ input-output relationships can be accomplished
via a neural network with $O(\sqrt{n})$ neurons. Calculating the output of this
neural network can be accomplished using $O(\sqrt{n})$ elementary arithmetic
operations of additions, multiplications and comparisons for each input. Using
a conditional ReLU network, we show that the same task can be accomplished
using only $O(\log n)$ operations per input. This represents an almost
exponential improvement as compared to networks without conditional
computation. We also show that the $\Theta(\log n)$ rate is the best possible.
Our achievability result utilizes a general methodology to synthesize a
conditional network out of an unconditional network in a
computationally-efficient manner, bridging the gap between unconditional and
conditional architectures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be presented at International Conference on Learning
  Representations (ICLR), 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Make Landscape Flatter in Differentially Private Federated Learning <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11242v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11242v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Shi, Yingqi Liu, Kang Wei, Li Shen, Xueqian Wang, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To defend the inference attacks and mitigate the sensitive information
leakages in Federated Learning (FL), client-level Differentially Private FL
(DPFL) is the de-facto standard for privacy protection by clipping local
updates and adding random noise. However, existing DPFL methods tend to make a
sharper loss landscape and have poorer weight perturbation robustness,
resulting in severe performance degradation. To alleviate these issues, we
propose a novel DPFL algorithm named DP-FedSAM, which leverages gradient
perturbation to mitigate the negative impact of DP. Specifically, DP-FedSAM
integrates Sharpness Aware Minimization (SAM) optimizer to generate local
flatness models with better stability and weight perturbation robustness, which
results in the small norm of local updates and robustness to DP noise, thereby
improving the performance. From the theoretical perspective, we analyze in
detail how DP-FedSAM mitigates the performance degradation induced by DP.
Meanwhile, we give rigorous privacy guarantees with R\'enyi DP and present the
sensitivity analysis of local updates. At last, we empirically confirm that our
algorithm achieves state-of-the-art (SOTA) performance compared with existing
SOTA baselines in DPFL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR2023, 18 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Training Invertible Neural Networks as Autoencoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11239v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11239v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        The-Gia Leo Nguyen, Lynton Ardizzone, Ullrich Koethe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autoencoders are able to learn useful data representations in an unsupervised
matter and have been widely used in various machine learning and computer
vision tasks. In this work, we present methods to train Invertible Neural
Networks (INNs) as (variational) autoencoders which we call INN (variational)
autoencoders. Our experiments on MNIST, CIFAR and CelebA show that for low
bottleneck sizes our INN autoencoder achieves results similar to the classical
autoencoder. However, for large bottleneck sizes our INN autoencoder
outperforms its classical counterpart. Based on the empirical results, we
hypothesize that INN autoencoders might not have any intrinsic information loss
and thereby are not bounded to a maximal number of layers (depth) after which
only suboptimal results can be achieved.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Conference Paper at GCPR2019</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cascaded Latent Diffusion Models for High-Resolution Chest X-ray
  Synthesis <span class="chip">PAKDD 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11224v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11224v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tobias Weber, Michael Ingrisch, Bernd Bischl, David Rügamer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While recent advances in large-scale foundational models show promising
results, their application to the medical domain has not yet been explored in
detail. In this paper, we progress into the realms of large-scale modeling in
medical synthesis by proposing Cheff - a foundational cascaded latent diffusion
model, which generates highly-realistic chest radiographs providing
state-of-the-art quality on a 1-megapixel scale. We further propose MaCheX,
which is a unified interface for public chest datasets and forms the largest
open collection of chest X-rays up to date. With Cheff conditioned on
radiological reports, we further guide the synthesis process over text prompts
and unveil the research area of report-to-chest-X-ray generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at PAKDD 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Inverse problem regularization with hierarchical variational
  autoencoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11217v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11217v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jean Prost, Antoine Houdard, Andrés Almansa, Nicolas Papadakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose to regularize ill-posed inverse problems using a
deep hierarchical variational autoencoder (HVAE) as an image prior. The
proposed method synthesizes the advantages of i) denoiser-based Plug \& Play
approaches and ii) generative model based approaches to inverse problems.
First, we exploit VAE properties to design an efficient algorithm that benefits
from convergence guarantees of Plug-and-Play (PnP) methods. Second, our
approach is not restricted to specialized datasets and the proposed PnP-HVAE
model is able to solve image restoration problems on natural images of any
size. Our experiments show that the proposed PnP-HVAE method is competitive
with both SOTA denoiser-based PnP approaches, and other SOTA restoration
methods based on generative models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Generate 3D Representations of Building Roofs Using
  Single-View Aerial Imagery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11215v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11215v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maxim Khomiakov, Alejandro Valverde Mahou, Alba Reinders Sánchez, Jes Frellsen, Michael Riis Andersen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel pipeline for learning the conditional distribution of a
building roof mesh given pixels from an aerial image, under the assumption that
roof geometry follows a set of regular patterns. Unlike alternative methods
that require multiple images of the same object, our approach enables
estimating 3D roof meshes using only a single image for predictions. The
approach employs the PolyGen, a deep generative transformer architecture for 3D
meshes. We apply this model in a new domain and investigate the sensitivity of
the image resolution. We propose a novel metric to evaluate the performance of
the inferred meshes, and our results show that the model is robust even at
lower resolutions, while qualitatively producing realistic representations for
out-of-distribution samples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Copyright 2023 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Investigating Topological Order using Recurrent Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11207v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11207v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Hibat-Allah, Roger G. Melko, Juan Carrasquilla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recurrent neural networks (RNNs), originally developed for natural language
processing, hold great promise for accurately describing strongly correlated
quantum many-body systems. Here, we employ 2D RNNs to investigate two
prototypical quantum many-body Hamiltonians exhibiting topological order.
Specifically, we demonstrate that RNN wave functions can effectively capture
the topological order of the toric code and a Bose-Hubbard spin liquid on the
kagome lattice by estimating their topological entanglement entropies. We also
find that RNNs favor coherent superpositions of minimally-entangled states over
minimally-entangled states themselves. Overall, our findings demonstrate that
RNN wave functions constitute a powerful tool to study phases of matter beyond
Landau's symmetry-breaking paradigm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 7 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> of Demonstration Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11191v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11191v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        André Correia, Luís A. Alexandre
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the fast improvement of machine learning, reinforcement learning (RL)
has been used to automate human tasks in different areas. However, training
such agents is difficult and restricted to expert users. Moreover, it is mostly
limited to simulation environments due to the high cost and safety concerns of
interactions in the real world. Demonstration Learning is a paradigm in which
an agent learns to perform a task by imitating the behavior of an expert shown
in demonstrations. It is a relatively recent area in machine learning, but it
is gaining significant traction due to having tremendous potential for learning
complex behaviors from demonstrations. Learning from demonstration accelerates
the learning process by improving sample efficiency, while also reducing the
effort of the programmer. Due to learning without interacting with the
environment, demonstration learning would allow the automation of a wide range
of real world applications such as robotics and healthcare. This paper provides
a survey of demonstration learning, where we formally introduce the
demonstration problem along with its main challenges and provide a
comprehensive overview of the process of learning from demonstrations from the
creation of the demonstration data set, to learning methods from
demonstrations, and optimization by combining demonstration learning with
different machine learning methods. We also review the existing benchmarks and
identify their strengths and limitations. Additionally, we discuss the
advantages and disadvantages of the paradigm as well as its main applications.
Lastly, we discuss our perspective on open problems and research directions for
this rapidly growing field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Unified Framework of Policy Learning for Contextual Bandit with
  Confounding Bias and Missing Observations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11187v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11187v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyu Chen, Yitan Wang, Zhaoran Wang, Zhuoran Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the offline contextual bandit problem, where we aim to acquire an
optimal policy using observational data. However, this data usually contains
two deficiencies: (i) some variables that confound actions are not observed,
and (ii) missing observations exist in the collected data. Unobserved
confounders lead to a confounding bias and missing observations cause bias and
inefficiency problems. To overcome these challenges and learn the optimal
policy from the observed dataset, we present a new algorithm called
Causal-Adjusted Pessimistic (CAP) policy learning, which forms the reward
function as the solution of an integral equation system, builds a confidence
set, and greedily takes action with pessimism. With mild assumptions on the
data, we develop an upper bound to the suboptimality of CAP for the offline
contextual bandit problem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>76 page, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Architecture, <span class="highlight-title">Dataset</span> and Model-Scale Agnostic Data-free Meta-Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11183v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11183v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixuan Hu, Li Shen, Zhenyi Wang, Tongliang Liu, Chun Yuan, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of data-free meta-learning is to learn useful prior knowledge from a
collection of pre-trained models without accessing their training data.
However, existing works only solve the problem in parameter space, which (i)
ignore the fruitful data knowledge contained in the pre-trained models; (ii)
can not scale to large-scale pre-trained models; (iii) can only meta-learn
pre-trained models with the same network architecture. To address those issues,
we propose a unified framework, dubbed PURER, which contains: (1) ePisode
cUrriculum inveRsion (ECI) during data-free meta training; and (2) invErsion
calibRation following inner loop (ICFIL) during meta testing. During meta
training, we propose ECI to perform pseudo episode training for learning to
adapt fast to new unseen tasks. Specifically, we progressively synthesize a
sequence of pseudo episodes by distilling the training data from each
pre-trained model. The ECI adaptively increases the difficulty level of pseudo
episodes according to the real-time feedback of the meta model. We formulate
the optimization process of meta training with ECI as an adversarial form in an
end-to-end manner. During meta testing, we further propose a simple
plug-and-play supplement-ICFIL-only used during meta testing to narrow the gap
between meta training and meta testing task distribution. Extensive experiments
in various real-world scenarios show the superior performance of ours.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Integration of Radiomics and Tumor Biomarkers in Interpretable Machine
  Learning Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11177v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11177v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lennart Brocki, Neo Christopher Chung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the unprecedented performance of deep neural networks (DNNs) in
computer vision, their practical application in the diagnosis and prognosis of
cancer using medical imaging has been limited. One of the critical challenges
for integrating diagnostic DNNs into radiological and oncological applications
is their lack of interpretability, preventing clinicians from understanding the
model predictions. Therefore, we study and propose the integration of
expert-derived radiomics and DNN-predicted biomarkers in interpretable
classifiers which we call ConRad, for computerized tomography (CT) scans of
lung cancer. Importantly, the tumor biomarkers are predicted from a concept
bottleneck model (CBM) such that once trained, our ConRad models do not require
labor-intensive and time-consuming biomarkers. In our evaluation and practical
application, the only input to ConRad is a segmented CT scan. The proposed
model is compared to convolutional neural networks (CNNs) which act as a black
box classifier. We further investigated and evaluated all combinations of
radiomics, predicted biomarkers and CNN features in five different classifiers.
We found the ConRad models using non-linear SVM and the logistic regression
with the Lasso outperform others in five-fold cross-validation, although we
highlight that interpretability of ConRad is its primary advantage. The Lasso
is used for feature selection, which substantially reduces the number of
non-zero weights while increasing the accuracy. Overall, the proposed ConRad
model combines CBM-derived biomarkers and radiomics features in an
interpretable ML model which perform excellently for the lung nodule malignancy
classification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Imitating Graph-Based Planning with Goal-Conditioned Policies <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11166v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11166v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junsu Kim, Younggyo Seo, Sungsoo Ahn, Kyunghwan Son, Jinwoo Shin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, graph-based planning algorithms have gained much attention to solve
goal-conditioned reinforcement learning (RL) tasks: they provide a sequence of
subgoals to reach the target-goal, and the agents learn to execute
subgoal-conditioned policies. However, the sample-efficiency of such RL schemes
still remains a challenge, particularly for long-horizon tasks. To address this
issue, we present a simple yet effective self-imitation scheme which distills a
subgoal-conditioned policy into the target-goal-conditioned policy. Our
intuition here is that to reach a target-goal, an agent should pass through a
subgoal, so target-goal- and subgoal- conditioned policies should be similar to
each other. We also propose a novel scheme of stochastically skipping executed
subgoals in a planned path, which further improves performance. Unlike prior
methods that only utilize graph-based planning in an execution phase, our
method transfers knowledge from a planner along with a graph into policy
learning. We empirically show that our method can significantly boost the
sample-efficiency of the existing goal-conditioned RL methods under various
long-horizon control tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Computationally Budgeted Continual Learning: What Does Matter? <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11165v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11165v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ameya Prabhu, Hasan Abed Al Kader Hammoud, Puneet Dokania, Philip H. S. Torr, Ser-Nam Lim, Bernard Ghanem, Adel Bibi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual Learning (CL) aims to sequentially train models on streams of
incoming data that vary in distribution by preserving previous knowledge while
adapting to new data. Current CL literature focuses on restricted access to
previously seen data, while imposing no constraints on the computational budget
for training. This is unreasonable for applications in-the-wild, where systems
are primarily constrained by computational and time budgets, not storage. We
revisit this problem with a large-scale benchmark and analyze the performance
of traditional CL approaches in a compute-constrained setting, where effective
memory samples used in training can be implicitly restricted as a consequence
of limited computation. We conduct experiments evaluating various CL sampling
strategies, distillation losses, and partial fine-tuning on two large-scale
datasets, namely ImageNet2K and Continual Google Landmarks V2 in data
incremental, class incremental, and time incremental settings. Through
extensive experiments amounting to a total of over 1500 GPU-hours, we find
that, under compute-constrained setting, traditional CL approaches, with no
exception, fail to outperform a simple minimal baseline that samples uniformly
from memory. Our conclusions are consistent in a different number of stream
time steps, e.g., 20 to 200, and under several computational budgets. This
suggests that most existing CL methods are particularly too computationally
expensive for realistic budgeted deployment. Code for this project is available
at: https://github.com/drimpossible/BudgetCL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Appearing in CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adversarial Attacks against Binary Similarity Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11143v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11143v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gianluca Capozzi, Daniele Cono D'Elia, Giuseppe Antonio Di Luna, Leonardo Querzoni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, binary analysis gained traction as a fundamental approach to
inspect software and guarantee its security. Due to the exponential increase of
devices running software, much research is now moving towards new autonomous
solutions based on deep learning models, as they have been showing
state-of-the-art performances in solving binary analysis problems. One of the
hot topics in this context is binary similarity, which consists in determining
if two functions in assembly code are compiled from the same source code.
However, it is unclear how deep learning models for binary similarity behave in
an adversarial context. In this paper, we study the resilience of binary
similarity models against adversarial examples, showing that they are
susceptible to both targeted and untargeted attacks (w.r.t. similarity goals)
performed by black-box and white-box attackers. In more detail, we extensively
test three current state-of-the-art solutions for binary similarity against two
black-box greedy attacks, including a new technique that we call Spatial
Greedy, and one white-box attack in which we repurpose a gradient-guided
strategy used in attacks to image classifiers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fault Detection via Occupation Kernel Principal Component Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11138v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11138v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zachary Morrison, Benjamin P. Russo, Yingzhao Lian, Rushikesh Kamalapurkar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The reliable operation of automatic systems is heavily dependent on the
ability to detect faults in the underlying dynamical system. While traditional
model-based methods have been widely used for fault detection, data-driven
approaches have garnered increasing attention due to their ease of deployment
and minimal need for expert knowledge. In this paper, we present a novel
principal component analysis (PCA) method that uses occupation kernels.
Occupation kernels result in feature maps that are tailored to the measured
data, have inherent noise-robustness due to the use of integration, and can
utilize irregularly sampled system trajectories of variable lengths for PCA.
The occupation kernel PCA method is used to develop a reconstruction error
approach to fault detection and its efficacy is validated using numerical
simulations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TWINS: A Fine-Tuning Framework for Improved Transferability of
  Adversarial Robustness and Generalization <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11135v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11135v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziquan Liu, Yi Xu, Xiangyang Ji, Antoni B. Chan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have seen the ever-increasing importance of pre-trained models
and their downstream training in deep learning research and applications. At
the same time, the defense for adversarial examples has been mainly
investigated in the context of training from random initialization on simple
classification tasks. To better exploit the potential of pre-trained models in
adversarial robustness, this paper focuses on the fine-tuning of an
adversarially pre-trained model in various classification tasks. Existing
research has shown that since the robust pre-trained model has already learned
a robust feature extractor, the crucial question is how to maintain the
robustness in the pre-trained model when learning the downstream task. We study
the model-based and data-based approaches for this goal and find that the two
common approaches cannot achieve the objective of improving both generalization
and adversarial robustness. Thus, we propose a novel statistics-based approach,
Two-WIng NormliSation (TWINS) fine-tuning framework, which consists of two
neural networks where one of them keeps the population means and variances of
pre-training data in the batch normalization layers. Besides the robust
information transfer, TWINS increases the effective learning rate without
hurting the training stability since the relationship between a weight norm and
its gradient norm in standard batch normalization layer is broken, resulting in
a faster escape from the sub-optimal initialization and alleviating the robust
overfitting. Finally, TWINS is shown to be effective on a wide range of image
classification datasets in terms of both generalization and robustness. Our
code is available at https://github.com/ziquanliu/CVPR2023-TWINS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cocktail Hu<span class="highlight-title">BERT</span>: Generalized <span class="highlight-title">Self-Supervised</span> <span class="highlight-title">Pre-train</span>ing for Mixture
  and Single-Source Speech <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11131v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11131v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maryam Fazel-Zarandi, Wei-Ning Hsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning leverages unlabeled data effectively, improving
label efficiency and generalization to domains without labeled data. While
recent work has studied generalization to more acoustic/linguistic domains,
languages, and modalities, these investigations are limited to single-source
speech with one primary speaker in the recording. This paper presents Cocktail
HuBERT, a self-supervised learning framework that generalizes to mixture speech
using a masked pseudo source separation objective. This objective encourages
the model to identify the number of sources, separate and understand the
context, and infer the content of masked regions represented as discovered
units. Cocktail HuBERT outperforms state-of-the-art results with 69% lower WER
on multi-speaker ASR, 31% lower DER on diarization, and is competitive on
single- and multi-speaker tasks from SUPERB.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep learning automated quantification of lung disease in pulmonary
  hypertension on CT pulmonary angiography: A preliminary clinical study with
  external validation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11130v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11130v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael J. Sharkey, Krit Dwivedi, Samer Alabed, Andrew J. Swift
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Purpose: Lung disease assessment in precapillary pulmonary hypertension (PH)
is essential for appropriate patient management. This study aims to develop an
artificial intelligence (AI) deep learning model for lung texture
classification in CT Pulmonary Angiography (CTPA), and evaluate its correlation
with clinical assessment methods.
  Materials and Methods: In this retrospective study with external validation,
122 patients with pre-capillary PH were used to train (n=83), validate (n=17)
and test (n=10 internal test, n=12 external test) a patch based DenseNet-121
classification model. "Normal", "Ground glass", "Ground glass with
reticulation", "Honeycombing", and "Emphysema" were classified as per the
Fleishner Society glossary of terms. Ground truth classes were segmented by two
radiologists with patches extracted from the labelled regions. Proportion of
lung volume for each texture was calculated by classifying patches throughout
the entire lung volume to generate a coarse texture classification mapping
throughout the lung parenchyma. AI output was assessed against diffusing
capacity of carbon monoxide (DLCO) and specialist radiologist reported disease
severity.
  Results: Micro-average AUCs for the validation, internal test, and external
test were 0.92, 0.95, and 0.94, respectively. The model had consistent
performance across parenchymal textures, demonstrated strong correlation with
diffusing capacity of carbon monoxide (DLCO), and showed good correspondence
with disease severity reported by specialist radiologists.
  Conclusion: The classification model demonstrates excellent performance on
external validation. The clinical utility of its output has been demonstrated.
This objective, repeatable measure of disease severity can aid in patient
management in adjunct to radiological reporting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 3 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sionna RT: Differentiable Ray Tracing for Radio Propagation Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11103v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11103v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jakob Hoydis, Fayçal Aït Aoudia, Sebastian Cammerer, Merlin Nimier-David, Nikolaus Binder, Guillermo Marcus, Alexander Keller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sionna is a GPU-accelerated open-source library for link-level simulations
based on TensorFlow. Its latest release (v0.14) integrates a differentiable ray
tracer (RT) for the simulation of radio wave propagation. This unique feature
allows for the computation of gradients of the channel impulse response and
other related quantities with respect to many system and environment
parameters, such as material properties, antenna patterns, array geometries, as
well as transmitter and receiver orientations and positions. In this paper, we
outline the key components of Sionna RT and showcase example applications such
as learning radio materials and optimizing transmitter orientations by gradient
descent. While classic ray tracing is a crucial tool for 6G research topics
like reconfigurable intelligent surfaces, integrated sensing and
communications, as well as user localization, differentiable ray tracing is a
key enabler for many novel and exciting research directions, for example,
digital twins.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Differentially Private Algorithms for Synthetic Power System <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11079v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11079v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vladimir Dvorkin, Audun Botterud
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While power systems research relies on the availability of real-world network
datasets, data owners (e.g., system operators) are hesitant to share data due
to security and privacy risks. To control these risks, we develop
privacy-preserving algorithms for the synthetic generation of optimization and
machine learning datasets. Taking a real-world dataset as input, the algorithms
output its noisy, synthetic version, which preserves the accuracy of the real
data on a specific downstream model or even a large population of those. We
control the privacy loss using Laplace and Exponential mechanisms of
differential privacy and preserve data accuracy using a post-processing convex
optimization. We apply the algorithms to generate synthetic network parameters
and wind power data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Discovering Interpretable Directions in the Semantic Latent Space of
  Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11073v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11073v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        René Haas, Inbar Huberman-Spiegelglas, Rotem Mulayoff, Tomer Michaeli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Denoising Diffusion Models (DDMs) have emerged as a strong competitor to
Generative Adversarial Networks (GANs). However, despite their widespread use
in image synthesis and editing applications, their latent space is still not as
well understood. Recently, a semantic latent space for DDMs, coined
`$h$-space', was shown to facilitate semantic image editing in a way
reminiscent of GANs. The $h$-space is comprised of the bottleneck activations
in the DDM's denoiser across all timesteps of the diffusion process. In this
paper, we explore the properties of h-space and propose several novel methods
for finding meaningful semantic directions within it. We start by studying
unsupervised methods for revealing interpretable semantic directions in
pretrained DDMs. Specifically, we show that global latent directions emerge as
the principal components in the latent space. Additionally, we provide a novel
method for discovering image-specific semantic directions by spectral analysis
of the Jacobian of the denoiser w.r.t. the latent code. Next, we extend the
analysis by finding directions in a supervised fashion in unconditional DDMs.
We demonstrate how such directions can be found by relying on either a labeled
data set of real images or by annotating generated samples with a
domain-specific attribute classifier. We further show how to semantically
disentangle the found direction by simple linear projection. Our approaches are
applicable without requiring any architectural modifications, text-based
guidance, CLIP-based optimization, or model fine-tuning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantile and moment neural networks for learning functionals of
  distributions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11060v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11060v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xavier Warin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study news neural networks to approximate function of distributions in a
probability space. Two classes of neural networks based on quantile and moment
approximation are proposed to learn these functions and are theoretically
supported by universal approximation theorems. By mixing the quantile and
moment features in other new networks, we develop schemes that outperform
existing networks on numerical test cases involving univariate distributions.
For bivariate distributions, the moment neural network outperforms all other
networks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hospitalization Length of Stay Prediction using Patient Event Sequences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11042v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11042v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emil Riis Hansen, Thomas Dyhre Nielsen, Thomas Mulvad, Mads Nibe Strausholm, Tomer Sagi, Katja Hose
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predicting patients hospital length of stay (LOS) is essential for improving
resource allocation and supporting decision-making in healthcare organizations.
This paper proposes a novel approach for predicting LOS by modeling patient
information as sequences of events. Specifically, we present a
transformer-based model, termed Medic-BERT (M-BERT), for LOS prediction using
the unique features describing patients medical event sequences. We performed
empirical experiments on a cohort of more than 45k emergency care patients from
a large Danish hospital. Experimental results show that M-BERT can achieve high
accuracy on a variety of LOS problems and outperforms traditional
nonsequence-based machine learning approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MAQA: A Quantum Framework for Supervised Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11028v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11028v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antonio Macaluso, Matthias Klusch, Stefano Lodi, Claudio Sartori
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantum Machine Learning has the potential to improve traditional machine
learning methods and overcome some of the main limitations imposed by the
classical computing paradigm. However, the practical advantages of using
quantum resources to solve pattern recognition tasks are still to be
demonstrated.
  This work proposes a universal, efficient framework that can reproduce the
output of a plethora of classical supervised machine learning algorithms
exploiting quantum computation's advantages. The proposed framework is named
Multiple Aggregator Quantum Algorithm (MAQA) due to its capability to combine
multiple and diverse functions to solve typical supervised learning problems.
In its general formulation, MAQA can be potentially adopted as the quantum
counterpart of all those models falling into the scheme of aggregation of
multiple functions, such as ensemble algorithms and neural networks. From a
computational point of view, the proposed framework allows generating an
exponentially large number of different transformations of the input at the
cost of increasing the depth of the corresponding quantum circuit linearly.
Thus, MAQA produces a model with substantial descriptive power to broaden the
horizon of possible applications of quantum machine learning with a
computational advantage over classical methods. As a second meaningful
addition, we discuss the adoption of the proposed framework as hybrid
quantum-classical and fault-tolerant quantum algorithm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>1 Figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Controllable Ancient Chinese Lyrics Generation Based on Phrase Prototype
  Retrieving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11005v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11005v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Yi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating lyrics and poems is one of the essential downstream tasks in the
Natural Language Processing (NLP) field. Current methods have performed well in
some lyrics generation scenarios but need further improvements in tasks
requiring fine control. We propose a novel method for generating ancient
Chinese lyrics (Song Ci), a type of ancient lyrics that involves precise
control of song structure. The system is equipped with a phrase retriever and a
phrase connector. Based on an input prompt, the phrase retriever picks phrases
from a database to construct a phrase pool. The phrase connector then selects a
series of phrases from the phrase pool that minimizes a multi-term loss
function that considers rhyme, song structure, and fluency. Experimental
results show that our method can generate high-quality ancient Chinese lyrics
while performing well on topic and song structure control. We also expect our
approach to be generalized to other lyrics-generating tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Late Meta-learning Fusion Using Representation Learning for Time Series
  Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11000v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11000v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Terence L. van Zyl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Meta-learning, decision fusion, hybrid models, and representation learning
are topics of investigation with significant traction in time-series
forecasting research. Of these two specific areas have shown state-of-the-art
results in forecasting: hybrid meta-learning models such as Exponential
Smoothing - Recurrent Neural Network (ES-RNN) and Neural Basis Expansion
Analysis (N-BEATS) and feature-based stacking ensembles such as Feature-based
FORecast Model Averaging (FFORMA). However, a unified taxonomy for model fusion
and an empirical comparison of these hybrid and feature-based stacking ensemble
approaches is still missing. This study presents a unified taxonomy
encompassing these topic areas. Furthermore, the study empirically evaluates
several model fusion approaches and a novel combination of hybrid and feature
stacking algorithms called Deep-learning FORecast Model Averaging (DeFORMA).
The taxonomy contextualises the considered methods. Furthermore, the empirical
analysis of the results shows that the proposed model, DeFORMA, can achieve
state-of-the-art results in the M4 data set. DeFORMA, increases the mean
Overall Weighted Average (OWA) in the daily, weekly and yearly subsets with
competitive results in the hourly, monthly and quarterly subsets. The taxonomy
and empirical results lead us to argue that significant progress is still to be
made by continuing to explore the intersection of these research areas.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Induced Feature Selection by Structured Pruning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10999v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10999v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathan Hubens, Victor Delvigne, Matei Mancas, Bernard Gosselin, Marius Preda, Titus Zaharia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of sparsity inducing techniques in neural networks has been of a
great help in the last few years. Indeed, those methods allowed to find lighter
and faster networks, able to perform more efficiently in resource-constrained
environment such as mobile devices or highly requested servers. Such a sparsity
is generally imposed on the weights of neural networks, reducing the footprint
of the architecture. In this work, we go one step further by imposing sparsity
jointly on the weights and on the input data. This can be achieved following a
three-step process: 1) impose a certain structured sparsity on the weights of
the network; 2) track back input features corresponding to zeroed blocks of
weight; 3) remove useless weights and input features and retrain the network.
Performing pruning both on the network and on input data not only allows for
extreme reduction in terms of parameters and operations but can also serve as
an interpretation process. Indeed, with the help of data pruning, we now have
information about which input feature is useful for the network to keep its
performance. Experiments conducted on a variety of architectures and datasets:
MLP validated on MNIST, CIFAR10/100 and ConvNets (VGG16 and ResNet18),
validated on CIFAR10/100 and CALTECH101 respectively, show that it is possible
to achieve additional gains in terms of total parameters and in FLOPs by
performing pruning on input data, while also increasing accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> on Oversmoothing in Graph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10993v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10993v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        T. Konstantin Rusch, Michael M. Bronstein, Siddhartha Mishra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Node features of graph neural networks (GNNs) tend to become more similar
with the increase of the network depth. This effect is known as over-smoothing,
which we axiomatically define as the exponential convergence of suitable
similarity measures on the node features. Our definition unifies previous
approaches and gives rise to new quantitative measures of over-smoothing.
Moreover, we empirically demonstrate this behavior for several over-smoothing
measures on different graphs (small-, medium-, and large-scale). We also review
several approaches for mitigating over-smoothing and empirically test their
effectiveness on real-world graph datasets. Through illustrative examples, we
demonstrate that mitigating over-smoothing is a necessary but not sufficient
condition for building deep GNNs that are expressive on a wide range of graph
learning tasks. Finally, we extend our definition of over-smoothing to the
rapidly emerging field of continuous-time GNNs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semantic segmentation of surgical hyperspectral images under geometric
  domain shifts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10972v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10972v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan Sellner, Silvia Seidlitz, Alexander Studier-Fischer, Alessandro Motta, Berkin Özdemir, Beat Peter Müller-Stich, Felix Nickel, Lena Maier-Hein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robust semantic segmentation of intraoperative image data could pave the way
for automatic surgical scene understanding and autonomous robotic surgery.
Geometric domain shifts, however, although common in real-world open surgeries
due to variations in surgical procedures or situs occlusions, remain a topic
largely unaddressed in the field. To address this gap in the literature, we (1)
present the first analysis of state-of-the-art (SOA) semantic segmentation
networks in the presence of geometric out-of-distribution (OOD) data, and (2)
address generalizability with a dedicated augmentation technique termed "Organ
Transplantation" that we adapted from the general computer vision community.
According to a comprehensive validation on six different OOD data sets
comprising 600 RGB and hyperspectral imaging (HSI) cubes from 33 pigs
semantically annotated with 19 classes, we demonstrate a large performance drop
of SOA organ segmentation networks applied to geometric OOD data. Surprisingly,
this holds true not only for conventional RGB data (drop of Dice similarity
coefficient (DSC) by 46 %) but also for HSI data (drop by 45 %), despite the
latter's rich information content per pixel. Using our augmentation scheme
improves on the SOA DSC by up to 67 % (RGB) and 90 % (HSI) and renders
performance on par with in-distribution performance on real OOD test data. The
simplicity and effectiveness of our augmentation scheme makes it a valuable
network-independent tool for addressing geometric domain shifts in semantic
scene segmentation of intraoperative data. Our code and pre-trained models will
be made publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors (Jan Sellner and Silvia Seidlitz) contributed
  equally to this paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncertainty-aware deep learning for digital twin-driven monitoring:
  Application to fault detection in power lines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10954v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10954v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Laya Das, Blazhe Gjorgiev, Giovanni Sansavini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks (DNNs) are often coupled with physics-based models or
data-driven surrogate models to perform fault detection and health monitoring
of systems in the low data regime. These models serve as digital twins to
generate large quantities of data to train DNNs which would otherwise be
difficult to obtain from the real-life system. However, such models can exhibit
parametric uncertainty that propagates to the generated data. In addition, DNNs
exhibit uncertainty in the parameters learnt during training. In such a
scenario, the performance of the DNN model will be influenced by the
uncertainty in the physics-based model as well as the parameters of the DNN. In
this article, we quantify the impact of both these sources of uncertainty on
the performance of the DNN. We perform explicit propagation of uncertainty in
input data through all layers of the DNN, as well as implicit prediction of
output uncertainty to capture the former. Furthermore, we adopt Monte Carlo
dropout to capture uncertainty in DNN parameters. We demonstrate the approach
for fault detection of power lines with a physics-based model, two types of
input data and three different neural network architectures. We compare the
performance of such uncertainty-aware probabilistic models with their
deterministic counterparts. The results show that the probabilistic models
provide important information regarding the confidence of predictions, while
also delivering an improvement in performance over deterministic models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EMC2-Net: Joint Equalization and Modulation Classification based on
  Constellation Network <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10934v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10934v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyun Ryu, Junil Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modulation classification (MC) is the first step performed at the receiver
side unless the modulation type is explicitly indicated by the transmitter.
Machine learning techniques have been widely used for MC recently. In this
paper, we propose a novel MC technique dubbed as Joint Equalization and
Modulation Classification based on Constellation Network (EMC2-Net). Unlike
prior works that considered the constellation points as an image, the proposed
EMC2-Net directly uses a set of 2D constellation points to perform MC. In order
to obtain clear and concrete constellation despite multipath fading channels,
the proposed EMC2-Net consists of equalizer and classifier having separate and
explainable roles via novel three-phase training and noise-curriculum
pretraining. Numerical results with linear modulation types under different
channel models show that the proposed EMC2-Net achieves the performance of
state-of-the-art MC techniques with significantly less complexity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICASSP 2023 (5 pages, 2 figures, 1 table)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Approaching an unknown communication system by latent space exploration
  and causal inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10931v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10931v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gašper Beguš, Andrej Leban, Shane Gero
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a methodology for discovering meaningful properties in
data by exploring the latent space of unsupervised deep generative models. We
combine manipulation of individual latent variables to extreme values outside
the training range with methods inspired by causal inference into an approach
we call causal disentanglement with extreme values (CDEV) and show that this
approach yields insights for model interpretability. Using this technique, we
can infer what properties of unknown data the model encodes as meaningful. We
apply the methodology to test what is meaningful in the communication system of
sperm whales, one of the most intriguing and understudied animal communication
systems. We train a network that has been shown to learn meaningful
representations of speech and test whether we can leverage such unsupervised
learning to decipher the properties of another vocal communication system for
which we have no ground truth. The proposed technique suggests that sperm
whales encode information using the number of clicks in a sequence, the
regularity of their timing, and audio properties such as the spectral mean and
the acoustic regularity of the sequences. Some of these findings are consistent
with existing hypotheses, while others are proposed for the first time. We also
argue that our models uncover rules that govern the structure of communication
units in the sperm whale communication system and apply them while generating
innovative data not shown during training. This paper suggests that an
interpretation of the outputs of deep neural networks with causal methodology
can be a viable strategy for approaching data about which little is known and
presents another case of how deep learning can limit the hypothesis space.
Finally, the proposed approach combining latent space manipulation and causal
inference can be extended to other architectures and arbitrary datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 23 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Representation Learning for Small-Footprint Keyword Spotting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10912v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10912v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Cui, Liyong Guo, Quandong Wang, Peng Gao, Yujun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we investigate representation learning for low-resource
keyword spotting (KWS). The main challenges of KWS are limited labeled data and
limited available device resources. To address those challenges, we explore
representation learning for KWS by self-supervised contrastive learning and
self-training with pretrained model. First, local-global contrastive siamese
networks (LGCSiam) are designed to learn similar utterance-level
representations for similar audio samplers by proposed local-global contrastive
loss without requiring ground-truth. Second, a self-supervised pretrained
Wav2Vec 2.0 model is applied as a constraint module (WVC) to force the KWS
model to learn frame-level acoustic representations. By the LGCSiam and WVC
modules, the proposed small-footprint KWS model can be pretrained with
unlabeled data. Experiments on speech commands dataset show that the
self-training WVC module and the self-supervised LGCSiam module significantly
improve accuracy, especially in the case of training on a small labeled
dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph Neural Rough Differential Equations for Traffic Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10909v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10909v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeongwhan Choi, Noseong Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traffic forecasting is one of the most popular spatio-temporal tasks in the
field of machine learning. A prevalent approach in the field is to combine
graph convolutional networks and recurrent neural networks for the
spatio-temporal processing. There has been fierce competition and many novel
methods have been proposed. In this paper, we present the method of
spatio-temporal graph neural rough differential equation (STG-NRDE). Neural
rough differential equations (NRDEs) are a breakthrough concept for processing
time-series data. Their main concept is to use the log-signature transform to
convert a time-series sample into a relatively shorter series of feature
vectors. We extend the concept and design two NRDEs: one for the temporal
processing and the other for the spatial processing. After that, we combine
them into a single framework. We conduct experiments with 6 benchmark datasets
and 21 baselines. STG-NRDE shows the best accuracy in all cases, outperforming
all those 21 baselines by non-trivial margins.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review by ACM TIST. arXiv admin note: substantial text overlap
  with arXiv:2112.03558</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Tiny Machine Learning Model for Point Cloud Object Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10898v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10898v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Min Zhang, Jintang Xue, Pranav Kadam, Hardik Prajapati, Shan Liu, C. -C. Jay Kuo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The design of a tiny machine learning model, which can be deployed in mobile
and edge devices, for point cloud object classification is investigated in this
work. To achieve this objective, we replace the multi-scale representation of a
point cloud object with a single-scale representation for complexity reduction,
and exploit rich 3D geometric information of a point cloud object for
performance improvement. The proposed solution is named Green-PointHop due to
its low computational complexity. We evaluate the performance of Green-PointHop
on ModelNet40 and ScanObjectNN two datasets. Green-PointHop has a model size of
64K parameters. It demands 2.3M floating-point operations (FLOPs) to classify a
ModelNet40 object of 1024 down-sampled points. Its classification performance
gaps against the state-of-the-art DGCNN method are 3% and 7% for ModelNet40 and
ScanObjectNN, respectively. On the other hand, the model size and inference
complexity of DGCNN are 42X and 1203X of those of Green-PointHop, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Machine Learning Automated Approach for Enormous Synchrotron X-Ray
  Diffraction Data Interpretation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10881v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10881v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaodong Zhao, YiXuan Luo, Juejing Liu, Wenjun Liu, Kevin M. Rosso, Xiaofeng Guo, Tong Geng, Ang Li, Xin Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Manual analysis of XRD data is usually laborious and time consuming. The deep
neural network (DNN) based models trained by synthetic XRD patterns are proved
to be an automatic, accurate, and high throughput method to analysis common XRD
data collected from solid sample in ambient environment. However, it remains
unknown that whether synthetic XRD based models are capable to solve u-XRD
mapping data for in-situ experiments involving liquid phase exhibiting lower
quality with significant artifacts. In this study, we collected u-XRD mapping
data from an LaCl3-calcite hydrothermal fluid system and trained two categories
of models to solve the experimental XRD patterns. The models trained by
synthetic XRD patterns show low accuracy (as low as 64%) when solving
experimental u-XRD mapping data. The accuracy of the DNN models was
significantly improved (90% or above) when training them with the dataset
containing both synthetic and small number of labeled experimental u-XRD
patterns. This study highlighted the importance of labeled experimental
patterns on the training of DNN models to solve u-XRD mapping data from in-situ
experiments involving liquid phase.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>See link below for supporting information
  https://docs.google.com/document/d/1m2SyaBDej4BhkWCA38GRXJe5Jd7Di7cp/edit?usp=sharing&ouid=108731997922646321851&rtpof=true&sd=true</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rotating without Seeing: Towards In-hand Dexterity through Touch 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10880v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10880v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhao-Heng Yin, Binghao Huang, Yuzhe Qin, Qifeng Chen, Xiaolong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tactile information plays a critical role in human dexterity. It reveals
useful contact information that may not be inferred directly from vision. In
fact, humans can even perform in-hand dexterous manipulation without using
vision. Can we enable the same ability for the multi-finger robot hand? In this
paper, we propose to perform in-hand object rotation using only touching
without seeing the object. Instead of relying on precise tactile sensing in a
small region, we introduce a new system design using dense binary force sensors
(touch or no touch) overlaying one side of the whole robot hand (palm, finger
links, fingertips). Such a design is low-cost, giving a larger coverage of the
object, and minimizing the Sim2Real gap at the same time. We train an in-hand
rotation policy using Reinforcement Learning on diverse objects in simulation.
Relying on touch-only sensing, we can directly deploy the policy in a real
robot hand and rotate novel objects that are not presented in training.
Extensive ablations are performed on how tactile information help in-hand
manipulation. Our project is available at https://touchdexterity.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://touchdexterity.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hardware-Aware Graph Neural Network Automated Design for Edge Computing
  Platforms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10875v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10875v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ao Zhou, Jianlei Yang, Yingjie Qi, Yumeng Shi, Tong Qiao, Weisheng Zhao, Chunming Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph neural networks (GNNs) have emerged as a popular strategy for handling
non-Euclidean data due to their state-of-the-art performance. However, most of
the current GNN model designs mainly focus on task accuracy, lacking in
considering hardware resources limitation and real-time requirements of edge
application scenarios. Comprehensive profiling of typical GNN models indicates
that their execution characteristics are significantly affected across
different computing platforms, which demands hardware awareness for efficient
GNN designs. In this work, HGNAS is proposed as the first Hardware-aware Graph
Neural Architecture Search framework targeting resource constraint edge
devices. By decoupling the GNN paradigm, HGNAS constructs a fine-grained design
space and leverages an efficient multi-stage search strategy to explore optimal
architectures within a few GPU hours. Moreover, HGNAS achieves hardware
awareness during the GNN architecture design by leveraging a hardware
performance predictor, which could balance the GNN model accuracy and
efficiency corresponding to the characteristics of targeted devices.
Experimental results show that HGNAS can achieve about $10.6\times$ speedup and
$88.2\%$ peak memory reduction with a negligible accuracy loss compared to
DGCNN on various edge devices, including Nvidia RTX3080, Jetson TX2, Intel
i7-8700K and Raspberry Pi 3B+.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by DAC'23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improved Sample Complexity for Reward-free Reinforcement Learning under
  Low-rank MDPs <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10859v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10859v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Cheng, Ruiquan Huang, Jing Yang, Yingbin Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In reward-free reinforcement learning (RL), an agent explores the environment
first without any reward information, in order to achieve certain learning
goals afterwards for any given reward. In this paper we focus on reward-free RL
under low-rank MDP models, in which both the representation and linear weight
vectors are unknown. Although various algorithms have been proposed for
reward-free low-rank MDPs, the corresponding sample complexity is still far
from being satisfactory. In this work, we first provide the first known sample
complexity lower bound that holds for any algorithm under low-rank MDPs. This
lower bound implies it is strictly harder to find a near-optimal policy under
low-rank MDPs than under linear MDPs. We then propose a novel model-based
algorithm, coined RAFFLE, and show it can both find an $\epsilon$-optimal
policy and achieve an $\epsilon$-accurate system identification via reward-free
exploration, with a sample complexity significantly improving the previous
results. Such a sample complexity matches our lower bound in the dependence on
$\epsilon$, as well as on $K$ in the large $d$ regime, where $d$ and $K$
respectively denote the representation dimension and action space cardinality.
Finally, we provide a planning algorithm (without further interaction with true
environment) for RAFFLE to learn a near-accurate representation, which is the
first known representation learning guarantee under the same setting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting Realistic Test-Time Training: Sequential Inference and
  Adaptation by Anchored Clustering Regularized Self-Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10856v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10856v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongyi Su, Xun Xu, Tianrui Li, Kui Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deploying models on target domain data subject to distribution shift requires
adaptation. Test-time training (TTT) emerges as a solution to this adaptation
under a realistic scenario where access to full source domain data is not
available, and instant inference on the target domain is required. Despite many
efforts into TTT, there is a confusion over the experimental settings, thus
leading to unfair comparisons. In this work, we first revisit TTT assumptions
and categorize TTT protocols by two key factors. Among the multiple protocols,
we adopt a realistic sequential test-time training (sTTT) protocol, under which
we develop a test-time anchored clustering (TTAC) approach to enable stronger
test-time feature learning. TTAC discovers clusters in both source and target
domains and matches the target clusters to the source ones to improve
adaptation. When source domain information is strictly absent (i.e.
source-free) we further develop an efficient method to infer source domain
distributions for anchored clustering. Finally, self-training~(ST) has
demonstrated great success in learning from unlabeled data and we empirically
figure out that applying ST alone to TTT is prone to confirmation bias.
Therefore, a more effective TTT approach is introduced by regularizing
self-training with anchored clustering, and the improved model is referred to
as TTAC++. We demonstrate that, under all TTT protocols, TTAC++ consistently
outperforms the state-of-the-art methods on five TTT datasets, including
corrupted target domain, selected hard samples, synthetic-to-real adaptation
and adversarially attacked target domain. We hope this work will provide a fair
benchmarking of TTT methods, and future research should be compared within
respective protocols.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Test-time training, Self-training. arXiv admin note: substantial text
  overlap with arXiv:2206.02721</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ref-NeuS: Ambiguity-Reduced Neural Implicit Surface Learning for
  Multi-View Reconstruction with Reflection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10840v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10840v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhang Ge, Tao Hu, Haoyu Zhao, Shu Liu, Ying-Cong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural implicit surface learning has shown significant progress in multi-view
3D reconstruction, where an object is represented by multilayer perceptrons
that provide continuous implicit surface representation and view-dependent
radiance. However, current methods often fail to accurately reconstruct
reflective surfaces, leading to severe ambiguity. To overcome this issue, we
propose Ref-NeuS, which aims to reduce ambiguity by attenuating the importance
of reflective surfaces. Specifically, we utilize an anomaly detector to
estimate an explicit reflection score with the guidance of multi-view context
to localize reflective surfaces. Afterward, we design a reflection-aware
photometric loss that adaptively reduces ambiguity by modeling rendered color
as a Gaussian distribution, with the reflection score representing the
variance. We show that together with a reflection direction-dependent radiance,
our model achieves high-quality surface reconstruction on reflective surfaces
and outperforms the state-of-the-arts by a large margin. Besides, our model is
also comparable on general surfaces.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project webpage: https://g3956.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FedML-HE: An Efficient Homomorphic-Encryption-Based Privacy-Preserving
  Federated Learning System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10837v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10837v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weizhao Jin, Yuhang Yao, Shanshan Han, Carlee Joe-Wong, Srivatsan Ravi, Salman Avestimehr, Chaoyang He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) enables machine learning model training on
distributed edge devices by aggregating local model updates rather than local
data. However, privacy concerns arise as the FL server's access to local model
updates can potentially reveal sensitive personal information by performing
attacks like gradient inversion recovery. To address these concerns,
privacy-preserving methods, such as Homomorphic Encryption (HE)-based
approaches, have been proposed. Despite HE's post-quantum security advantages,
its applications suffer from impractical overheads. In this paper, we present
FedML-HE, the first practical system for efficient HE-based secure federated
aggregation that provides a user/device-friendly deployment platform. FL-HE
utilizes a novel universal overhead optimization scheme, significantly reducing
both computation and communication overheads during deployment while providing
customizable privacy guarantees. Our optimized system demonstrates considerable
overhead reduction, particularly for large models (e.g., ~10x reduction for
HE-federated training of ResNet-50 and ~40x reduction for BERT), demonstrating
the potential for scalable HE-based FL deployment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Object-Centric Slot Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10834v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10834v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jindong Jiang, Fei Deng, Gautam Singh, Sungjin Ahn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite remarkable recent advances, making object-centric learning work for
complex natural scenes remains the main challenge. The recent success of
adopting the transformer-based image generative model in object-centric
learning suggests that having a highly expressive image generator is crucial
for dealing with complex scenes. In this paper, inspired by this observation,
we aim to answer the following question: can we benefit from the other pillar
of modern deep generative models, i.e., the diffusion models, for
object-centric learning and what are the pros and cons of such a model? To this
end, we propose a new object-centric learning model, Latent Slot Diffusion
(LSD). LSD can be seen from two perspectives. From the perspective of
object-centric learning, it replaces the conventional slot decoders with a
latent diffusion model conditioned on the object slots. Conversely, from the
perspective of diffusion models, it is the first unsupervised compositional
conditional diffusion model which, unlike traditional diffusion models, does
not require supervised annotation such as a text description to learn to
compose. In experiments on various object-centric tasks, including the FFHQ
dataset for the first time in this line of research, we demonstrate that LSD
significantly outperforms the state-of-the-art transformer-based decoder,
particularly when the scene is more complex. We also show a superior quality in
unsupervised compositional generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data Might be Enough: Bridge Real-World Traffic Signal Control Using
  Offline Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10828v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10828v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liang Zhang, Jianming Deng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Applying reinforcement learning (RL) to traffic signal control (TSC) has
become a promising solution. However, most RL-based methods focus solely on
optimization within simulators and give little thought to deployment issues in
the real world. Online RL-based methods, which require interaction with the
environment, are limited in their interactions with the real-world environment.
Additionally, acquiring an offline dataset for offline RL is challenging in the
real world. Moreover, most real-world intersections prefer a cyclical phase
structure. To address these challenges, we propose: (1) a cyclical offline
dataset (COD), designed based on common real-world scenarios to facilitate easy
collection; (2) an offline RL model called DataLight, capable of learning
satisfactory control strategies from the COD; and (3) a method called Arbitrary
To Cyclical (ATC), which can transform most RL-based methods into cyclical
signal control. Extensive experiments using real-world datasets on simulators
demonstrate that: (1) DataLight outperforms most existing methods and achieves
comparable results with the best-performing method; (2) introducing ATC into
some recent RL-based methods achieves satisfactory performance; and (3) COD is
reliable, with DataLight remaining robust even with a small amount of data.
These results suggest that the cyclical offline dataset might be enough for
offline RL for TSC. Our proposed methods make significant contributions to the
TSC field and successfully bridge the gap between simulation experiments and
real-world applications. Our code is released on Github.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Global Model Approach to Robust Few-Shot SAR Automatic Target
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10800v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10800v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathan Inkawhich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In real-world scenarios, it may not always be possible to collect hundreds of
labeled samples per class for training deep learning-based SAR Automatic Target
Recognition (ATR) models. This work specifically tackles the few-shot SAR ATR
problem, where only a handful of labeled samples may be available to support
the task of interest. Our approach is composed of two stages. In the first, a
global representation model is trained via self-supervised learning on a large
pool of diverse and unlabeled SAR data. In the second stage, the global model
is used as a fixed feature extractor and a classifier is trained to partition
the feature space given the few-shot support samples, while simultaneously
being calibrated to detect anomalous inputs. Unlike competing approaches which
require a pristine labeled dataset for pretraining via meta-learning, our
approach learns highly transferable features from unlabeled data that have
little-to-no relation to the downstream task. We evaluate our method in
standard and extended MSTAR operating conditions and find it to achieve high
accuracy and robust out-of-distribution detection in many different few-shot
settings. Our results are particularly significant because they show the merit
of a global model approach to SAR ATR, which makes minimal assumptions, and
provides many axes for extendability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FlexVDW: A machine learning approach to account for protein flexibility
  in ligand docking <span class="chip">ICLR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11494v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11494v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patricia Suriana, Joseph M. Paggi, Ron O. Dror
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most widely used ligand docking methods assume a rigid protein structure.
This leads to problems when the structure of the target protein deforms upon
ligand binding. In particular, the ligand's true binding pose is often scored
very unfavorably due to apparent clashes between ligand and protein atoms,
which lead to extremely high values of the calculated van der Waals energy
term. Traditionally, this problem has been addressed by explicitly searching
for receptor conformations to account for the flexibility of the receptor in
ligand binding. Here we present a deep learning model trained to take receptor
flexibility into account implicitly when predicting van der Waals energy. We
show that incorporating this machine-learned energy term into a
state-of-the-art physics-based scoring function improves small molecule ligand
pose prediction results in cases with substantial protein deformation, without
degrading performance in cases with minimal protein deformation. This work
demonstrates the feasibility of learning effects of protein flexibility on
ligand binding without explicitly modeling changes in protein structure.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at the MLDD workshop, International Conference on Learning
  Representations (ICLR) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatic Measures for Evaluating Generative Design Methods for
  Architects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11483v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11483v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eric Yeh, Briland Hitaj, Vidyasagar Sadhu, Anirban Roy, Takuma Nakabayashi, Yoshito Tsuji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent explosion of high-quality image-to-image methods has prompted
interest in applying image-to-image methods towards artistic and design tasks.
Of interest for architects is to use these methods to generate design proposals
from conceptual sketches, usually hand-drawn sketches that are quickly
developed and can embody a design intent. More specifically, instantiating a
sketch into a visual that can be used to elicit client feedback is typically a
time consuming task, and being able to speed up this iteration time is
important. While the body of work in generative methods has been impressive,
there has been a mismatch between the quality measures used to evaluate the
outputs of these systems and the actual expectations of architects. In
particular, most recent image-based works place an emphasis on realism of
generated images. While important, this is one of several criteria architects
look for. In this work, we describe the expectations architects have for design
proposals from conceptual sketches, and identify corresponding automated
metrics from the literature. We then evaluate several image-to-image generative
methods that may address these criteria and examine their performance across
these metrics. From these results, we identify certain challenges with
hand-drawn conceptual sketches and describe possible future avenues of
investigation to address them.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 6 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sandwiched Video Compression: Efficiently Extending the Reach of
  Standard Codecs with Neural Wrappers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11473v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11473v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Berivan Isik, Onur G. Guleryuz, Danhang Tang, Jonathan Taylor, Philip A. Chou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose sandwiched video compression -- a video compression system that
wraps neural networks around a standard video codec. The sandwich framework
consists of a neural pre- and post-processor with a standard video codec
between them. The networks are trained jointly to optimize a rate-distortion
loss function with the goal of significantly improving over the standard codec
in various compression scenarios. End-to-end training in this setting requires
a differentiable proxy for the standard video codec, which incorporates
temporal processing with motion compensation, inter/intra mode decisions, and
in-loop filtering. We propose differentiable approximations to key video codec
components and demonstrate that the neural codes of the sandwich lead to
significantly better rate-distortion performance compared to compressing the
original frames of the input video in two important scenarios. When
transporting high-resolution video via low-resolution HEVC, the sandwich system
obtains 6.5 dB improvements over standard HEVC. More importantly, using the
well-known perceptual similarity metric, LPIPS, we observe $~30 \%$
improvements in rate at the same quality over HEVC. Last but not least we show
that pre- and post-processors formed by very modestly-parameterized,
light-weight networks can closely approximate these results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Did You Train on My <span class="highlight-title">Dataset</span>? Towards Public <span class="highlight-title">Dataset</span> Protection with
  Clean-Label Backdoor Watermarking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11470v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11470v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruixiang Tang, Qizhang Feng, Ninghao Liu, Fan Yang, Xia Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The huge supporting training data on the Internet has been a key factor in
the success of deep learning models. However, this abundance of
public-available data also raises concerns about the unauthorized exploitation
of datasets for commercial purposes, which is forbidden by dataset licenses. In
this paper, we propose a backdoor-based watermarking approach that serves as a
general framework for safeguarding public-available data. By inserting a small
number of watermarking samples into the dataset, our approach enables the
learning model to implicitly learn a secret function set by defenders. This
hidden function can then be used as a watermark to track down third-party
models that use the dataset illegally. Unfortunately, existing backdoor
insertion methods often entail adding arbitrary and mislabeled data to the
training set, leading to a significant drop in performance and easy detection
by anomaly detection algorithms. To overcome this challenge, we introduce a
clean-label backdoor watermarking framework that uses imperceptible
perturbations to replace mislabeled samples. As a result, the watermarking
samples remain consistent with the original labels, making them difficult to
detect. Our experiments on text, image, and audio datasets demonstrate that the
proposed framework effectively safeguards datasets with minimal impact on
original task performance. We also show that adding just 1% of watermarking
samples can inject a traceable watermarking function and that our watermarking
samples are stealthy and look benign upon visual inspection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Seven open problems in applied combinatorics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11464v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11464v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sinan G. Aksoy, Ryan Bennink, Yuzhou Chen, José Frías, Yulia R. Gel, Bill Kay, Uwe Naumann, Carlos Ortiz Marrero, Anthony V. Petyuk, Sandip Roy, Ignacio Segovia-Dominguez, Nate Veldt, Stephen J. Young
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present and discuss seven different open problems in applied
combinatorics. The application areas relevant to this compilation include
quantum computing, algorithmic differentiation, topological data analysis,
iterative methods, hypergraph cut algorithms, and power systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>43 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fairness-Aware Graph Filter Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11459v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11459v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        O. Deniz Kose, Yanning Shen, Gonzalo Mateos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graphs are mathematical tools that can be used to represent complex
real-world systems, such as financial markets and social networks. Hence,
machine learning (ML) over graphs has attracted significant attention recently.
However, it has been demonstrated that ML over graphs amplifies the already
existing bias towards certain under-represented groups in various
decision-making problems due to the information aggregation over biased graph
structures. Faced with this challenge, in this paper, we design a fair graph
filter that can be employed in a versatile manner for graph-based learning
tasks. The design of the proposed filter is based on a bias analysis and its
optimality in mitigating bias compared to its fairness-agnostic counterpart is
established. Experiments on real-world networks for node classification
demonstrate the efficacy of the proposed filter design in mitigating bias,
while attaining similar utility and better stability compared to baseline
algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 1 figure, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models and Simple, Stupid Bugs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11455v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11455v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Jesse, Toufique Ahmed, Premkumar T. Devanbu, Emily Morgan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the advent of powerful neural language models, AI-based systems to
assist developers in coding tasks are becoming widely available; Copilot is one
such system. Copilot uses Codex, a large language model (LLM), to complete code
conditioned on a preceding "prompt". Codex, however, is trained on public
GitHub repositories, viz., on code that may include bugs and vulnerabilities.
Previous studies [1], [2] show Codex reproduces vulnerabilities seen in
training. In this study, we examine how prone Codex is to generate an
interesting bug category, single statement bugs, commonly referred to as
simple, stupid bugs or SStuBs in the MSR community. We find that Codex and
similar LLMs do help avoid some SStuBs, but do produce known, verbatim SStuBs
as much as 2x as likely than known, verbatim correct code. We explore the
consequences of the Codex generated SStuBs and propose avoidance strategies
that suggest the possibility of reducing the production of known, verbatim
SStubs, and increase the possibility of producing known, verbatim fixes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at International Conference on Mining Software Repositories
  (MSR-2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How (Implicit) Regularization of ReLU Neural Networks Characterizes the
  Learned Function -- Part II: the Multi-D Case of Two Layers with Random First
  Layer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11454v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11454v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jakob Heiss, Josef Teichmann, Hanna Wutte
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Randomized neural networks (randomized NNs), where only the terminal layer's
weights are optimized constitute a powerful model class to reduce computational
time in training the neural network model. At the same time, these models
generalize surprisingly well in various regression and classification tasks. In
this paper, we give an exact macroscopic characterization (i.e., a
characterization in function space) of the generalization behavior of
randomized, shallow NNs with ReLU activation (RSNs). We show that RSNs
correspond to a generalized additive model (GAM)-typed regression in which
infinitely many directions are considered: the infinite generalized additive
model (IGAM). The IGAM is formalized as solution to an optimization problem in
function space for a specific regularization functional and a fairly general
loss. This work is an extension to multivariate NNs of prior work, where we
showed how wide RSNs with ReLU activation behave like spline regression under
certain conditions and if the input is one-dimensional.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages + appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Greedy Pruning with Group Lasso Provably Generalizes for Matrix Sensing
  and Neural Networks with Quadratic Activations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11453v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11453v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nived Rajaraman,  Devvrit, Aryan Mokhtari, Kannan Ramchandran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pruning schemes have been widely used in practice to reduce the complexity of
trained models with a massive number of parameters. Several practical studies
have shown that pruning an overparameterized model and fine-tuning generalizes
well to new samples. Although the above pipeline, which we refer to as pruning
+ fine-tuning, has been extremely successful in lowering the complexity of
trained models, there is very little known about the theory behind this
success. In this paper we address this issue by investigating the pruning +
fine-tuning framework on the overparameterized matrix sensing problem, with the
ground truth denoted $U_\star \in \mathbb{R}^{d \times r}$ and the
overparameterized model $U \in \mathbb{R}^{d \times k}$ with $k \gg r$. We
study the approximate local minima of the empirical mean square error,
augmented with a smooth version of a group Lasso regularizer, $\sum_{i=1}^k \|
U e_i \|_2$ and show that pruning the low $\ell_2$-norm columns results in a
solution $U_{\text{prune}}$ which has the minimum number of columns $r$, yet is
close to the ground truth in training loss. Initializing the subsequent
fine-tuning phase from $U_{\text{prune}}$, the resulting solution converges
linearly to a generalization error of $O(\sqrt{rd/n})$ ignoring lower order
terms, which is statistically optimal. While our analysis provides insights
into the role of regularization in pruning, we also show that running gradient
descent in the absence of regularization results in models which {are not
suitable for greedy pruning}, i.e., many columns could have their $\ell_2$ norm
comparable to that of the maximum. Lastly, we extend our results for the
training and pruning of two-layer neural networks with quadratic activation
functions. Our results provide the first rigorous insights on why greedy
pruning + fine-tuning leads to smaller models which also generalize well.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>60 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bias mitigation techniques in image classification: fair machine
  learning in human heritage collections 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11449v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11449v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dalia Ortiz Pablo, Sushruth Badri, Erik Norén, Christoph Nötzli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A major problem with using automated classification systems is that if they
are not engineered correctly and with fairness considerations, they could be
detrimental to certain populations. Furthermore, while engineers have developed
cutting-edge technologies for image classification, there is still a gap in the
application of these models in human heritage collections, where data sets
usually consist of low-quality pictures of people with diverse ethnicity,
gender, and age. In this work, we evaluate three bias mitigation techniques
using two state-of-the-art neural networks, Xception and EfficientNet, for
gender classification. Moreover, we explore the use of transfer learning using
a fair data set to overcome the training data scarcity. We evaluated the
effectiveness of the bias mitigation pipeline on a cultural heritage collection
of photographs from the 19th and 20th centuries, and we used the FairFace data
set for the transfer learning experiments. After the evaluation, we found that
transfer learning is a good technique that allows better performance when
working with a small data set. Moreover, the fairest classifier was found to be
accomplished using transfer learning, threshold change, re-weighting and image
augmentation as bias mitigation methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Geometrical aspects of lattice gauge equivariant convolutional neural
  networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11448v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11448v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jimmy Aronsson, David I. Müller, Daniel Schuh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lattice gauge equivariant convolutional neural networks (L-CNNs) are a
framework for convolutional neural networks that can be applied to non-Abelian
lattice gauge theories without violating gauge symmetry. We demonstrate how
L-CNNs can be equipped with global group equivariance. This allows us to extend
the formulation to be equivariant not just under translations but under global
lattice symmetries such as rotations and reflections. Additionally, we provide
a geometric formulation of L-CNNs and show how convolutions in L-CNNs arise as
a special case of gauge equivariant neural networks on SU($N$) principal
bundles.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Inversion by Direct Iteration: An Alternative to Denoising Diffusion for
  Image Restoration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11435v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11435v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mauricio Delbracio, Peyman Milanfar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inversion by Direct Iteration (InDI) is a new formulation for supervised
image restoration that avoids the so-called ``regression to the mean'' effect
and produces more realistic and detailed images than existing regression-based
methods. It does this by gradually improving image quality in small steps,
similar to generative denoising diffusion models.
  Image restoration is an ill-posed problem where multiple high-quality images
are plausible reconstructions of a given low-quality input. Therefore, the
outcome of a single step regression model is typically an aggregate of all
possible explanations, therefore lacking details and realism. % The main
advantage of InDI is that it does not try to predict the clean target image in
a single step but instead gradually improves the image in small steps,
resulting in better perceptual quality.
  While generative denoising diffusion models also work in small steps, our
formulation is distinct in that it does not require knowledge of any analytic
form of the degradation process. Instead, we directly learn an iterative
restoration process from low-quality and high-quality paired examples. InDI can
be applied to virtually any image degradation, given paired training data. In
conditional denoising diffusion image restoration the denoising network
generates the restored image by repeatedly denoising an initial image of pure
noise, conditioned on the degraded input. Contrary to conditional denoising
formulations, InDI directly proceeds by iteratively restoring the input
low-quality image, producing high-quality results on a variety of image
restoration tasks, including motion and out-of-focus deblurring,
super-resolution, compression artifact removal, and denoising.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ResDTA: Predicting Drug-Target Binding Affinity Using Residual Skip
  Connections 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11434v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11434v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Partho Ghosh, Md. Aynal Haque
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The discovery of novel drug target (DT) interactions is an important step in
the drug development process. The majority of computer techniques for
predicting DT interactions have focused on binary classification, with the goal
of determining whether or not a DT pair interacts. Protein ligand interactions,
on the other hand, assume a continuous range of binding strength values, also
known as binding affinity, and forecasting this value remains a difficulty. As
the amount of affinity data in DT knowledge-bases grows, advanced learning
techniques such as deep learning architectures can be used to predict binding
affinities. In this paper, we present a deep-learning-based methodology for
predicting DT binding affinities using just sequencing information from both
targets and drugs. The results show that the proposed deep learning-based model
that uses the 1D representations of targets and drugs is an effective approach
for drug target binding affinity prediction and it does not require additional
chemical domain knowledge to work with. The model in which high-level
representations of a drug and a target are constructed via CNNs that uses
residual skip connections and also with an additional stream to create a
high-level combined representation of the drug-target pair achieved the best
Concordance Index (CI) performance in one of the largest benchmark datasets,
outperforming the recent state-of-the-art method AttentionDTA and many other
machine-learning and deep-learning based baseline methods for DT binding
affinity prediction that uses the 1D representations of targets and drugs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>40 pages, 10 figures, 2 tables. arXiv admin note: substantial text
  overlap with arXiv:1801.10193, arXiv:1902.04166 by other authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lamarr: LHCb ultra-fast simulation based on machine learning models
  deployed within Gauss 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11428v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11428v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Barbetti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  About 90% of the computing resources available to the LHCb experiment has
been spent to produce simulated data samples for Run 2 of the Large Hadron
Collider at CERN. The upgraded LHCb detector will be able to collect larger
data samples, requiring many more simulated events to analyze the data to be
collected in Run 3. Simulation is a key necessity of analysis to interpret
signal vs background and measure efficiencies. The needed simulation will far
exceed the pledged resources, requiring an evolution in technologies and
techniques to produce these simulated data samples. In this contribution, we
discuss Lamarr, a Gaudi-based framework to speed-up the simulation production
parametrizing both the detector response and the reconstruction algorithms of
the LHCb experiment. Deep Generative Models powered by several algorithms and
strategies are employed to effectively parametrize the high-level response of
the single components of the LHCb detector, encoding within neural networks the
experimental errors and uncertainties introduced in the detection and
reconstruction phases. Where possible, models are trained directly on real
data, statistically subtracting any background components through weights
application. Embedding Lamarr in the general LHCb Gauss Simulation framework
allows to combine its execution with any of the available generators in a
seamless way. The resulting software package enables a simulation process
completely independent of the Detailed Simulation used to date.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review in Journal of Physics: Conference Series (ACAT 2022)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond mAP: Towards better evaluation of instance segmentation <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.01614v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.01614v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rohit Jena, Lukas Zhornyak, Nehal Doiphode, Pratik Chaudhari, Vivek Buch, James Gee, Jianbo Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Correctness of instance segmentation constitutes counting the number of
objects, correctly localizing all predictions and classifying each localized
prediction. Average Precision is the de-facto metric used to measure all these
constituents of segmentation. However, this metric does not penalize duplicate
predictions in the high-recall range, and cannot distinguish instances that are
localized correctly but categorized incorrectly. This weakness has
inadvertently led to network designs that achieve significant gains in AP but
also introduce a large number of false positives. We therefore cannot rely on
AP to choose a model that provides an optimal tradeoff between false positives
and high recall. To resolve this dilemma, we review alternative metrics in the
literature and propose two new measures to explicitly measure the amount of
both spatial and categorical duplicate predictions. We also propose a Semantic
Sorting and NMS module to remove these duplicates based on a pixel occupancy
matching scheme. Experiments show that modern segmentation networks have
significant gains in AP, but also contain a considerable amount of duplicates.
Our Semantic Sorting and NMS can be added as a plug-and-play module to mitigate
hedged predictions and preserve AP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fighting Money Laundering with Statistics and Machine Learning: An
  Introduction and <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.04207v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.04207v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rasmus Jensen, Alexandros Iosifidis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Money laundering is a profound global problem. Nonetheless, there is little
scientific literature on statistical and machine learning methods for
anti-money laundering. In this paper, we focus on anti-money laundering in
banks and provide an introduction and review of the literature. We propose a
unifying terminology with two central elements: (i) client risk profiling and
(ii) suspicious behavior flagging. We find that client risk profiling is
characterized by diagnostics, i.e., efforts to find and explain risk factors.
On the other hand, suspicious behavior flagging is characterized by
non-disclosed features and hand-crafted risk indices. Finally, we discuss
directions for future research. One major challenge is the need for more public
data sets. This may potentially be addressed by synthetic data generation.
Other possible research directions include semi-supervised and deep learning,
interpretability, and fairness of the results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in IEEE Access, vol. 11, pp. 8889-8903,
  doi:10.1109/ACCESS.2023.3239549</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scalable Multi-Agent Lab Framework for Lab Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.09099v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.09099v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        A. Gilad Kusne, Austin McDannald
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous materials research systems allow scientists to fail smarter, learn
faster, and spend less resources in their studies. As these systems grow in
number, capability, and complexity, a new challenge arises - how will they work
together across large facilities? We explore one solution to this question - a
multi-agent laboratory control frame-work. We demonstrate this framework with
an autonomous material science lab in mind - where information from diverse
research campaigns can be combined to ad-dress the scientific question at hand.
This framework can 1) account for realistic resource limits such as equipment
use, 2) allow for machine learning agents with diverse learning capabilities
and goals capable of running re-search campaigns, and 3) facilitate multi-agent
collaborations and teams. The framework is dubbed the MULTI-agent auTonomous
fAcilities - a Scalable frameworK aka MULTITASK. MULTITASK makes possible
facility-wide simulations, including agent-instrument and agent-agent
interactions. Through MULTITASK's modularity, real-world facilities can come
on-line in phases, with simulated instruments gradually replaced by real-world
instruments. We hope MULTITASK opens new areas of study in large-scale
autonomous and semi-autonomous research campaigns and facilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Concentration inequalities and optimal number of layers for stochastic
  deep neural networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.11241v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.11241v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michele Caprio, Sayan Mukherjee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We state concentration inequalities for the output of the hidden layers of a
stochastic deep neural network (SDNN), as well as for the output of the whole
SDNN. These results allow us to introduce an expected classifier (EC), and to
give probabilistic upper bound for the classification error of the EC. We also
state the optimal number of layers for the SDNN via an optimal stopping
procedure. We apply our analysis to a stochastic version of a feedforward
neural network with ReLU activation function.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-armed Bandit Learning on a Graph 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.09419v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.09419v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianpeng Zhang, Kasper Johansson, Na Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The multi-armed bandit(MAB) problem is a simple yet powerful framework that
has been extensively studied in the context of decision-making under
uncertainty. In many real-world applications, such as robotic applications,
selecting an arm corresponds to a physical action that constrains the choices
of the next available arms (actions). Motivated by this, we study an extension
of MAB called the graph bandit, where an agent travels over a graph to maximize
the reward collected from different nodes. The graph defines the agent's
freedom in selecting the next available nodes at each step. We assume the graph
structure is fully available, but the reward distributions are unknown. Built
upon an offline graph-based planning algorithm and the principle of optimism,
we design a learning algorithm, G-UCB, that balances long-term
exploration-exploitation using the principle of optimism. We show that our
proposed algorithm achieves $O(\sqrt{|S|T\log(T)}+D|S|\log T)$ learning regret,
where $|S|$ is the number of nodes and $D$ is the diameter of the graph, which
matches the theoretical lower bound $\Omega(\sqrt{|S|T})$ up to logarithmic
factors. To our knowledge, this result is among the first tight regret bounds
in non-episodic, un-discounted learning problems with known deterministic
transitions. Numerical experiments confirm that our algorithm outperforms
several benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bilevel Imaging Learning Problems as Mathematical Programs with
  Complementarity Constraints: Reformulation and Theory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.02273v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.02273v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juan Carlos De los Reyes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate a family of bilevel imaging learning problems where the
lower-level instance corresponds to a convex variational model involving first-
and second-order nonsmooth sparsity-based regularizers. By using geometric
properties of the primal-dual reformulation of the lower-level problem and
introducing suitable auxiliar variables, we are able to reformulate the
original bilevel problems as Mathematical Programs with Complementarity
Constraints (MPCC). For the latter, we prove tight constraint qualification
conditions (MPCC-RCPLD and partial MPCC-LICQ) and derive Mordukhovich (M-) and
Strong (S-) stationarity conditions. The stationarity systems for the MPCC turn
also into stationarity conditions for the original formulation. Second-order
sufficient optimality conditions are derived as well, together with a local
uniqueness result for stationary points. The proposed reformulation may be
extended to problems in function spaces, leading to MPCC's with constraints on
the gradient of the state. The MPCC reformulation also leads to the efficient
use of available large-scale nonlinear programming solvers, as shown in a
companion paper, where different imaging applications are studied.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Meta-GNN approach to personalized seizure detection and classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.02642v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.02642v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdellah Rahmani, Arun Venkitaraman, Pascal Frossard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a personalized seizure detection and classification
framework that quickly adapts to a specific patient from limited seizure
samples. We achieve this by combining two novel paradigms that have recently
seen much success in a wide variety of real-world applications: graph neural
networks (GNN), and meta-learning. We train a Meta-GNN based classifier that
learns a global model from a set of training patients such that this global
model can eventually be adapted to a new unseen patient using very limited
samples. We apply our approach on the TUSZ-dataset, one of the largest and
publicly available benchmark datasets for epilepsy. We show that our method
outperforms the baselines by reaching 82.7% on accuracy and 82.08% on F1 score
after only 20 iterations on new unseen patients.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spatially heterogeneous learning by a deep student machine 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.07419v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.07419v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hajime Yoshino
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the spectacular successes, deep neural networks (DNN) with a huge
number of adjustable parameters remain largely black boxes. To shed light on
the hidden layers of DNN, we study supervised learning by a DNN of width $N$
and depth $L$ consisting of perceptrons with $c$ inputs by a statistical
mechanics approach called the teacher-student setting. We consider an ensemble
of student machines that exactly reproduce $M$ sets of $N$ dimensional
input/output relations provided by a teacher machine. We analyze the ensemble
theoretically using a replica method (H. Yoshino (2020)) and numerically
performing greedy Monte Carlo simulations. The replica theory which works on
high dimensional data $N \gg 1$ becomes exact in 'dense limit' $N \gg c \gg 1$
and $M \gg 1$ with fixed $\alpha=M/c$. Both the theory and the simulation
suggest learning by the DNN is quite heterogeneous in the network space:
configurations of the machines are more correlated within the layers closer to
the input/output boundaries while the central region remains much less
correlated due to over-parametrization. Deep enough systems relax faster thanks
to the less correlated central region. Remarkably both the theory and
simulation suggest generalization-ability of the student machines does not
vanish even in the deep limit $L \gg 1$ where the system becomes strongly
over-parametrized. We also consider the impact of effective dimension $D(\leq
N)$ of data by incorporating the hidden manifold model (S. Goldt et al (2020))
into our model. The replica theory implies that the loop corrections to the
dense limit, which reflect correlations between different nodes in the network,
become enhanced by either decreasing the width $\ N$ or decreasing the
effective dimension $D$ of the data. Simulation suggests both leads to
significant improvements in generalization-ability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 page, 18 figures (revised version with normalized squared
  overlaps)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data-Driven Constitutive Relation Reveals Scaling Law for Hydrodynamic
  Transport Coefficients 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2108.00413v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2108.00413v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Candi Zheng, Yang Wang, Shiyi Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Finding extended hydrodynamics equations valid from the dense gas region to
the rarefied gas region remains a great challenge. The key to success is to
obtain accurate constitutive relations for stress and heat flux. Data-driven
models offer a new phenomenological approach to learning constitutive relations
from data. Such models enable complex constitutive relations that extend
Newton's law of viscosity and Fourier's law of heat conduction by regression on
higher derivatives. However, the choices of derivatives in these models are
ad-hoc without a clear physical explanation. We investigated data-driven models
theoretically on a linear system. We argue that these models are equivalent to
non-linear length scale scaling laws of transport coefficients. The equivalence
to scaling laws justified the physical plausibility and revealed the limitation
of data-driven models. Our argument also points out that modeling the scaling
law could avoid practical difficulties in data-driven models like derivative
estimation and variable selection on noisy data. We further proposed a
constitutive relation model based on scaling law and tested it on the
calculation of Rayleigh scattering spectra. The result shows our data-driven
model has a clear advantage over the Chapman-Enskog expansion and moment
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FrankenSplit: Saliency Guided Neural Feature Compression with Shallow
  Variational Bottleneck Injection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.10681v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.10681v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alireza Furutanpey, Philipp Raith, Schahram Dustdar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of mobile AI accelerators allows latency-sensitive applications to
execute lightweight Deep Neural Networks (DNNs) on the client side. However,
critical applications require powerful models that edge devices cannot host and
must therefore offload requests, where the high-dimensional data will compete
for limited bandwidth. This work proposes shifting away from focusing on
executing shallow layers of partitioned DNNs. Instead, it advocates
concentrating the local resources on variational compression optimized for
machine interpretability. We introduce a novel framework for resource-conscious
compression models and extensively evaluate our method in an environment
reflecting the asymmetric resource distribution between edge devices and
servers. Our method achieves 60\% lower bitrate than a state-of-the-art SC
method without decreasing accuracy and is up to 16x faster than offloading with
existing codec standards.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ABAW: Valence-Arousal Estimation, Expression Recognition, Action Unit
  Detection & Emotional Reaction Intensity Estimation Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.01498v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.01498v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dimitrios Kollias, Panagiotis Tzirakis, Alice Baird, Alan Cowen, Stefanos Zafeiriou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The fifth Affective Behavior Analysis in-the-wild (ABAW) Competition is part
of the respective ABAW Workshop which will be held in conjunction with IEEE
Computer Vision and Pattern Recognition Conference (CVPR), 2023. The 5th ABAW
Competition is a continuation of the Competitions held at ECCV 2022, IEEE CVPR
2022, ICCV 2021, IEEE FG 2020 and CVPR 2017 Conferences, and is dedicated at
automatically analyzing affect. For this year's Competition, we feature two
corpora: i) an extended version of the Aff-Wild2 database and ii) the
Hume-Reaction dataset. The former database is an audiovisual one of around 600
videos of around 3M frames and is annotated with respect to:a) two continuous
affect dimensions -valence (how positive/negative a person is) and arousal (how
active/passive a person is)-; b) basic expressions (e.g. happiness, sadness,
neutral state); and c) atomic facial muscle actions (i.e., action units). The
latter dataset is an audiovisual one in which reactions of individuals to
emotional stimuli have been annotated with respect to seven emotional
expression intensities. Thus the 5th ABAW Competition encompasses four
Challenges: i) uni-task Valence-Arousal Estimation, ii) uni-task Expression
Classification, iii) uni-task Action Unit Detection, and iv) Emotional Reaction
Intensity Estimation. In this paper, we present these Challenges, along with
their corpora, we outline the evaluation metrics, we present the baseline
systems and illustrate their obtained performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2202.10659</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Teachable Autotelic Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2105.11977v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2105.11977v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olivier Sigaud, Ahmed Akakzia, Hugo Caselles-Dupré, Cédric Colas, Pierre-Yves Oudeyer, Mohamed Chetouani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous discovery and direct instruction are two distinct sources of
learning in children but education sciences demonstrate that mixed approaches
such as assisted discovery or guided play result in improved skill acquisition.
In the field of Artificial Intelligence, these extremes respectively map to
autonomous agents learning from their own signals and interactive learning
agents fully taught by their teachers. In between should stand teachable
autotelic agents (TAA): agents that learn from both internal and teaching
signals to benefit from the higher efficiency of assisted discovery. Designing
such agents will enable real-world non-expert users to orient the learning
trajectories of agents towards their expectations. More fundamentally, this may
also be a key step to build agents with human-level intelligence. This paper
presents a roadmap towards the design of teachable autonomous agents. Building
on developmental psychology and education sciences, we start by identifying key
features enabling assisted discovery processes in child-tutor interactions.
This leads to the production of a checklist of features that future TAA will
need to demonstrate. The checklist allows us to precisely pinpoint the various
limitations of current reinforcement learning agents and to identify the
promising first steps towards TAA. It also shows the way forward by
highlighting key research directions towards the design or autonomous agents
that can be taught by ordinary people via natural pedagogy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Calibrating Semantic Segmentation Models: Analyses and An Algorithm <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.12053v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.12053v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongdong Wang, Boqing Gong, Liqiang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of semantic segmentation calibration. Lots of solutions
have been proposed to approach model miscalibration of confidence in image
classification. However, to date, confidence calibration research on semantic
segmentation is still limited. We provide a systematic study on the calibration
of semantic segmentation models and propose a simple yet effective approach.
First, we find that model capacity, crop size, multi-scale testing, and
prediction correctness have impact on calibration. Among them, prediction
correctness, especially misprediction, is more important to miscalibration due
to over-confidence. Next, we propose a simple, unifying, and effective
approach, namely selective scaling, by separating correct/incorrect prediction
for scaling and more focusing on misprediction logit smoothing. Then, we study
popular existing calibration methods and compare them with selective scaling on
semantic segmentation calibration. We conduct extensive experiments with a
variety of benchmarks on both in-domain and domain-shift calibration, and show
that selective scaling consistently outperforms other methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR2023 (8 pages, 4 figures)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Few-shot human motion prediction for heterogeneous sensors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.11771v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.11771v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rafael Rego Drumond, Lukas Brinkmeyer, Lars Schmidt-Thieme
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human motion prediction is a complex task as it involves forecasting
variables over time on a graph of connected sensors. This is especially true in
the case of few-shot learning, where we strive to forecast motion sequences for
previously unseen actions based on only a few examples. Despite this, almost
all related approaches for few-shot motion prediction do not incorporate the
underlying graph, while it is a common component in classical motion
prediction. Furthermore, state-of-the-art methods for few-shot motion
prediction are restricted to motion tasks with a fixed output space meaning
these tasks are all limited to the same sensor graph. In this work, we propose
to extend recent works on few-shot time-series forecasting with heterogeneous
attributes with graph neural networks to introduce the first few-shot motion
approach that explicitly incorporates the spatial graph while also generalizing
across motion tasks with heterogeneous sensors. In our experiments on motion
tasks with heterogeneous sensors, we demonstrate significant performance
improvements with lifts from 10.4% up to 39.3% compared to best
state-of-the-art models. Moreover, we show that our model can perform on par
with the best approach so far when evaluating on tasks with a fixed output
space while maintaining two magnitudes fewer parameters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SimGRACE: A Simple Framework for Graph Contrastive Learning without Data
  Augmentation <span class="chip">WWW 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.03104v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.03104v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Xia, Lirong Wu, Jintao Chen, Bozhen Hu, Stan Z. Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph contrastive learning (GCL) has emerged as a dominant technique for
graph representation learning which maximizes the mutual information between
paired graph augmentations that share the same semantics. Unfortunately, it is
difficult to preserve semantics well during augmentations in view of the
diverse nature of graph data. Currently, data augmentations in GCL that are
designed to preserve semantics broadly fall into three unsatisfactory ways.
First, the augmentations can be manually picked per dataset by
trial-and-errors. Second, the augmentations can be selected via cumbersome
search. Third, the augmentations can be obtained by introducing expensive
domain-specific knowledge as guidance. All of these limit the efficiency and
more general applicability of existing GCL methods. To circumvent these crucial
issues, we propose a \underline{Sim}ple framework for \underline{GRA}ph
\underline{C}ontrastive l\underline{E}arning, \textbf{SimGRACE} for brevity,
which does not require data augmentations. Specifically, we take original graph
as input and GNN model with its perturbed version as two encoders to obtain two
correlated views for contrast. SimGRACE is inspired by the observation that
graph data can preserve their semantics well during encoder perturbations while
not requiring manual trial-and-errors, cumbersome search or expensive domain
knowledge for augmentations selection. Also, we explain why SimGRACE can
succeed. Furthermore, we devise adversarial training scheme, dubbed
\textbf{AT-SimGRACE}, to enhance the robustness of graph contrastive learning
and theoretically explain the reasons. Albeit simple, we show that SimGRACE can
yield competitive or better performance compared with state-of-the-art methods
in terms of generalizability, transferability and robustness, while enjoying
unprecedented degree of flexibility and efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by The Web Conference 2022 (WWW 2022)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Convergence Rates of Stochastic Zeroth-order Gradient Descent for Ł
  ojasiewicz Functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.16997v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.16997v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyu Wang, Yasong Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We prove convergence rates of Stochastic Zeroth-order Gradient Descent (SZGD)
algorithms for Lojasiewicz functions. The SZGD algorithm iterates as
\begin{align*}
  \mathbf{x}_{t+1} = \mathbf{x}_t - \eta_t \widehat{\nabla} f (\mathbf{x}_t),
\qquad t = 0,1,2,3,\cdots , \end{align*} where $f$ is the objective function
that satisfies the \L ojasiewicz inequality with \L ojasiewicz exponent
$\theta$, $\eta_t$ is the step size (learning rate), and $ \widehat{\nabla} f
(\mathbf{x}_t) $ is the approximate gradient estimated using zeroth-order
information only.
  Our results show that $ \{ f (\mathbf{x}_t) - f (\mathbf{x}_\infty) \}_{t \in
\mathbb{N} } $ can converge faster than $ \{ \| \mathbf{x}_t -
\mathbf{x}_\infty \| \}_{t \in \mathbb{N} }$, regardless of whether the
objective $f$ is smooth or nonsmooth.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Highly Efficient Real-Time Streaming and Fully On-Device Speaker
  Diarization with Multi-Stage Clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.13690v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.13690v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quan Wang, Yiling Huang, Han Lu, Guanlong Zhao, Ignacio Lopez Moreno
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While recent research advances in speaker diarization mostly focus on
improving the quality of diarization results, there is also an increasing
interest in improving the efficiency of diarization systems. In this paper, we
demonstrate that a multi-stage clustering strategy that uses different
clustering algorithms for input of different lengths can address multi-faceted
challenges of on-device speaker diarization applications. Specifically, a
fallback clusterer is used to handle short-form inputs; a main clusterer is
used to handle medium-length inputs; and a pre-clusterer is used to compress
long-form inputs before they are processed by the main clusterer. Both the main
clusterer and the pre-clusterer can be configured with an upper bound of the
computational complexity to adapt to devices with different resource
constraints. This multi-stage clustering strategy is critical for streaming
on-device speaker diarization systems, where the budgets of CPU, memory and
battery are tight.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Symmetric (Optimistic) Natural Policy Gradient for Multi-agent Learning
  with Parameter Convergence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.12812v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.12812v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sarath Pattathil, Kaiqing Zhang, Asuman Ozdaglar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-agent interactions are increasingly important in the context of
reinforcement learning, and the theoretical foundations of policy gradient
methods have attracted surging research interest. We investigate the global
convergence of natural policy gradient (NPG) algorithms in multi-agent
learning. We first show that vanilla NPG may not have parameter convergence,
i.e., the convergence of the vector that parameterizes the policy, even when
the costs are regularized (which enabled strong convergence guarantees in the
policy space in the literature). This non-convergence of parameters leads to
stability issues in learning, which becomes especially relevant in the function
approximation setting, where we can only operate on low-dimensional parameters,
instead of the high-dimensional policy. We then propose variants of the NPG
algorithm, for several standard multi-agent learning scenarios: two-player
zero-sum matrix and Markov games, and multi-player monotone games, with global
last-iterate parameter convergence guarantees. We also generalize the results
to certain function approximation settings. Note that in our algorithms, the
agents take symmetric roles. Our results might also be of independent interest
for solving nonconvex-nonconcave minimax optimization problems with certain
structures. Simulations are also provided to corroborate our theoretical
findings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Initially submitted for publication in January 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sionna: An Open-Source Library for Next-Generation Physical Layer
  Research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11854v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11854v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jakob Hoydis, Sebastian Cammerer, Fayçal Ait Aoudia, Avinash Vem, Nikolaus Binder, Guillermo Marcus, Alexander Keller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sionna is a GPU-accelerated open-source library for link-level simulations
based on TensorFlow. It enables the rapid prototyping of complex communication
system architectures and provides native support for the integration of neural
networks. Sionna implements a wide breadth of carefully tested state-of-the-art
algorithms that can be used for benchmarking and end-to-end performance
evaluation. This allows researchers to focus on their research, making it more
impactful and reproducible, while saving time implementing components outside
their area of expertise. This white paper provides a brief introduction to
Sionna, explains its design principles and features, as well as future
extensions, such as integrated ray tracing and custom CUDA kernels. We believe
that Sionna is a valuable tool for research on next-generation communication
systems, such as 6G, and we welcome contributions from our community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 1 figure, 4 code listings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Predictive Inference with Feature Conformal Prediction <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.00173v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.00173v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaye Teng, Chuan Wen, Dinghuai Zhang, <span class="highlight-author">Yoshua Bengio</span>, Yang Gao, Yang Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conformal prediction is a distribution-free technique for establishing valid
prediction intervals. Although conventionally people conduct conformal
prediction in the output space, this is not the only possibility. In this
paper, we propose feature conformal prediction, which extends the scope of
conformal prediction to semantic feature spaces by leveraging the inductive
bias of deep representation learning. From a theoretical perspective, we
demonstrate that feature conformal prediction provably outperforms regular
conformal prediction under mild assumptions. Our approach could be combined
with not only vanilla conformal prediction, but also other adaptive conformal
prediction methods. Apart from experiments on existing predictive inference
benchmarks, we also demonstrate the state-of-the-art performance of the
proposed methods on large-scale tasks such as ImageNet classification and
Cityscapes image segmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Estimation for Longitudinal Network via Adaptive Merging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.07866v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.07866v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Zhang, Junhui Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Longitudinal network consists of a sequence of temporal edges among multiple
nodes, where the temporal edges are observed in real time. It has become
ubiquitous with the rise of online social platform and e-commerce, but largely
under-investigated in literature. In this paper, we propose an efficient
estimation framework for longitudinal network, leveraging strengths of adaptive
network merging, tensor decomposition and point process. It merges neighboring
sparse networks so as to enlarge the number of observed edges and reduce
estimation variance, whereas the estimation bias introduced by network merging
is controlled by exploiting local temporal structures for adaptive network
neighborhood. A projected gradient descent algorithm is proposed to facilitate
estimation, where the upper bound of the estimation error in each iteration is
established. A thorough analysis is conducted to quantify the asymptotic
behavior of the proposed method, which shows that it can significantly reduce
the estimation error and also provides guideline for network merging under
various scenarios. We further demonstrate the advantage of the proposed method
through extensive numerical experiments on synthetic datasets and a militarized
interstate dispute dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages and 2 figures; the appendix including technical proof will
  be uploaded later</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards an AI-enabled Connected Industry: AGV Communication and Sensor
  Measurement <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.03364v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.03364v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rodrigo Hernangómez, Alexandros Palaios, Cara Watermann, Daniel Schäufele, Philipp Geuer, Rafail Ismayilov, Mohammad Parvini, Anton Krause, Martin Kasparick, Thomas Neugebauer, Oscar D. Ramos-Cantor, Hugues Tchouankem, Jose Leon Calvo, Bo Chen, Gerhard Fettweis, Sławomir Stańczak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents two wireless measurement campaigns in industrial
testbeds: industrial Vehicle-to-vehicle (iV2V) and industrial
Vehicle-to-infrastructure plus Sensor (iV2I+). Detailed information about the
two captured datasets is provided as well. iV2V covers sidelink communication
scenarios between Automated Guided Vehicles (AGVs), while iV2I+ is conducted at
an industrial setting where an autonomous cleaning robot is connected to a
private cellular network. The combination of different communication
technologies, together with a common measurement methodology, provides insights
that can be exploited by Machine Learning (ML) for tasks such as
fingerprinting, line-of-sight detection, prediction of quality of service or
link selection. Moreover, the datasets are labelled and pre-filtered for fast
on-boarding and applicability. The corresponding testbeds and measurements are
also presented in detail for both datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 3 figures. Submitted to a special issue magazine. Datasets
  available at
  https://ieee-dataport.org/open-access/ai4mobile-industrial-wireless-datasets-iv2v-and-iv2i</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GraphCFC: A Directed Graph based Cross-modal Feature Complementation
  Approach for Multimodal Conversational Emotion Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.12261v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.12261v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiang Li, Xiaoping Wang, Guoqing Lv, Zhigang Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emotion Recognition in Conversation (ERC) plays a significant part in
Human-Computer Interaction (HCI) systems since it can provide empathetic
services. Multimodal ERC can mitigate the drawbacks of uni-modal approaches.
Recently, Graph Neural Networks (GNNs) have been widely used in a variety of
fields due to their superior performance in relation modeling. In multimodal
ERC, GNNs are capable of extracting both long-distance contextual information
and inter-modal interactive information. Unfortunately, since existing methods
such as MMGCN directly fuse multiple modalities, redundant information may be
generated and diverse information may be lost. In this work, we present a
directed Graph based Cross-modal Feature Complementation (GraphCFC) module that
can efficiently model contextual and interactive information. GraphCFC
alleviates the problem of heterogeneity gap in multimodal fusion by utilizing
multiple subspace extractors and Pair-wise Cross-modal Complementary (PairCC)
strategy. We extract various types of edges from the constructed graph for
encoding, thus enabling GNNs to extract crucial contextual and interactive
information more accurately when performing message passing. Furthermore, we
design a GNN structure called GAT-MLP, which can provide a new unified network
framework for multimodal learning. The experimental results on two benchmark
datasets show that our GraphCFC outperforms the state-of-the-art (SOTA)
approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Who Is the Strongest Enemy? Towards Optimal and Efficient Evasion
  Attacks in Deep RL <span class="chip">ICLR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.05087v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.05087v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanchao Sun, Ruijie Zheng, Yongyuan Liang, Furong Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating the worst-case performance of a reinforcement learning (RL) agent
under the strongest/optimal adversarial perturbations on state observations
(within some constraints) is crucial for understanding the robustness of RL
agents. However, finding the optimal adversary is challenging, in terms of both
whether we can find the optimal attack and how efficiently we can find it.
Existing works on adversarial RL either use heuristics-based methods that may
not find the strongest adversary, or directly train an RL-based adversary by
treating the agent as a part of the environment, which can find the optimal
adversary but may become intractable in a large state space. This paper
introduces a novel attacking method to find the optimal attacks through
collaboration between a designed function named "actor" and an RL-based learner
named "director". The actor crafts state perturbations for a given policy
perturbation direction, and the director learns to propose the best policy
perturbation directions. Our proposed algorithm, PA-AD, is theoretically
optimal and significantly more efficient than prior RL-based works in
environments with large state spaces. Empirical results show that our proposed
PA-AD universally outperforms state-of-the-art attacking methods in various
Atari and MuJoCo environments. By applying PA-AD to adversarial training, we
achieve state-of-the-art empirical robustness in multiple tasks under strong
adversaries. The codebase is released at
https://github.com/umd-huang-lab/paad_adv_rl.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In the 10th International Conference on Learning Representations
  (ICLR 2022)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Aug<span class="highlight-title">GPT</span>: Leveraging Chat<span class="highlight-title">GPT</span> for Text Data Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.13007v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.13007v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haixing Dai, Zhengliang Liu, Wenxiong Liao, Xiaoke Huang, Yihan Cao, Zihao Wu, Lin Zhao, Shaochen Xu, Wei Liu, Ninghao Liu, Sheng Li, Dajiang Zhu, Hongmin Cai, Lichao Sun, Quanzheng Li, Dinggang Shen, Tianming Liu, Xiang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text data augmentation is an effective strategy for overcoming the challenge
of limited sample sizes in many natural language processing (NLP) tasks. This
challenge is especially prominent in the few-shot learning scenario, where the
data in the target domain is generally much scarcer and of lowered quality. A
natural and widely-used strategy to mitigate such challenges is to perform data
augmentation to better capture the data invariance and increase the sample
size. However, current text data augmentation methods either can't ensure the
correct labeling of the generated data (lacking faithfulness) or can't ensure
sufficient diversity in the generated data (lacking compactness), or both.
Inspired by the recent success of large language models, especially the
development of ChatGPT, which demonstrated improved language comprehension
abilities, in this work, we propose a text data augmentation approach based on
ChatGPT (named AugGPT). AugGPT rephrases each sentence in the training samples
into multiple conceptually similar but semantically different samples. The
augmented samples can then be used in downstream model training. Experiment
results on few-shot learning text classification tasks show the superior
performance of the proposed AugGPT approach over state-of-the-art text data
augmentation methods in terms of testing accuracy and distribution of the
augmented samples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Shape, Pose, and Appearance from a Single Image via Bootstrapped
  Radiance Field Inversion <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.11674v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.11674v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dario Pavllo, David Joseph Tan, Marie-Julie Rakotosaona, Federico Tombari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Radiance Fields (NeRF) coupled with GANs represent a promising
direction in the area of 3D reconstruction from a single view, owing to their
ability to efficiently model arbitrary topologies. Recent work in this area,
however, has mostly focused on synthetic datasets where exact ground-truth
poses are known, and has overlooked pose estimation, which is important for
certain downstream applications such as augmented reality (AR) and robotics. We
introduce a principled end-to-end reconstruction framework for natural images,
where accurate ground-truth poses are not available. Our approach recovers an
SDF-parameterized 3D shape, pose, and appearance from a single image of an
object, without exploiting multiple views during training. More specifically,
we leverage an unconditional 3D-aware generator, to which we apply a hybrid
inversion scheme where a model produces a first guess of the solution which is
then refined via optimization. Our framework can de-render an image in as few
as 10 steps, enabling its use in practical scenarios. We demonstrate
state-of-the-art results on a variety of real and synthetic benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023. Code and models are available at
  https://github.com/google-research/nerf-from-image</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for
  Text-to-Image Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.08453v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.08453v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, Ying Shan, Xiaohu Qie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The incredible generative ability of large-scale text-to-image (T2I) models
has demonstrated strong power of learning complex structures and meaningful
semantics. However, relying solely on text prompts cannot fully take advantage
of the knowledge learned by the model, especially when flexible and accurate
controlling (e.g., color and structure) is needed. In this paper, we aim to
``dig out" the capabilities that T2I models have implicitly learned, and then
explicitly use them to control the generation more granularly. Specifically, we
propose to learn simple and lightweight T2I-Adapters to align internal
knowledge in T2I models with external control signals, while freezing the
original large T2I models. In this way, we can train various adapters according
to different conditions, achieving rich control and editing effects in the
color and structure of the generation results. Further, the proposed
T2I-Adapters have attractive properties of practical value, such as
composability and generalization ability. Extensive experiments demonstrate
that our T2I-Adapter has promising generation quality and a wide range of
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Tech Report. GitHub: https://github.com/TencentARC/T2I-Adapter</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Active Exploration for Inverse Reinforcement Learning <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08645v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08645v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Lindner, Andreas Krause, Giorgia Ramponi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inverse Reinforcement Learning (IRL) is a powerful paradigm for inferring a
reward function from expert demonstrations. Many IRL algorithms require a known
transition model and sometimes even a known expert policy, or they at least
require access to a generative model. However, these assumptions are too strong
for many real-world applications, where the environment can be accessed only
through sequential interaction. We propose a novel IRL algorithm: Active
exploration for Inverse Reinforcement Learning (AceIRL), which actively
explores an unknown environment and expert policy to quickly learn the expert's
reward function and identify a good policy. AceIRL uses previous observations
to construct confidence intervals that capture plausible reward functions and
find exploration policies that focus on the most informative regions of the
environment. AceIRL is the first approach to active IRL with sample-complexity
bounds that does not require a generative model of the environment. AceIRL
matches the sample complexity of active IRL with a generative model in the
worst case. Additionally, we establish a problem-dependent bound that relates
the sample complexity of AceIRL to the suboptimality gap of a given IRL
problem. We empirically evaluate AceIRL in simulations and find that it
significantly outperforms more naive exploration strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at Conference on Neural Information Processing Systems
  (NeurIPS), 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative Adversarial Network for Personalized Art Therapy in Melanoma
  Disease Management 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.09232v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.09232v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lennart Jütte, Ning Wang, Bernhard Roth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Melanoma is the most lethal type of skin cancer. Patients are vulnerable to
mental health illnesses which can reduce the effectiveness of the cancer
treatment and the patients adherence to drug plans. It is crucial to preserve
the mental health of patients while they are receiving treatment. However,
current art therapy approaches are not personal and unique to the patient. We
aim to provide a well-trained image style transfer model that can quickly
generate unique art from personal dermoscopic melanoma images as an additional
tool for art therapy in disease management of melanoma. Visual art appreciation
as a common form of art therapy in disease management that measurably reduces
the degree of psychological distress. We developed a network based on the
cycle-consistent generative adversarial network for style transfer that
generates personalized and unique artworks from dermoscopic melanoma images. We
developed a model that converts melanoma images into unique flower-themed
artworks that relate to the shape of the lesion and are therefore personal to
the patient. Further, we altered the initial framework and made comparisons and
evaluations of the results. With this, we increased the options in the toolbox
for art therapy in disease management of melanoma. The development of an
easy-to-use user interface ensures the availability of the approach to
stakeholders. The transformation of melanoma into flower-themed artworks is
achieved by the proposed model and the graphical user interface. This
contribution opens a new field of GANs in art therapy and could lead to more
personalized disease management.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Message Passing Neural PDE Solvers <span class="chip">ICLR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.03376v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.03376v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Johannes Brandstetter, Daniel Worrall, Max Welling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The numerical solution of partial differential equations (PDEs) is difficult,
having led to a century of research so far. Recently, there have been pushes to
build neural--numerical hybrid solvers, which piggy-backs the modern trend
towards fully end-to-end learned systems. Most works so far can only generalize
over a subset of properties to which a generic solver would be faced,
including: resolution, topology, geometry, boundary conditions, domain
discretization regularity, dimensionality, etc. In this work, we build a
solver, satisfying these properties, where all the components are based on
neural message passing, replacing all heuristically designed components in the
computation graph with backprop-optimized neural function approximators. We
show that neural message passing solvers representationally contain some
classical methods, such as finite differences, finite volumes, and WENO
schemes. In order to encourage stability in training autoregressive models, we
put forward a method that is based on the principle of zero-stability, posing
stability as a domain adaptation problem. We validate our method on various
fluid-like flow problems, demonstrating fast, stable, and accurate performance
across different domain topologies, equation parameters, discretizations, etc.,
in 1D and 2D.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at ICLR 2022 (Spotlight paper), Github:
  https://github.com/brandstetter-johannes/MP-Neural-PDE-Solvers</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multitrack Music <span class="highlight-title">Transformer</span> <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06983v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06983v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao-Wen Dong, Ke Chen, Shlomo Dubnov, Julian McAuley, Taylor Berg-Kirkpatrick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing approaches for generating multitrack music with transformer models
have been limited in terms of the number of instruments, the length of the
music segments and slow inference. This is partly due to the memory
requirements of the lengthy input sequences necessitated by existing
representations. In this work, we propose a new multitrack music representation
that allows a diverse set of instruments while keeping a short sequence length.
Our proposed Multitrack Music Transformer (MMT) achieves comparable performance
with state-of-the-art systems, landing in between two recently proposed models
in a subjective listening test, while achieving substantial speedups and memory
reductions over both, making the method attractive for real time improvisation
or near real time creative applications. Further, we propose a new measure for
analyzing musical self-attention and show that the trained model attends more
to notes that form a consonant interval with the current note and to notes that
are 4N beats away from the current step.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2023. Audio samples available at
  https://salu133445.github.io/mmt/ . Source code available at
  https://github.com/salu133445/mmt</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Watch or Listen: Robust Audio-Visual Speech Recognition with Visual
  Corruption Modeling and Reliability Scoring <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08536v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08536v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joanna Hong, Minsu Kim, Jeongsoo Choi, Yong Man Ro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper deals with Audio-Visual Speech Recognition (AVSR) under multimodal
input corruption situations where audio inputs and visual inputs are both
corrupted, which is not well addressed in previous research directions.
Previous studies have focused on how to complement the corrupted audio inputs
with the clean visual inputs with the assumption of the availability of clean
visual inputs. However, in real life, clean visual inputs are not always
accessible and can even be corrupted by occluded lip regions or noises. Thus,
we firstly analyze that the previous AVSR models are not indeed robust to the
corruption of multimodal input streams, the audio and the visual inputs,
compared to uni-modal models. Then, we design multimodal input corruption
modeling to develop robust AVSR models. Lastly, we propose a novel AVSR
framework, namely Audio-Visual Reliability Scoring module (AV-RelScore), that
is robust to the corrupted multimodal inputs. The AV-RelScore can determine
which input modal stream is reliable or not for the prediction and also can
exploit the more reliable streams in prediction. The effectiveness of the
proposed method is evaluated with comprehensive experiments on popular
benchmark databases, LRS2 and LRS3. We also show that the reliability scores
obtained by AV-RelScore well reflect the degree of corruption and make the
proposed model focus on the reliable multimodal representations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2023. Implementation available:
  https://github.com/joannahong/AV-RelScore</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Falsification-Based Robust Adversarial Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2007.00691v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2007.00691v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Wang, Saasha Nair, Matthias Althoff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) has achieved enormous progress in solving various
sequential decision-making problems, such as control tasks in robotics. Since
policies are overfitted to training environments, RL methods have often failed
to be generalized to safety-critical test scenarios. Robust adversarial RL
(RARL) was previously proposed to train an adversarial network that applies
disturbances to a system, which improves the robustness in test scenarios.
However, an issue of neural network-based adversaries is that integrating
system requirements without handcrafting sophisticated reward signals are
difficult. Safety falsification methods allow one to find a set of initial
conditions and an input sequence, such that the system violates a given
property formulated in temporal logic. In this paper, we propose
falsification-based RARL (FRARL): this is the first generic framework for
integrating temporal logic falsification in adversarial learning to improve
policy robustness. By applying our falsification method, we do not need to
construct an extra reward function for the adversary. Moreover, we evaluate our
approach on a braking assistance system and an adaptive cruise control system
of autonomous vehicles. Our experimental results demonstrate that policies
trained with a falsification-based adversary generalize better and show less
violation of the safety specification in test scenarios than those trained
without an adversary or with an adversarial network.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GANs as Gradient Flows that Converge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.02910v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.02910v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu-Jui Huang, Yuchong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper approaches the unsupervised learning problem by gradient descent
in the space of probability density functions. A main result shows that along
the gradient flow induced by a distribution-dependent ordinary differential
equation (ODE), the unknown data distribution emerges as the long-time limit.
That is, one can uncover the data distribution by simulating the
distribution-dependent ODE. Intriguingly, the simulation of the ODE is shown
equivalent to the training of generative adversarial networks (GANs). This
equivalence provides a new "cooperative" view of GANs and, more importantly,
sheds new light on the divergence of GANs. In particular, it reveals that the
GAN algorithm implicitly minimizes the mean squared error (MSE) between two
sets of samples, and this MSE fitting alone can cause GANs to diverge. To
construct a solution to the distribution-dependent ODE, we first show that the
associated nonlinear Fokker-Planck equation has a unique weak solution, by the
Crandall-Liggett theorem for differential equations in Banach spaces. Based on
this solution to the Fokker-Planck equation, we construct a unique solution to
the ODE, using Trevisan's superposition principle. The convergence of the
induced gradient flow to the data distribution is obtained by analyzing the
Fokker-Planck equation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Safe Exploration Method for Reinforcement Learning under Existence of
  Disturbance <span class="chip">ECML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.15452v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.15452v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yoshihiro Okawa, Tomotake Sasaki, Hitoshi Yanami, Toru Namerikawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent rapid developments in reinforcement learning algorithms have been
giving us novel possibilities in many fields. However, due to their exploring
property, we have to take the risk into consideration when we apply those
algorithms to safety-critical problems especially in real environments. In this
study, we deal with a safe exploration problem in reinforcement learning under
the existence of disturbance. We define the safety during learning as
satisfaction of the constraint conditions explicitly defined in terms of the
state and propose a safe exploration method that uses partial prior knowledge
of a controlled object and disturbance. The proposed method assures the
satisfaction of the explicit state constraints with a pre-specified probability
even if the controlled object is exposed to a stochastic disturbance following
a normal distribution. As theoretical results, we introduce sufficient
conditions to construct conservative inputs not containing an exploring aspect
used in the proposed method and prove that the safety in the above explained
sense is guaranteed with the proposed method. Furthermore, we illustrate the
validity and effectiveness of the proposed method through numerical simulations
of an inverted pendulum and a four-bar parallel link robot manipulator.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the European Conference on Machine Learning and
  Principles and Practice of Knowledge Discovery in Databases (ECMLPKDD) 2022.
  The Version of Record is available at
  https://doi.org/10.1007/978-3-031-26412-2_9</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PCT-CycleGAN: Paired Complementary Temporal Cycle-Consistent Adversarial
  Networks for Radar-Based Precipitation Nowcasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.15046v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.15046v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaeho Choi, Yura Kim, Kwang-Ho Kim, Sung-Hwa Jung, Ikhyun Cho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The precipitation nowcasting methods have been elaborated over the centuries
because rain has a crucial impact on human life. Not only quantitative
precipitation forecast (QPF) models and convolutional long short-term memory
(ConvLSTM), but also various sophisticated methods such as the latest MetNet-2
are emerging. In this paper, we propose a paired complementary temporal
cycle-consistent adversarial networks (PCT-CycleGAN) for radar-based
precipitation nowcasting, inspired by cycle-consistent adversarial networks
(CycleGAN), which shows strong performance in image-to-image translation.
PCT-CycleGAN generates temporal causality using two generator networks with
forward and backward temporal dynamics in paired complementary cycles. Each
generator network learns a huge number of one-to-one mappings about
time-dependent radar-based precipitation data to approximate a mapping function
representing the temporal dynamics in each direction. To create robust temporal
causality between paired complementary cycles, novel connection loss is
proposed. The generator network learning forward temporal dynamics in
PCT-CycleGAN generates radar-based precipitation data 10 minutes from the
current time. Also, it provides a reliable prediction of up to 2 hours with
iterative forecasting. The superiority of PCT-CycleGAN is demonstrated through
qualitative and quantitative comparisons with several previous methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EcoFormer: Energy-Saving Attention with Linear Complexity <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.09004v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.09004v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Liu, Zizheng Pan, Haoyu He, Jianfei Cai, Bohan Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer is a transformative framework that models sequential data and has
achieved remarkable performance on a wide range of tasks, but with high
computational and energy cost. To improve its efficiency, a popular choice is
to compress the models via binarization which constrains the floating-point
values into binary ones to save resource consumption owing to cheap bitwise
operations significantly. However, existing binarization methods only aim at
minimizing the information loss for the input distribution statistically, while
ignoring the pairwise similarity modeling at the core of the attention. To this
end, we propose a new binarization paradigm customized to high-dimensional
softmax attention via kernelized hashing, called EcoFormer, to map the original
queries and keys into low-dimensional binary codes in Hamming space. The
kernelized hash functions are learned to match the ground-truth similarity
relations extracted from the attention map in a self-supervised way. Based on
the equivalence between the inner product of binary codes and the Hamming
distance as well as the associative property of matrix multiplication, we can
approximate the attention in linear complexity by expressing it as a
dot-product of binary codes. Moreover, the compact binary representations of
queries and keys enable us to replace most of the expensive multiply-accumulate
operations in attention with simple accumulations to save considerable on-chip
energy footprint on edge devices. Extensive experiments on both vision and
language tasks show that EcoFormer consistently achieves comparable performance
with standard attentions while consuming much fewer resources. For example,
based on PVTv2-B0 and ImageNet-1K, Ecoformer achieves a 73% on-chip energy
footprint reduction with only a 0.33% performance drop compared to the standard
attention. Code is available at https://github.com/ziplab/EcoFormer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2022 camera ready; First two authors contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Stochastic Generative Flow Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.09465v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.09465v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ling Pan, Dinghuai Zhang, Moksh Jain, Longbo Huang, <span class="highlight-author">Yoshua Bengio</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative Flow Networks (or GFlowNets for short) are a family of
probabilistic agents that learn to sample complex combinatorial structures
through the lens of "inference as control". They have shown great potential in
generating high-quality and diverse candidates from a given energy landscape.
However, existing GFlowNets can be applied only to deterministic environments,
and fail in more general tasks with stochastic dynamics, which can limit their
applicability. To overcome this challenge, this paper introduces Stochastic
GFlowNets, a new algorithm that extends GFlowNets to stochastic environments.
By decomposing state transitions into two steps, Stochastic GFlowNets isolate
environmental stochasticity and learn a dynamics model to capture it. Extensive
experimental results demonstrate that Stochastic GFlowNets offer significant
advantages over standard GFlowNets as well as MCMC- and RL-based approaches, on
a variety of standard benchmarks with stochastic dynamics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reward Reports for Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.10817v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.10817v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Krendl Gilbert, Nathan Lambert, Sarah Dean, Tom Zick, Aaron Snoswell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building systems that are good for society in the face of complex societal
effects requires a dynamic approach. Recent approaches to machine learning (ML)
documentation have demonstrated the promise of discursive frameworks for
deliberation about these complexities. However, these developments have been
grounded in a static ML paradigm, leaving the role of feedback and
post-deployment performance unexamined. Meanwhile, recent work in reinforcement
learning has shown that the effects of feedback and optimization objectives on
system behavior can be wide-ranging and unpredictable. In this paper we sketch
a framework for documenting deployed and iteratively updated learning systems,
which we call Reward Reports. Taking inspiration from various contributions to
the technical literature on reinforcement learning, we outline Reward Reports
as living documents that track updates to design choices and assumptions behind
what a particular automated system is optimizing for. They are intended to
track dynamic phenomena arising from system deployment, rather than merely
static properties of models or data. After presenting the elements of a Reward
Report, we discuss a concrete example: Meta's BlenderBot 3 chatbot. Several
others for game-playing (DeepMind's MuZero), content recommendation
(MovieLens), and traffic control (Project Flow) are included in the appendix.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Approximate Newton policy gradient algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.02398v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.02398v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoya Li, Samarth Gupta, Hsiangfu Yu, Lexing Ying, Inderjit Dhillon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Policy gradient algorithms have been widely applied to Markov decision
processes and reinforcement learning problems in recent years. Regularization
with various entropy functions is often used to encourage exploration and
improve stability. This paper proposes an approximate Newton method for the
policy gradient algorithm with entropy regularization. In the case of Shannon
entropy, the resulting algorithm reproduces the natural policy gradient
algorithm. For other entropy functions, this method results in brand-new policy
gradient algorithms. We prove that all these algorithms enjoy Newton-type
quadratic convergence and that the corresponding gradient flow converges
globally to the optimal solution. We use synthetic and industrial-scale
examples to demonstrate that the proposed approximate Newton method typically
converges in single-digit iterations, often orders of magnitude faster than
other state-of-the-art algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Optical XNOR-Bitcount Based Accelerator for Efficient Inference of
  Binary Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.06405v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.06405v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sairam Sri Vatsavai, Venkata Sai Praneeth Karempudi, Ishan Thakkar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Binary Neural Networks (BNNs) are increasingly preferred over full-precision
Convolutional Neural Networks(CNNs) to reduce the memory and computational
requirements of inference processing with minimal accuracy drop. BNNs convert
CNN model parameters to 1-bit precision, allowing inference of BNNs to be
processed with simple XNOR and bitcount operations. This makes BNNs amenable to
hardware acceleration. Several photonic integrated circuits (PICs) based BNN
accelerators have been proposed. Although these accelerators provide remarkably
higher throughput and energy efficiency than their electronic counterparts, the
utilized XNOR and bitcount circuits in these accelerators need to be further
enhanced to improve their area, energy efficiency, and throughput. This paper
aims to fulfill this need. For that, we invent a single-MRR-based optical XNOR
gate (OXG). Moreover, we present a novel design of bitcount circuit which we
refer to as Photo-Charge Accumulator (PCA). We employ multiple OXGs in a
cascaded manner using dense wavelength division multiplexing (DWDM) and connect
them to the PCA, to forge a novel Optical XNOR-Bitcount based Binary Neural
Network Accelerator (OXBNN). Our evaluation for the inference of four modern
BNNs indicates that OXBNN provides improvements of up to 62x and 7.6x in
frames-per-second (FPS) and FPS/W (energy efficiency), respectively, on
geometric mean over two PIC-based BNN accelerators from prior work. We
developed a transaction-level, event-driven python-based simulator for
evaluation of accelerators (https://github.com/uky-UCAT/B_ONN_SIM).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To Appear at IEEE ISQED 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Nonparametric Estimation of Intrinsic Data Structures by Chart
  Autoencoders: Generalization Error and Robustness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.09863v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.09863v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Liu, Alex Havrilla, Rongjie Lai, Wenjing Liao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autoencoders have demonstrated remarkable success in learning low-dimensional
latent features of high-dimensional data across various applications. Assuming
that data are sampled near a low-dimensional manifold, we employ chart
autoencoders, which encode data into low-dimensional latent features on a
collection of charts, preserving the topology and geometry of the data
manifold. Our paper establishes statistical guarantees on the generalization
error of chart autoencoders, and we demonstrate their denoising capabilities by
considering $n$ noisy training samples, along with their noise-free
counterparts, on a $d$-dimensional manifold. By training autoencoders, we show
that chart autoencoders can effectively denoise the input data with normal
noise. We prove that, under proper network architectures, chart autoencoders
achieve a squared generalization error in the order of $\displaystyle
n^{-\frac{2}{d+2}}\log^4 n$, which depends on the intrinsic dimension of the
manifold and only weakly depends on the ambient dimension and noise level. We
further extend our theory on data with noise containing both normal and
tangential components, where chart autoencoders still exhibit a denoising
effect for the normal component. As a special case, our theory also applies to
classical autoencoders, as long as the data manifold has a global
parametrization. Our results provide a solid theoretical foundation for the
effectiveness of autoencoders, which is further validated through several
numerical experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Catch-22 of Reservoir Computing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.10211v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.10211v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanzhao Zhang, Sean P. Cornelius
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reservoir Computing (RC) is a simple and efficient model-free framework for
forecasting the behavior of nonlinear dynamical systems from data. Here, we
show that there exist commonly-studied systems for which leading RC frameworks
struggle to learn the dynamics unless key information about the underlying
system is already known. We focus on the important problem of basin prediction
-- determining which attractor a system will converge to from its initial
conditions. First, we show that the predictions of standard RC models (echo
state networks) depend critically on warm-up time, requiring a warm-up
trajectory containing almost the entire transient in order to identify the
correct attractor even after being trained with optimal hyperparameters.
Accordingly, we turn to Next-Generation Reservoir Computing (NGRC), an
attractive variant of RC that requires negligible warm-up time. By
incorporating the exact nonlinearities in the original equations, we show that
NGRC can accurately reconstruct intricate and high-dimensional basins of
attraction, even with sparse training data (e.g., a single transient
trajectory). Yet, a tiny uncertainty on the exact nonlinearity can already
break NGRC, rendering the prediction accuracy no better than chance. Our
results highlight the challenges faced by data-driven methods in learning the
dynamics of multistable systems and suggest potential avenues to make these
approaches more robust.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>added new results on standard RC; expanded intro and discussion</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A generalization gap estimation for overparameterized models via the
  Langevin functional variance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.03660v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.03660v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akifumi Okuno, Keisuke Yano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper discusses the estimation of the generalization gap, the difference
between generalization performance and training performance, for
overparameterized models including neural networks. We first show that a
functional variance, a key concept in defining a widely-applicable information
criterion, characterizes the generalization gap even in overparameterized
settings where a conventional theory cannot be applied. As the computational
cost of the functional variance is expensive for the overparameterized models,
we propose an efficient approximation of the function variance, the Langevin
approximation of the functional variance (Langevin FV). This method leverages
only the $1$st-order gradient of the squared loss function, without referencing
the $2$nd-order gradient; this ensures that the computation is efficient and
the implementation is consistent with gradient-based optimization algorithms.
We demonstrate the Langevin FV numerically by estimating the generalization
gaps of overparameterized linear regression and non-linear neural network
models, containing more than a thousand of parameters therein.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>40 pages, no figure, accepted to Journal of Computational and
  Graphical Statistics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diffusion probabilistic modeling of protein backbones in 3D for the
  motif-scaffolding problem <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.04119v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.04119v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brian L. Trippe, Jason Yim, Doug Tischer, David Baker, Tamara Broderick, Regina Barzilay, Tommi Jaakkola
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Construction of a scaffold structure that supports a desired motif,
conferring protein function, shows promise for the design of vaccines and
enzymes. But a general solution to this motif-scaffolding problem remains open.
Current machine-learning techniques for scaffold design are either limited to
unrealistically small scaffolds (up to length 20) or struggle to produce
multiple diverse scaffolds. We propose to learn a distribution over diverse and
longer protein backbone structures via an E(3)-equivariant graph neural
network. We develop SMCDiff to efficiently sample scaffolds from this
distribution conditioned on a given motif; our algorithm is the first to
theoretically guarantee conditional samples from a diffusion model in the
large-compute limit. We evaluate our designed backbones by how well they align
with AlphaFold2-predicted structures. We show that our method can (1) sample
scaffolds up to 80 residues and (2) achieve structurally diverse scaffolds for
a fixed motif.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Appearing in ICLR 2023. Code available:
  github.com/blt2114/ProtDiff_SMCDiff</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Holistic Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.15829v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.15829v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dimitris Bertsimas, Kimberly Villalobos Carballo, Léonard Boussioux, Michael Lingzhi Li, Alex Paskov, Ivan Paskov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel holistic deep learning framework that
simultaneously addresses the challenges of vulnerability to input
perturbations, overparametrization, and performance instability from different
train-validation splits. The proposed framework holistically improves accuracy,
robustness, sparsity, and stability over standard deep learning models, as
demonstrated by extensive experiments on both tabular and image data sets. The
results are further validated by ablation experiments and SHAP value analysis,
which reveal the interactions and trade-offs between the different evaluation
metrics. To support practitioners applying our framework, we provide a
prescriptive approach that offers recommendations for selecting an appropriate
training loss function based on their specific objectives. All the code to
reproduce the results can be found at https://github.com/kimvc7/HDL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review at Machine Learning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large-scale End-of-Life Prediction of Hard Disks in Distributed
  Datacenters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08955v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08955v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rohan Mohapatra, Austin Coursey, Saptarshi Sengupta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  On a daily basis, data centers process huge volumes of data backed by the
proliferation of inexpensive hard disks. Data stored in these disks serve a
range of critical functional needs from financial, and healthcare to aerospace.
As such, premature disk failure and consequent loss of data can be
catastrophic. To mitigate the risk of failures, cloud storage providers perform
condition-based monitoring and replace hard disks before they fail. By
estimating the remaining useful life of hard disk drives, one can predict the
time-to-failure of a particular device and replace it at the right time,
ensuring maximum utilization whilst reducing operational costs. In this work,
large-scale predictive analyses are performed using severely skewed health
statistics data by incorporating customized feature engineering and a suite of
sequence learners. Past work suggests using LSTMs as an excellent approach to
predicting remaining useful life. To this end, we present an encoder-decoder
LSTM model where the context gained from understanding health statistics
sequences aid in predicting an output sequence of the number of days remaining
before a disk potentially fails. The models developed in this work are trained
and tested across an exhaustive set of all of the 10 years of S.M.A.R.T. health
data in circulation from Backblaze and on a wide variety of disk instances. It
closes the knowledge gap on what full-scale training achieves on thousands of
devices and advances the state-of-the-art by providing tangible metrics for
evaluation and generalization for practitioners looking to extend their
workflow to all years of health data in circulation across disk manufacturers.
The encoder-decoder LSTM posted an RMSE of 0.83 during training and 0.86 during
testing over the exhaustive 10 year data while being able to generalize
competitively over other drives from the Seagate family.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 9 figures and 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contextual Bandits with Packing and Covering Constraints: A Modular
  Lagrangian Approach via Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.07484v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.07484v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aleksandrs Slivkins, Karthik Abinav Sankararaman, Dylan Foster
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider a variant of contextual bandits in which the algorithm consumes
multiple resources subject to linear constraints on total consumption. This
problem generalizes contextual bandits with knapsacks (CBwK), allowing for
packing and covering constraints, as well as positive and negative resource
consumption. We present a new algorithm that is simple, computationally
efficient, and admits vanishing regret. It is statistically optimal for CBwK
when an algorithm must stop once some constraint is violated. Our algorithm
builds on LagrangeBwK (Immorlica et al., FOCS 2019) , a Lagrangian-based
technique for CBwK, and SquareCB (Foster and Rakhlin, ICML 2020), a
regression-based technique for contextual bandits. Our analysis leverages the
inherent modularity of both techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ D3G: Learning Multi-robot Coordination from Demonstrations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08892v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08892v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuan Wang, Yizhi Zhou, Wanxin Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper develops a Distributed Differentiable Dynamic Game (D3G)
framework, which enables learning multi-robot coordination from demonstrations.
We represent multi-robot coordination as a dynamic game, where the behavior of
a robot is dictated by its own dynamics and objective that also depends on
others' behavior. The coordination thus can be adapted by tuning the objective
and dynamics of each robot. The proposed D3G enables each robot to
automatically tune its individual dynamics and objectives in a distributed
manner by minimizing the mismatch between its trajectory and demonstrations.
This learning framework features a new design, including a forward-pass, where
all robots collaboratively seek Nash equilibrium of a game, and a
backward-pass, where gradients are propagated via the communication graph. We
test the D3G in simulation with two types of robots given different task
configurations. The results validate the capability of D3G for learning
multi-robot coordination from demonstrations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mitigating Covertly Unsafe Text within Natural Language Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.09306v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.09306v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex Mei, Anisha Kabir, Sharon Levy, Melanie Subbiah, Emily Allaway, John Judge, Desmond Patton, Bruce Bimber, Kathleen McKeown, William Yang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An increasingly prevalent problem for intelligent technologies is text
safety, as uncontrolled systems may generate recommendations to their users
that lead to injury or life-threatening consequences. However, the degree of
explicitness of a generated statement that can cause physical harm varies. In
this paper, we distinguish types of text that can lead to physical harm and
establish one particularly underexplored category: covertly unsafe text. Then,
we further break down this category with respect to the system's information
and discuss solutions to mitigate the generation of text in each of these
subcategories. Ultimately, our work defines the problem of covertly unsafe
language that causes physical harm and argues that this subtle yet dangerous
issue needs to be prioritized by stakeholders and regulators. We highlight
mitigation strategies to inspire future researchers to tackle this challenging
problem and help improve safety within smart systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Findings of the 2022 Conference on Empirical Methods in Natural
  Language Processing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AirTrack: Onboard Deep Learning Framework for Long-Range Aircraft
  Detection and Tracking <span class="chip">ICRA 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.12849v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.12849v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sourish Ghosh, Jay Patrikar, Brady Moon, Milad Moghassem Hamidi, Sebastian Scherer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detect-and-Avoid (DAA) capabilities are critical for safe operations of
unmanned aircraft systems (UAS). This paper introduces, AirTrack, a real-time
vision-only detect and tracking framework that respects the size, weight, and
power (SWaP) constraints of sUAS systems. Given the low Signal-to-Noise ratios
(SNR) of far away aircraft, we propose using full resolution images in a deep
learning framework that aligns successive images to remove ego-motion. The
aligned images are then used downstream in cascaded primary and secondary
classifiers to improve detection and tracking performance on multiple metrics.
We show that AirTrack outperforms state-of-the art baselines on the Amazon
Airborne Object Tracking (AOT) Dataset. Multiple real world flight tests with a
Cessna 182 interacting with general aviation traffic and additional
near-collision flight tests with a Bell helicopter flying towards a UAS in a
controlled setting showcase that the proposed approach satisfies the newly
introduced ASTM F3442/F3442M standard for DAA. Empirical evaluations show that
our system has a probability of track of more than 95% up to a range of 700m.
Video available at https://youtu.be/H3lL_Wjxjpw .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 5 figures, ICRA 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lossy Compression of Noisy Data for Private and Data-Efficient Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.02892v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.02892v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Berivan Isik, Tsachy Weissman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Storage-efficient privacy-preserving learning is crucial due to increasing
amounts of sensitive user data required for modern learning tasks. We propose a
framework for reducing the storage cost of user data while at the same time
providing privacy guarantees, without essential loss in the utility of the data
for learning. Our method comprises noise injection followed by lossy
compression. We show that, when appropriately matching the lossy compression to
the distribution of the added noise, the compressed examples converge, in
distribution, to that of the noise-free training data as the sample size of the
training data (or the dimension of the training data) increases. In this sense,
the utility of the data for learning is essentially maintained, while reducing
storage and privacy leakage by quantifiable amounts. We present experimental
results on the CelebA dataset for gender classification and find that our
suggested pipeline delivers in practice on the promise of the theory: the
individuals in the images are unrecognizable (or less recognizable, depending
on the noise level), overall storage of the data is substantially reduced, with
no essential loss (and in some cases a slight boost) to the classification
accuracy. As an added bonus, our experiments suggest that our method yields a
substantial boost to robustness in the face of adversarial test data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at the IEEE Journal on Selected Areas in Information Theory
  (JSAIT). Preliminary version was presented at the IEEE International
  Symposium on Information Theory (ISIT), 2022, with a slightly different
  title, "Learning under Storage and Privacy Constraints."</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DeepSense 6G: A Large-Scale Real-World Multi-Modal Sensing and
  Communication <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.09769v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.09769v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed Alkhateeb, Gouranga Charan, Tawfik Osman, Andrew Hredzak, João Morais, Umut Demirhan, Nikhil Srinivas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article presents the DeepSense 6G dataset, which is a large-scale
dataset based on real-world measurements of co-existing multi-modal sensing and
communication data. The DeepSense 6G dataset is built to advance deep learning
research in a wide range of applications in the intersection of multi-modal
sensing, communication, and positioning. This article provides a detailed
overview of the DeepSense dataset structure, adopted testbeds, data collection
and processing methodology, deployment scenarios, and example applications,
with the objective of facilitating the adoption and reproducibility of
multi-modal sensing and communication datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The dataset is available on the DeepSense 6G website
  http://deepsense6g.net/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Opto-UNet: Optimized UNet for Segmentation of Varicose Veins in Optical
  Coherence Tomography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.14808v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.14808v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maryam Viqar, Violeta Madjarova, Vipul Baghel, Elena Stoykova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human veins are important for carrying the blood from the body-parts to the
heart. The improper functioning of the human veins may arise from several
venous diseases. Varicose vein is one such disease wherein back flow of blood
can occur, often resulting in increased venous pressure or restricted blood
flow due to changes in the structure of vein. To examine the functional
characteristics of the varicose vein, it is crucial to study the physical and
bio mechanical properties of the vein. This work proposes a segmentation model
Opto-UNet, for segmenting the venous wall structure. Optical Coherence
Tomography system is used to acquire images of varicose vein. As the extracted
vein is not uniform in shape, hence adequate method of segmentation is required
to segment the venous wall. Opto-UNet model is based on the U-Net architecture
wherein a new block is integrated into the architecture, employing atrous and
separable convolution to extract spatially wide-range and separable features
maps for attaining advanced performance. Furthermore, the depth wise separable
convolution significantly reduces the complexity of the network by optimizing
the number of parameters. The model achieves accuracy of 0.9830, sensitivity of
0.8425 and specificity of 0.9980 using 8.54 million number of parameters. These
results indicate that model is highly adequate in segmenting the varicose vein
wall without deteriorating the segmentation quality along with reduced
complexity
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Case Study on Designing Evaluations of ML Explanations with Simulated
  User Studies <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.07444v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.07444v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ada Martin, Valerie Chen, Sérgio Jesus, Pedro Saleiro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When conducting user studies to ascertain the usefulness of model
explanations in aiding human decision-making, it is important to use real-world
use cases, data, and users. However, this process can be resource-intensive,
allowing only a limited number of explanation methods to be evaluated.
Simulated user evaluations (SimEvals), which use machine learning models as a
proxy for human users, have been proposed as an intermediate step to select
promising explanation methods. In this work, we conduct the first SimEvals on a
real-world use case to evaluate whether explanations can better support
ML-assisted decision-making in e-commerce fraud detection. We study whether
SimEvals can corroborate findings from a user study conducted in this fraud
detection context. In particular, we find that SimEvals suggest that all
considered explainers are equally performant, and none beat a baseline without
explanations -- this matches the conclusions of the original user study. Such
correspondences between our results and the original user study provide initial
evidence in favor of using SimEvals before running user studies. We also
explore the use of SimEvals as a cheap proxy to explore an alternative user
study set-up. We hope that this work motivates further study of when and how
SimEvals should be used to aid in the design of real-world evaluations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 2 figures. Will appear in ICLR 2023's TrustML-(un)Limited
  workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neighborhood Gradient Clustering: An Efficient Decentralized Learning
  Method for Non-IID Data Distributions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.14390v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.14390v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sai Aparna Aketi, Sangamesh Kodge, Kaushik Roy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decentralized learning over distributed datasets can have significantly
different data distributions across the agents. The current state-of-the-art
decentralized algorithms mostly assume the data distributions to be Independent
and Identically Distributed. This paper focuses on improving decentralized
learning over non-IID data. We propose \textit{Neighborhood Gradient Clustering
(NGC)}, a novel decentralized learning algorithm that modifies the local
gradients of each agent using self- and cross-gradient information.
Cross-gradients for a pair of neighboring agents are the derivatives of the
model parameters of an agent with respect to the dataset of the other agent. In
particular, the proposed method replaces the local gradients of the model with
the weighted mean of the self-gradients, model-variant cross-gradients
(derivatives of the neighbors' parameters with respect to the local dataset),
and data-variant cross-gradients (derivatives of the local model with respect
to its neighbors' datasets). The data-variant cross-gradients are aggregated
through an additional communication round without breaking the privacy
constraints. Further, we present \textit{CompNGC}, a compressed version of
\textit{NGC} that reduces the communication overhead by $32 \times$. We
theoretically analyze the convergence rate of the proposed algorithm and
demonstrate its efficiency over non-IID data sampled from {various vision and
language} datasets trained. Our experiments demonstrate that \textit{NGC} and
\textit{CompNGC} outperform (by $0-6\%$) the existing SoTA decentralized
learning algorithm over non-IID data with significantly less compute and memory
requirements. Further, our experiments show that the model-variant
cross-gradient information available locally at each agent can improve the
performance over non-IID data by $1-35\%$ without additional communication
cost.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 5 figures, 16 tables. arXiv admin note: text overlap with
  arXiv:2103.02051 by other authors</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">9</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LFACon: Introducing Anglewise Attention to No-Reference Quality
  Assessment in Light Field Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10961v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10961v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiang Qu, Xiaoming Chen, Yuk Ying Chung, Weidong Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Light field imaging can capture both the intensity information and the
direction information of light rays. It naturally enables a
six-degrees-of-freedom viewing experience and deep user engagement in virtual
reality. Compared to 2D image assessment, light field image quality assessment
(LFIQA) needs to consider not only the image quality in the spatial domain but
also the quality consistency in the angular domain. However, there is a lack of
metrics to effectively reflect the angular consistency and thus the angular
quality of a light field image (LFI). Furthermore, the existing LFIQA metrics
suffer from high computational costs due to the excessive data volume of LFIs.
In this paper, we propose a novel concept of "anglewise attention" by
introducing a multihead self-attention mechanism to the angular domain of an
LFI. This mechanism better reflects the LFI quality. In particular, we propose
three new attention kernels, including anglewise self-attention, anglewise grid
attention, and anglewise central attention. These attention kernels can realize
angular self-attention, extract multiangled features globally or selectively,
and reduce the computational cost of feature extraction. By effectively
incorporating the proposed kernels, we further propose our light field
attentional convolutional neural network (LFACon) as an LFIQA metric. Our
experimental results show that the proposed LFACon metric significantly
outperforms the state-of-the-art LFIQA metrics. For the majority of distortion
types, LFACon attains the best performance with lower complexity and less
computational time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for IEEE VR 2023 (TVCG Special Issues) (Early Access)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sandwiched Video Compression: Efficiently Extending the Reach of
  Standard Codecs with Neural Wrappers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11473v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11473v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Berivan Isik, Onur G. Guleryuz, Danhang Tang, Jonathan Taylor, Philip A. Chou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose sandwiched video compression -- a video compression system that
wraps neural networks around a standard video codec. The sandwich framework
consists of a neural pre- and post-processor with a standard video codec
between them. The networks are trained jointly to optimize a rate-distortion
loss function with the goal of significantly improving over the standard codec
in various compression scenarios. End-to-end training in this setting requires
a differentiable proxy for the standard video codec, which incorporates
temporal processing with motion compensation, inter/intra mode decisions, and
in-loop filtering. We propose differentiable approximations to key video codec
components and demonstrate that the neural codes of the sandwich lead to
significantly better rate-distortion performance compared to compressing the
original frames of the input video in two important scenarios. When
transporting high-resolution video via low-resolution HEVC, the sandwich system
obtains 6.5 dB improvements over standard HEVC. More importantly, using the
well-known perceptual similarity metric, LPIPS, we observe $~30 \%$
improvements in rate at the same quality over HEVC. Last but not least we show
that pre- and post-processors formed by very modestly-parameterized,
light-weight networks can closely approximate these results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Did You Train on My <span class="highlight-title">Dataset</span>? Towards Public <span class="highlight-title">Dataset</span> Protection with
  Clean-Label Backdoor Watermarking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11470v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11470v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruixiang Tang, Qizhang Feng, Ninghao Liu, Fan Yang, Xia Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The huge supporting training data on the Internet has been a key factor in
the success of deep learning models. However, this abundance of
public-available data also raises concerns about the unauthorized exploitation
of datasets for commercial purposes, which is forbidden by dataset licenses. In
this paper, we propose a backdoor-based watermarking approach that serves as a
general framework for safeguarding public-available data. By inserting a small
number of watermarking samples into the dataset, our approach enables the
learning model to implicitly learn a secret function set by defenders. This
hidden function can then be used as a watermark to track down third-party
models that use the dataset illegally. Unfortunately, existing backdoor
insertion methods often entail adding arbitrary and mislabeled data to the
training set, leading to a significant drop in performance and easy detection
by anomaly detection algorithms. To overcome this challenge, we introduce a
clean-label backdoor watermarking framework that uses imperceptible
perturbations to replace mislabeled samples. As a result, the watermarking
samples remain consistent with the original labels, making them difficult to
detect. Our experiments on text, image, and audio datasets demonstrate that the
proposed framework effectively safeguards datasets with minimal impact on
original task performance. We also show that adding just 1% of watermarking
samples can inject a traceable watermarking function and that our watermarking
samples are stealthy and look benign upon visual inspection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GraphCFC: A Directed Graph based Cross-modal Feature Complementation
  Approach for Multimodal Conversational Emotion Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.12261v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.12261v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiang Li, Xiaoping Wang, Guoqing Lv, Zhigang Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emotion Recognition in Conversation (ERC) plays a significant part in
Human-Computer Interaction (HCI) systems since it can provide empathetic
services. Multimodal ERC can mitigate the drawbacks of uni-modal approaches.
Recently, Graph Neural Networks (GNNs) have been widely used in a variety of
fields due to their superior performance in relation modeling. In multimodal
ERC, GNNs are capable of extracting both long-distance contextual information
and inter-modal interactive information. Unfortunately, since existing methods
such as MMGCN directly fuse multiple modalities, redundant information may be
generated and diverse information may be lost. In this work, we present a
directed Graph based Cross-modal Feature Complementation (GraphCFC) module that
can efficiently model contextual and interactive information. GraphCFC
alleviates the problem of heterogeneity gap in multimodal fusion by utilizing
multiple subspace extractors and Pair-wise Cross-modal Complementary (PairCC)
strategy. We extract various types of edges from the constructed graph for
encoding, thus enabling GNNs to extract crucial contextual and interactive
information more accurately when performing message passing. Furthermore, we
design a GNN structure called GAT-MLP, which can provide a new unified network
framework for multimodal learning. The experimental results on two benchmark
datasets show that our GraphCFC outperforms the state-of-the-art (SOTA)
approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MedLocker: A Transferable Adversarial Watermarking for Preventing
  Unauthorized Analysis of Medical Image <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.09858v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.09858v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bangzheng Pu, Xingxing Wei, Shiji Zhao, Huazhu Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The collection of medical image datasets is a demanding and laborious process
that requires significant resources. Furthermore, these medical datasets may
contain personally identifiable information, necessitating measures to ensure
that unauthorized access is prevented. Failure to do so could violate the
intellectual property rights of the dataset owner and potentially compromise
the privacy of patients. As a result, safeguarding medical datasets and
preventing unauthorized usage by AI diagnostic models is a pressing challenge.
To address this challenge, we propose a novel visible adversarial watermarking
method for medical image copyright protection, called MedLocker. Our approach
involves continuously optimizing the position and transparency of a watermark
logo, which reduces the performance of the target model, leading to incorrect
predictions. Importantly, we ensure that our method minimizes the impact on
clinical visualization by constraining watermark positions using semantical
masks (WSM), which are bounding boxes of lesion regions based on semantic
segmentation. To ensure the transferability of the watermark across different
models, we verify the cross-model transferability of the watermark generated on
a single model. Additionally, we generate a unique watermark parameter list
each time, which can be used as a certification to verify the authorization. We
evaluate the performance of MedLocker on various mainstream backbones and
validate the feasibility of adversarial watermarking for copyright protection
on two widely-used diabetic retinopathy detection datasets. Our results
demonstrate that MedLocker can effectively protect the copyright of medical
datasets and prevent unauthorized users from analyzing medical images with AI
diagnostic models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for
  Text-to-Image Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.08453v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.08453v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, Ying Shan, Xiaohu Qie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The incredible generative ability of large-scale text-to-image (T2I) models
has demonstrated strong power of learning complex structures and meaningful
semantics. However, relying solely on text prompts cannot fully take advantage
of the knowledge learned by the model, especially when flexible and accurate
controlling (e.g., color and structure) is needed. In this paper, we aim to
``dig out" the capabilities that T2I models have implicitly learned, and then
explicitly use them to control the generation more granularly. Specifically, we
propose to learn simple and lightweight T2I-Adapters to align internal
knowledge in T2I models with external control signals, while freezing the
original large T2I models. In this way, we can train various adapters according
to different conditions, achieving rich control and editing effects in the
color and structure of the generation results. Further, the proposed
T2I-Adapters have attractive properties of practical value, such as
composability and generalization ability. Extensive experiments demonstrate
that our T2I-Adapter has promising generation quality and a wide range of
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Tech Report. GitHub: https://github.com/TencentARC/T2I-Adapter</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multitrack Music <span class="highlight-title">Transformer</span> <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06983v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06983v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao-Wen Dong, Ke Chen, Shlomo Dubnov, Julian McAuley, Taylor Berg-Kirkpatrick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing approaches for generating multitrack music with transformer models
have been limited in terms of the number of instruments, the length of the
music segments and slow inference. This is partly due to the memory
requirements of the lengthy input sequences necessitated by existing
representations. In this work, we propose a new multitrack music representation
that allows a diverse set of instruments while keeping a short sequence length.
Our proposed Multitrack Music Transformer (MMT) achieves comparable performance
with state-of-the-art systems, landing in between two recently proposed models
in a subjective listening test, while achieving substantial speedups and memory
reductions over both, making the method attractive for real time improvisation
or near real time creative applications. Further, we propose a new measure for
analyzing musical self-attention and show that the trained model attends more
to notes that form a consonant interval with the current note and to notes that
are 4N beats away from the current step.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2023. Audio samples available at
  https://salu133445.github.io/mmt/ . Source code available at
  https://github.com/salu133445/mmt</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Watch or Listen: Robust Audio-Visual Speech Recognition with Visual
  Corruption Modeling and Reliability Scoring <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08536v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08536v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joanna Hong, Minsu Kim, Jeongsoo Choi, Yong Man Ro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper deals with Audio-Visual Speech Recognition (AVSR) under multimodal
input corruption situations where audio inputs and visual inputs are both
corrupted, which is not well addressed in previous research directions.
Previous studies have focused on how to complement the corrupted audio inputs
with the clean visual inputs with the assumption of the availability of clean
visual inputs. However, in real life, clean visual inputs are not always
accessible and can even be corrupted by occluded lip regions or noises. Thus,
we firstly analyze that the previous AVSR models are not indeed robust to the
corruption of multimodal input streams, the audio and the visual inputs,
compared to uni-modal models. Then, we design multimodal input corruption
modeling to develop robust AVSR models. Lastly, we propose a novel AVSR
framework, namely Audio-Visual Reliability Scoring module (AV-RelScore), that
is robust to the corrupted multimodal inputs. The AV-RelScore can determine
which input modal stream is reliable or not for the prediction and also can
exploit the more reliable streams in prediction. The effectiveness of the
proposed method is evaluated with comprehensive experiments on popular
benchmark databases, LRS2 and LRS3. We also show that the reliability scores
obtained by AV-RelScore well reflect the degree of corruption and make the
proposed model focus on the reliable multimodal representations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2023. Implementation available:
  https://github.com/joannahong/AV-RelScore</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Semantic2Graph: Graph-based Multi-modal Feature Fusion for Action
  Segmentation in Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.05653v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.05653v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junbin Zhang, Pei-Hsuan Tsai, Meng-Hsun Tsai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video action segmentation and recognition tasks have been widely applied in
many fields. Most previous studies employ large-scale, high computational
visual models to understand videos comprehensively. However, few studies
directly employ the graph model to reason about the video. The graph model
provides the benefits of fewer parameters, low computational cost, a large
receptive field, and flexible neighborhood message aggregation. In this paper,
we present a graph-based method named Semantic2Graph, to turn the video action
segmentation and recognition problem into node classification of graphs. To
preserve fine-grained relations in videos, we construct the graph structure of
videos at the frame-level and design three types of edges: temporal, semantic,
and self-loop. We combine visual, structural, and semantic features as node
attributes. Semantic edges are used to model long-term spatio-temporal
relations, while the semantic features are the embedding of the label-text
based on the textual prompt. A Graph Neural Networks (GNNs) model is used to
learn multi-modal feature fusion. Experimental results show that Semantic2Graph
achieves improvement on GTEA and 50Salads, compared to the state-of-the-art
results. Multiple ablation experiments further confirm the effectiveness of
semantic features in improving model performance, and semantic edges enable
Semantic2Graph to capture long-term dependencies at a low cost.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 3 figures, 9 tables. This paper was submitted to Springer</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-03-19T00:00:00Z">2023-03-19</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">17</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PheME: A deep ensemble framework for improving phenotype prediction from
  multi-modal data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10794v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10794v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shenghan Zhang, Haoxuan Li, Ruixiang Tang, Sirui Ding, Laila Rasmy, Degui Zhi, Na Zou, Xia Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detailed phenotype information is fundamental to accurate diagnosis and risk
estimation of diseases. As a rich source of phenotype information, electronic
health records (EHRs) promise to empower diagnostic variant interpretation.
However, how to accurately and efficiently extract phenotypes from the
heterogeneous EHR data remains a challenge. In this work, we present PheME, an
Ensemble framework using Multi-modality data of structured EHRs and
unstructured clinical notes for accurate Phenotype prediction. Firstly, we
employ multiple deep neural networks to learn reliable representations from the
sparse structured EHR data and redundant clinical notes. A multi-modal model
then aligns multi-modal features onto the same latent space to predict
phenotypes. Secondly, we leverage ensemble learning to combine outputs from
single-modal models and multi-modal models to improve phenotype predictions. We
choose seven diseases to evaluate the phenotyping performance of the proposed
framework. Experimental results show that using multi-modal data significantly
improves phenotype prediction in all diseases, the proposed ensemble learning
framework can further boost the performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Importance of Signer Overlap for Sign Language Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10782v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10782v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhilash Pal, Stephan Huber, Cyrine Chaabani, Alessandro Manzotti, Oscar Koller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sign language detection, identifying if someone is signing or not, is
becoming crucially important for its applications in remote conferencing
software and for selecting useful sign data for training sign language
recognition or translation tasks. We argue that the current benchmark data sets
for sign language detection estimate overly positive results that do not
generalize well due to signer overlap between train and test partitions. We
quantify this with a detailed analysis of the effect of signer overlap on
current sign detection benchmark data sets. Comparing accuracy with and without
overlap on the DGS corpus and Signing in the Wild, we observed a relative
decrease in accuracy of 4.17% and 6.27%, respectively. Furthermore, we propose
new data set partitions that are free of overlap and allow for more realistic
performance assessment. We hope this work will contribute to improving the
accuracy and generalization of sign language detection systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FVQA 2.0: Introducing Adversarial Samples into Fact-based Visual
  Question Answering <span class="chip">EACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10699v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10699v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weizhe Lin, Zhilin Wang, Bill Byrne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The widely used Fact-based Visual Question Answering (FVQA) dataset contains
visually-grounded questions that require information retrieval using common
sense knowledge graphs to answer. It has been observed that the original
dataset is highly imbalanced and concentrated on a small portion of its
associated knowledge graph. We introduce FVQA 2.0 which contains adversarial
variants of test questions to address this imbalance. We show that systems
trained with the original FVQA train sets can be vulnerable to adversarial
samples and we demonstrate an augmentation scheme to reduce this vulnerability
without human annotations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EACL 2023 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ COVID-19 event extraction from Twitter via extractive question answering
  with continuous <span class="highlight-title">prompt</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10659v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10659v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhang Jiang, Ramakanth Kavuluru
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As COVID-19 ravages the world, social media analytics could augment
traditional surveys in assessing how the pandemic evolves and capturing
consumer chatter that could help healthcare agencies in addressing it. This
typically involves mining disclosure events that mention testing positive for
the disease or discussions surrounding perceptions and beliefs in preventative
or treatment options. The 2020 shared task on COVID-19 event extraction
(conducted as part of the W-NUT workshop during the EMNLP conference)
introduced a new Twitter dataset for benchmarking event extraction from
COVID-19 tweets. In this paper, we cast the problem of event extraction as
extractive question answering using recent advances in continuous prompting in
language models. On the shared task test dataset, our approach leads to over 5%
absolute micro-averaged F1-score improvement over prior best results, across
all COVID-19 event slots. Our ablation study shows that continuous prompts have
a major impact on the eventual performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to appear in MEDINFO 2023. Dataset:
  https://github.com/viczong/extract_COVID19_events_from_Twitter; Code:
  https://github.com/bionlproc/twitter-covid-QA-extraction</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bangla Grammatical Error Detection Using T5 <span class="highlight-title">Transformer</span> Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10612v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10612v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        H. A. Z. Sameen Shahgir, Khondker Salman Sayeed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a method for detecting grammatical errors in Bangla using
a Text-to-Text Transfer Transformer (T5) Language Model, using the small
variant of BanglaT5, fine-tuned on a corpus of 9385 sentences where errors were
bracketed by the dedicated demarcation symbol. The T5 model was primarily
designed for translation and is not specifically designed for this task, so
extensive post-processing was necessary to adapt it to the task of error
detection. Our experiments show that the T5 model can achieve low Levenshtein
Distance in detecting grammatical errors in Bangla, but post-processing is
essential to achieve optimal performance. The final average Levenshtein
Distance after post-processing the output of the fine-tuned model was 1.0394 on
a test set of 5000 sentences. This paper also presents a detailed analysis of
the errors detected by the model and discusses the challenges of adapting a
translation model for grammar. Our approach can be extended to other languages,
demonstrating the potential of T5 models for detecting grammatical errors in a
wide range of languages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CTRAN: CNN-<span class="highlight-title">Transformer</span>-based Network for Natural Language Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10606v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10606v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehrdad Rafiepour, Javad Salimi Sartakhti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intent-detection and slot-filling are the two main tasks in natural language
understanding. In this study, we propose CTRAN, a novel encoder-decoder
CNN-Transformer-based architecture for intent-detection and slot-filling. In
the encoder, we use BERT, followed by several convolutional layers, and
rearrange the output using window feature sequence. We use stacked Transformer
encoders after the window feature sequence. For the intent-detection decoder,
we utilize self-attention followed by a linear layer. In the slot-filling
decoder, we introduce the aligned Transformer decoder, which utilizes a zero
diagonal mask, aligning output tags with input tokens. We apply our network on
ATIS and SNIPS, and surpass the current state-of-the-art in slot-filling on
both datasets. Furthermore, we incorporate the language model as word
embeddings, and show that this strategy yields a better result when compared to
the language model as an encoder.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Toward Artificial Empathy for Human-Centered Design: A Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10583v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10583v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qihao Zhu, Jianxi Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the early stages of the design process, designers explore opportunities by
discovering unmet needs and developing innovative concepts as potential
solutions. From a human-centered design perspective, designers must develop
empathy with people to truly understand their needs. However, developing
empathy is a complex and subjective process that relies heavily on the
designer's empathetic capability. Therefore, the development of empathetic
understanding is intuitive, and the discovery of underlying needs is often
serendipitous. This paper aims to provide insights from artificial intelligence
research to indicate the future direction of AI-driven human-centered design,
taking into account the essential role of empathy. Specifically, we conduct an
interdisciplinary investigation of research areas such as data-driven user
studies, empathetic understanding development, and artificial empathy. Based on
this foundation, we discuss the role that artificial empathy can play in
human-centered design and propose an artificial empathy framework for
human-centered design. Building on the mechanisms behind empathy and insights
from empathetic design research, the framework aims to break down the rather
complex and subjective concept of empathy into components and modules that can
potentially be modeled computationally. Furthermore, we discuss the expected
benefits of developing such systems and identify current research gaps to
encourage future research efforts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IDETC2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Extracting Incidents, Effects, and Requested Advice from MeToo Posts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10573v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10573v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vaibhav Garg, Jiaqing Yuan, Rujie Xi, Munindar P. Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Survivors of sexual harassment frequently share their experiences on social
media, revealing their feelings and emotions and seeking advice. We observed
that on Reddit, survivors regularly share long posts that describe a
combination of (i) a sexual harassment incident, (ii) its effect on the
survivor, including their feelings and emotions, and (iii) the advice being
sought. We term such posts MeToo posts, even though they may not be so tagged
and may appear in diverse subreddits. A prospective helper (such as a counselor
or even a casual reader) must understand a survivor's needs from such posts.
But long posts can be time-consuming to read and respond to.
  Accordingly, we address the problem of extracting key information from a long
MeToo post. We develop a natural language-based model to identify sentences
from a post that describe any of the above three categories.
  On ten-fold cross-validation of a dataset, our model achieves a macro F1
score of 0.82.
  In addition, we contribute MeThree, a dataset comprising 8,947 labeled
sentences extracted from Reddit posts. We apply the LIWC-22 toolkit on MeThree
to understand how different language patterns in sentences of the three
categories can reveal differences in emotional tone, authenticity, and other
aspects.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How People Respond to the COVID-19 Pandemic on Twitter: A Comparative
  Analysis of Emotional Expressions from US and India 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10560v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10560v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brandon Siyuan Loh, Raj Kumar Gupta, Ajay Vishwanath, Andrew Ortony, Yinping Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The COVID-19 pandemic has claimed millions of lives worldwide and elicited
heightened emotions. This study examines the expression of various emotions
pertaining to COVID-19 in the United States and India as manifested in over 54
million tweets, covering the fifteen-month period from February 2020 through
April 2021, a period which includes the beginnings of the huge and disastrous
increase in COVID-19 cases that started to ravage India in March 2021.
Employing pre-trained emotion analysis and topic modeling algorithms, four
distinct types of emotions (fear, anger, happiness, and sadness) and their
time- and location-associated variations were examined. Results revealed
significant country differences and temporal changes in the relative
proportions of fear, anger, and happiness, with fear declining and anger and
happiness fluctuating in 2020 until new situations over the first four months
of 2021 reversed the trends. Detected differences are discussed briefly in
terms of the latent topics revealed and through the lens of appraisal theories
of emotions, and the implications of the findings are discussed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 3 figures, 1 table, 2 appendices</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Two Kinds of Recall 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10527v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10527v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yoav Goldberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is an established assumption that pattern-based models are good at
precision, while learning based models are better at recall. But is that really
the case? I argue that there are two kinds of recall: d-recall, reflecting
diversity, and e-recall, reflecting exhaustiveness. I demonstrate through
experiments that while neural methods are indeed significantly better at
d-recall, it is sometimes the case that pattern-based methods are still
substantially better at e-recall. Ideal methods should aim for both kinds, and
this ideal should in turn be reflected in our evaluations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.04088v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.04088v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chan Hee Song, Jiaman Wu, Clayton Washington, Brian M. Sadler, Wei-Lun Chao, Yu Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study focuses on using large language models (LLMs) as a planner for
embodied agents that can follow natural language instructions to complete
complex tasks in a visually-perceived environment. The high data cost and poor
sample efficiency of existing methods hinders the development of versatile
agents that are capable of many tasks and can learn new tasks quickly. In this
work, we propose a novel method, LLM-Planner, that harnesses the power of large
language models to do few-shot planning for embodied agents. We further propose
a simple but effective way to enhance LLMs with physical grounding to generate
and update plans that are grounded in the current environment. Experiments on
the ALFRED dataset show that our method can achieve very competitive few-shot
performance: Despite using less than 0.5% of paired training data, LLM-Planner
achieves competitive performance with recent baselines that are trained using
the full training data. Existing methods can barely complete any task
successfully under the same few-shot setting. Our work opens the door for
developing versatile and sample-efficient embodied agents that can quickly
learn many tasks. Website: https://dki-lab.github.io/LLM-Planner
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MTEB: Massive Text Embedding Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.07316v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.07316v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niklas Muennighoff, Nouamane Tazi, Loïc Magne, Nils Reimers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text embeddings are commonly evaluated on a small set of datasets from a
single task not covering their possible applications to other tasks. It is
unclear whether state-of-the-art embeddings on semantic textual similarity
(STS) can be equally well applied to other tasks like clustering or reranking.
This makes progress in the field difficult to track, as various models are
constantly being proposed without proper evaluation. To solve this problem, we
introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding
tasks covering a total of 58 datasets and 112 languages. Through the
benchmarking of 33 models on MTEB, we establish the most comprehensive
benchmark of text embeddings to date. We find that no particular text embedding
method dominates across all tasks. This suggests that the field has yet to
converge on a universal text embedding method and scale it up sufficiently to
provide state-of-the-art results on all embedding tasks. MTEB comes with
open-source code and a public leaderboard at
https://github.com/embeddings-benchmark/mteb.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 14 tables, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning from Noisy Crowd Labels with Logics <span class="chip">ICDE-2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.06337v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.06337v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhijun Chen, Hailong Sun, Haoqian He, Pengpeng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores the integration of symbolic logic knowledge into deep
neural networks for learning from noisy crowd labels. We introduce Logic-guided
Learning from Noisy Crowd Labels (Logic-LNCL), an EM-alike iterative logic
knowledge distillation framework that learns from both noisy labeled data and
logic rules of interest. Unlike traditional EM methods, our framework contains
a ``pseudo-E-step'' that distills from the logic rules a new type of learning
target, which is then used in the ``pseudo-M-step'' for training the
classifier. Extensive evaluations on two real-world datasets for text sentiment
classification and named entity recognition demonstrate that the proposed
framework improves the state-of-the-art and provides a new solution to learning
from noisy crowd labels.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 7 figures, accepted by ICDE-2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MN-DS: A Multilabeled News <span class="highlight-title">Dataset</span> for News Articles Hierarchical
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.12061v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.12061v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alina Petukhova, Nuno Fachada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article presents a dataset of 10,917 news articles with hierarchical
news categories collected between January 1st 2019, and December 31st 2019. We
manually labelled the articles based on a hierarchical taxonomy with 17
first-level and 109 second-level categories. This dataset can be used to train
machine learning models for automatically classifying news articles by topic.
This dataset can be helpful for researchers working on news structuring,
classification, and predicting future events based on released news.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Is Chat<span class="highlight-title">GPT</span> A Good Translator? Yes With <span class="highlight-title">GPT</span>-4 As The Engine 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08745v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08745v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Xing Wang, Zhaopeng Tu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This report provides a preliminary evaluation of ChatGPT for machine
translation, including translation prompt, multilingual translation, and
translation robustness. We adopt the prompts advised by ChatGPT to trigger its
translation ability and find that the candidate prompts generally work well and
show minor performance differences. By evaluating on a number of benchmark test
sets, we find that ChatGPT performs competitively with commercial translation
products (e.g., Google Translate) on high-resource European languages but lags
behind significantly on low-resource or distant languages. For distant
languages, we explore an interesting strategy named $\mathbf{pivot~prompting}$
that asks ChatGPT to translate the source sentence into a high-resource pivot
language before into the target language, which improves the translation
performance significantly. As for the translation robustness, ChatGPT does not
perform as well as the commercial systems on biomedical abstracts or Reddit
comments but exhibits good results on spoken language. With the launch of the
GPT-4 engine, the translation performance of ChatGPT is significantly boosted,
becoming comparable to commercial translation products, even for distant
languages. In other words,
$\mathbf{ChatGPT~has~already~become~a~good~translator!}$ Scripts and data:
https://github.com/wxjiao/Is-ChatGPT-A-Good-Translator
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages; added GPT-3 data statistics reference; added GPT-4 results</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Construction and Applications of Billion-Scale <span class="highlight-title">Pre-Train</span>ed Multimodal
  Business Knowledge Graph <span class="chip">ICDE 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.15214v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.15214v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shumin Deng, Chengming Wang, Zhoubo Li, Ningyu Zhang, Zelin Dai, Hehong Chen, Feiyu Xiong, Ming Yan, Qiang Chen, Mosha Chen, Jiaoyan Chen, Jeff Z. Pan, Bryan Hooi, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Business Knowledge Graphs (KGs) are important to many enterprises today,
providing factual knowledge and structured data that steer many products and
make them more intelligent. Despite their promising benefits, building business
KG necessitates solving prohibitive issues of deficient structure and multiple
modalities. In this paper, we advance the understanding of the practical
challenges related to building KG in non-trivial real-world systems. We
introduce the process of building an open business knowledge graph (OpenBG)
derived from a well-known enterprise, Alibaba Group. Specifically, we define a
core ontology to cover various abstract products and consumption demands, with
fine-grained taxonomy and multimodal facts in deployed applications. OpenBG is
an open business KG of unprecedented scale: 2.6 billion triples with more than
88 million entities covering over 1 million core classes/concepts and 2,681
types of relations. We release all the open resources (OpenBG benchmarks)
derived from it for the community and report experimental results of KG-centric
tasks. We also run up an online competition based on OpenBG benchmarks, and has
attracted thousands of teams. We further pre-train OpenBG and apply it to many
KG- enhanced downstream tasks in business scenarios, demonstrating the
effectiveness of billion-scale multimodal knowledge for e-commerce. All the
resources with codes have been released at
\url{https://github.com/OpenBGBenchmark/OpenBG}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>OpenBG. Accepted by ICDE 2023. The project is released at
  https://github.com/OpenBGBenchmark/OpenBG . Website: https://kg.alibaba.com/
  , Leaderboard: https://tianchi.aliyun.com/dataset/dataDetail?dataId=122271</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PACIFIC: Towards Proactive Conversational Question Answering over
  Tabular and Textual Data in Finance <span class="chip">EMNLP 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.08817v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.08817v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Deng, Wenqiang Lei, Wenxuan Zhang, Wai Lam, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To facilitate conversational question answering (CQA) over hybrid contexts in
finance, we present a new dataset, named PACIFIC. Compared with existing CQA
datasets, PACIFIC exhibits three key features: (i) proactivity, (ii) numerical
reasoning, and (iii) hybrid context of tables and text. A new task is defined
accordingly to study Proactive Conversational Question Answering (PCQA), which
combines clarification question generation and CQA. In addition, we propose a
novel method, namely UniPCQA, to adapt a hybrid format of input and output
content in PCQA into the Seq2Seq problem, including the reformulation of the
numerical reasoning process as code generation. UniPCQA performs multi-task
learning over all sub-tasks in PCQA and incorporates a simple ensemble strategy
to alleviate the error propagation issue in the multi-task learning by
cross-validating top-$k$ sampled Seq2Seq outputs. We benchmark the PACIFIC
dataset with extensive baselines and provide comprehensive evaluations on each
sub-task of PCQA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EMNLP 2022 (main conference)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">5</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Textless Speech-to-Music Retrieval Using Emotion Similarity <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10539v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10539v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        SeungHeon Doh, Minz Won, Keunwoo Choi, Juhan Nam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a framework that recommends music based on the emotions of
speech. In content creation and daily life, speech contains information about
human emotions, which can be enhanced by music. Our framework focuses on a
cross-domain retrieval system to bridge the gap between speech and music via
emotion labels. We explore different speech representations and report their
impact on different speech types, including acting voice and wake-up words. We
also propose an emotion similarity regularization term in cross-domain
retrieval tasks. By incorporating the regularization term into training,
similar speech-and-music pairs in the emotion space are closer in the joint
embedding space. Our comprehensive experimental results show that the proposed
model is effective in textless speech-to-music retrieval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To Appear IEEE ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Discovery and Recognition of Formula Concepts using Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.01994v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.01994v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philipp Scharpf, Moritz Schubotz, Howard S. Cohl, Corinna Breitinger, Bela Gipp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Citation-based Information Retrieval (IR) methods for scientific documents
have proven effective for IR applications, such as Plagiarism Detection or
Literature Recommender Systems in academic disciplines that use many
references. In science, technology, engineering, and mathematics, researchers
often employ mathematical concepts through formula notation to refer to prior
knowledge. Our long-term goal is to generalize citation-based IR methods and
apply this generalized method to both classical references and mathematical
concepts. In this paper, we suggest how mathematical formulas could be cited
and define a Formula Concept Retrieval task with two subtasks: Formula Concept
Discovery (FCD) and Formula Concept Recognition (FCR). While FCD aims at the
definition and exploration of a 'Formula Concept' that names bundled equivalent
representations of a formula, FCR is designed to match a given formula to a
prior assigned unique mathematical concept identifier. We present machine
learning-based approaches to address the FCD and FCR tasks. We then evaluate
these approaches on a standardized test collection (NTCIR arXiv dataset). Our
FCD approach yields a precision of 68% for retrieving equivalent
representations of frequent formulas and a recall of 72% for extracting the
formula name from the surrounding text. FCD and FCR enable the citation of
formulas within mathematical documents and facilitate semantic search and
question answering as well as document similarity assessments for plagiarism
detection or recommender systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Scientometrics (Springer) journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MTEB: Massive Text Embedding Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.07316v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.07316v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niklas Muennighoff, Nouamane Tazi, Loïc Magne, Nils Reimers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text embeddings are commonly evaluated on a small set of datasets from a
single task not covering their possible applications to other tasks. It is
unclear whether state-of-the-art embeddings on semantic textual similarity
(STS) can be equally well applied to other tasks like clustering or reranking.
This makes progress in the field difficult to track, as various models are
constantly being proposed without proper evaluation. To solve this problem, we
introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding
tasks covering a total of 58 datasets and 112 languages. Through the
benchmarking of 33 models on MTEB, we establish the most comprehensive
benchmark of text embeddings to date. We find that no particular text embedding
method dominates across all tasks. This suggests that the field has yet to
converge on a universal text embedding method and scale it up sufficiently to
provide state-of-the-art results on all embedding tasks. MTEB comes with
open-source code and a public leaderboard at
https://github.com/embeddings-benchmark/mteb.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 14 tables, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Construction and Applications of Billion-Scale <span class="highlight-title">Pre-Train</span>ed Multimodal
  Business Knowledge Graph <span class="chip">ICDE 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.15214v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.15214v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shumin Deng, Chengming Wang, Zhoubo Li, Ningyu Zhang, Zelin Dai, Hehong Chen, Feiyu Xiong, Ming Yan, Qiang Chen, Mosha Chen, Jiaoyan Chen, Jeff Z. Pan, Bryan Hooi, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Business Knowledge Graphs (KGs) are important to many enterprises today,
providing factual knowledge and structured data that steer many products and
make them more intelligent. Despite their promising benefits, building business
KG necessitates solving prohibitive issues of deficient structure and multiple
modalities. In this paper, we advance the understanding of the practical
challenges related to building KG in non-trivial real-world systems. We
introduce the process of building an open business knowledge graph (OpenBG)
derived from a well-known enterprise, Alibaba Group. Specifically, we define a
core ontology to cover various abstract products and consumption demands, with
fine-grained taxonomy and multimodal facts in deployed applications. OpenBG is
an open business KG of unprecedented scale: 2.6 billion triples with more than
88 million entities covering over 1 million core classes/concepts and 2,681
types of relations. We release all the open resources (OpenBG benchmarks)
derived from it for the community and report experimental results of KG-centric
tasks. We also run up an online competition based on OpenBG benchmarks, and has
attracted thousands of teams. We further pre-train OpenBG and apply it to many
KG- enhanced downstream tasks in business scenarios, demonstrating the
effectiveness of billion-scale multimodal knowledge for e-commerce. All the
resources with codes have been released at
\url{https://github.com/OpenBGBenchmark/OpenBG}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>OpenBG. Accepted by ICDE 2023. The project is released at
  https://github.com/OpenBGBenchmark/OpenBG . Website: https://kg.alibaba.com/
  , Leaderboard: https://tianchi.aliyun.com/dataset/dataDetail?dataId=122271</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Comprehensive Privacy Analysis on Federated Recommender System against
  Attribute Inference Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.11857v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.11857v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shijie Zhang, Wei Yuan, Hongzhi Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, recommender systems are crucially important for the delivery
of personalized services that satisfy users' preferences. With personalized
recommendation services, users can enjoy a variety of recommendations such as
movies, books, ads, restaurants, and more. Despite the great benefits,
personalized recommendations typically require the collection of personal data
for user modelling and analysis, which can make users susceptible to attribute
inference attacks. Specifically, the vulnerability of existing centralized
recommenders under attribute inference attacks leaves malicious attackers a
backdoor to infer users' private attributes, as the systems remember
information of their training data (i.e., interaction data and side
information). An emerging practice is to implement recommender systems in the
federated setting, which enables all user devices to collaboratively learn a
shared global recommender while keeping all the training data on device.
However, the privacy issues in federated recommender systems have been rarely
explored. In this paper, we first design a novel attribute inference attacker
to perform a comprehensive privacy analysis of the state-of-the-art federated
recommender models. The experimental results show that the vulnerability of
each model component against attribute inference attack is varied, highlighting
the need for new defense approaches. Therefore, we propose a novel adaptive
privacy-preserving approach to protect users' sensitive data in the presence of
attribute inference attacks and meanwhile maximize the recommendation accuracy.
Extensive experimental results on two real-world datasets validate the superior
performance of our model on both recommendation effectiveness and resistance to
inference attacks.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">49</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PheME: A deep ensemble framework for improving phenotype prediction from
  multi-modal data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10794v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10794v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shenghan Zhang, Haoxuan Li, Ruixiang Tang, Sirui Ding, Laila Rasmy, Degui Zhi, Na Zou, Xia Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detailed phenotype information is fundamental to accurate diagnosis and risk
estimation of diseases. As a rich source of phenotype information, electronic
health records (EHRs) promise to empower diagnostic variant interpretation.
However, how to accurately and efficiently extract phenotypes from the
heterogeneous EHR data remains a challenge. In this work, we present PheME, an
Ensemble framework using Multi-modality data of structured EHRs and
unstructured clinical notes for accurate Phenotype prediction. Firstly, we
employ multiple deep neural networks to learn reliable representations from the
sparse structured EHR data and redundant clinical notes. A multi-modal model
then aligns multi-modal features onto the same latent space to predict
phenotypes. Secondly, we leverage ensemble learning to combine outputs from
single-modal models and multi-modal models to improve phenotype predictions. We
choose seven diseases to evaluate the phenotyping performance of the proposed
framework. Experimental results show that using multi-modal data significantly
improves phenotype prediction in all diseases, the proposed ensemble learning
framework can further boost the performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A hybrid CNN-RNN approach for survival analysis in a Lung Cancer
  Screening study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10789v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10789v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaozhi Lu, Shahab Aslani, An Zhao, Ahmed Shahin, David Barber, Mark Emberton, Daniel C. Alexander, Joseph Jacob
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we present a hybrid CNN-RNN approach to investigate long-term
survival of subjects in a lung cancer screening study. Subjects who died of
cardiovascular and respiratory causes were identified whereby the CNN model was
used to capture imaging features in the CT scans and the RNN model was used to
investigate time series and thus global information. The models were trained on
subjects who underwent cardiovascular and respiratory deaths and a control
cohort matched to participant age, gender, and smoking history. The combined
model can achieve an AUC of 0.76 which outperforms humans at cardiovascular
mortality prediction. The corresponding F1 and Matthews Correlation Coefficient
are 0.63 and 0.42 respectively. The generalisability of the model is further
validated on an 'external' cohort. The same models were applied to survival
analysis with the Cox Proportional Hazard model. It was demonstrated that
incorporating the follow-up history can lead to improvement in survival
prediction. The Cox neural network can achieve an IPCW C-index of 0.75 on the
internal dataset and 0.69 on an external dataset. Delineating imaging features
associated with long-term survival can help focus preventative interventions
appropriately, particularly for under-recognised pathologies thereby
potentially reducing patient morbidity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comprehensive <span class="highlight-title">Review</span> of Spiking Neural Networks: Interpretation,
  Optimization, Efficiency, and Best Practices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10780v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10780v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Malcom, Josue Casco-Rodriguez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Biological neural networks continue to inspire breakthroughs in neural
network performance. And yet, one key area of neural computation that has been
under-appreciated and under-investigated is biologically plausible,
energy-efficient spiking neural networks, whose potential is especially
attractive for low-power, mobile, or otherwise hardware-constrained settings.
We present a literature review of recent developments in the interpretation,
optimization, efficiency, and accuracy of spiking neural networks. Key
contributions include identification, discussion, and comparison of
cutting-edge methods in spiking neural network optimization, energy-efficiency,
and evaluation, starting from first principles so as to be accessible to new
practitioners.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Declarative Dynamic Time Warping for End-to-End Learning of
  Alignment Paths <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10778v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10778v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ming Xu, Sourav Garg, Michael Milford, Stephen Gould
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses learning end-to-end models for time series data that
include a temporal alignment step via dynamic time warping (DTW). Existing
approaches to differentiable DTW either differentiate through a fixed warping
path or apply a differentiable relaxation to the min operator found in the
recursive steps used to solve the DTW problem. We instead propose a DTW layer
based around bi-level optimisation and deep declarative networks, which we name
DecDTW. By formulating DTW as a continuous, inequality constrained optimisation
problem, we can compute gradients for the solution of the optimal alignment
(with respect to the underlying time series) using implicit differentiation. An
interesting byproduct of this formulation is that DecDTW outputs the optimal
warping path between two time series as opposed to a soft approximation,
recoverable from Soft-DTW. We show that this property is particularly useful
for applications where downstream loss functions are defined on the optimal
alignment path itself. This naturally occurs, for instance, when learning to
improve the accuracy of predicted alignments against ground truth alignments.
We evaluate DecDTW on two such applications, namely the audio-to-score
alignment task in music information retrieval and the visual place recognition
task in robotics, demonstrating state-of-the-art results in both.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2023 (Poster)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cross-GAN Auditing: Unsupervised Identification of Attribute Level
  Similarities and Differences between <span class="highlight-title">Pretrain</span>ed Generative Models <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10774v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10774v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew L. Olson, Shusen Liu, Rushil Anirudh, Jayaraman J. Thiagarajan, Peer-Timo Bremer, Weng-Keen Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative Adversarial Networks (GANs) are notoriously difficult to train
especially for complex distributions and with limited data. This has driven the
need for tools to audit trained networks in human intelligible format, for
example, to identify biases or ensure fairness. Existing GAN audit tools are
restricted to coarse-grained, model-data comparisons based on summary
statistics such as FID or recall. In this paper, we propose an alternative
approach that compares a newly developed GAN against a prior baseline. To this
end, we introduce Cross-GAN Auditing (xGA) that, given an established
"reference" GAN and a newly proposed "client" GAN, jointly identifies
intelligible attributes that are either common across both GANs, novel to the
client GAN, or missing from the client GAN. This provides both users and model
developers an intuitive assessment of similarity and differences between GANs.
We introduce novel metrics to evaluate attribute-based GAN auditing approaches
and use these metrics to demonstrate quantitatively that xGA outperforms
baseline approaches. We also include qualitative results that illustrate the
common, novel and missing attributes identified by xGA from GANs trained on a
variety of image datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023. Source code is available at
  https://github.com/mattolson93/cross_gan_auditing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Calibration of Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10761v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10761v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruslan Vasilev, Alexander D'yakonov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural networks solving real-world problems are often required not only to
make accurate predictions but also to provide a confidence level in the
forecast. The calibration of a model indicates how close the estimated
confidence is to the true probability. This paper presents a survey of
confidence calibration problems in the context of neural networks and provides
an empirical comparison of calibration methods. We analyze problem statement,
calibration definitions, and different approaches to evaluation: visualizations
and scalar measures that estimate whether the model is well-calibrated. We
review modern calibration techniques: based on post-processing or requiring
changes in training. Empirical experiments cover various datasets and models,
comparing calibration methods according to different criteria.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lower Generalization Bounds for GD and SGD in Smooth Stochastic Convex
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10758v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10758v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peiyuan Zhang, Jiaye Teng, Jingzhao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent progress was made in characterizing the generalization error of
gradient methods for general convex loss by the learning theory community. In
this work, we focus on how training longer might affect generalization in
smooth stochastic convex optimization (SCO) problems. We first provide tight
lower bounds for general non-realizable SCO problems. Furthermore, existing
upper bound results suggest that sample complexity can be improved by assuming
the loss is realizable, i.e. an optimal solution simultaneously minimizes all
the data points. However, this improvement is compromised when training time is
long and lower bounds are lacking. Our paper examines this observation by
providing excess risk lower bounds for gradient descent (GD) and stochastic
gradient descent (SGD) in two realizable settings: 1) realizable with $T =
O(n)$, and (2) realizable with $T = \Omega(n)$, where $T$ denotes the number of
training iterations and $n$ is the size of the training dataset. These bounds
are novel and informative in characterizing the relationship between $T$ and
$n$. In the first small training horizon case, our lower bounds almost tightly
match and provide the first optimal certificates for the corresponding upper
bounds. However, for the realizable case with $T = \Omega(n)$, a gap exists
between the lower and upper bounds. We provide a conjecture to address this
problem, that the gap can be closed by improving upper bounds, which is
supported by our analyses in one-dimensional and linear regression scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multiscale Audio Spectrogram <span class="highlight-title">Transformer</span> for Efficient Audio
  Classification <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10757v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10757v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wentao Zhu, Mohamed Omar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio event has a hierarchical architecture in both time and frequency and
can be grouped together to construct more abstract semantic audio classes. In
this work, we develop a multiscale audio spectrogram Transformer (MAST) that
employs hierarchical representation learning for efficient audio
classification. Specifically, MAST employs one-dimensional (and
two-dimensional) pooling operators along the time (and frequency domains) in
different stages, and progressively reduces the number of tokens and increases
the feature dimensions. MAST significantly outperforms AST~\cite{gong2021ast}
by 22.2\%, 4.4\% and 4.7\% on Kinetics-Sounds, Epic-Kitchens-100 and VGGSound
in terms of the top-1 accuracy without external training data. On the
downloaded AudioSet dataset, which has over 20\% missing audios, MAST also
achieves slightly better accuracy than AST. In addition, MAST is 5x more
efficient in terms of multiply-accumulates (MACs) with 42\% reduction in the
number of parameters compared to AST. Through clustering metrics and
visualizations, we demonstrate that the proposed MAST can learn semantically
more separable feature representations from audio signals.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Computer Vision Estimation of Emotion Reaction Intensity in the Wild 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10741v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10741v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Qian, Ali Kargarandehkordi, Onur Cezmi Mutlu, Saimourya Surabhi, Mohammadmahdi Honarmand, Dennis Paul Wall, Peter Washington
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emotions play an essential role in human communication. Developing computer
vision models for automatic recognition of emotion expression can aid in a
variety of domains, including robotics, digital behavioral healthcare, and
media analytics. There are three types of emotional representations which are
traditionally modeled in affective computing research: Action Units, Valence
Arousal (VA), and Categorical Emotions. As part of an effort to move beyond
these representations towards more fine-grained labels, we describe our
submission to the newly introduced Emotional Reaction Intensity (ERI)
Estimation challenge in the 5th competition for Affective Behavior Analysis
in-the-Wild (ABAW). We developed four deep neural networks trained in the
visual domain and a multimodal model trained with both visual and audio
features to predict emotion reaction intensity. Our best performing model on
the Hume-Reaction dataset achieved an average Pearson correlation coefficient
of 0.4080 on the test set using a pre-trained ResNet50 model. This work
provides a first step towards the development of production-grade models which
predict emotion reaction intensities rather than discrete emotion categories.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MIA-3DCNN: COVID-19 Detection Based on a 3D CNN 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10738v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10738v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Igor Kenzo Ishikawa Oshiro Nakashima, Giovanna Vendramini, Helio Pedrini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Early and accurate diagnosis of COVID-19 is essential to control the rapid
spread of the pandemic and mitigate sequelae in the population. Current
diagnostic methods, such as RT-PCR, are effective but require time to provide
results and can quickly overwhelm clinics, requiring individual laboratory
analysis. Automatic detection methods have the potential to significantly
reduce diagnostic time. To this end, learning-based methods using lung imaging
have been explored. Although they require specialized hardware, automatic
evaluation methods can be performed simultaneously, making diagnosis faster.
Convolutional neural networks have been widely used to detect pneumonia caused
by COVID-19 in lung images. This work describes an architecture based on 3D
convolutional neural networks for detecting COVID-19 in computed tomography
images. Despite the challenging scenario present in the dataset, the results
obtained with our architecture demonstrated to be quite promising.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AutoEn: An AutoML method based on ensembles of predefined Machine
  Learning pipelines for supervised Traffic Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10732v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10732v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juan S. Angarita-Zapata, Antonio D. Masegosa, Isaac Triguero
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intelligent Transportation Systems are producing tons of hardly manageable
traffic data, which motivates the use of Machine Learning (ML) for data-driven
applications, such as Traffic Forecasting (TF). TF is gaining relevance due to
its ability to mitigate traffic congestion by forecasting future traffic
states. However, TF poses one big challenge to the ML paradigm, known as the
Model Selection Problem (MSP): deciding the most suitable combination of data
preprocessing techniques and ML method for traffic data collected under
different transportation circumstances. In this context, Automated Machine
Learning (AutoML), the automation of the ML workflow from data preprocessing to
model validation, arises as a promising strategy to deal with the MSP in
problem domains wherein expert ML knowledge is not always an available or
affordable asset, such as TF. Various AutoML frameworks have been used to
approach the MSP in TF. Most are based on online optimisation processes to
search for the best-performing pipeline on a given dataset. This online
optimisation could be complemented with meta-learning to warm-start the search
phase and/or the construction of ensembles using pipelines derived from the
optimisation process. However, given the complexity of the search space and the
high computational cost of tuning-evaluating pipelines generated, online
optimisation is only beneficial when there is a long time to obtain the final
model. Thus, we introduce AutoEn, which is a simple and efficient method for
automatically generating multi-classifier ensembles from a predefined set of ML
pipelines. We compare AutoEn against Auto-WEKA and Auto-sklearn, two AutoML
methods commonly used in TF. Experimental results demonstrate that AutoEn can
lead to better or more competitive results in the general-purpose domain and in
TF.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Pretrain</span>ed Vision Models for Predicting High-Risk Breast Cancer Stage <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10730v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10730v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bonaventure F. P. Dossou, Yenoukoume S. K. Gbenou, Miglanche Ghomsi Nono
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cancer is increasingly a global health issue. Seconding cardiovascular
diseases, cancers are the second biggest cause of death in the world with
millions of people succumbing to the disease every year. According to the World
Health Organization (WHO) report, by the end of 2020, more than 7.8 million
women have been diagnosed with breast cancer, making it the world's most
prevalent cancer. In this paper, using the Nightingale Open Science dataset of
digital pathology (breast biopsy) images, we leverage the capabilities of
pre-trained computer vision models for the breast cancer stage prediction task.
While individual models achieve decent performances, we find out that the
predictions of an ensemble model are more efficient, and offer a winning
solution\footnote{https://www.nightingalescience.org/updates/hbc1-results}. We
also provide analyses of the results and explore pathways for better
interpretability and generalization. Our code is open-source at
\url{https://github.com/bonaventuredossou/nightingale_winning_solution}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Machine Learning for Global Health Workshop, ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Training Deep Boltzmann Networks with Sparse Ising Machines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10728v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10728v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaila Niazi, Navid Anjum Aadit, Masoud Mohseni, Shuvro Chowdhury, Yao Qin, Kerem Y. Camsari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The slowing down of Moore's law has driven the development of unconventional
computing paradigms, such as specialized Ising machines tailored to solve
combinatorial optimization problems. In this paper, we show a new application
domain for probabilistic bit (p-bit) based Ising machines by training deep
generative AI models with them. Using sparse, asynchronous, and massively
parallel Ising machines we train deep Boltzmann networks in a hybrid
probabilistic-classical computing setup. We use the full MNIST dataset without
any downsampling or reduction in hardware-aware network topologies implemented
in moderately sized Field Programmable Gate Arrays (FPGA). Our machine, which
uses only 4,264 nodes (p-bits) and about 30,000 parameters, achieves the same
classification accuracy (90%) as an optimized software-based restricted
Boltzmann Machine (RBM) with approximately 3.25 million parameters.
Additionally, the sparse deep Boltzmann network can generate new handwritten
digits, a task the 3.25 million parameter RBM fails at despite achieving the
same accuracy. Our hybrid computer takes a measured 50 to 64 billion
probabilistic flips per second, which is at least an order of magnitude faster
than superficially similar Graphics and Tensor Processing Unit (GPU/TPU) based
implementations. The massively parallel architecture can comfortably perform
the contrastive divergence algorithm (CD-n) with up to n = 10 million sweeps
per update, beyond the capabilities of existing software implementations. These
results demonstrate the potential of using Ising machines for traditionally
hard-to-train deep generative Boltzmann networks, with further possible
improvement in nanodevice-based realizations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ERSAM: Neural Architecture Search For Energy-Efficient and Real-Time
  Social Ambiance Measurement <span class="chip">ICASSP'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10727v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10727v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaojian Li, Wenwan Chen, Jiayi Yuan,  Yingyan,  Lin, Ashutosh Sabharwal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Social ambiance describes the context in which social interactions happen,
and can be measured using speech audio by counting the number of concurrent
speakers. This measurement has enabled various mental health tracking and
human-centric IoT applications. While on-device Socal Ambiance Measure (SAM) is
highly desirable to ensure user privacy and thus facilitate wide adoption of
the aforementioned applications, the required computational complexity of
state-of-the-art deep neural networks (DNNs) powered SAM solutions stands at
odds with the often constrained resources on mobile devices. Furthermore, only
limited labeled data is available or practical when it comes to SAM under
clinical settings due to various privacy constraints and the required human
effort, further challenging the achievable accuracy of on-device SAM solutions.
To this end, we propose a dedicated neural architecture search framework for
Energy-efficient and Real-time SAM (ERSAM). Specifically, our ERSAM framework
can automatically search for DNNs that push forward the achievable accuracy vs.
hardware efficiency frontier of mobile SAM solutions. For example,
ERSAM-delivered DNNs only consume 40 mW x 12 h energy and 0.05 seconds
processing latency for a 5 seconds audio segment on a Pixel 3 phone, while only
achieving an error rate of 14.3% on a social ambiance dataset generated by
LibriSpeech. We can expect that our ERSAM framework can pave the way for
ubiquitous on-device SAM solutions which are in growing demand.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP'23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SIESTA: Efficient Online Continual Learning with Sleep 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10725v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10725v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Yousuf Harun, Jhair Gallardo, Tyler L. Hayes, Ronald Kemker, Christopher Kanan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In supervised continual learning, a deep neural network (DNN) is updated with
an ever-growing data stream. Unlike the offline setting where data is shuffled,
we cannot make any distributional assumptions about the data stream. Ideally,
only one pass through the dataset is needed for computational efficiency.
However, existing methods are inadequate and make many assumptions that cannot
be made for real-world applications, while simultaneously failing to improve
computational efficiency. In this paper, we do not propose a novel method.
Instead, we present SIESTA, an incremental improvement to the continual
learning algorithm REMIND. Unlike REMIND, SIESTA uses a wake/sleep framework
for training, which is well aligned to the needs of on-device learning. SIESTA
is far more computationally efficient than existing methods, enabling continual
learning on ImageNet-1K in under 3 hours on a single GPU; moreover, in the
augmentation-free setting it matches the performance of the offline learner, a
milestone critical to driving adoption of continual learning in real-world
applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Q-RBSA: High-Resolution 3D EBSD Map Generation Using An Efficient
  Quaternion <span class="highlight-title">Transformer</span> Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10722v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10722v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Devendra K. Jangid, Neal R. Brodnik, McLean P. Echlin, Tresa M. Pollock, Samantha H. Daly, B. S. Manjunath
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gathering 3D material microstructural information is time-consuming,
expensive, and energy-intensive. Acquisition of 3D data has been accelerated by
developments in serial sectioning instrument capabilities; however, for
crystallographic information, the electron backscatter diffraction (EBSD)
imaging modality remains rate limiting. We propose a physics-based efficient
deep learning framework to reduce the time and cost of collecting 3D EBSD maps.
Our framework uses a quaternion residual block self-attention network (QRBSA)
to generate high-resolution 3D EBSD maps from sparsely sectioned EBSD maps. In
QRBSA, quaternion-valued convolution effectively learns local relations in
orientation space, while self-attention in the quaternion domain captures
long-range correlations. We apply our framework to 3D data collected from
commercially relevant titanium alloys, showing both qualitatively and
quantitatively that our method can predict missing samples (EBSD information
between sparsely sectioned mapping points) as compared to high-resolution
ground truth 3D EBSD maps.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Trainable Projected Gradient Method for Robust Fine-tuning <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10720v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10720v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjiao Tian, Xiaoliang Dai, Chih-Yao Ma, Zecheng He, Yen-Cheng Liu, Zsolt Kira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies on transfer learning have shown that selectively fine-tuning a
subset of layers or customizing different learning rates for each layer can
greatly improve robustness to out-of-distribution (OOD) data and retain
generalization capability in the pre-trained models. However, most of these
methods employ manually crafted heuristics or expensive hyper-parameter
searches, which prevent them from scaling up to large datasets and neural
networks. To solve this problem, we propose Trainable Projected Gradient Method
(TPGM) to automatically learn the constraint imposed for each layer for a
fine-grained fine-tuning regularization. This is motivated by formulating
fine-tuning as a bi-level constrained optimization problem. Specifically, TPGM
maintains a set of projection radii, i.e., distance constraints between the
fine-tuned model and the pre-trained model, for each layer, and enforces them
through weight projections. To learn the constraints, we propose a bi-level
optimization to automatically learn the best set of projection radii in an
end-to-end manner. Theoretically, we show that the bi-level optimization
formulation is the key to learning different constraints for each layer.
Empirically, with little hyper-parameter search cost, TPGM outperforms existing
fine-tuning methods in OOD performance while matching the best in-distribution
(ID) performance. For example, when fine-tuned on DomainNet-Real and ImageNet,
compared to vanilla fine-tuning, TPGM shows $22\%$ and $10\%$ relative OOD
improvement respectively on their sketch counterparts. Code is available at
\url{https://github.com/PotatoTian/TPGM}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluation of Convolution Primitives for Embedded Neural Networks on
  32-bit Microcontrollers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10702v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10702v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baptiste Nguyen, Pierre-Alain Moellic, Sylvain Blayac
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deploying neural networks on constrained hardware platforms such as 32-bit
microcontrollers is a challenging task because of the large memory, computing
and energy requirements of their inference process. To tackle these issues,
several convolution primitives have been proposed to make the standard
convolution more computationally efficient. However, few of these primitives
are really implemented for 32-bit microcontrollers. In this work, we collect
different state-of-the-art convolutional primitives and propose an
implementation for ARM Cortex-M processor family with an open source deployment
platform (NNoM). Then, we carry out experimental characterization tests on
these implementations. Our benchmark reveals a linear relationship between
theoretical MACs and energy consumption. Thus showing the advantages of using
computationally efficient primitives like shift convolution. We discuss about
the significant reduction in latency and energy consumption due to the use of
SIMD instructions and highlight the importance of data reuse in those
performance gains. For reproducibility purpose and further experiments, codes
and experiments are publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ISDA 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Convergence of Decentralized Federated Learning Under Imperfect
  Information Sharing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10695v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10695v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vishnu Pandi Chellapandi, Antesh Upadhyay, Abolfazl Hashemi, Stanislaw H /. Zak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decentralized learning and optimization is a central problem in control that
encompasses several existing and emerging applications, such as federated
learning. While there exists a vast literature on this topic and most methods
centered around the celebrated average-consensus paradigm, less attention has
been devoted to scenarios where the communication between the agents may be
imperfect. To this end, this paper presents three different algorithms of
Decentralized Federated Learning (DFL) in the presence of imperfect information
sharing modeled as noisy communication channels. The first algorithm, Federated
Noisy Decentralized Learning (FedNDL1), comes from the literature, where the
noise is added to their parameters to simulate the scenario of the presence of
noisy communication channels. This algorithm shares parameters to form a
consensus with the clients based on a communication graph topology through a
noisy communication channel. The proposed second algorithm (FedNDL2) is similar
to the first algorithm but with added noise to the parameters, and it performs
the gossip averaging before the gradient optimization. The proposed third
algorithm
  (FedNDL3), on the other hand, shares the gradients through noisy
communication channels instead of the parameters. Theoretical and experimental
results demonstrate that under imperfect information sharing, the third scheme
that mixes gradients is more robust in the presence of a noisy channel compared
with the algorithms from the literature that mix the parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Uncertainty Quantification of Deep Classifiers via
  Neighborhood Conformal Prediction: Novel Algorithm and Theoretical Analysis <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10694v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10694v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Subhankar Ghosh, Taha Belkhouja, Yan Yan, Janardhan Rao Doppa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Safe deployment of deep neural networks in high-stake real-world applications
requires theoretically sound uncertainty quantification. Conformal prediction
(CP) is a principled framework for uncertainty quantification of deep models in
the form of prediction set for classification tasks with a user-specified
coverage (i.e., true class label is contained with high probability). This
paper proposes a novel algorithm referred to as Neighborhood Conformal
Prediction (NCP) to improve the efficiency of uncertainty quantification from
CP for deep classifiers (i.e., reduce prediction set size). The key idea behind
NCP is to use the learned representation of the neural network to identify k
nearest-neighbors calibration examples for a given testing input and assign
them importance weights proportional to their distance to create adaptive
prediction sets. We theoretically show that if the learned data representation
of the neural network satisfies some mild conditions, NCP will produce smaller
prediction sets than traditional CP algorithms. Our comprehensive experiments
on CIFAR-10, CIFAR-100, and ImageNet datasets using diverse deep neural
networks strongly demonstrate that NCP leads to significant reduction in
prediction set size over prior CP methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in the Proceedings of the AAAI Conference on Artificial
  Intelligence, 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative Adversarial Classification Network with Application to
  Network Traffic Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10681v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10681v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rozhina Ghanavi, Ben Liang, Ali Tizghadam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large datasets in machine learning often contain missing data, which
necessitates the imputation of missing data values. In this work, we are
motivated by network traffic classification, where traditional data imputation
methods do not perform well. We recognize that no existing method directly
accounts for classification accuracy during data imputation. Therefore, we
propose a joint data imputation and data classification method, termed
generative adversarial classification network (GACN), whose architecture
contains a generator network, a discriminator network, and a classification
network, which are iteratively optimized toward the ultimate objective of
classification accuracy. For the scenario where some data samples are
unlabeled, we further propose an extension termed semi-supervised GACN
(SSGACN), which is able to use the partially labeled data to improve
classification accuracy. We conduct experiments with real-world network traffic
data traces, which demonstrate that GACN and SS-GACN can more accurately impute
data features that are more important for classification, and they outperform
existing methods in terms of classification accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 4 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> of Federated Learning for Connected and Automated Vehicles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10677v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10677v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vishnu Pandi Chellapandi, Liangqi Yuan, Stanislaw H /. Zak, Ziran Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Connected and Automated Vehicles (CAVs) are one of the emerging technologies
in the automotive domain that has the potential to alleviate the issues of
accidents, traffic congestion, and pollutant emissions, leading to a safe,
efficient, and sustainable transportation system. Machine learning-based
methods are widely used in CAVs for crucial tasks like perception, motion
planning, and motion control, where machine learning models in CAVs are solely
trained using the local vehicle data, and the performance is not certain when
exposed to new environments or unseen conditions. Federated learning (FL) is an
effective solution for CAVs that enables a collaborative model development with
multiple vehicles in a distributed learning framework. FL enables CAVs to learn
from a wide range of driving environments and improve their overall performance
while ensuring the privacy and security of local vehicle data. In this paper,
we review the progress accomplished by researchers in applying FL to CAVs. A
broader view of the various data modalities and algorithms that have been
implemented on CAVs is provided. Specific applications of FL are reviewed in
detail, and an analysis of the challenges and future scope of research are
presented.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ URM4DMU: an user represention model for darknet markets users 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10674v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10674v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongmeng Liu, Jiapeng Zhao, Yixuan Huo, Yuyan Wang, Chun Liao, Liyan Shen, Shiyao Cui, Jinqiao Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Darknet markets provide a large platform for trading illicit goods and
services due to their anonymity. Learning an invariant representation of each
user based on their posts on different markets makes it easy to aggregate user
information across different platforms, which helps identify anonymous users.
Traditional user representation methods mainly rely on modeling the text
information of posts and cannot capture the temporal content and the forum
interaction of posts. While recent works mainly use CNN to model the text
information of posts, failing to effectively model posts whose length changes
frequently in an episode. To address the above problems, we propose a model
named URM4DMU(User Representation Model for Darknet Markets Users) which mainly
improves the post representation by augmenting convolutional operators and
self-attention with an adaptive gate mechanism. It performs much better when
combined with the temporal content and the forum interaction of posts. We
demonstrate the effectiveness of URM4DMU on four darknet markets. The average
improvements on MRR value and Recall@10 are 22.5% and 25.5% over the
state-of-the-art method respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Agent Reinforcement Learning via Mean Field Control: Common Noise,
  Major Agents and Approximation Properties 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10665v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10665v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Cui, Christian Fabian, Heinz Koeppl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, mean field control (MFC) has provided a tractable and theoretically
founded approach to otherwise difficult cooperative multi-agent control.
However, the strict assumption of many independent, homogeneous agents may be
too stringent in practice. In this work, we propose a novel discrete-time
generalization of Markov decision processes and MFC to both many minor agents
and potentially complex major agents -- major-minor mean field control (M3FC).
In contrast to deterministic MFC, M3FC allows for stochastic minor agent
distributions with strong correlation between minor agents through the major
agent state, which can model arbitrary problem details not bound to any agent.
Theoretically, we give rigorous approximation properties with novel proofs for
both M3FC and existing MFC models in the finite multi-agent problem, together
with a dynamic programming principle for solving such problems. In the
infinite-horizon discounted case, existence of an optimal stationary policy
follows. Algorithmically, we propose the major-minor mean field proximal policy
optimization algorithm (M3FPPO) as a novel multi-agent reinforcement learning
algorithm and demonstrate its success in illustrative M3FC-type problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Randomized Adversarial Training via Taylor Expansion <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10653v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10653v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gaojie Jin, Xinping Yi, Dengyu Wu, Ronghui Mu, Xiaowei Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, there has been an explosion of research into developing more
robust deep neural networks against adversarial examples. Adversarial training
appears as one of the most successful methods. To deal with both the robustness
against adversarial examples and the accuracy over clean examples, many works
develop enhanced adversarial training methods to achieve various trade-offs
between them. Leveraging over the studies that smoothed update on weights
during training may help find flat minima and improve generalization, we
suggest reconciling the robustness-accuracy trade-off from another perspective,
i.e., by adding random noise into deterministic weights. The randomized weights
enable our design of a novel adversarial training method via Taylor expansion
of a small Gaussian noise, and we show that the new adversarial training method
can flatten loss landscape and find flat minima. With PGD, CW, and Auto
Attacks, an extensive set of experiments demonstrate that our method enhances
the state-of-the-art adversarial training methods, boosting both robustness and
clean accuracy. The code is available at
https://github.com/Alexkael/Randomized-Adversarial-Training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Logic of Differentiable Logics: Towards a Uniform Semantics of DL 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10650v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10650v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Natalia Ślusarz, Ekaterina Komendantskaya, Matthew L. Daggitt, Robert Stewart, Kathrin Stark
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Differentiable logics (DL) have recently been proposed as a method of
training neural networks to satisfy logical specifications. A DL consists of a
syntax in which specifications are stated and an interpretation function that
translates expressions in the syntax into loss functions. These loss functions
can then be used during training with standard gradient descent algorithms. The
variety of existing DLs and the differing levels of formality with which they
are treated makes a systematic comparative study of their properties and
implementations difficult. This paper remedies this problem by suggesting a
meta-language for defining DLs that we call the Logic of Differentiable Logics,
or LDL. Syntactically, it generalises the syntax of existing DLs to FOL, and
for the first time introduces the formalism for reasoning about vectors and
learners. Semantically, it introduces a general interpretation function that
can be instantiated to define loss functions arising from different existing
DLs. We use LDL to establish several theoretical properties of existing DLs,
and to conduct their empirical study in neural network verification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Experimenting with Normalization Layers in Federated Learning on non-IID
  scenarios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10630v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10630v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bruno Casella, Roberto Esposito, Antonio Sciarappa, Carlo Cavazzoni, Marco Aldinucci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training Deep Learning (DL) models require large, high-quality datasets,
often assembled with data from different institutions. Federated Learning (FL)
has been emerging as a method for privacy-preserving pooling of datasets
employing collaborative training from different institutions by iteratively
globally aggregating locally trained models. One critical performance challenge
of FL is operating on datasets not independently and identically distributed
(non-IID) among the federation participants. Even though this fragility cannot
be eliminated, it can be debunked by a suitable optimization of two
hyper-parameters: layer normalization methods and collaboration frequency
selection. In this work, we benchmark five different normalization layers for
training Neural Networks (NNs), two families of non-IID data skew, and two
datasets. Results show that Batch Normalization, widely employed for
centralized DL, is not the best choice for FL, whereas Group and Layer
Normalization consistently outperform Batch Normalization. Similarly, frequent
model aggregation decreases convergence speed and mode quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, Submitted to IEEE Transactions on Neural Networks and
  Learning Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PFSL: Personalized & Fair Split Learning with Data & Label Privacy for
  thin clients 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10624v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10624v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manas Wadhwa, Gagan Raj Gupta, Ashutosh Sahu, Rahul Saini, Vidhi Mittal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The traditional framework of federated learning (FL) requires each client to
re-train their models in every iteration, making it infeasible for
resource-constrained mobile devices to train deep-learning (DL) models. Split
learning (SL) provides an alternative by using a centralized server to offload
the computation of activations and gradients for a subset of the model but
suffers from problems of slow convergence and lower accuracy. In this paper, we
implement PFSL, a new framework of distributed split learning where a large
number of thin clients perform transfer learning in parallel, starting with a
pre-trained DL model without sharing their data or labels with a central
server. We implement a lightweight step of personalization of client models to
provide high performance for their respective data distributions. Furthermore,
we evaluate performance fairness amongst clients under a work fairness
constraint for various scenarios of non-i.i.d. data distributions and unequal
sample sizes. Our accuracy far exceeds that of current SL algorithms and is
very close to that of centralized learning on several real-life benchmarks. It
has a very low computation cost compared to FL variants and promises to deliver
the full benefits of DL to extremely thin, resource-constrained clients.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in : THE 23RD IEEE/ACM INTERNATIONAL SYMPOSIUM ON
  Cluster, Cloud and Internet Computing. Granted: Open Research Objects (ORO)
  and Research Objects Reviewed (ROR) badges. See
  https://www.niso.org/publications/rp-31-2021-badging for definitions of the
  badges. Code available at: https://github.com/mnswdhw/PFSL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fully Bayesian inference for latent variable Gaussian process models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.02218v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.02218v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suraj Yerramilli, Akshay Iyer, Wei Chen, Daniel W. Apley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real engineering and scientific applications often involve one or more
qualitative inputs. Standard Gaussian processes (GPs), however, cannot directly
accommodate qualitative inputs. The recently introduced latent variable
Gaussian process (LVGP) overcomes this issue by first mapping each qualitative
factor to underlying latent variables (LVs), and then uses any standard GP
covariance function over these LVs. The LVs are estimated similarly to the
other GP hyperparameters through maximum likelihood estimation, and then
plugged into the prediction expressions. However, this plug-in approach will
not account for uncertainty in estimation of the LVs, which can be significant
especially with limited training data. In this work, we develop a fully
Bayesian approach for the LVGP model and for visualizing the effects of the
qualitative inputs via their LVs. We also develop approximations for scaling up
LVGPs and fully Bayesian inference for the LVGP hyperparameters. We conduct
numerical studies comparing plug-in inference against fully Bayesian inference
over a few engineering models and material design applications. In contrast to
previous studies on standard GP modeling that have largely concluded that a
fully Bayesian treatment offers limited improvements, our results show that for
LVGP modeling it offers significant improvements in prediction accuracy and
uncertainty quantification over the plug-in approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Theoretical Understanding of Shallow Vision <span class="highlight-title">Transformer</span>s: Learning,
  Generalization, and Sample Complexity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.06015v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.06015v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongkang Li, Meng Wang, Sijia Liu, Pin-yu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Transformers (ViTs) with self-attention modules have recently achieved
great empirical success in many vision tasks. Due to non-convex interactions
across layers, however, theoretical learning and generalization analysis is
mostly elusive. Based on a data model characterizing both label-relevant and
label-irrelevant tokens, this paper provides the first theoretical analysis of
training a shallow ViT, i.e., one self-attention layer followed by a two-layer
perceptron, for a classification task. We characterize the sample complexity to
achieve a zero generalization error. Our sample complexity bound is positively
correlated with the inverse of the fraction of label-relevant tokens, the token
noise level, and the initial model error. We also prove that a training process
using stochastic gradient descent (SGD) leads to a sparse attention map, which
is a formal verification of the general intuition about the success of
attention. Moreover, this paper indicates that a proper token sparsification
can improve the test performance by removing label-irrelevant and/or noisy
tokens, including spurious correlations. Empirical experiments on synthetic
data and CIFAR-10 dataset justify our theoretical results and generalize to
deeper ViTs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Seq-HyGAN: Sequence Classification via Hypergraph Attention Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02393v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02393v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khaled Mohammed Saifuddin, Corey May, Farhan Tanvir, Muhammad Ifte Khairul Islam, Esra Akbas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequence classification has a wide range of real-world applications in
different domains, such as genome classification in health and anomaly
detection in business. However, the lack of explicit features in sequence data
makes it difficult for machine learning models. While Neural Network (NN)
models address this with learning features automatically, they are limited to
capturing adjacent structural connections and ignore global, higher-order
information between the sequences. To address these challenges in the sequence
classification problems, we propose a novel Hypergraph Attention Network model,
namely Seq-HyGAN. To capture the complex structural similarity between sequence
data, we first create a hypergraph where the sequences are depicted as
hyperedges and subsequences extracted from sequences are depicted as nodes.
Additionally, we introduce an attention-based Hypergraph Neural Network model
that utilizes a two-level attention mechanism. This model generates a sequence
representation as a hyperedge while simultaneously learning the crucial
subsequences for each sequence. We conduct extensive experiments on four data
sets to assess and compare our model with several state-of-the-art methods.
Experimental results demonstrate that our proposed Seq-HyGAN model can
effectively classify sequence data and significantly outperform the baselines.
We also conduct case studies to investigate the contribution of each module in
Seq-HyGAN.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Forecasting Cryptocurrency Returns from Sentiment Signals: An Analysis
  of <span class="highlight-title">BERT</span> Classifiers and Weak Supervision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.05781v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.05781v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Duygu Ider, Stefan Lessmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anticipating price developments in financial markets is a topic of continued
interest in forecasting. Funneled by advancements in deep learning and natural
language processing (NLP) together with the availability of vast amounts of
textual data in form of news articles, social media postings, etc., an
increasing number of studies incorporate text-based predictors in forecasting
models. We contribute to this literature by introducing weak learning, a
recently proposed NLP approach to address the problem that text data is
unlabeled. Without a dependent variable, it is not possible to finetune
pretrained NLP models on a custom corpus. We confirm that finetuning using weak
labels enhances the predictive value of text-based features and raises forecast
accuracy in the context of predicting cryptocurrency returns. More
fundamentally, the modeling paradigm we present, weak labeling domain-specific
text and finetuning pretrained NLP models, is universally applicable in
(financial) forecasting and unlocks new ways to leverage text data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.04088v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.04088v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chan Hee Song, Jiaman Wu, Clayton Washington, Brian M. Sadler, Wei-Lun Chao, Yu Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study focuses on using large language models (LLMs) as a planner for
embodied agents that can follow natural language instructions to complete
complex tasks in a visually-perceived environment. The high data cost and poor
sample efficiency of existing methods hinders the development of versatile
agents that are capable of many tasks and can learn new tasks quickly. In this
work, we propose a novel method, LLM-Planner, that harnesses the power of large
language models to do few-shot planning for embodied agents. We further propose
a simple but effective way to enhance LLMs with physical grounding to generate
and update plans that are grounded in the current environment. Experiments on
the ALFRED dataset show that our method can achieve very competitive few-shot
performance: Despite using less than 0.5% of paired training data, LLM-Planner
achieves competitive performance with recent baselines that are trained using
the full training data. Existing methods can barely complete any task
successfully under the same few-shot setting. Our work opens the door for
developing versatile and sample-efficient embodied agents that can quickly
learn many tasks. Website: https://dki-lab.github.io/LLM-Planner
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning an Adaptive Forwarding Strategy for Mobile Wireless Networks:
  Resource Usage vs. Latency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.11386v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.11386v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Victoria Manfredi, Alicia P. Wolfe, Xiaolan Zhang, Bing Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Designing effective routing strategies for mobile wireless networks is
challenging due to the need to seamlessly adapt routing behavior to spatially
diverse and temporally changing network conditions. In this work, we use deep
reinforcement learning (DeepRL) to learn a scalable and generalizable
single-copy routing strategy for such networks. We make the following
contributions: i) we design a reward function that enables the DeepRL agent to
explicitly trade-off competing network goals, such as minimizing delay vs. the
number of transmissions per packet; ii) we propose a novel set of relational
neighborhood, path, and context features to characterize mobile wireless
networks and model device mobility independently of a specific network
topology; and iii) we use a flexible training approach that allows us to
combine data from all packets and devices into a single offline centralized
training set to train a single DeepRL agent. To evaluate generalizeability and
scalability, we train our DeepRL agent on one mobile network scenario and then
test it on other mobile scenarios, varying the number of devices and
transmission ranges. Our results show our learned single-copy routing strategy
outperforms all other strategies in terms of delay except for the optimal
strategy, even on scenarios on which the DeepRL agent was not trained.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>39 pages, 9 figures. Added new features as input to DRL model, added
  new mobility model to evaluation, updated related work, and updated
  simulations</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Discovery and Recognition of Formula Concepts using Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.01994v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.01994v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philipp Scharpf, Moritz Schubotz, Howard S. Cohl, Corinna Breitinger, Bela Gipp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Citation-based Information Retrieval (IR) methods for scientific documents
have proven effective for IR applications, such as Plagiarism Detection or
Literature Recommender Systems in academic disciplines that use many
references. In science, technology, engineering, and mathematics, researchers
often employ mathematical concepts through formula notation to refer to prior
knowledge. Our long-term goal is to generalize citation-based IR methods and
apply this generalized method to both classical references and mathematical
concepts. In this paper, we suggest how mathematical formulas could be cited
and define a Formula Concept Retrieval task with two subtasks: Formula Concept
Discovery (FCD) and Formula Concept Recognition (FCR). While FCD aims at the
definition and exploration of a 'Formula Concept' that names bundled equivalent
representations of a formula, FCR is designed to match a given formula to a
prior assigned unique mathematical concept identifier. We present machine
learning-based approaches to address the FCD and FCR tasks. We then evaluate
these approaches on a standardized test collection (NTCIR arXiv dataset). Our
FCD approach yields a precision of 68% for retrieving equivalent
representations of frequent formulas and a recall of 72% for extracting the
formula name from the surrounding text. FCD and FCR enable the citation of
formulas within mathematical documents and facilitate semantic search and
question answering as well as document similarity assessments for plagiarism
detection or recommender systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Scientometrics (Springer) journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Principal Component Analysis based frameworks for efficient missing data
  imputation algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.15150v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.15150v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thu Nguyen, Hoang Thien Ly, Michael Alexander Riegler, Pål Halvorsen, Hugo L. Hammer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Missing data is a commonly occurring problem in practice. Many imputation
methods have been developed to fill in the missing entries. However, not all of
them can scale to high-dimensional data, especially the multiple imputation
techniques. Meanwhile, the data nowadays tends toward high-dimensional.
Therefore, in this work, we propose Principal Component Analysis Imputation
(PCAI), a simple but versatile framework based on Principal Component Analysis
(PCA) to speed up the imputation process and alleviate memory issues of many
available imputation techniques, without sacrificing the imputation quality in
term of MSE. In addition, the frameworks can be used even when some or all of
the missing features are categorical, or when the number of missing features is
large. Next, we introduce PCA Imputation - Classification (PIC), an application
of PCAI for classification problems with some adjustments. We validate our
approach by experiments on various scenarios, which shows that PCAI and PIC can
work with various imputation algorithms, including the state-of-the-art ones
and improve the imputation speed significantly, while achieving competitive
mean square error/classification accuracy compared to direct imputation (i.e.,
impute directly on the missing data).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ L-SVRG and L-Katyusha with Adaptive Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.13387v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.13387v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boxin Zhao, Boxiang Lyu, Mladen Kolar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stochastic gradient-based optimization methods, such as L-SVRG and its
accelerated variant L-Katyusha (Kovalev et al., 2020), are widely used to train
machine learning models.The theoretical and empirical performance of L-SVRG and
L-Katyusha can be improved by sampling observations from a non-uniform
distribution (Qian et al., 2021). However,designing a desired sampling
distribution requires prior knowledge of smoothness constants, which can be
computationally intractable to obtain in practice when the dimension of the
model parameter is high. To address this issue, we propose an adaptive sampling
strategy for L-SVRG and L-Katyusha that can learn the sampling distribution
with little computational overhead, while allowing it to change with iterates,
and at the same time does not require any prior knowledge of the problem
parameters. We prove convergence guarantees for L-SVRG and L-Katyusha for
convex objectives when the sampling distribution changes with iterates. Our
results show that even without prior information, the proposed adaptive
sampling strategy matches, and in some cases even surpasses, the performance of
the sampling scheme in Qian et al. (2021). Extensive simulations support our
theory and the practical utility of the proposed sampling scheme on real data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Group conditional validity via multi-group learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03995v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03995v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Deng, Navid Ardeshir, Daniel Hsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of distribution-free conformal prediction and the
criterion of group conditional validity. This criterion is motivated by many
practical scenarios including hidden stratification and group fairness.
Existing methods achieve such guarantees under either restrictive grouping
structure or distributional assumptions, or they are overly-conservative under
heteroskedastic noise. We propose a simple reduction to the problem of
achieving validity guarantees for individual populations by leveraging
algorithms for a problem called multi-group learning. This allows us to port
theoretical guarantees from multi-group learning to obtain obtain sample
complexity guarantees for conformal prediction. We also provide a new algorithm
for multi-group learning for groups with hierarchical structure. Using this
algorithm in our reduction leads to improved sample complexity guarantees with
a simpler predictor structure.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Valid prediction intervals constructed by proposed method do not
  appear to be any shorter than those constructed by baseline methods</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bispectral Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.03416v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.03416v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sophia Sanborn, Christian Shewmake, Bruno Olshausen, Christopher Hillar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a neural network architecture, Bispectral Neural Networks (BNNs)
for learning representations that are invariant to the actions of compact
commutative groups on the space over which a signal is defined. The model
incorporates the ansatz of the bispectrum, an analytically defined group
invariant that is complete -- that is, it preserves all signal structure while
removing only the variation due to group actions. Here, we demonstrate that
BNNs are able to simultaneously learn groups, their irreducible
representations, and corresponding equivariant and complete-invariant maps
purely from the symmetries implicit in data. Further, we demonstrate that the
completeness property endows these networks with strong invariance-based
adversarial robustness. This work establishes Bispectral Neural Networks as a
powerful computational primitive for robust invariant representation learning
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SpaText: Spatio-Textual Representation for Controllable Image Generation <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.14305v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.14305v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Omri Avrahami, Thomas Hayes, Oran Gafni, Sonal Gupta, Yaniv Taigman, Devi Parikh, Dani Lischinski, Ohad Fried, Xi Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent text-to-image diffusion models are able to generate convincing results
of unprecedented quality. However, it is nearly impossible to control the
shapes of different regions/objects or their layout in a fine-grained fashion.
Previous attempts to provide such controls were hindered by their reliance on a
fixed set of labels. To this end, we present SpaText - a new method for
text-to-image generation using open-vocabulary scene control. In addition to a
global text prompt that describes the entire scene, the user provides a
segmentation map where each region of interest is annotated by a free-form
natural language description. Due to lack of large-scale datasets that have a
detailed textual description for each region in the image, we choose to
leverage the current large-scale text-to-image datasets and base our approach
on a novel CLIP-based spatio-textual representation, and show its effectiveness
on two state-of-the-art diffusion models: pixel-based and latent-based. In
addition, we show how to extend the classifier-free guidance method in
diffusion models to the multi-conditional case and present an alternative
accelerated inference algorithm. Finally, we offer several automatic evaluation
metrics and use them, in addition to FID scores and a user study, to evaluate
our method and show that it achieves state-of-the-art results on image
generation with free-form textual scene control.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023. Project page available at:
  https://omriavrahami.com/spatext</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Computably Continuous Reinforcement-Learning Objectives are
  PAC-learnable 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05518v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05518v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cambridge Yang, Michael Littman, Michael Carbin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In reinforcement learning, the classic objectives of maximizing discounted
and finite-horizon cumulative rewards are PAC-learnable: There are algorithms
that learn a near-optimal policy with high probability using a finite amount of
samples and computation. In recent years, researchers have introduced
objectives and corresponding reinforcement-learning algorithms beyond the
classic cumulative rewards, such as objectives specified as linear temporal
logic formulas. However, questions about the PAC-learnability of these new
objectives have remained open.
  This work demonstrates the PAC-learnability of general reinforcement-learning
objectives through sufficient conditions for PAC-learnability in two analysis
settings. In particular, for the analysis that considers only sample
complexity, we prove that if an objective given as an oracle is uniformly
continuous, then it is PAC-learnable. Further, for the analysis that considers
computational complexity, we prove that if an objective is computable, then it
is PAC-learnable. In other words, if a procedure computes successive
approximations of the objective's value, then the objective is PAC-learnable.
  We give three applications of our condition on objectives from the literature
with previously unknown PAC-learnability and prove that these objectives are
PAC-learnable. Overall, our result helps verify existing objectives'
PAC-learnability. Also, as some studied objectives that are not uniformly
continuous have been shown to be not PAC-learnable, our results could guide the
design of new PAC-learnable objectives.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hierarchical Training of Deep Neural Networks Using Early Exiting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02384v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02384v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yamin Sepehri, Pedram Pad, Ahmet Caner Yüzügüler, Pascal Frossard, L. Andrea Dunbar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks provide state-of-the-art accuracy for vision tasks but
they require significant resources for training. Thus, they are trained on
cloud servers far from the edge devices that acquire the data. This issue
increases communication cost, runtime and privacy concerns. In this study, a
novel hierarchical training method for deep neural networks is proposed that
uses early exits in a divided architecture between edge and cloud workers to
reduce the communication cost, training runtime and privacy concerns. The
method proposes a brand-new use case for early exits to separate the backward
pass of neural networks between the edge and the cloud during the training
phase. We address the issues of most available methods that due to the
sequential nature of the training phase, cannot train the levels of hierarchy
simultaneously or they do it with the cost of compromising privacy. In
contrast, our method can use both edge and cloud workers simultaneously, does
not share the raw input data with the cloud and does not require communication
during the backward pass. Several simulations and on-device experiments for
different neural network architectures demonstrate the effectiveness of this
method. It is shown that the proposed method reduces the training runtime by
29% and 61% in CIFAR-10 classification experiment for VGG-16 and ResNet-18 when
the communication with the cloud is done at a low bit rate channel. This gain
in the runtime is achieved whilst the accuracy drop is negligible. This method
is advantageous for online learning of high-accuracy deep neural networks on
low-resource devices such as mobile phones or robots as a part of an edge-cloud
system, making them more flexible in facing new tasks and classes of data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 9 figures, 1 Table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DBSCAN of Multi-Slice Clustering for Third-Order Tensors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07768v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07768v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dina Faneva Andriantsiory, Joseph Ben Geloun, Mustapha Lebbah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Several methods for triclustering three-dimensional data require the cluster
size or the number of clusters in each dimension to be specified. To address
this issue, the Multi-Slice Clustering (MSC) for 3-order tensor finds signal
slices that lie in a low dimensional subspace for a rank-one tensor dataset in
order to find a cluster based on the threshold similarity. We propose an
extension algorithm called MSC-DBSCAN to extract the different clusters of
slices that lie in the different subspaces from the data if the dataset is a
sum of r rank-one tensor (r > 1). Our algorithm uses the same input as the MSC
algorithm and can find the same solution for rank-one tensor data as MSC.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ KoopmanLab: machine learning for solving complex physics equations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.01104v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.01104v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Xiong, Muyuan Ma, Xiaomeng Huang, Ziyang Zhang, Pei Sun, Yang Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Numerous physics theories are rooted in partial differential equations
(PDEs). However, the increasingly intricate physics equations, especially those
that lack analytic solutions or closed forms, have impeded the further
development of physics. Computationally solving PDEs by classic numerical
approaches suffers from the trade-off between accuracy and efficiency and is
not applicable to the empirical data generated by unknown latent PDEs. To
overcome this challenge, we present KoopmanLab, an efficient module of the
Koopman neural operator family, for learning PDEs without analytic solutions or
closed forms. Our module consists of multiple variants of the Koopman neural
operator (KNO), a kind of mesh-independent neural-network-based PDE solvers
developed following dynamic system theory. The compact variants of KNO can
accurately solve PDEs with small model sizes while the large variants of KNO
are more competitive in predicting highly complicated dynamic systems govern by
unknown, high-dimensional, and non-linear PDEs. All variants are validated by
mesh-independent and long-term prediction experiments implemented on
representative PDEs (e.g., the Navier-Stokes equation and the Bateman-Burgers
equation in fluid mechanics) and ERA5 (i.e., one of the largest high-resolution
global-scale climate data sets in earth physics). These demonstrations suggest
the potential of KoopmanLab to be a fundamental tool in diverse physics studies
related to equations or dynamic systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MTEB: Massive Text Embedding Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.07316v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.07316v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niklas Muennighoff, Nouamane Tazi, Loïc Magne, Nils Reimers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text embeddings are commonly evaluated on a small set of datasets from a
single task not covering their possible applications to other tasks. It is
unclear whether state-of-the-art embeddings on semantic textual similarity
(STS) can be equally well applied to other tasks like clustering or reranking.
This makes progress in the field difficult to track, as various models are
constantly being proposed without proper evaluation. To solve this problem, we
introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding
tasks covering a total of 58 datasets and 112 languages. Through the
benchmarking of 33 models on MTEB, we establish the most comprehensive
benchmark of text embeddings to date. We find that no particular text embedding
method dominates across all tasks. This suggests that the field has yet to
converge on a universal text embedding method and scale it up sufficiently to
provide state-of-the-art results on all embedding tasks. MTEB comes with
open-source code and a public leaderboard at
https://github.com/embeddings-benchmark/mteb.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 14 tables, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning from Noisy Crowd Labels with Logics <span class="chip">ICDE-2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.06337v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.06337v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhijun Chen, Hailong Sun, Haoqian He, Pengpeng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores the integration of symbolic logic knowledge into deep
neural networks for learning from noisy crowd labels. We introduce Logic-guided
Learning from Noisy Crowd Labels (Logic-LNCL), an EM-alike iterative logic
knowledge distillation framework that learns from both noisy labeled data and
logic rules of interest. Unlike traditional EM methods, our framework contains
a ``pseudo-E-step'' that distills from the logic rules a new type of learning
target, which is then used in the ``pseudo-M-step'' for training the
classifier. Extensive evaluations on two real-world datasets for text sentiment
classification and named entity recognition demonstrate that the proposed
framework improves the state-of-the-art and provides a new solution to learning
from noisy crowd labels.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 7 figures, accepted by ICDE-2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Merak: An Efficient Distributed DNN Training Framework with Automated 3D
  Parallelism for Giant Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.04959v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.04959v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiquan Lai, Shengwei Li, Xudong Tang, Keshi Ge, Weijie Liu, Yabo Duan, Linbo Qiao, Dongsheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models are becoming the dominant deep learning technologies.
Pretraining a foundation model is always time-consumed due to the large scale
of both the model parameter and training dataset. Besides being
computing-intensive, the training process is extremely memory-intensive and
communication-intensive. These features make it necessary to apply 3D
parallelism, which integrates data parallelism, pipeline model parallelism and
tensor model parallelism, to achieve high training efficiency.
  To achieve this goal, some custom software frameworks such as Megatron-LM and
DeepSpeed are developed. However, current 3D parallelism frameworks still meet
two issues: i) they are not transparent to model developers, which need to
manually modify the model to parallelize training. ii) their utilization of
computation, GPU memory and network bandwidth are not sufficient. We propose
Merak, an automated 3D parallelism deep learning training framework with high
resource utilization. Merak automatically deploys with an automatic model
partitioner, which uses a graph sharding algorithm on a proxy representation of
the model. Merak also presents the non-intrusive API for scaling out foundation
model training with minimal code modification. In addition, we design a
high-performance 3D parallel runtime engine in Merak. It uses several
techniques to exploit available training resources, including shifted critical
path pipeline schedule that brings a higher computation utilization,
stage-aware recomputation that makes use of idle worker memory, and
sub-pipelined tensor model parallelism that overlaps communication and
computation. Experiments on 64 GPUs show Merak can speedup the training
performance over the state-of-the-art 3D parallelism frameworks of models with
1.5, 2.5, 8.3, and 20 billion parameters by up to 1.42X, 1.39X, 1.43X, and
1.61X, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MN-DS: A Multilabeled News <span class="highlight-title">Dataset</span> for News Articles Hierarchical
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.12061v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.12061v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alina Petukhova, Nuno Fachada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article presents a dataset of 10,917 news articles with hierarchical
news categories collected between January 1st 2019, and December 31st 2019. We
manually labelled the articles based on a hierarchical taxonomy with 17
first-level and 109 second-level categories. This dataset can be used to train
machine learning models for automatically classifying news articles by topic.
This dataset can be helpful for researchers working on news structuring,
classification, and predicting future events based on released news.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Construction and Applications of Billion-Scale <span class="highlight-title">Pre-Train</span>ed Multimodal
  Business Knowledge Graph <span class="chip">ICDE 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.15214v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.15214v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shumin Deng, Chengming Wang, Zhoubo Li, Ningyu Zhang, Zelin Dai, Hehong Chen, Feiyu Xiong, Ming Yan, Qiang Chen, Mosha Chen, Jiaoyan Chen, Jeff Z. Pan, Bryan Hooi, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Business Knowledge Graphs (KGs) are important to many enterprises today,
providing factual knowledge and structured data that steer many products and
make them more intelligent. Despite their promising benefits, building business
KG necessitates solving prohibitive issues of deficient structure and multiple
modalities. In this paper, we advance the understanding of the practical
challenges related to building KG in non-trivial real-world systems. We
introduce the process of building an open business knowledge graph (OpenBG)
derived from a well-known enterprise, Alibaba Group. Specifically, we define a
core ontology to cover various abstract products and consumption demands, with
fine-grained taxonomy and multimodal facts in deployed applications. OpenBG is
an open business KG of unprecedented scale: 2.6 billion triples with more than
88 million entities covering over 1 million core classes/concepts and 2,681
types of relations. We release all the open resources (OpenBG benchmarks)
derived from it for the community and report experimental results of KG-centric
tasks. We also run up an online competition based on OpenBG benchmarks, and has
attracted thousands of teams. We further pre-train OpenBG and apply it to many
KG- enhanced downstream tasks in business scenarios, demonstrating the
effectiveness of billion-scale multimodal knowledge for e-commerce. All the
resources with codes have been released at
\url{https://github.com/OpenBGBenchmark/OpenBG}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>OpenBG. Accepted by ICDE 2023. The project is released at
  https://github.com/OpenBGBenchmark/OpenBG . Website: https://kg.alibaba.com/
  , Leaderboard: https://tianchi.aliyun.com/dataset/dataDetail?dataId=122271</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PheME: A deep ensemble framework for improving phenotype prediction from
  multi-modal data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10794v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10794v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shenghan Zhang, Haoxuan Li, Ruixiang Tang, Sirui Ding, Laila Rasmy, Degui Zhi, Na Zou, Xia Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detailed phenotype information is fundamental to accurate diagnosis and risk
estimation of diseases. As a rich source of phenotype information, electronic
health records (EHRs) promise to empower diagnostic variant interpretation.
However, how to accurately and efficiently extract phenotypes from the
heterogeneous EHR data remains a challenge. In this work, we present PheME, an
Ensemble framework using Multi-modality data of structured EHRs and
unstructured clinical notes for accurate Phenotype prediction. Firstly, we
employ multiple deep neural networks to learn reliable representations from the
sparse structured EHR data and redundant clinical notes. A multi-modal model
then aligns multi-modal features onto the same latent space to predict
phenotypes. Secondly, we leverage ensemble learning to combine outputs from
single-modal models and multi-modal models to improve phenotype predictions. We
choose seven diseases to evaluate the phenotyping performance of the proposed
framework. Experimental results show that using multi-modal data significantly
improves phenotype prediction in all diseases, the proposed ensemble learning
framework can further boost the performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Textless Speech-to-Music Retrieval Using Emotion Similarity <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10539v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10539v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        SeungHeon Doh, Minz Won, Keunwoo Choi, Juhan Nam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a framework that recommends music based on the emotions of
speech. In content creation and daily life, speech contains information about
human emotions, which can be enhanced by music. Our framework focuses on a
cross-domain retrieval system to bridge the gap between speech and music via
emotion labels. We explore different speech representations and report their
impact on different speech types, including acting voice and wake-up words. We
also propose an emotion similarity regularization term in cross-domain
retrieval tasks. By incorporating the regularization term into training,
similar speech-and-music pairs in the emotion space are closer in the joint
embedding space. Our comprehensive experimental results show that the proposed
model is effective in textless speech-to-music retrieval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To Appear IEEE ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CapEnrich: Enriching Caption Semantics for Web Images via Cross-modal
  <span class="highlight-title">Pre-train</span>ed Knowledge <span class="chip">WWW2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.09371v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.09371v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linli Yao, Weijing Chen, Qin Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatically generating textual descriptions for massive unlabeled images on
the web can greatly benefit realistic web applications, e.g. multimodal
retrieval and recommendation. However, existing models suffer from the problem
of generating ``over-generic'' descriptions, such as their tendency to generate
repetitive sentences with common concepts for different images. These generic
descriptions fail to provide sufficient textual semantics for ever-changing web
images. Inspired by the recent success of Vision-Language Pre-training (VLP)
models that learn diverse image-text concept alignment during pretraining, we
explore leveraging their cross-modal pre-trained knowledge to automatically
enrich the textual semantics of image descriptions. With no need for additional
human annotations, we propose a plug-and-play framework, i.e CapEnrich, to
complement the generic image descriptions with more semantic details.
Specifically, we first propose an automatic data-building strategy to get
desired training sentences, based on which we then adopt prompting strategies,
i.e. learnable and template prompts, to incentivize VLP models to generate more
textual details. For learnable templates, we fix the whole VLP model and only
tune the prompt vectors, which leads to two advantages: 1) the pre-training
knowledge of VLP models can be reserved as much as possible to describe diverse
visual concepts; 2) only lightweight trainable parameters are required, so it
is friendly to low data resources. Extensive experiments show that our method
significantly improves the descriptiveness and diversity of generated sentences
for web images. The code is available at https://github.com/yaolinli/CapEnrich.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to WWW2023</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-03-18T00:00:00Z">2023-03-18</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">18</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning <span class="chip">ICLR
  2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10512v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10512v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen, Tuo Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning large pre-trained language models on downstream tasks has become
an important paradigm in NLP. However, common practice fine-tunes all of the
parameters in a pre-trained model, which becomes prohibitive when a large
number of downstream tasks are present. Therefore, many fine-tuning methods are
proposed to learn incremental updates of pre-trained weights in a parameter
efficient way, e.g., low-rank increments. These methods often evenly distribute
the budget of incremental updates across all pre-trained weight matrices, and
overlook the varying importance of different weight parameters. As a
consequence, the fine-tuning performance is suboptimal. To bridge this gap, we
propose AdaLoRA, which adaptively allocates the parameter budget among weight
matrices according to their importance score. In particular, AdaLoRA
parameterizes the incremental updates in the form of singular value
decomposition. Such a novel approach allows us to effectively prune the
singular values of unimportant updates, which is essentially to reduce their
parameter budget but circumvent intensive exact SVD computations. We conduct
extensive experiments with several pre-trained models on natural language
processing, question answering, and natural language generation to validate the
effectiveness of AdaLoRA. Results demonstrate that AdaLoRA manifests notable
improvement over baselines, especially in the low budget settings. Our code is
publicly available at https://github.com/QingruZhang/AdaLoRA .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The 11th International Conference on Learning Representations (ICLR
  2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Deep Learning System for Domain-specific speech Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10510v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10510v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanan Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As human-machine voice interfaces provide easy access to increasingly
intelligent machines, many state-of-the-art automatic speech recognition (ASR)
systems are proposed. However, commercial ASR systems usually have poor
performance on domain-specific speech especially under low-resource settings.
The author works with pre-trained DeepSpeech2 and Wav2Vec2 acoustic models to
develop benefit-specific ASR systems. The domain-specific data are collected
using proposed semi-supervised learning annotation with little human
intervention. The best performance comes from a fine-tuned Wav2Vec2-Large-LV60
acoustic model with an external KenLM, which surpasses the Google and AWS ASR
systems on benefit-specific speech. The viability of using error prone ASR
transcriptions as part of spoken language understanding (SLU) is also
investigated. Results of a benefit-specific natural language understanding
(NLU) task show that the domain-specific fine-tuned ASR system can outperform
the commercial ASR systems even when its transcriptions have higher word error
rate (WER), and the results between fine-tuned ASR and human transcriptions are
similar.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4th International Conference on Natural Language Processing and
  Computational Linguistics (NLPCL 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Is <span class="highlight-title">Prompt</span> All You Need? No. A Comprehensive and Broader View of
  Instruction Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10475v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10475v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renze Lou, Kai Zhang, Wenpeng Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Task semantics can be expressed by a set of input-to-output examples or a
piece of textual instruction. Conventional machine learning approaches for
natural language processing (NLP) mainly rely on the availability of
large-scale sets of task-specific examples. Two issues arise: first, collecting
task-specific labeled examples does not apply to scenarios where tasks may be
too complicated or costly to annotate, or the system is required to handle a
new task immediately; second, this is not user-friendly since end-users are
probably more willing to provide task description rather than a set of examples
before using the system. Therefore, the community is paying increasing interest
in a new supervision-seeking paradigm for NLP: learning from task instructions.
Despite its impressive progress, there are some common issues that the
community struggles with. This survey paper tries to summarize the current
research on instruction learning, particularly, by answering the following
questions: (i) what is task instruction, and what instruction types exist? (ii)
how to model instructions? (iii) what factors influence and explain the
instructions' performance? (iv) what challenges remain in instruction learning?
To our knowledge, this is the first comprehensive survey about textual
instructions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work is still in progress. The paper list is available at
  https://github.com/RenzeLou/awesome-instruction-learning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SPDF: Sparse <span class="highlight-title">Pre-train</span>ing and Dense Fine-tuning for Large Language
  Models <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10464v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10464v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vithursan Thangarasa, Abhay Gupta, William Marshall, Tianda Li, Kevin Leong, Dennis DeCoste, Sean Lie, Shreyas Saxena
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The pre-training and fine-tuning paradigm has contributed to a number of
breakthroughs in Natural Language Processing (NLP). Instead of directly
training on a downstream task, language models are first pre-trained on large
datasets with cross-domain knowledge (e.g., Pile, MassiveText, etc.) and then
fine-tuned on task-specific data (e.g., natural language generation, text
summarization, etc.). Scaling the model and dataset size has helped improve the
performance of LLMs, but unfortunately, this also leads to highly prohibitive
computational costs. Pre-training LLMs often require orders of magnitude more
FLOPs than fine-tuning and the model capacity often remains the same between
the two phases. To achieve training efficiency w.r.t training FLOPs, we propose
to decouple the model capacity between the two phases and introduce Sparse
Pre-training and Dense Fine-tuning (SPDF). In this work, we show the benefits
of using unstructured weight sparsity to train only a subset of weights during
pre-training (Sparse Pre-training) and then recover the representational
capacity by allowing the zeroed weights to learn (Dense Fine-tuning). We
demonstrate that we can induce up to 75% sparsity into a 1.3B parameter GPT-3
XL model resulting in a 2.5x reduction in pre-training FLOPs, without a
significant loss in accuracy on the downstream tasks relative to the dense
baseline. By rigorously evaluating multiple downstream tasks, we also establish
a relationship between sparsity, task complexity, and dataset size. Our work
presents a promising direction to train large GPT models at a fraction of the
training FLOPs using weight sparsity while retaining the benefits of
pre-trained textual representations for downstream tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at the ICLR 2023 Workshop on Sparsity in Neural Networks</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GazeReader: Detecting Unknown Word Using Webcam for English as a Second
  Language (ESL) Learners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10443v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10443v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiexin Ding, Bowen Zhao, Yuqi Huang, Yuntao Wang, Yuanchun Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic unknown word detection techniques can enable new applications for
assisting English as a Second Language (ESL) learners, thus improving their
reading experiences. However, most modern unknown word detection methods
require dedicated eye-tracking devices with high precision that are not easily
accessible to end-users. In this work, we propose GazeReader, an unknown word
detection method only using a webcam. GazeReader tracks the learner's gaze and
then applies a transformer-based machine learning model that encodes the text
information to locate the unknown word. We applied knowledge enhancement
including term frequency, part of speech, and named entity recognition to
improve the performance. The user study indicates that the accuracy and
F1-score of our method were 98.09% and 75.73%, respectively. Lastly, we
explored the design scope for ESL reading and discussed the findings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by ACM CHI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stop Words for Processing Software Engineering Documents: Do they
  Matter? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10439v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10439v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaohou Fan, Chetan Arora, Christoph Treude
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stop words, which are considered non-predictive, are often eliminated in
natural language processing tasks. However, the definition of uninformative
vocabulary is vague, so most algorithms use general knowledge-based stop lists
to remove stop words. There is an ongoing debate among academics about the
usefulness of stop word elimination, especially in domain-specific settings. In
this work, we investigate the usefulness of stop word removal in a software
engineering context. To do this, we replicate and experiment with three
software engineering research tools from related work. Additionally, we
construct a corpus of software engineering domain-related text from 10,000
Stack Overflow questions and identify 200 domain-specific stop words using
traditional information-theoretic methods. Our results show that the use of
domain-specific stop words significantly improved the performance of research
tools compared to the use of a general stop list and that 17 out of 19
evaluation measures showed better performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at the 2nd Intl. Workshop on NL-based
  Software Engineering (NLBSE 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NoisyHate: Benchmarking Content Moderation Machine Learning Models with
  Human-Written Perturbations Online 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10430v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10430v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiran Ye, Thai Le, Dongwon Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online texts with toxic content are a threat in social media that might cause
cyber harassment. Although many platforms applied measures, such as machine
learning-based hate-speech detection systems, to diminish their effect, those
toxic content publishers can still evade the system by modifying the spelling
of toxic words. Those modified words are also known as human-written text
perturbations. Many research works developed certain techniques to generate
adversarial samples to help the machine learning models obtain the ability to
recognize those perturbations. However, there is still a gap between those
machine-generated perturbations and human-written perturbations. In this paper,
we introduce a benchmark test set containing human-written perturbations online
for toxic speech detection models. We also recruited a group of workers to
evaluate the quality of this test set and dropped low-quality samples.
Meanwhile, to check if our perturbation can be normalized to its clean version,
we applied spell corrector algorithms on this dataset. Finally, we test this
data on state-of-the-art language models, such as BERT and RoBERTa, and black
box APIs, such as perspective API, to demonstrate the adversarial attack with
real human-written perturbations is still effective.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comprehensive Capability Analysis of <span class="highlight-title">GPT</span>-3 and <span class="highlight-title">GPT</span>-3.5 Series Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10420v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10420v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjie Ye, Xuanting Chen, Nuo Xu, Can Zu, Zekai Shao, Shichun Liu, Yuhan Cui, Zeyang Zhou, Chao Gong, Yang Shen, Jie Zhou, Siming Chen, Tao Gui, Qi Zhang, Xuanjing Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  GPT series models, such as GPT-3, CodeX, InstructGPT, ChatGPT, and so on,
have gained considerable attention due to their exceptional natural language
processing capabilities. However, despite the abundance of research on the
difference in capabilities between GPT series models and fine-tuned models,
there has been limited attention given to the evolution of GPT series models'
capabilities over time. To conduct a comprehensive analysis of the capabilities
of GPT series models, we select six representative models, comprising two GPT-3
series models (i.e., davinci and text-davinci-001) and four GPT-3.5 series
models (i.e., code-davinci-002, text-davinci-002, text-davinci-003, and
gpt-3.5-turbo). We evaluate their performance on nine natural language
understanding (NLU) tasks using 21 datasets. In particular, we compare the
performance and robustness of different models for each task under zero-shot
and few-shot scenarios. Our extensive experiments reveal that the overall
ability of GPT series models on NLU tasks does not increase gradually as the
models evolve, especially with the introduction of the RLHF training strategy.
While this strategy enhances the models' ability to generate human-like
responses, it also compromises their ability to solve some tasks. Furthermore,
our findings indicate that there is still room for improvement in areas such as
model robustness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Graph-Guided Reasoning Approach for Open-ended Commonsense Question
  Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10395v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10395v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Han, Yue Feng, Mingming Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, end-to-end trained models for multiple-choice commonsense question
answering (QA) have delivered promising results. However, such
question-answering systems cannot be directly applied in real-world scenarios
where answer candidates are not provided. Hence, a new benchmark challenge set
for open-ended commonsense reasoning (OpenCSR) has been recently released,
which contains natural science questions without any predefined choices. On the
OpenCSR challenge set, many questions require implicit multi-hop reasoning and
have a large decision space, reflecting the difficult nature of this task.
Existing work on OpenCSR sorely focuses on improving the retrieval process,
which extracts relevant factual sentences from a textual knowledge base,
leaving the important and non-trivial reasoning task outside the scope. In this
work, we extend the scope to include a reasoner that constructs a
question-dependent open knowledge graph based on retrieved supporting facts and
employs a sequential subgraph reasoning process to predict the answer. The
subgraph can be seen as a concise and compact graphical explanation of the
prediction. Experiments on two OpenCSR datasets show that the proposed model
achieves great performance on benchmark OpenCSR datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Powerful and Extensible WFST Framework for RNN-Transducer Losses <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10384v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10384v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aleksandr Laptev, Vladimir Bataev, Igor Gitman, Boris Ginsburg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a framework based on Weighted Finite-State Transducers
(WFST) to simplify the development of modifications for RNN-Transducer (RNN-T)
loss. Existing implementations of RNN-T use CUDA-related code, which is hard to
extend and debug. WFSTs are easy to construct and extend, and allow debugging
through visualization. We introduce two WFST-powered RNN-T implementations: (1)
"Compose-Transducer", based on a composition of the WFST graphs from acoustic
and textual schema -- computationally competitive and easy to modify; (2)
"Grid-Transducer", which constructs the lattice directly for further
computations -- most compact, and computationally efficient. We illustrate the
ease of extensibility through introduction of a new W-Transducer loss -- the
adaptation of the Connectionist Temporal Classification with Wild Cards.
W-Transducer (W-RNNT) consistently outperforms the standard RNN-T in a
weakly-supervised data setup with missing parts of transcriptions at the
beginning and end of utterances. All RNN-T losses are implemented with the k2
framework and are available in the NeMo toolkit.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in Proc. ICASSP 2023, June 04-10, 2023, Rhodes island,
  Greece. 5 pages, 5 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Empirical Study of <span class="highlight-title">Pre-train</span>ed Language Models in Simple Knowledge
  Graph Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10368v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10368v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nan Hu, Yike Wu, Guilin Qi, Dehai Min, Jiaoyan Chen, Jeff Z. Pan, Zafar Ali
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale pre-trained language models (PLMs) such as BERT have recently
achieved great success and become a milestone in natural language processing
(NLP). It is now the consensus of the NLP community to adopt PLMs as the
backbone for downstream tasks. In recent works on knowledge graph question
answering (KGQA), BERT or its variants have become necessary in their KGQA
models. However, there is still a lack of comprehensive research and comparison
of the performance of different PLMs in KGQA. To this end, we summarize two
basic KGQA frameworks based on PLMs without additional neural network modules
to compare the performance of nine PLMs in terms of accuracy and efficiency. In
addition, we present three benchmarks for larger-scale KGs based on the popular
SimpleQuestions benchmark to investigate the scalability of PLMs. We carefully
analyze the results of all PLMs-based KGQA basic frameworks on these benchmarks
and two other popular datasets, WebQuestionSP and FreebaseQA, and find that
knowledge distillation techniques and knowledge enhancement methods in PLMs are
promising for KGQA. Furthermore, we test ChatGPT, which has drawn a great deal
of attention in the NLP community, demonstrating its impressive capabilities
and limitations in zero-shot KGQA. We have released the code and benchmarks to
promote the use of PLMs on KGQA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by World Wide Web Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Partial Knowledge Base Inference in Biomedical Entity Linking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10330v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10330v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyi Yuan, Keming Lu, Zheng Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Biomedical entity linking (EL) consists of named entity recognition (NER) and
named entity disambiguation (NED). EL models are trained on corpora labeled by
a predefined KB. However, it is a common scenario that only entities within a
subset of the KB are precious to stakeholders. We name this scenario partial
knowledge base inference: training an EL model with one KB and inferring on the
part of it without further training. In this work, we give a detailed
definition and evaluation procedures for this practically valuable but
significantly understudied scenario and evaluate methods from three
representative EL paradigms. We construct partial KB inference benchmarks and
witness a catastrophic degradation in EL performance due to dramatically
precision drop. Our findings reveal these EL paradigms can not correctly handle
unlinkable mentions (NIL), so they are not robust to partial KB inference. We
also propose two simple-and-effective redemption methods to combat the NIL
issue with little computational overhead.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 3 figures. The first two authors are contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting Automatic Question Summarization Evaluation in the Biomedical
  Domain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10328v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10328v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyi Yuan, Yaoyun Zhang, Fei Huang, Songfang Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic evaluation metrics have been facilitating the rapid development of
automatic summarization methods by providing instant and fair assessments of
the quality of summaries. Most metrics have been developed for the general
domain, especially news and meeting notes, or other language-generation tasks.
However, these metrics are applied to evaluate summarization systems in
different domains, such as biomedical question summarization. To better
understand whether commonly used evaluation metrics are capable of evaluating
automatic summarization in the biomedical domain, we conduct human evaluations
of summarization quality from four different aspects of a biomedical question
summarization task. Based on human judgments, we identify different noteworthy
features for current automatic metrics and summarization systems as well. We
also release a dataset of our human annotations to aid the research of
summarization evaluation metrics in the biomedical domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the rise of fear speech in online social media 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10311v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10311v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Punyajoy Saha, Kiran Garimella, Narla Komal Kalyan, Saurabh Kumar Pandey, Pauras Mangesh Meher, Binny Mathew, Animesh Mukherjee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, social media platforms are heavily moderated to prevent the spread
of online hate speech, which is usually fertile in toxic words and is directed
toward an individual or a community. Owing to such heavy moderation, newer and
more subtle techniques are being deployed. One of the most striking among these
is fear speech. Fear speech, as the name suggests, attempts to incite fear
about a target community. Although subtle, it might be highly effective, often
pushing communities toward a physical conflict. Therefore, understanding their
prevalence in social media is of paramount importance. This article presents a
large-scale study to understand the prevalence of 400K fear speech and over
700K hate speech posts collected from Gab.com. Remarkably, users posting a
large number of fear speech accrue more followers and occupy more central
positions in social networks than users posting a large number of hate speech.
They can also reach out to benign users more effectively than hate speech users
through replies, reposts, and mentions. This connects to the fact that, unlike
hate speech, fear speech has almost zero toxic content, making it look
plausible. Moreover, while fear speech topics mostly portray a community as a
perpetrator using a (fake) chain of argumentation, hate speech topics hurl
direct multitarget insults, thus pointing to why general users could be more
gullible to fear speech. Our findings transcend even to other platforms
(Twitter and Facebook) and thus necessitate using sophisticated moderation
policies and mass awareness to combat fear speech.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 9 tables, 15 figures, accepted in Proceedings of the
  National Academy of Sciences of the United States of America</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contextual Semantic Embeddings for Ontology Subsumption Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.09791v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.09791v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaoyan Chen, Yuan He, Yuxia Geng, Ernesto Jimenez-Ruiz, Hang Dong, Ian Horrocks
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automating ontology construction and curation is an important but challenging
task in knowledge engineering and artificial intelligence. Prediction by
machine learning techniques such as contextual semantic embedding is a
promising direction, but the relevant research is still preliminary especially
for expressive ontologies in Web Ontology Language (OWL). In this paper, we
present a new subsumption prediction method named BERTSubs for classes of OWL
ontology. It exploits the pre-trained language model BERT to compute contextual
embeddings of a class, where customized templates are proposed to incorporate
the class context (e.g., neighbouring classes) and the logical existential
restriction. BERTSubs is able to predict multiple kinds of subsumers including
named classes from the same ontology or another ontology, and existential
restrictions from the same ontology. Extensive evaluation on five real-world
ontologies for three different subsumption tasks has shown the effectiveness of
the templates and that BERTSubs can dramatically outperform the baselines that
use (literal-aware) knowledge graph embeddings, non-contextual word embeddings
and the state-of-the-art OWL ontology embeddings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by World Wide Web Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FGSI: Distant Supervision for Relation Extraction method based on
  Fine-Grained Semantic Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.02078v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.02078v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenghong Sun, Weidong Ji, Guohui Zhou, Hui Guo, Zengxiang Yin, Yuqi Yue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The main purpose of relation extraction is to extract the semantic
relationships between tagged pairs of entities in a sentence, which plays an
important role in the semantic understanding of sentences and the construction
of knowledge graphs. In this paper, we propose that the key semantic
information within a sentence plays a key role in the relationship extraction
of entities. We propose the hypothesis that the key semantic information inside
the sentence plays a key role in entity relationship extraction. And based on
this hypothesis, we split the sentence into three segments according to the
location of the entity from the inside of the sentence, and find the
fine-grained semantic features inside the sentence through the intra-sentence
attention mechanism to reduce the interference of irrelevant noise information.
The proposed relational extraction model can make full use of the available
positive semantic information. The experimental results show that the proposed
relation extraction model improves the accuracy-recall curves and P@N values
compared with existing methods, which proves the effectiveness of this model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Uzbek text's correspondence with the educational potential of pupils: a
  case study of the School corpus 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.00465v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.00465v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khabibulla Madatov, Sanatbek Matlatipov, Mersaid Aripov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the major challenges of an educational system is choosing appropriate
content considering pupils' age and intellectual potential. In this article the
experiment of primary school grades (from 1st to 4th grades) is considered for
automatically determining the correspondence of an educational materials
recommended for pupils by using the School corpus where it includes the dataset
of 25 school textbooks confirmed by the Ministry of preschool and school
education of the Republic of Uzbekistan. In this case, TF-IDF scores of the
texts are determined, they are converted into a vector representation, and the
given educational materials are compared with the corresponding class of the
School corpus using the cosine similarity algorithm. Based on the results of
the calculation, it is determined whether the given educational material is
appropriate or not appropriate for the pupils' educational potential.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint of the paper accepted to The 10th Language & Technology
  Conference: Human Language Technologies as a Challenge for Computer Science
  and Linguistics. April 21-23, 2023, Poznan, Poland</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Active Learning for Event Extraction with Memory-based Loss Prediction
  Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.03073v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.03073v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shirong Shen, Zhen Li, Guilin Qi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event extraction (EE) plays an important role in many industrial application
scenarios, and high-quality EE methods require a large amount of manual
annotation data to train supervised learning models. However, the cost of
obtaining annotation data is very high, especially for annotation of domain
events, which requires the participation of experts from corresponding domain.
So we introduce active learning (AL) technology to reduce the cost of event
annotation. But the existing AL methods have two main problems, which make them
not well used for event extraction. Firstly, the existing pool-based selection
strategies have limitations in terms of computational cost and sample validity.
Secondly, the existing evaluation of sample importance lacks the use of local
sample information. In this paper, we present a novel deep AL method for EE. We
propose a batch-based selection strategy and a Memory-Based Loss Prediction
model (MBLP) to select unlabeled samples efficiently. During the selection
process, we use an internal-external sample loss ranking method to evaluate the
sample importance by using local information. Finally, we propose a delayed
training strategy to train the MBLP model. Extensive experiments are performed
on three domain datasets, and our method outperforms other state-of-the-art
methods.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">4</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Examining the Potential for Conversational Exploratory Search using a
  Smart Speaker Digital Assistant 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10497v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10497v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhishek Kaushik, Gareth J. F. Jones
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online Digital Assistants, such as Amazon Alexa, Google Assistant, Apple Siri
are very popular and provide a range or services to their users, a key function
is their ability to satisfy user information needs from the sources available
to them. Users may often regard these applications as providing search services
similar to Google type search engines. However, while it is clear that they are
in general able to answer factoid questions effectively, it is much less
obvious how well they support less specific or exploratory type search tasks.
We describe an investigation examining the behaviour of the standard Amazon
Alexa for exploratory search tasks. The results of our study show that it not
effective in addressing these types of information needs. We propose extensions
to Alexa designed to overcome these shortcomings. Our Custom Alexa application
extends Alexa's conversational functionality for exploratory search. A user
study shows that our extended Alexa application both enables users to more
successfully complete exploratory search tasks and is well accepted by our test
users.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Practical Cross-System Shilling Attacks with Limited Access to Data <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.07145v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.07145v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meifang Zeng, Ke Li, Bingchuan Jiang, Liujuan Cao, Hui Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In shilling attacks, an adversarial party injects a few fake user profiles
into a Recommender System (RS) so that the target item can be promoted or
demoted. Although much effort has been devoted to developing shilling attack
methods, we find that existing approaches are still far from practical. In this
paper, we analyze the properties a practical shilling attack method should have
and propose a new concept of Cross-system Attack. With the idea of Cross-system
Attack, we design a Practical Cross-system Shilling Attack (PC-Attack)
framework that requires little information about the victim RS model and the
target RS data for conducting attacks. PC-Attack is trained to capture graph
topology knowledge from public RS data in a self-supervised manner. Then, it is
fine-tuned on a small portion of target data that is easy to access to
construct fake profiles. Extensive experiments have demonstrated the
superiority of PC-Attack over state-of-the-art baselines. Our implementation of
PC-Attack is available at https://github.com/KDEGroup/PC-Attack.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph-less Collaborative Filtering <span class="chip">WWW 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08537v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08537v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lianghao Xia, Chao Huang, Jiao Shi, Yong Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph neural networks (GNNs) have shown the power in representation learning
over graph-structured user-item interaction data for collaborative filtering
(CF) task. However, with their inherently recursive message propagation among
neighboring nodes, existing GNN-based CF models may generate indistinguishable
and inaccurate user (item) representations due to the over-smoothing and noise
effect with low-pass Laplacian smoothing operators. In addition, the recursive
information propagation with the stacked aggregators in the entire graph
structures may result in poor scalability in practical applications. Motivated
by these limitations, we propose a simple and effective collaborative filtering
model (SimRec) that marries the power of knowledge distillation and contrastive
learning. In SimRec, adaptive transferring knowledge is enabled between the
teacher GNN model and a lightweight student network, to not only preserve the
global collaborative signals, but also address the over-smoothing issue with
representation recalibration. Empirical results on public datasets show that
SimRec archives better efficiency while maintaining superior recommendation
performance compared with various strong baselines. Our implementations are
publicly available at: https://github.com/HKUDS/SimRec.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM WWW 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lorentz Equivariant Model for Knowledge-Enhanced Hyperbolic
  Collaborative Filtering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.04545v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.04545v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bosong Huang, Weihao Yu, Ruzhong Xie, Jing Xiao, Jin Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Introducing prior auxiliary information from the knowledge graph (KG) to
assist the user-item graph can improve the comprehensive performance of the
recommender system. Many recent studies show that the ensemble properties of
hyperbolic spaces fit the scale-free and hierarchical characteristics exhibited
in the above two types of graphs well. However, existing hyperbolic methods
ignore the consideration of equivariance, thus they cannot generalize symmetric
features under given transformations, which seriously limits the capability of
the model. Moreover, they cannot balance preserving the heterogeneity and
mining the high-order entity information to users across two graphs. To fill
these gaps, we propose a rigorously Lorentz group equivariant
knowledge-enhanced collaborative filtering model (LECF). Innovatively, we
jointly update the attribute embeddings (containing the high-order entity
signals from the KG) and hyperbolic embeddings (the distance between hyperbolic
embeddings reveals the recommendation tendency) by the LECF layer with Lorentz
Equivariant Transformation. Moreover, we propose Hyperbolic Sparse Attention
Mechanism to sample the most informative neighbor nodes. Lorentz equivariance
is strictly maintained throughout the entire model, and enforcing equivariance
is proven necessary experimentally. Extensive experiments on three real-world
benchmarks demonstrate that LECF remarkably outperforms state-of-the-art
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">5</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Content Adaptive Learnable Time-Frequency Representation For Audio
  Signal Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10446v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10446v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prateek Verma, Chris Chafe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a learnable content adaptive front end for audio signal
processing. Before the modern advent of deep learning, we used fixed
representation non-learnable front-ends like spectrogram or mel-spectrogram
with/without neural architectures. With convolutional architectures supporting
various applications such as ASR and acoustic scene understanding, a shift to a
learnable front ends occurred in which both the type of basis functions and the
weight were learned from scratch and optimized for the particular task of
interest. With the shift to transformer-based architectures with no
convolutional blocks present, a linear layer projects small waveform patches
onto a small latent dimension before feeding them to a transformer
architecture. In this work, we propose a way of computing a content-adaptive
learnable time-frequency representation. We pass each audio signal through a
bank of convolutional filters, each giving a fixed-dimensional vector. It is
akin to learning a bank of finite impulse-response filterbanks and passing the
input signal through the optimum filter bank depending on the content of the
input signal. A content-adaptive learnable time-frequency representation may be
more broadly applicable, beyond the experiments in this paper.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 4 figures. 2023 IEEE International Conference on Acoustics,
  Speech, and Signal Processing, Rhodes, Greece</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Just Noticeable Visual Redundancy Forecasting: A Deep Multimodal-driven
  Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10372v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10372v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wuyuan Xie, Shukang Wang, Sukun Tian, Lirong Huang, Ye Liu, Miaohui Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Just noticeable difference (JND) refers to the maximum visual change that
human eyes cannot perceive, and it has a wide range of applications in
multimedia systems. However, most existing JND approaches only focus on a
single modality, and rarely consider the complementary effects of multimodal
information. In this article, we investigate the JND modeling from an
end-to-end homologous multimodal perspective, namely hmJND-Net. Specifically,
we explore three important visually sensitive modalities, including saliency,
depth, and segmentation. To better utilize homologous multimodal information,
we establish an effective fusion method via summation enhancement and
subtractive offset, and align homologous multimodal features based on a
self-attention driven encoder-decoder paradigm. Extensive experimental results
on eight different benchmark datasets validate the superiority of our hmJND-Net
over eight representative methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Blind Multimodal Quality Assessment: A Brief <span class="highlight-title">Survey</span> and A Case Study of
  Low-light Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10369v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10369v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miaohui Wang, Zhuowei Xu, Mai Xu, Weisi Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Blind image quality assessment (BIQA) aims at automatically and accurately
forecasting objective scores for visual signals, which has been widely used to
monitor product and service quality in low-light applications, covering
smartphone photography, video surveillance, autonomous driving, etc. Recent
developments in this field are dominated by unimodal solutions inconsistent
with human subjective rating patterns, where human visual perception is
simultaneously reflected by multiple sensory information (e.g., sight and
hearing). In this article, we present a unique blind multimodal quality
assessment (BMQA) of low-light images from subjective evaluation to objective
score. To investigate the multimodal mechanism, we first establish a multimodal
low-light image quality (MLIQ) database with authentic low-light distortions,
containing image and audio modality pairs. Further, we specially design the key
modules of BMQA, considering multimodal quality representation, latent feature
alignment and fusion, and hybrid self-supervised and supervised learning.
Extensive experiments show that our BMQA yields state-of-the-art accuracy on
the proposed MLIQ benchmark database. In particular, we also build an
independent single-image modality Dark-4K database, which is used to verify its
applicability and generalization performance in mainstream unimodal
applications. Qualitative and quantitative results on Dark-4K show that BMQA
achieves superior performance to existing BIQA approaches as long as a
pre-trained quality semantic description model is provided. The proposed
framework and two databases as well as the collected BIQA methods and
evaluation metrics are made publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Continuous Emotion Recognition: A Technical Report for ABAW5 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10335v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10335v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Su Zhang, Ziyuan Zhao, Cuntai Guan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We used two multimodal models for continuous valence-arousal recognition
using visual, audio, and linguistic information. The first model is the same as
we used in ABAW2 and ABAW3, which employs the leader-follower attention. The
second model has the same architecture for spatial and temporal encoding. As
for the fusion block, it employs a compact and straightforward channel
attention, borrowed from the End2You toolkit. Unlike our previous attempts that
use Vggish feature directly as the audio feature, this time we feed the
pre-trained VGG model using logmel-spectrogram and finetune it during the
training. To make full use of the data and alleviate over-fitting,
cross-validation is carried out. The fold with the highest concordance
correlation coefficient is selected for submission. The code is to be available
at https://github.com/sucv/ABAW5.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages. arXiv admin note: substantial text overlap with
  arXiv:2203.13031</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Smartbanner: Intelligent banner design framework that strikes a balance
  between creative freedom and design rules 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10325v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10325v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guandong Li, Xian Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Companies use banners extensively to promote their products, and the
intelligent automatic synthesis of banners is a challenging event. Under the
premise of inputting only a small amount of information such as product, text
and size, it can synthesize styles with high freedom and richness, but at the
same time, it must satisfy the design specifications of advertisers for
advertising and scenes. We propose an intelligent banner design framework that
strikes a balance between creative freedom and design rules, called
smartbanner. Smartbanner consists of planner, actuator, adjuster and generator.
The banner is synthesized through the combined framework, which fully liberates
the designer and reduces the threshold and cost of design. It increases the
click-through rate by 30%, improves the human efficiency of designers by 500%
under the condition of ensuring the quality of creation, and synthesizes
hundreds of millions of pictures in batches throughout the year.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2023-03-26T05:20:41.403983136Z">
            2023-03-26 05:20:41 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
