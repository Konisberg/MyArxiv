{"2023-03-20T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2303.11331v1","updated":"2023-03-20T17:59:59Z","published":"2023-03-20T17:59:59Z","title":"EVA-02: A Visual Representation for Neon Genesis","summary":"  We launch EVA-02, a next-generation Transformer-based visual representation\npre-trained to reconstruct strong and robust language-aligned vision features\nvia masked image modeling. With an updated plain Transformer architecture as\nwell as extensive pre-training from an open & accessible giant CLIP vision\nencoder, EVA-02 demonstrates superior performance compared to prior\nstate-of-the-art approaches across various representative vision tasks, while\nutilizing significantly fewer parameters and compute budgets. Notably, using\nexclusively publicly accessible training data, EVA-02 with only 304M parameters\nachieves a phenomenal 90.0 fine-tuning top-1 accuracy on ImageNet-1K val set.\nAdditionally, our EVA-02-CLIP can reach up to 80.4 zero-shot top-1 on\nImageNet-1K, outperforming the previous largest & best open-sourced CLIP with\nonly ~1/6 parameters and ~1/6 image-text training data. We offer four EVA-02\nvariants in various model sizes, ranging from 6M to 304M parameters, all with\nimpressive performance. To facilitate open access and open research, we release\nthe complete suite of EVA-02 to the community at\nhttps://github.com/baaivision/EVA/tree/master/EVA-02.\n","authors":["Yuxin Fang","Quan Sun","Xinggang Wang","Tiejun Huang","Xinlong Wang","Yue Cao"],"pdf_url":"https://arxiv.org/pdf/2303.11331v1.pdf","comment":"To Asuka. Code & Models:\n  https://github.com/baaivision/EVA/tree/master/EVA-02"},{"id":"http://arxiv.org/abs/2303.11315v1","updated":"2023-03-20T17:54:58Z","published":"2023-03-20T17:54:58Z","title":"Context-faithful Prompting for Large Language Models","summary":"  Large language models (LLMs) encode parametric knowledge about world facts\nand have shown remarkable performance in knowledge-driven NLP tasks. However,\ntheir reliance on parametric knowledge may cause them to overlook contextual\ncues, leading to incorrect predictions in context-sensitive NLP tasks (e.g.,\nknowledge acquisition tasks). In this paper, we seek to assess and enhance\nLLMs' contextual faithfulness in two aspects: knowledge conflict and prediction\nwith abstention. We demonstrate that LLMs' faithfulness can be significantly\nimproved using carefully designed prompting strategies. In particular, we\nidentify opinion-based prompts and counterfactual demonstrations as the most\neffective methods. Opinion-based prompts reframe the context as a narrator's\nstatement and inquire about the narrator's opinions, while counterfactual\ndemonstrations use instances containing false facts to improve faithfulness in\nknowledge conflict situations. Neither technique requires additional training.\nWe conduct experiments on three datasets of two standard NLP tasks, machine\nreading comprehension and relation extraction, and the results demonstrate\nsignificant improvement in faithfulness to contexts.\n","authors":["Wenxuan Zhou","Sheng Zhang","Hoifung Poon","Muhao Chen"],"pdf_url":"https://arxiv.org/pdf/2303.11315v1.pdf","comment":"Code and data will be released at\n  https://github.com/wzhouad/context-faithful-llm"},{"id":"http://arxiv.org/abs/2303.11192v1","updated":"2023-03-20T15:22:11Z","published":"2023-03-20T15:22:11Z","title":"Multimodal Shannon Game with Images","summary":"  The Shannon game has long been used as a thought experiment in linguistics\nand NLP, asking participants to guess the next letter in a sentence based on\nits preceding context. We extend the game by introducing an optional extra\nmodality in the form of image information. To investigate the impact of\nmultimodal information in this game, we use human participants and a language\nmodel (LM, GPT-2). We show that the addition of image information improves both\nself-reported confidence and accuracy for both humans and LM. Certain word\nclasses, such as nouns and determiners, benefit more from the additional\nmodality information. The priming effect in both humans and the LM becomes more\napparent as the context size (extra modality information + sentence context)\nincreases. These findings highlight the potential of multimodal information in\nimproving language understanding and modeling.\n","authors":["Vilém Zouhar","Sunit Bhattacharya","Ondřej Bojar"],"pdf_url":"https://arxiv.org/pdf/2303.11192v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11184v1","updated":"2023-03-20T15:10:45Z","published":"2023-03-20T15:10:45Z","title":"Conversation Modeling to Predict Derailment","summary":"  Conversations among online users sometimes derail, i.e., break down into\npersonal attacks. Such derailment has a negative impact on the healthy growth\nof cyberspace communities. The ability to predict whether ongoing conversations\nare likely to derail could provide valuable real-time insight to interlocutors\nand moderators. Prior approaches predict conversation derailment\nretrospectively without the ability to forestall the derailment proactively.\nSome works attempt to make dynamic prediction as the conversation develops, but\nfail to incorporate multisource information, such as conversation structure and\ndistance to derailment.\n  We propose a hierarchical transformer-based framework that combines\nutterance-level and conversation-level information to capture fine-grained\ncontextual semantics. We propose a domain-adaptive pretraining objective to\nintegrate conversational structure information and a multitask learning scheme\nto leverage the distance from each utterance to derailment. An evaluation of\nour framework on two conversation derailment datasets yields improvement over\nF1 score for the prediction of derailment. These results demonstrate the\neffectiveness of incorporating multisource information.\n","authors":["Jiaqing Yuan","Munindar P. Singh"],"pdf_url":"https://arxiv.org/pdf/2303.11184v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11141v1","updated":"2023-03-20T14:19:58Z","published":"2023-03-20T14:19:58Z","title":"DocRED-FE: A Document-Level Fine-Grained Entity And Relation Extraction\n  Dataset","summary":"  Joint entity and relation extraction (JERE) is one of the most important\ntasks in information extraction. However, most existing works focus on\nsentence-level coarse-grained JERE, which have limitations in real-world\nscenarios. In this paper, we construct a large-scale document-level\nfine-grained JERE dataset DocRED-FE, which improves DocRED with Fine-Grained\nEntity Type. Specifically, we redesign a hierarchical entity type schema\nincluding 11 coarse-grained types and 119 fine-grained types, and then\nre-annotate DocRED manually according to this schema. Through comprehensive\nexperiments we find that: (1) DocRED-FE is challenging to existing JERE models;\n(2) Our fine-grained entity types promote relation classification. We make\nDocRED-FE with instruction and the code for our baselines publicly available at\nhttps://github.com/PKU-TANGENT/DOCRED-FE.\n","authors":["Hongbo Wang","Weimin Xiong","Yifan Song","Dawei Zhu","Yu Xia","Sujian Li"],"pdf_url":"https://arxiv.org/pdf/2303.11141v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11131v1","updated":"2023-03-20T14:07:13Z","published":"2023-03-20T14:07:13Z","title":"Cocktail HuBERT: Generalized Self-Supervised Pre-training for Mixture\n  and Single-Source Speech","summary":"  Self-supervised learning leverages unlabeled data effectively, improving\nlabel efficiency and generalization to domains without labeled data. While\nrecent work has studied generalization to more acoustic/linguistic domains,\nlanguages, and modalities, these investigations are limited to single-source\nspeech with one primary speaker in the recording. This paper presents Cocktail\nHuBERT, a self-supervised learning framework that generalizes to mixture speech\nusing a masked pseudo source separation objective. This objective encourages\nthe model to identify the number of sources, separate and understand the\ncontext, and infer the content of masked regions represented as discovered\nunits. Cocktail HuBERT outperforms state-of-the-art results with 69% lower WER\non multi-speaker ASR, 31% lower DER on diarization, and is competitive on\nsingle- and multi-speaker tasks from SUPERB.\n","authors":["Maryam Fazel-Zarandi","Wei-Ning Hsu"],"pdf_url":"https://arxiv.org/pdf/2303.11131v1.pdf","comment":"ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.11117v1","updated":"2023-03-20T13:58:35Z","published":"2023-03-20T13:58:35Z","title":"EmotionIC: Emotional Inertia and Contagion-driven Dependency Modelling\n  for Emotion Recognition in Conversation","summary":"  Emotion Recognition in Conversation (ERC) has attracted growing attention in\nrecent years as a result of the advancement and implementation of\nhuman-computer interface technologies. However, previous approaches to modeling\nglobal and local context dependencies lost the diversity of dependency\ninformation and do not take the context dependency into account at the\nclassification level. In this paper, we propose a novel approach to dependency\nmodeling driven by Emotional Inertia and Contagion (EmotionIC) for\nconversational emotion recognition at the feature extraction and classification\nlevels. At the feature extraction level, our designed Identity Masked\nMulti-head Attention (IM-MHA) captures the identity-based long-distant context\nin the dialogue to contain the diverse influence of different participants and\nconstruct the global emotional atmosphere, while the devised Dialogue-based\nGate Recurrent Unit (DialogGRU) that aggregates the emotional tendencies of\ndyadic dialogue is applied to refine the contextual features with inter- and\nintra-speaker dependencies. At the classification level, by introducing skip\nconnections in Conditional Random Field (CRF), we elaborate the Skip-chain CRF\n(SkipCRF) to capture the high-order dependencies within and between speakers,\nand to emulate the emotional flow of distant participants. Experimental results\nshow that our method can significantly outperform the state-of-the-art models\non four benchmark datasets. The ablation studies confirm that our modules can\neffectively model emotional inertia and contagion.\n","authors":["Liu Yingjian","Li Jiang","Wang Xiaoping","Zeng Zhigang"],"pdf_url":"https://arxiv.org/pdf/2303.11117v1.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2207.12261v2","updated":"2023-03-20T13:34:48Z","published":"2022-07-06T13:56:48Z","title":"GraphCFC: A Directed Graph based Cross-modal Feature Complementation\n  Approach for Multimodal Conversational Emotion Recognition","summary":"  Emotion Recognition in Conversation (ERC) plays a significant part in\nHuman-Computer Interaction (HCI) systems since it can provide empathetic\nservices. Multimodal ERC can mitigate the drawbacks of uni-modal approaches.\nRecently, Graph Neural Networks (GNNs) have been widely used in a variety of\nfields due to their superior performance in relation modeling. In multimodal\nERC, GNNs are capable of extracting both long-distance contextual information\nand inter-modal interactive information. Unfortunately, since existing methods\nsuch as MMGCN directly fuse multiple modalities, redundant information may be\ngenerated and diverse information may be lost. In this work, we present a\ndirected Graph based Cross-modal Feature Complementation (GraphCFC) module that\ncan efficiently model contextual and interactive information. GraphCFC\nalleviates the problem of heterogeneity gap in multimodal fusion by utilizing\nmultiple subspace extractors and Pair-wise Cross-modal Complementary (PairCC)\nstrategy. We extract various types of edges from the constructed graph for\nencoding, thus enabling GNNs to extract crucial contextual and interactive\ninformation more accurately when performing message passing. Furthermore, we\ndesign a GNN structure called GAT-MLP, which can provide a new unified network\nframework for multimodal learning. The experimental results on two benchmark\ndatasets show that our GraphCFC outperforms the state-of-the-art (SOTA)\napproaches.\n","authors":["Jiang Li","Xiaoping Wang","Guoqing Lv","Zhigang Zeng"],"pdf_url":"https://arxiv.org/pdf/2207.12261v2.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2110.04845v4","updated":"2023-03-20T13:34:47Z","published":"2021-10-10T16:23:54Z","title":"What Makes Sentences Semantically Related: A Textual Relatedness Dataset\n  and Empirical Study","summary":"  The degree of semantic relatedness of two units of language has long been\nconsidered fundamental to understanding meaning. Additionally, automatically\ndetermining relatedness has many applications such as question answering and\nsummarization. However, prior NLP work has largely focused on semantic\nsimilarity, a subset of relatedness, because of a lack of relatedness datasets.\nIn this paper, we introduce a dataset for Semantic Textual Relatedness,\nSTR-2022, that has 5,500 English sentence pairs manually annotated using a\ncomparative annotation framework, resulting in fine-grained scores. We show\nthat human intuition regarding relatedness of sentence pairs is highly\nreliable, with a repeat annotation correlation of 0.84. We use the dataset to\nexplore questions on what makes sentences semantically related. We also show\nthe utility of STR-2022 for evaluating automatic methods of sentence\nrepresentation and for various downstream NLP tasks.\n  Our dataset, data statement, and annotation questionnaire can be found at:\nhttps://doi.org/10.5281/zenodo.7599667\n","authors":["Mohamed Abdalla","Krishnapriya Vishnubhotla","Saif M. Mohammad"],"pdf_url":"https://arxiv.org/pdf/2110.04845v4.pdf","comment":"Accepted to EACL 2023; Our dataset, data statement, and annotation\n  questionnaire can be found at: https://doi.org/10.5281/zenodo.7599667"},{"id":"http://arxiv.org/abs/2303.11082v1","updated":"2023-03-20T13:14:59Z","published":"2023-03-20T13:14:59Z","title":"Evaluating Language Models for Knowledge Base Completion","summary":"  Structured knowledge bases (KBs) are a foundation of many intelligent\napplications, yet are notoriously incomplete. Language models (LMs) have\nrecently been proposed for unsupervised knowledge base completion (KBC), yet,\ndespite encouraging initial results, questions regarding their suitability\nremain open. Existing evaluations often fall short because they only evaluate\non popular subjects, or sample already existing facts from KBs. In this work,\nwe introduce a novel, more challenging benchmark dataset, and a methodology\ntailored for a realistic assessment of the KBC potential of LMs. For automated\nassessment, we curate a dataset called WD-KNOWN, which provides an unbiased\nrandom sample of Wikidata, containing over 3.9 million facts. In a second step,\nwe perform a human evaluation on predictions that are not yet in the KB, as\nonly this provides real insights into the added value over existing KBs. Our\nkey finding is that biases in dataset conception of previous benchmarks lead to\na systematic overestimate of LM performance for KBC. However, our results also\nreveal strong areas of LMs. We could, for example, perform a significant\ncompletion of Wikidata on the relations nativeLanguage, by a factor of ~21\n(from 260k to 5.8M) at 82% precision, usedLanguage, by a factor of ~2.1 (from\n2.1M to 6.6M) at 82% precision, and citizenOf by a factor of ~0.3 (from 4.2M to\n5.3M) at 90% precision. Moreover, we find that LMs possess surprisingly strong\ngeneralization capabilities: even on relations where most facts were not\ndirectly observed in LM training, prediction quality can be high.\n","authors":["Blerta Veseli","Sneha Singhania","Simon Razniewski","Gerhard Weikum"],"pdf_url":"https://arxiv.org/pdf/2303.11082v1.pdf","comment":"Data and code available at https://github.com/bveseli/LMsForKBC"},{"id":"http://arxiv.org/abs/2302.13007v3","updated":"2023-03-20T11:39:47Z","published":"2023-02-25T06:58:16Z","title":"AugGPT: Leveraging ChatGPT for Text Data Augmentation","summary":"  Text data augmentation is an effective strategy for overcoming the challenge\nof limited sample sizes in many natural language processing (NLP) tasks. This\nchallenge is especially prominent in the few-shot learning scenario, where the\ndata in the target domain is generally much scarcer and of lowered quality. A\nnatural and widely-used strategy to mitigate such challenges is to perform data\naugmentation to better capture the data invariance and increase the sample\nsize. However, current text data augmentation methods either can't ensure the\ncorrect labeling of the generated data (lacking faithfulness) or can't ensure\nsufficient diversity in the generated data (lacking compactness), or both.\nInspired by the recent success of large language models, especially the\ndevelopment of ChatGPT, which demonstrated improved language comprehension\nabilities, in this work, we propose a text data augmentation approach based on\nChatGPT (named AugGPT). AugGPT rephrases each sentence in the training samples\ninto multiple conceptually similar but semantically different samples. The\naugmented samples can then be used in downstream model training. Experiment\nresults on few-shot learning text classification tasks show the superior\nperformance of the proposed AugGPT approach over state-of-the-art text data\naugmentation methods in terms of testing accuracy and distribution of the\naugmented samples.\n","authors":["Haixing Dai","Zhengliang Liu","Wenxiong Liao","Xiaoke Huang","Yihan Cao","Zihao Wu","Lin Zhao","Shaochen Xu","Wei Liu","Ninghao Liu","Sheng Li","Dajiang Zhu","Hongmin Cai","Lichao Sun","Quanzheng Li","Dinggang Shen","Tianming Liu","Xiang Li"],"pdf_url":"https://arxiv.org/pdf/2302.13007v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11032v1","updated":"2023-03-20T11:34:37Z","published":"2023-03-20T11:34:37Z","title":"DeID-GPT: Zero-shot Medical Text De-Identification by GPT-4","summary":"  The digitization of healthcare has facilitated the sharing and re-using of\nmedical data but has also raised concerns about confidentiality and privacy.\nHIPAA (Health Insurance Portability and Accountability Act) mandates removing\nre-identifying information before the dissemination of medical records. Thus,\neffective and efficient solutions for de-identifying medical data, especially\nthose in free-text forms, are highly needed. While various computer-assisted\nde-identification methods, including both rule-based and learning-based, have\nbeen developed and used in prior practice, such solutions still lack\ngeneralizability or need to be fine-tuned according to different scenarios,\nsignificantly imposing restrictions in wider use. The advancement of large\nlanguage models (LLM), such as ChatGPT and GPT-4, have shown great potential in\nprocessing text data in the medical domain with zero-shot in-context learning,\nespecially in the task of privacy protection, as these models can identify\nconfidential information by their powerful named entity recognition (NER)\ncapability. In this work, we developed a novel GPT4-enabled de-identification\nframework (\"DeID-GPT\") to automatically identify and remove the identifying\ninformation. Compared to existing commonly used medical text data\nde-identification methods, our developed DeID-GPT showed the highest accuracy\nand remarkable reliability in masking private information from the unstructured\nmedical text while preserving the original structure and meaning of the text.\nThis study is one of the earliest to utilize ChatGPT and GPT-4 for medical text\ndata processing and de-identification, which provides insights for further\nresearch and solution development on the use of LLMs such as ChatGPT/GPT-4 in\nhealthcare. Codes and benchmarking data information are available at\nhttps://github.com/yhydhx/ChatGPT-API.\n","authors":["Zhengliang Liu","Xiaowei Yu","Lu Zhang","Zihao Wu","Chao Cao","Haixing Dai","Lin Zhao","Wei Liu","Dinggang Shen","Quanzheng Li","Tianming Liu","Dajiang Zhu","Xiang Li"],"pdf_url":"https://arxiv.org/pdf/2303.11032v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11005v1","updated":"2023-03-20T10:33:06Z","published":"2023-03-20T10:33:06Z","title":"Controllable Ancient Chinese Lyrics Generation Based on Phrase Prototype\n  Retrieving","summary":"  Generating lyrics and poems is one of the essential downstream tasks in the\nNatural Language Processing (NLP) field. Current methods have performed well in\nsome lyrics generation scenarios but need further improvements in tasks\nrequiring fine control. We propose a novel method for generating ancient\nChinese lyrics (Song Ci), a type of ancient lyrics that involves precise\ncontrol of song structure. The system is equipped with a phrase retriever and a\nphrase connector. Based on an input prompt, the phrase retriever picks phrases\nfrom a database to construct a phrase pool. The phrase connector then selects a\nseries of phrases from the phrase pool that minimizes a multi-term loss\nfunction that considers rhyme, song structure, and fluency. Experimental\nresults show that our method can generate high-quality ancient Chinese lyrics\nwhile performing well on topic and song structure control. We also expect our\napproach to be generalized to other lyrics-generating tasks.\n","authors":["Li Yi"],"pdf_url":"https://arxiv.org/pdf/2303.11005v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.01685v2","updated":"2023-03-20T10:11:41Z","published":"2022-06-03T17:01:46Z","title":"Toward a realistic model of speech processing in the brain with\n  self-supervised learning","summary":"  Several deep neural networks have recently been shown to generate activations\nsimilar to those of the brain in response to the same input. These algorithms,\nhowever, remain largely implausible: they require (1) extraordinarily large\namounts of data, (2) unobtainable supervised labels, (3) textual rather than\nraw sensory input, and / or (4) implausibly large memory (e.g. thousands of\ncontextual words). These elements highlight the need to identify algorithms\nthat, under these limitations, would suffice to account for both behavioral and\nbrain responses. Focusing on the issue of speech processing, we here\nhypothesize that self-supervised algorithms trained on the raw waveform\nconstitute a promising candidate. Specifically, we compare a recent\nself-supervised architecture, Wav2Vec 2.0, to the brain activity of 412\nEnglish, French, and Mandarin individuals recorded with functional Magnetic\nResonance Imaging (fMRI), while they listened to ~1h of audio books. Our\nresults are four-fold. First, we show that this algorithm learns brain-like\nrepresentations with as little as 600 hours of unlabelled speech -- a quantity\ncomparable to what infants can be exposed to during language acquisition.\nSecond, its functional hierarchy aligns with the cortical hierarchy of speech\nprocessing. Third, different training regimes reveal a functional\nspecialization akin to the cortex: Wav2Vec 2.0 learns sound-generic,\nspeech-specific and language-specific representations similar to those of the\nprefrontal and temporal cortices. Fourth, we confirm the similarity of this\nspecialization with the behavior of 386 additional participants. These\nelements, resulting from the largest neuroimaging benchmark to date, show how\nself-supervised learning can account for a rich organization of speech\nprocessing in the brain, and thus delineate a path to identify the laws of\nlanguage acquisition which shape the human brain.\n","authors":["Juliette Millet","Charlotte Caucheteux","Pierre Orhan","Yves Boubenec","Alexandre Gramfort","Ewan Dunbar","Christophe Pallier","Jean-Remi King"],"pdf_url":"https://arxiv.org/pdf/2206.01685v2.pdf","comment":"Accepted to NeurIPS 2022"},{"id":"http://arxiv.org/abs/2303.10974v1","updated":"2023-03-20T09:52:52Z","published":"2023-03-20T09:52:52Z","title":"Translate your gibberish: black-box adversarial attack on machine\n  translation systems","summary":"  Neural networks are deployed widely in natural language processing tasks on\nthe industrial scale, and perhaps the most often they are used as compounds of\nautomatic machine translation systems. In this work, we present a simple\napproach to fool state-of-the-art machine translation tools in the task of\ntranslation from Russian to English and vice versa. Using a novel black-box\ngradient-free tensor-based optimizer, we show that many online translation\ntools, such as Google, DeepL, and Yandex, may both produce wrong or offensive\ntranslations for nonsensical adversarial input queries and refuse to translate\nseemingly benign input phrases. This vulnerability may interfere with\nunderstanding a new language and simply worsen the user's experience while\nusing machine translation systems, and, hence, additional improvements of these\ntools are required to establish better translation.\n","authors":["Andrei Chertkov","Olga Tsymboi","Mikhail Pautov","Ivan Oseledets"],"pdf_url":"https://arxiv.org/pdf/2303.10974v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10966v1","updated":"2023-03-20T09:41:28Z","published":"2023-03-20T09:41:28Z","title":"Towards Reliable Neural Machine Translation with Consistency-Aware\n  Meta-Learning","summary":"  Neural machine translation (NMT) has achieved remarkable success in producing\nhigh-quality translations. However, current NMT systems suffer from a lack of\nreliability, as their outputs that are often affected by lexical or syntactic\nchanges in inputs, resulting in large variations in quality. This limitation\nhinders the practicality and trustworthiness of NMT. A contributing factor to\nthis problem is that NMT models trained with the one-to-one paradigm struggle\nto handle the source diversity phenomenon, where inputs with the same meaning\ncan be expressed differently. In this work, we treat this problem as a bilevel\noptimization problem and present a consistency-aware meta-learning (CAML)\nframework derived from the model-agnostic meta-learning (MAML) algorithm to\naddress it. Specifically, the NMT model with CAML (named CoNMT) first learns a\nconsistent meta representation of semantically equivalent sentences in the\nouter loop. Subsequently, a mapping from the meta representation to the output\nsentence is learned in the inner loop, allowing the NMT model to translate\nsemantically equivalent sentences to the same target sentence. We conduct\nexperiments on the NIST Chinese to English task, three WMT translation tasks,\nand the TED M2O task. The results demonstrate that CoNMT effectively improves\noverall translation quality and reliably handles diverse inputs.\n","authors":["Rongxiang Weng","Qiang Wang","Wensen Cheng","Changfeng Zhu","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.10966v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10949v1","updated":"2023-03-20T09:13:27Z","published":"2023-03-20T09:13:27Z","title":"Code-Switching Text Generation and Injection in Mandarin-English ASR","summary":"  Code-switching speech refers to a means of expression by mixing two or more\nlanguages within a single utterance. Automatic Speech Recognition (ASR) with\nEnd-to-End (E2E) modeling for such speech can be a challenging task due to the\nlack of data. In this study, we investigate text generation and injection for\nimproving the performance of an industry commonly-used streaming model,\nTransformer-Transducer (T-T), in Mandarin-English code-switching speech\nrecognition. We first propose a strategy to generate code-switching text data\nand then investigate injecting generated text into T-T model explicitly by\nText-To-Speech (TTS) conversion or implicitly by tying speech and text latent\nspaces. Experimental results on the T-T model trained with a dataset containing\n1,800 hours of real Mandarin-English code-switched speech show that our\napproaches to inject generated code-switching text significantly boost the\nperformance of T-T models, i.e., 16% relative Token-based Error Rate (TER)\nreduction averaged on three evaluation sets, and the approach of tying speech\nand text latent spaces is superior to that of TTS conversion on the evaluation\nset which contains more homogeneous data with the training set.\n","authors":["Haibin Yu","Yuxuan Hu","Yao Qian","Ma Jin","Linquan Liu","Shujie Liu","Yu Shi","Yanmin Qian","Edward Lin","Michael Zeng"],"pdf_url":"https://arxiv.org/pdf/2303.10949v1.pdf","comment":"Accepted by ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.07205v2","updated":"2023-03-20T08:59:41Z","published":"2023-02-04T04:49:17Z","title":"The Science of Detecting LLM-Generated Texts","summary":"  The emergence of large language models (LLMs) has resulted in the production\nof LLM-generated texts that is highly sophisticated and almost\nindistinguishable from texts written by humans. However, this has also sparked\nconcerns about the potential misuse of such texts, such as spreading\nmisinformation and causing disruptions in the education system. Although many\ndetection approaches have been proposed, a comprehensive understanding of the\nachievements and challenges is still lacking. This survey aims to provide an\noverview of existing LLM-generated text detection techniques and enhance the\ncontrol and regulation of language generation models. Furthermore, we emphasize\ncrucial considerations for future research, including the development of\ncomprehensive evaluation metrics and the threat posed by open-source LLMs, to\ndrive progress in the area of LLM-generated text detection.\n","authors":["Ruixiang Tang","Yu-Neng Chuang","Xia Hu"],"pdf_url":"https://arxiv.org/pdf/2303.07205v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10942v1","updated":"2023-03-20T08:54:40Z","published":"2023-03-20T08:54:40Z","title":"On-the-fly Text Retrieval for End-to-End ASR Adaptation","summary":"  End-to-end speech recognition models are improved by incorporating external\ntext sources, typically by fusion with an external language model. Such\nlanguage models have to be retrained whenever the corpus of interest changes.\nFurthermore, since they store the entire corpus in their parameters, rare words\ncan be challenging to recall. In this work, we propose augmenting a\ntransducer-based ASR model with a retrieval language model, which directly\nretrieves from an external text corpus plausible completions for a partial ASR\nhypothesis. These completions are then integrated into subsequent predictions\nby an adapter, which is trained once, so that the corpus of interest can be\nswitched without incurring the computational overhead of retraining. Our\nexperiments show that the proposed model significantly improves the performance\nof a transducer baseline on a pair of question-answering datasets. Further, it\noutperforms shallow fusion on recognition of named entities by about 7\nrelative; when the two are combined, the relative improvement increases to 13%.\n","authors":["Bolaji Yusuf","Aditya Gourav","Ankur Gandhe","Ivan Bulyko"],"pdf_url":"https://arxiv.org/pdf/2303.10942v1.pdf","comment":"Accepted to ICASSP 2023; Appendix added to include ablations that\n  could not fit into the conference 4-page limit"},{"id":"http://arxiv.org/abs/2301.06323v2","updated":"2023-03-20T08:37:45Z","published":"2023-01-16T09:27:45Z","title":"An Error-Guided Correction Model for Chinese Spelling Error Correction","summary":"  Although existing neural network approaches have achieved great success on\nChinese spelling correction, there is still room to improve. The model is\nrequired to avoid over-correction and to distinguish a correct token from its\nphonological and visually similar ones. In this paper, we propose an\nerror-guided correction model (EGCM) to improve Chinese spelling correction. By\nborrowing the powerful ability of BERT, we propose a novel zero-shot error\ndetection method to do a preliminary detection, which guides our model to\nattend more on the probably wrong tokens in encoding and to avoid modifying the\ncorrect tokens in generating. Furthermore, we introduce a new loss function to\nintegrate the error confusion set, which enables our model to distinguish\neasily misused tokens. Moreover, our model supports highly parallel decoding to\nmeet real application requirements. Experiments are conducted on widely used\nbenchmarks. Our model achieves superior performance against state-of-the-art\napproaches by a remarkable margin, on both the correction quality and\ncomputation speed.\n","authors":["Rui Sun","Xiuyu Wu","Yunfang Wu"],"pdf_url":"https://arxiv.org/pdf/2301.06323v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.06924v4","updated":"2023-03-20T08:33:18Z","published":"2022-06-14T15:43:44Z","title":"The Maximum Linear Arrangement Problem for trees under projectivity and\n  planarity","summary":"  A linear arrangement is a mapping $\\pi$ from the $n$ vertices of a graph $G$\nto $n$ distinct consecutive integers. Linear arrangements can be represented by\ndrawing the vertices along a horizontal line and drawing the edges as\nsemicircles above said line. In this setting, the length of an edge is defined\nas the absolute value of the difference between the positions of its two\nvertices in the arrangement, and the cost of an arrangement as the sum of all\nedge lengths. Here we study two variants of the Maximum Linear Arrangement\nproblem (MaxLA), which consists of finding an arrangement that maximizes the\ncost. In the planar variant for free trees, vertices have to be arranged in\nsuch a way that there are no edge crossings. In the projective variant for\nrooted trees, arrangements have to be planar and the root of the tree cannot be\ncovered by any edge. In this paper we present algorithms that are linear in\ntime and space to solve planar and projective MaxLA for trees. We also prove\nseveral properties of maximum projective and planar arrangements, and show that\ncaterpillar trees maximize planar MaxLA over all trees of a fixed size thereby\ngeneralizing a previous extremal result on trees.\n","authors":["Lluís Alemany-Puig","Juan Luis Esteban","Ramon Ferrer-i-Cancho"],"pdf_url":"https://arxiv.org/pdf/2206.06924v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.05405v4","updated":"2023-03-20T08:29:29Z","published":"2022-11-10T08:19:44Z","title":"VieCap4H-VLSP 2021: ObjectAoA-Enhancing performance of Object Relation\n  Transformer with Attention on Attention for Vietnamese image captioning","summary":"  Image captioning is currently a challenging task that requires the ability to\nboth understand visual information and use human language to describe this\nvisual information in the image. In this paper, we propose an efficient way to\nimprove the image understanding ability of transformer-based method by\nextending Object Relation Transformer architecture with Attention on Attention\nmechanism. Experiments on the VieCap4H dataset show that our proposed method\nsignificantly outperforms its original structure on both the public test and\nprivate test of the Image Captioning shared task held by VLSP.\n","authors":["Nghia Hieu Nguyen","Duong T. D. Vo","Minh-Quan Ha"],"pdf_url":"https://arxiv.org/pdf/2211.05405v4.pdf","comment":"Accepted for publishing at the VNU Journal of Science: Computer\n  Science and Communication Engineering"},{"id":"http://arxiv.org/abs/2303.10912v1","updated":"2023-03-20T07:09:26Z","published":"2023-03-20T07:09:26Z","title":"Exploring Representation Learning for Small-Footprint Keyword Spotting","summary":"  In this paper, we investigate representation learning for low-resource\nkeyword spotting (KWS). The main challenges of KWS are limited labeled data and\nlimited available device resources. To address those challenges, we explore\nrepresentation learning for KWS by self-supervised contrastive learning and\nself-training with pretrained model. First, local-global contrastive siamese\nnetworks (LGCSiam) are designed to learn similar utterance-level\nrepresentations for similar audio samplers by proposed local-global contrastive\nloss without requiring ground-truth. Second, a self-supervised pretrained\nWav2Vec 2.0 model is applied as a constraint module (WVC) to force the KWS\nmodel to learn frame-level acoustic representations. By the LGCSiam and WVC\nmodules, the proposed small-footprint KWS model can be pretrained with\nunlabeled data. Experiments on speech commands dataset show that the\nself-training WVC module and the self-supervised LGCSiam module significantly\nimprove accuracy, especially in the case of training on a small labeled\ndataset.\n","authors":["Fan Cui","Liyong Guo","Quandong Wang","Peng Gao","Yujun Wang"],"pdf_url":"https://arxiv.org/pdf/2303.10912v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10897v1","updated":"2023-03-20T06:34:22Z","published":"2023-03-20T06:34:22Z","title":"Relate auditory speech to EEG by shallow-deep attention-based network","summary":"  Electroencephalography (EEG) plays a vital role in detecting how brain\nresponses to different stimulus. In this paper, we propose a novel Shallow-Deep\nAttention-based Network (SDANet) to classify the correct auditory stimulus\nevoking the EEG signal. It adopts the Attention-based Correlation Module (ACM)\nto discover the connection between auditory speech and EEG from global aspect,\nand the Shallow-Deep Similarity Classification Module (SDSCM) to decide the\nclassification result via the embeddings learned from the shallow and deep\nlayers. Moreover, various training strategies and data augmentation are used to\nboost the model robustness. Experiments are conducted on the dataset provided\nby Auditory EEG challenge (ICASSP Signal Processing Grand Challenge 2023).\nResults show that the proposed model has a significant gain over the baseline\non the match-mismatch track.\n","authors":["Fan Cui","Liyong Guo","Lang He","Jiyao Liu","ErCheng Pei","Yujun Wang","Dongmei Jiang"],"pdf_url":"https://arxiv.org/pdf/2303.10897v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10893v1","updated":"2023-03-20T06:20:03Z","published":"2023-03-20T06:20:03Z","title":"Character, Word, or Both? Revisiting the Segmentation Granularity for\n  Chinese Pre-trained Language Models","summary":"  Pretrained language models (PLMs) have shown marvelous improvements across\nvarious NLP tasks. Most Chinese PLMs simply treat an input text as a sequence\nof characters, and completely ignore word information. Although Whole Word\nMasking can alleviate this, the semantics in words is still not well\nrepresented. In this paper, we revisit the segmentation granularity of Chinese\nPLMs. We propose a mixed-granularity Chinese BERT (MigBERT) by considering both\ncharacters and words. To achieve this, we design objective functions for\nlearning both character and word-level representations. We conduct extensive\nexperiments on various Chinese NLP tasks to evaluate existing PLMs as well as\nthe proposed MigBERT. Experimental results show that MigBERT achieves new SOTA\nperformance on all these tasks. Further analysis demonstrates that words are\nsemantically richer than characters. More interestingly, we show that MigBERT\nalso works with Japanese. Our code and model have been released\nhere~\\footnote{https://github.com/xnliang98/MigBERT}.\n","authors":["Xinnian Liang","Zefan Zhou","Hui Huang","Shuangzhi Wu","Tong Xiao","Muyun Yang","Zhoujun Li","Chao Bian"],"pdf_url":"https://arxiv.org/pdf/2303.10893v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2303.10888v1","updated":"2023-03-20T06:13:03Z","published":"2023-03-20T06:13:03Z","title":"Self-Improving-Leaderboard(SIL): A Call for Real-World Centric Natural\n  Language Processing Leaderboards","summary":"  Leaderboard systems allow researchers to objectively evaluate Natural\nLanguage Processing (NLP) models and are typically used to identify models that\nexhibit superior performance on a given task in a predetermined setting.\nHowever, we argue that evaluation on a given test dataset is just one of many\nperformance indications of the model. In this paper, we claim leaderboard\ncompetitions should also aim to identify models that exhibit the best\nperformance in a real-world setting. We highlight three issues with current\nleaderboard systems: (1) the use of a single, static test set, (2) discrepancy\nbetween testing and real-world application (3) the tendency for\nleaderboard-centric competition to be biased towards the test set. As a\nsolution, we propose a new paradigm of leaderboard systems that addresses these\nissues of current leaderboard system. Through this study, we hope to induce a\nparadigm shift towards more real -world-centric leaderboard competitions.\n","authors":["Chanjun Park","Hyeonseok Moon","Seolhwa Lee","Jaehyung Seo","Sugyeong Eo","Heuiseok Lim"],"pdf_url":"https://arxiv.org/pdf/2303.10888v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10870v1","updated":"2023-03-20T05:11:22Z","published":"2023-03-20T05:11:22Z","title":"Multi-task Transformer with Relation-attention and Type-attention for\n  Named Entity Recognition","summary":"  Named entity recognition (NER) is an important research problem in natural\nlanguage processing. There are three types of NER tasks, including flat, nested\nand discontinuous entity recognition. Most previous sequential labeling models\nare task-specific, while recent years have witnessed the rising of generative\nmodels due to the advantage of unifying all NER tasks into the seq2seq model\nframework. Although achieving promising performance, our pilot studies\ndemonstrate that existing generative models are ineffective at detecting entity\nboundaries and estimating entity types. This paper proposes a multi-task\nTransformer, which incorporates an entity boundary detection task into the\nnamed entity recognition task. More concretely, we achieve entity boundary\ndetection by classifying the relations between tokens within the sentence. To\nimprove the accuracy of entity-type mapping during decoding, we adopt an\nexternal knowledge base to calculate the prior entity-type distributions and\nthen incorporate the information into the model via the self and\ncross-attention mechanisms. We perform experiments on an extensive set of NER\nbenchmarks, including two flat, three nested, and three discontinuous NER\ndatasets. Experimental results show that our approach considerably improves the\ngenerative NER model's performance.\n","authors":["Ying Mo","Hongyin Tang","Jiahao Liu","Qifan Wang","Zenglin Xu","Jingang Wang","Wei Wu","Zhoujun Li"],"pdf_url":"https://arxiv.org/pdf/2303.10870v1.pdf","comment":"5 pages,accepted ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.10868v1","updated":"2023-03-20T05:07:41Z","published":"2023-03-20T05:07:41Z","title":"Retrieving Multimodal Information for Augmented Generation: A Survey","summary":"  In this survey, we review methods that retrieve multimodal knowledge to\nassist and augment generative models. This group of works focuses on retrieving\ngrounding contexts from external sources, including images, codes, tables,\ngraphs, and audio. As multimodal learning and generative AI have become more\nand more impactful, such retrieval augmentation offers a promising solution to\nimportant concerns such as factuality, reasoning, interpretability, and\nrobustness. We provide an in-depth review of retrieval-augmented generation in\ndifferent modalities and discuss potential future directions. As this is an\nemerging field, we continue to add new papers and methods.\n","authors":["Ruochen Zhao","Hailin Chen","Weishi Wang","Fangkai Jiao","Xuan Long Do","Chengwei Qin","Bosheng Ding","Xiaobao Guo","Minzhi Li","Xingxuan Li","Shafiq Joty"],"pdf_url":"https://arxiv.org/pdf/2303.10868v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.09004v3","updated":"2023-03-20T04:49:10Z","published":"2022-09-19T13:28:32Z","title":"EcoFormer: Energy-Saving Attention with Linear Complexity","summary":"  Transformer is a transformative framework that models sequential data and has\nachieved remarkable performance on a wide range of tasks, but with high\ncomputational and energy cost. To improve its efficiency, a popular choice is\nto compress the models via binarization which constrains the floating-point\nvalues into binary ones to save resource consumption owing to cheap bitwise\noperations significantly. However, existing binarization methods only aim at\nminimizing the information loss for the input distribution statistically, while\nignoring the pairwise similarity modeling at the core of the attention. To this\nend, we propose a new binarization paradigm customized to high-dimensional\nsoftmax attention via kernelized hashing, called EcoFormer, to map the original\nqueries and keys into low-dimensional binary codes in Hamming space. The\nkernelized hash functions are learned to match the ground-truth similarity\nrelations extracted from the attention map in a self-supervised way. Based on\nthe equivalence between the inner product of binary codes and the Hamming\ndistance as well as the associative property of matrix multiplication, we can\napproximate the attention in linear complexity by expressing it as a\ndot-product of binary codes. Moreover, the compact binary representations of\nqueries and keys enable us to replace most of the expensive multiply-accumulate\noperations in attention with simple accumulations to save considerable on-chip\nenergy footprint on edge devices. Extensive experiments on both vision and\nlanguage tasks show that EcoFormer consistently achieves comparable performance\nwith standard attentions while consuming much fewer resources. For example,\nbased on PVTv2-B0 and ImageNet-1K, Ecoformer achieves a 73% on-chip energy\nfootprint reduction with only a 0.33% performance drop compared to the standard\nattention. Code is available at https://github.com/ziplab/EcoFormer.\n","authors":["Jing Liu","Zizheng Pan","Haoyu He","Jianfei Cai","Bohan Zhuang"],"pdf_url":"https://arxiv.org/pdf/2209.09004v3.pdf","comment":"NeurIPS 2022 camera ready; First two authors contributed equally"},{"id":"http://arxiv.org/abs/2303.10845v1","updated":"2023-03-20T03:39:27Z","published":"2023-03-20T03:39:27Z","title":"PanGu-Σ: Towards Trillion Parameter Language Model with Sparse\n  Heterogeneous Computing","summary":"  The scaling of large language models has greatly improved natural language\nunderstanding, generation, and reasoning. In this work, we develop a system\nthat trained a trillion-parameter language model on a cluster of Ascend 910 AI\nprocessors and MindSpore framework, and present the language model with 1.085T\nparameters named PanGu-{\\Sigma}. With parameter inherent from PanGu-{\\alpha},\nwe extend the dense Transformer model to sparse one with Random Routed Experts\n(RRE), and efficiently train the model over 329B tokens by using Expert\nComputation and Storage Separation(ECSS). This resulted in a 6.3x increase in\ntraining throughput through heterogeneous computing. Our experimental findings\nshow that PanGu-{\\Sigma} provides state-of-the-art performance in zero-shot\nlearning of various Chinese NLP downstream tasks. Moreover, it demonstrates\nstrong abilities when fine-tuned in application data of open-domain dialogue,\nquestion answering, machine translation and code generation.\n","authors":["Xiaozhe Ren","Pingyi Zhou","Xinfan Meng","Xinjing Huang","Yadao Wang","Weichao Wang","Pengfei Li","Xiaoda Zhang","Alexander Podolskiy","Grigory Arshinov","Andrey Bout","Irina Piontkovskaya","Jiansheng Wei","Xin Jiang","Teng Su","Qun Liu","Jun Yao"],"pdf_url":"https://arxiv.org/pdf/2303.10845v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.15042v3","updated":"2023-03-20T01:33:23Z","published":"2022-10-26T21:18:31Z","title":"Privately Fine-Tuning Large Language Models with Differential Privacy","summary":"  Pre-trained Large Language Models (LLMs) are an integral part of modern AI\nthat have led to breakthrough performances in complex AI tasks. Major AI\ncompanies with expensive infrastructures are able to develop and train these\nlarge models with billions and millions of parameters from scratch. Third\nparties, researchers, and practitioners are increasingly adopting these\npre-trained models and fine-tuning them on their private data to accomplish\ntheir downstream AI tasks. However, it has been shown that an adversary can\nextract/reconstruct the exact training samples from these LLMs, which can lead\nto revealing personally identifiable information. The issue has raised deep\nconcerns about the privacy of LLMs. Differential privacy (DP) provides a\nrigorous framework that allows adding noise in the process of training or\nfine-tuning LLMs such that extracting the training data becomes infeasible\n(i.e., with a cryptographically small success probability). While the\ntheoretical privacy guarantees offered in most extant studies assume learning\nmodels from scratch through many training iterations in an asymptotic setting,\nthis assumption does not hold in fine-tuning scenarios in which the number of\ntraining iterations is significantly smaller. To address the gap, we present\n\\ewtune, a DP framework for fine-tuning LLMs based on Edgeworth accountant with\nfinite-sample privacy guarantees. Our results across four well-established\nnatural language understanding (NLU) tasks show that while \\ewtune~adds privacy\nguarantees to LLM fine-tuning process, it directly contributes to decreasing\nthe induced noise to up to 5.6\\% and improves the state-of-the-art LLMs\nperformance by up to 1.1\\% across all NLU tasks. We have open-sourced our\nimplementations for wide adoption and public testing purposes.\n","authors":["Rouzbeh Behnia","Mohamamdreza Ebrahimi","Jason Pacheco","Balaji Padmanabhan"],"pdf_url":"https://arxiv.org/pdf/2210.15042v3.pdf","comment":"Publised at IEEE ICDM Workshop on Machine Learning for Cybersecurity\n  (MLC) 2022"},{"id":"http://arxiv.org/abs/2303.11504v1","updated":"2023-03-20T23:54:26Z","published":"2023-03-20T23:54:26Z","title":"Language Model Behavior: A Comprehensive Survey","summary":"  Transformer language models have received widespread public attention, yet\ntheir generated text is often surprising even to NLP researchers. In this\nsurvey, we discuss over 250 recent studies of English language model behavior\nbefore task-specific fine-tuning. Language models possess basic capabilities in\nsyntax, semantics, pragmatics, world knowledge, and reasoning, but these\ncapabilities are sensitive to specific inputs and surface features. Despite\ndramatic increases in generated text quality as models scale to hundreds of\nbillions of parameters, the models are still prone to unfactual responses,\ncommonsense errors, memorized text, and social biases. Many of these weaknesses\ncan be framed as over-generalizations or under-generalizations of learned\npatterns in text. We synthesize recent results to highlight what is currently\nknown about what large language models can and cannot do.\n","authors":["Tyler A. Chang","Benjamin K. Bergen"],"pdf_url":"https://arxiv.org/pdf/2303.11504v1.pdf","comment":"31 pages"},{"id":"http://arxiv.org/abs/2210.09306v2","updated":"2023-03-20T21:33:41Z","published":"2022-10-17T17:59:49Z","title":"Mitigating Covertly Unsafe Text within Natural Language Systems","summary":"  An increasingly prevalent problem for intelligent technologies is text\nsafety, as uncontrolled systems may generate recommendations to their users\nthat lead to injury or life-threatening consequences. However, the degree of\nexplicitness of a generated statement that can cause physical harm varies. In\nthis paper, we distinguish types of text that can lead to physical harm and\nestablish one particularly underexplored category: covertly unsafe text. Then,\nwe further break down this category with respect to the system's information\nand discuss solutions to mitigate the generation of text in each of these\nsubcategories. Ultimately, our work defines the problem of covertly unsafe\nlanguage that causes physical harm and argues that this subtle yet dangerous\nissue needs to be prioritized by stakeholders and regulators. We highlight\nmitigation strategies to inspire future researchers to tackle this challenging\nproblem and help improve safety within smart systems.\n","authors":["Alex Mei","Anisha Kabir","Sharon Levy","Melanie Subbiah","Emily Allaway","John Judge","Desmond Patton","Bruce Bimber","Kathleen McKeown","William Yang Wang"],"pdf_url":"https://arxiv.org/pdf/2210.09306v2.pdf","comment":"In Findings of the 2022 Conference on Empirical Methods in Natural\n  Language Processing"},{"id":"http://arxiv.org/abs/2303.11455v1","updated":"2023-03-20T21:14:06Z","published":"2023-03-20T21:14:06Z","title":"Large Language Models and Simple, Stupid Bugs","summary":"  With the advent of powerful neural language models, AI-based systems to\nassist developers in coding tasks are becoming widely available; Copilot is one\nsuch system. Copilot uses Codex, a large language model (LLM), to complete code\nconditioned on a preceding \"prompt\". Codex, however, is trained on public\nGitHub repositories, viz., on code that may include bugs and vulnerabilities.\nPrevious studies [1], [2] show Codex reproduces vulnerabilities seen in\ntraining. In this study, we examine how prone Codex is to generate an\ninteresting bug category, single statement bugs, commonly referred to as\nsimple, stupid bugs or SStuBs in the MSR community. We find that Codex and\nsimilar LLMs do help avoid some SStuBs, but do produce known, verbatim SStuBs\nas much as 2x as likely than known, verbatim correct code. We explore the\nconsequences of the Codex generated SStuBs and propose avoidance strategies\nthat suggest the possibility of reducing the production of known, verbatim\nSStubs, and increase the possibility of producing known, verbatim fixes.\n","authors":["Kevin Jesse","Toufique Ahmed","Premkumar T. Devanbu","Emily Morgan"],"pdf_url":"https://arxiv.org/pdf/2303.11455v1.pdf","comment":"Accepted at International Conference on Mining Software Repositories\n  (MSR-2023)"},{"id":"http://arxiv.org/abs/2303.09093v2","updated":"2023-03-20T20:40:15Z","published":"2023-03-16T05:36:38Z","title":"GLEN: General-Purpose Event Detection for Thousands of Types","summary":"  The development of event extraction systems has been hindered by the absence\nof wide-coverage, large-scale datasets. To make event extraction systems more\naccessible, we build a general-purpose event detection dataset GLEN, which\ncovers 3,465 different event types, making it over 20x larger in ontology than\nany current dataset. GLEN is created by utilizing the DWD Overlay, which\nprovides a mapping between Wikidata Qnodes and PropBank rolesets. This enables\nus to use the abundant existing annotation for PropBank as distant supervision.\nIn addition, we also propose a new multi-stage event detection model\nspecifically designed to handle the large ontology size and partial labels in\nGLEN. We show that our model exhibits superior performance (~10% F1 gain)\ncompared to both conventional classification baselines and newer\ndefinition-based models. Finally, we perform error analysis and show that label\nnoise is still the largest challenge for improving performance.\n","authors":["Qiusi Zhan","Sha Li","Kathryn Conger","Martha Palmer","Heng Ji","Jiawei Han"],"pdf_url":"https://arxiv.org/pdf/2303.09093v2.pdf","comment":"The first two authors contributed equally. (15 pages, 11 figures)"},{"id":"http://arxiv.org/abs/2303.11436v1","updated":"2023-03-20T20:28:26Z","published":"2023-03-20T20:28:26Z","title":"Mind meets machine: Unravelling GPT-4's cognitive psychology","summary":"  Commonsense reasoning is a basic ingredient of intelligence in humans,\nempowering the ability to deduce conclusions based on the observations of\nsurroundings. Large language models (LLMs) are emerging as potent tools\nincreasingly capable of performing human-level tasks. The recent development in\nthe form of GPT-4 and its demonstrated success in tasks complex to humans such\nas medical exam, bar exam and others has led to an increased confidence in the\nLLMs to become perfect instruments of intelligence. Though, the GPT-4 paper has\nshown performance on some common sense reasoning tasks, a comprehensive\nassessment of GPT-4 on common sense reasoning tasks, particularly on the\nexisting well-established datasets is missing. In this study, we focus on the\nevaluation of GPT-4's performance on a set of common sense reasoning questions\nfrom the widely used CommonsenseQA dataset along with tools from cognitive\npsychology. In doing so, we understand how GPT-4 processes and integrates\ncommon sense knowledge with contextual information, providing insight into the\nunderlying cognitive processes that enable its ability to generate common sense\nresponses. We show that GPT-4 exhibits a high level of accuracy in answering\ncommon sense questions, outperforming its predecessor, GPT-3 and GPT-3.5. We\nshow that the accuracy of GPT-4 on CommonSenseQA is 83 % and it has been shown\nin the original study that human accuracy over the same data was 89 %.\nAlthough, GPT-4 falls short of the human performance, it is a substantial\nimprovement from the original 56.5 % in the original language model used by the\nCommonSenseQA study. Our results strengthen the already available assessments\nand confidence on GPT-4's common sense reasoning abilities which have\nsignificant potential to revolutionize the field of AI, by enabling machines to\nbridge the gap between human and machine reasoning.\n","authors":[" Sifatkaur","Manmeet Singh","Vaisakh SB","Neetiraj Malviya"],"pdf_url":"https://arxiv.org/pdf/2303.11436v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11403v1","updated":"2023-03-20T19:20:34Z","published":"2023-03-20T19:20:34Z","title":"eP-ALM: Efficient Perceptual Augmentation of Language Models","summary":"  Large Language Models (LLMs) have so far impressed the world, with\nunprecedented capabilities that emerge in models at large scales. On the vision\nside, transformer models (i.e., ViT) are following the same trend, achieving\nthe best performance on challenging benchmarks. With the abundance of such\nunimodal models, a natural question arises; do we need also to follow this\ntrend to tackle multimodal tasks? In this work, we propose to rather direct\neffort to efficient adaptations of existing models, and propose to augment\nLanguage Models with perception. Existing approaches for adapting pretrained\nmodels for vision-language tasks still rely on several key components that\nhinder their efficiency. In particular, they still train a large number of\nparameters, rely on large multimodal pretraining, use encoders (e.g., CLIP)\ntrained on huge image-text datasets, and add significant inference overhead. In\naddition, most of these approaches have focused on Zero-Shot and In Context\nLearning, with little to no effort on direct finetuning. We investigate the\nminimal computational effort needed to adapt unimodal models for multimodal\ntasks and propose a new challenging setup, alongside different approaches, that\nefficiently adapts unimodal pretrained models. We show that by freezing more\nthan 99\\% of total parameters, training only one linear projection layer, and\nprepending only one trainable token, our approach (dubbed eP-ALM) significantly\noutperforms other baselines on VQA and Captioning across Image, Video, and\nAudio modalities, following the proposed setup. The code will be available\nhere: https://github.com/mshukor/eP-ALM.\n","authors":["Mustafa Shukor","Corentin Dancette","Matthieu Cord"],"pdf_url":"https://arxiv.org/pdf/2303.11403v1.pdf","comment":"Code: https://github.com/mshukor/eP-ALM"},{"id":"http://arxiv.org/abs/2303.11381v1","updated":"2023-03-20T18:31:47Z","published":"2023-03-20T18:31:47Z","title":"MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action","summary":"  We propose MM-REACT, a system paradigm that integrates ChatGPT with a pool of\nvision experts to achieve multimodal reasoning and action. In this paper, we\ndefine and explore a comprehensive list of advanced vision tasks that are\nintriguing to solve, but may exceed the capabilities of existing vision and\nvision-language models. To achieve such advanced visual intelligence, MM-REACT\nintroduces a textual prompt design that can represent text descriptions,\ntextualized spatial coordinates, and aligned file names for dense visual\nsignals such as images and videos. MM-REACT's prompt design allows language\nmodels to accept, associate, and process multimodal information, thereby\nfacilitating the synergetic combination of ChatGPT and various vision experts.\nZero-shot experiments demonstrate MM-REACT's effectiveness in addressing the\nspecified capabilities of interests and its wide application in different\nscenarios that require advanced visual understanding. Furthermore, we discuss\nand compare MM-REACT's system paradigm with an alternative approach that\nextends language models for multimodal scenarios through joint finetuning.\nCode, demo, video, and visualization are available at\nhttps://multimodal-react.github.io/\n","authors":["Zhengyuan Yang","Linjie Li","Jianfeng Wang","Kevin Lin","Ehsan Azarnasab","Faisal Ahmed","Zicheng Liu","Ce Liu","Michael Zeng","Lijuan Wang"],"pdf_url":"https://arxiv.org/pdf/2303.11381v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11366v1","updated":"2023-03-20T18:08:50Z","published":"2023-03-20T18:08:50Z","title":"Reflexion: an autonomous agent with dynamic memory and self-reflection","summary":"  Recent advancements in decision-making large language model (LLM) agents have\ndemonstrated impressive performance across various benchmarks. However, these\nstate-of-the-art approaches typically necessitate internal model fine-tuning,\nexternal model fine-tuning, or policy optimization over a defined state space.\nImplementing these methods can prove challenging due to the scarcity of\nhigh-quality training data or the lack of well-defined state space. Moreover,\nthese agents do not possess certain qualities inherent to human decision-making\nprocesses, specifically the ability to learn from mistakes. Self-reflection\nallows humans to efficiently solve novel problems through a process of trial\nand error. Building on recent research, we propose Reflexion, an approach that\nendows an agent with dynamic memory and self-reflection capabilities to enhance\nits existing reasoning trace and task-specific action choice abilities. To\nachieve full automation, we introduce a straightforward yet effective heuristic\nthat enables the agent to pinpoint hallucination instances, avoid repetition in\naction sequences, and, in some environments, construct an internal memory map\nof the given environment. To assess our approach, we evaluate the agent's\nability to complete decision-making tasks in AlfWorld environments and\nknowledge-intensive, search-based question-and-answer tasks in HotPotQA\nenvironments. We observe success rates of 97% and 51%, respectively, and\nprovide a discussion on the emergent property of self-reflection.\n","authors":["Noah Shinn","Beck Labash","Ashwin Gopinath"],"pdf_url":"https://arxiv.org/pdf/2303.11366v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11945v1","updated":"2023-03-20T06:19:49Z","published":"2023-03-20T06:19:49Z","title":"Unsupervised Cross-Domain Rumor Detection with Contrastive Learning and\n  Cross-Attention","summary":"  Massive rumors usually appear along with breaking news or trending topics,\nseriously hindering the truth. Existing rumor detection methods are mostly\nfocused on the same domain, and thus have poor performance in cross-domain\nscenarios due to domain shift. In this work, we propose an end-to-end\ninstance-wise and prototype-wise contrastive learning model with a\ncross-attention mechanism for cross-domain rumor detection. The model not only\nperforms cross-domain feature alignment but also enforces target samples to\nalign with the corresponding prototypes of a given source domain. Since target\nlabels in a target domain are unavailable, we use a clustering-based approach\nwith carefully initialized centers by a batch of source domain samples to\nproduce pseudo labels. Moreover, we use a cross-attention mechanism on a pair\nof source data and target data with the same labels to learn domain-invariant\nrepresentations. Because the samples in a domain pair tend to express similar\nsemantic patterns, especially on the people's attitudes (e.g., supporting or\ndenying) towards the same category of rumors, the discrepancy between a pair of\nthe source domain and target domain will be decreased. We conduct experiments\non four groups of cross-domain datasets and show that our proposed model\nachieves state-of-the-art performance.\n","authors":["Hongyan Ran","Caiyan Jia"],"pdf_url":"https://arxiv.org/pdf/2303.11945v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13314v1","updated":"2023-03-20T17:05:13Z","published":"2023-03-20T17:05:13Z","title":"Leveraging Foundation Models for Clinical Text Analysis","summary":"  Infectious diseases are a significant public health concern globally, and\nextracting relevant information from scientific literature can facilitate the\ndevelopment of effective prevention and treatment strategies. However, the\nlarge amount of clinical data available presents a challenge for information\nextraction. To address this challenge, this study proposes a natural language\nprocessing (NLP) framework that uses a pre-trained transformer model fine-tuned\non task-specific data to extract key information related to infectious diseases\nfrom free-text clinical data. The proposed framework includes three components:\na data layer for preparing datasets from clinical texts, a foundation model\nlayer for entity extraction, and an assessment layer for performance analysis.\nThe results of the evaluation indicate that the proposed method outperforms\nstandard methods, and leveraging prior knowledge through the pre-trained\ntransformer model makes it useful for investigating other infectious diseases\nin the future.\n","authors":["Shaina Raza","Syed Raza Bashir"],"pdf_url":"https://arxiv.org/pdf/2303.13314v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13475v1","updated":"2023-03-20T16:53:36Z","published":"2023-03-20T16:53:36Z","title":"Learning Semantic Text Similarity to rank Hypernyms of Financial Terms","summary":"  Over the years, there has been a paradigm shift in how users access financial\nservices. With the advancement of digitalization more users have been\npreferring the online mode of performing financial activities. This has led to\nthe generation of a huge volume of financial content. Most investors prefer to\ngo through these contents before making decisions. Every industry has terms\nthat are specific to the domain it operates in. Banking and Financial Services\nare not an exception to this. In order to fully comprehend these contents, one\nneeds to have a thorough understanding of the financial terms. Getting a basic\nidea about a term becomes easy when it is explained with the help of the broad\ncategory to which it belongs. This broad category is referred to as hypernym.\nFor example, \"bond\" is a hypernym of the financial term \"alternative\ndebenture\". In this paper, we propose a system capable of extracting and\nranking hypernyms for a given financial term. The system has been trained with\nfinancial text corpora obtained from various sources like DBpedia [4],\nInvestopedia, Financial Industry Business Ontology (FIBO), prospectus and so\non. Embeddings of these terms have been extracted using FinBERT [3], FinISH [1]\nand fine-tuned using SentenceBERT [54]. A novel approach has been used to\naugment the training set with negative samples. It uses the hierarchy present\nin FIBO. Finally, we benchmark the system performance with that of the existing\nones. We establish that it performs better than the existing ones and is also\nscalable.\n","authors":["Sohom Ghosh","Ankush Chopra","Sudip Kumar Naskar"],"pdf_url":"https://arxiv.org/pdf/2303.13475v1.pdf","comment":"Our code base:\n  https://github.com/sohomghosh/FinSim_Financial_Hypernym_detection"},{"id":"http://arxiv.org/abs/2303.13375v1","updated":"2023-03-20T16:18:38Z","published":"2023-03-20T16:18:38Z","title":"Capabilities of GPT-4 on Medical Challenge Problems","summary":"  Large language models (LLMs) have demonstrated remarkable capabilities in\nnatural language understanding and generation across various domains, including\nmedicine. We present a comprehensive evaluation of GPT-4, a state-of-the-art\nLLM, on medical competency examinations and benchmark datasets. GPT-4 is a\ngeneral-purpose model that is not specialized for medical problems through\ntraining or engineered to solve clinical tasks. Our analysis covers two sets of\nofficial practice materials for the USMLE, a three-step examination program\nused to assess clinical competency and grant licensure in the United States. We\nalso evaluate performance on the MultiMedQA suite of benchmark datasets. Beyond\nmeasuring model performance, experiments were conducted to investigate the\ninfluence of test questions containing both text and images on model\nperformance, probe for memorization of content during training, and study\nprobability calibration, which is of critical importance in high-stakes\napplications like medicine. Our results show that GPT-4, without any\nspecialized prompt crafting, exceeds the passing score on USMLE by over 20\npoints and outperforms earlier general-purpose models (GPT-3.5) as well as\nmodels specifically fine-tuned on medical knowledge (Med-PaLM, a prompt-tuned\nversion of Flan-PaLM 540B). In addition, GPT-4 is significantly better\ncalibrated than GPT-3.5, demonstrating a much-improved ability to predict the\nlikelihood that its answers are correct. We also explore the behavior of the\nmodel qualitatively through a case study that shows the ability of GPT-4 to\nexplain medical reasoning, personalize explanations to students, and\ninteractively craft new counterfactual scenarios around a medical case.\nImplications of the findings are discussed for potential uses of GPT-4 in\nmedical education, assessment, and clinical practice, with appropriate\nattention to challenges of accuracy and safety.\n","authors":["Harsha Nori","Nicholas King","Scott Mayer McKinney","Dean Carignan","Eric Horvitz"],"pdf_url":"https://arxiv.org/pdf/2303.13375v1.pdf","comment":"33 pages, 15 figures"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2303.11331v1","updated":"2023-03-20T17:59:59Z","published":"2023-03-20T17:59:59Z","title":"EVA-02: A Visual Representation for Neon Genesis","summary":"  We launch EVA-02, a next-generation Transformer-based visual representation\npre-trained to reconstruct strong and robust language-aligned vision features\nvia masked image modeling. With an updated plain Transformer architecture as\nwell as extensive pre-training from an open & accessible giant CLIP vision\nencoder, EVA-02 demonstrates superior performance compared to prior\nstate-of-the-art approaches across various representative vision tasks, while\nutilizing significantly fewer parameters and compute budgets. Notably, using\nexclusively publicly accessible training data, EVA-02 with only 304M parameters\nachieves a phenomenal 90.0 fine-tuning top-1 accuracy on ImageNet-1K val set.\nAdditionally, our EVA-02-CLIP can reach up to 80.4 zero-shot top-1 on\nImageNet-1K, outperforming the previous largest & best open-sourced CLIP with\nonly ~1/6 parameters and ~1/6 image-text training data. We offer four EVA-02\nvariants in various model sizes, ranging from 6M to 304M parameters, all with\nimpressive performance. To facilitate open access and open research, we release\nthe complete suite of EVA-02 to the community at\nhttps://github.com/baaivision/EVA/tree/master/EVA-02.\n","authors":["Yuxin Fang","Quan Sun","Xinggang Wang","Tiejun Huang","Xinlong Wang","Yue Cao"],"pdf_url":"https://arxiv.org/pdf/2303.11331v1.pdf","comment":"To Asuka. Code & Models:\n  https://github.com/baaivision/EVA/tree/master/EVA-02"},{"id":"http://arxiv.org/abs/2303.11330v1","updated":"2023-03-20T17:59:58Z","published":"2023-03-20T17:59:58Z","title":"Legs as Manipulator: Pushing Quadrupedal Agility Beyond Locomotion","summary":"  Locomotion has seen dramatic progress for walking or running across\nchallenging terrains. However, robotic quadrupeds are still far behind their\nbiological counterparts, such as dogs, which display a variety of agile skills\nand can use the legs beyond locomotion to perform several basic manipulation\ntasks like interacting with objects and climbing. In this paper, we take a step\ntowards bridging this gap by training quadruped robots not only to walk but\nalso to use the front legs to climb walls, press buttons, and perform object\ninteraction in the real world. To handle this challenging optimization, we\ndecouple the skill learning broadly into locomotion, which involves anything\nthat involves movement whether via walking or climbing a wall, and\nmanipulation, which involves using one leg to interact while balancing on the\nother three legs. These skills are trained in simulation using curriculum and\ntransferred to the real world using our proposed sim2real variant that builds\nupon recent locomotion success. Finally, we combine these skills into a robust\nlong-term plan by learning a behavior tree that encodes a high-level task\nhierarchy from one clean expert demonstration. We evaluate our method in both\nsimulation and real-world showing successful executions of both short as well\nas long-range tasks and how robustness helps confront external perturbations.\nVideos at https://robot-skills.github.io\n","authors":["Xuxin Cheng","Ashish Kumar","Deepak Pathak"],"pdf_url":"https://arxiv.org/pdf/2303.11330v1.pdf","comment":"Accepted at ICRA 2023. Videos at https://robot-skills.github.io"},{"id":"http://arxiv.org/abs/2303.11329v1","updated":"2023-03-20T17:59:55Z","published":"2023-03-20T17:59:55Z","title":"Sound Localization from Motion: Jointly Learning Sound Direction and\n  Camera Rotation","summary":"  The images and sounds that we perceive undergo subtle but geometrically\nconsistent changes as we rotate our heads. In this paper, we use these cues to\nsolve a problem we call Sound Localization from Motion (SLfM): jointly\nestimating camera rotation and localizing sound sources. We learn to solve\nthese tasks solely through self-supervision. A visual model predicts camera\nrotation from a pair of images, while an audio model predicts the direction of\nsound sources from binaural sounds. We train these models to generate\npredictions that agree with one another. At test time, the models can be\ndeployed independently. To obtain a feature representation that is well-suited\nto solving this challenging problem, we also propose a method for learning an\naudio-visual representation through cross-view binauralization: estimating\nbinaural sound from one view, given images and sound from another. Our model\ncan successfully estimate accurate rotations on both real and synthetic scenes,\nand localize sound sources with accuracy competitive with state-of-the-art\nself-supervised approaches. Project site: https://ificl.github.io/SLfM/\n","authors":["Ziyang Chen","Shengyi Qian","Andrew Owens"],"pdf_url":"https://arxiv.org/pdf/2303.11329v1.pdf","comment":"Project site: https://ificl.github.io/SLfM/"},{"id":"http://arxiv.org/abs/2303.11328v1","updated":"2023-03-20T17:59:50Z","published":"2023-03-20T17:59:50Z","title":"Zero-1-to-3: Zero-shot One Image to 3D Object","summary":"  We introduce Zero-1-to-3, a framework for changing the camera viewpoint of an\nobject given just a single RGB image. To perform novel view synthesis in this\nunder-constrained setting, we capitalize on the geometric priors that\nlarge-scale diffusion models learn about natural images. Our conditional\ndiffusion model uses a synthetic dataset to learn controls of the relative\ncamera viewpoint, which allow new images to be generated of the same object\nunder a specified camera transformation. Even though it is trained on a\nsynthetic dataset, our model retains a strong zero-shot generalization ability\nto out-of-distribution datasets as well as in-the-wild images, including\nimpressionist paintings. Our viewpoint-conditioned diffusion approach can\nfurther be used for the task of 3D reconstruction from a single image.\nQualitative and quantitative experiments show that our method significantly\noutperforms state-of-the-art single-view 3D reconstruction and novel view\nsynthesis models by leveraging Internet-scale pre-training.\n","authors":["Ruoshi Liu","Rundi Wu","Basile Van Hoorick","Pavel Tokmakov","Sergey Zakharov","Carl Vondrick"],"pdf_url":"https://arxiv.org/pdf/2303.11328v1.pdf","comment":"Website: https://zero123.cs.columbia.edu/"},{"id":"http://arxiv.org/abs/2303.11327v1","updated":"2023-03-20T17:59:49Z","published":"2023-03-20T17:59:49Z","title":"3D Concept Learning and Reasoning from Multi-View Images","summary":"  Humans are able to accurately reason in 3D by gathering multi-view\nobservations of the surrounding world. Inspired by this insight, we introduce a\nnew large-scale benchmark for 3D multi-view visual question answering\n(3DMV-VQA). This dataset is collected by an embodied agent actively moving and\ncapturing RGB images in an environment using the Habitat simulator. In total,\nit consists of approximately 5k scenes, 600k images, paired with 50k questions.\nWe evaluate various state-of-the-art models for visual reasoning on our\nbenchmark and find that they all perform poorly. We suggest that a principled\napproach for 3D reasoning from multi-view images should be to infer a compact\n3D representation of the world from the multi-view images, which is further\ngrounded on open-vocabulary semantic concepts, and then to execute reasoning on\nthese 3D representations. As the first step towards this approach, we propose a\nnovel 3D concept learning and reasoning (3D-CLR) framework that seamlessly\ncombines these components via neural fields, 2D pre-trained vision-language\nmodels, and neural reasoning operators. Experimental results suggest that our\nframework outperforms baseline models by a large margin, but the challenge\nremains largely unsolved. We further perform an in-depth analysis of the\nchallenges and highlight potential future directions.\n","authors":["Yining Hong","Chunru Lin","Yilun Du","Zhenfang Chen","Joshua B. Tenenbaum","Chuang Gan"],"pdf_url":"https://arxiv.org/pdf/2303.11327v1.pdf","comment":"CVPR 2023. Project page: https://vis-www.cs.umass.edu/3d-clr/"},{"id":"http://arxiv.org/abs/2303.11325v1","updated":"2023-03-20T17:59:03Z","published":"2023-03-20T17:59:03Z","title":"Towards Better 3D Knowledge Transfer via Masked Image Modeling for\n  Multi-view 3D Understanding","summary":"  Multi-view camera-based 3D detection is a challenging problem in computer\nvision. Recent works leverage a pretrained LiDAR detection model to transfer\nknowledge to a camera-based student network. However, we argue that there is a\nmajor domain gap between the LiDAR BEV features and the camera-based BEV\nfeatures, as they have different characteristics and are derived from different\nsources. In this paper, we propose Geometry Enhanced Masked Image Modeling\n(GeoMIM) to transfer the knowledge of the LiDAR model in a pretrain-finetune\nparadigm for improving the multi-view camera-based 3D detection. GeoMIM is a\nmulti-camera vision transformer with Cross-View Attention (CVA) blocks that\nuses LiDAR BEV features encoded by the pretrained BEV model as learning\ntargets. During pretraining, GeoMIM's decoder has a semantic branch completing\ndense perspective-view features and the other geometry branch reconstructing\ndense perspective-view depth maps. The depth branch is designed to be\ncamera-aware by inputting the camera's parameters for better transfer\ncapability. Extensive results demonstrate that GeoMIM outperforms existing\nmethods on nuScenes benchmark, achieving state-of-the-art performance for\ncamera-based 3D object detection and 3D segmentation.\n","authors":["Jihao Liu","Tai Wang","Boxiao Liu","Qihang Zhang","Yu Liu","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2303.11325v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11324v1","updated":"2023-03-20T17:58:48Z","published":"2023-03-20T17:58:48Z","title":"Open-vocabulary Panoptic Segmentation with Embedding Modulation","summary":"  Open-vocabulary image segmentation is attracting increasing attention due to\nits critical applications in the real world. Traditional closed-vocabulary\nsegmentation methods are not able to characterize novel objects, whereas\nseveral recent open-vocabulary attempts obtain unsatisfactory results, i.e.,\nnotable performance reduction on the closed vocabulary and massive demand for\nextra data. To this end, we propose OPSNet, an omnipotent and data-efficient\nframework for Open-vocabulary Panoptic Segmentation. Specifically, the\nexquisitely designed Embedding Modulation module, together with several\nmeticulous components, enables adequate embedding enhancement and information\nexchange between the segmentation model and the visual-linguistic well-aligned\nCLIP encoder, resulting in superior segmentation performance under both open-\nand closed-vocabulary settings with much fewer need of additional data.\nExtensive experimental evaluations are conducted across multiple datasets\n(e.g., COCO, ADE20K, Cityscapes, and PascalContext) under various\ncircumstances, where the proposed OPSNet achieves state-of-the-art results,\nwhich demonstrates the effectiveness and generality of the proposed approach.\nThe code and trained models will be made publicly available.\n","authors":["Xi Chen","Shuang Li","Ser-Nam Lim","Antonio Torralba","Hengshuang Zhao"],"pdf_url":"https://arxiv.org/pdf/2303.11324v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11320v1","updated":"2023-03-20T17:57:03Z","published":"2023-03-20T17:57:03Z","title":"ScribbleSeg: Scribble-based Interactive Image Segmentation","summary":"  Interactive segmentation enables users to extract masks by providing simple\nannotations to indicate the target, such as boxes, clicks, or scribbles. Among\nthese interaction formats, scribbles are the most flexible as they can be of\narbitrary shapes and sizes. This enables scribbles to provide more indications\nof the target object. However, previous works mainly focus on click-based\nconfiguration, and the scribble-based setting is rarely explored. In this work,\nwe attempt to formulate a standard protocol for scribble-based interactive\nsegmentation. Basically, we design diversified strategies to simulate scribbles\nfor training, propose a deterministic scribble generator for evaluation, and\nconstruct a challenging benchmark. Besides, we build a strong framework\nScribbleSeg, consisting of a Prototype Adaption Module(PAM) and a Corrective\nRefine Module (CRM), for the task. Extensive experiments show that ScribbleSeg\nperforms notably better than previous click-based methods. We hope this could\nserve as a more powerful and general solution for interactive segmentation. Our\ncode will be made available.\n","authors":["Xi Chen","Yau Shing Jonathan Cheung","Ser-Nam Lim","Hengshuang Zhao"],"pdf_url":"https://arxiv.org/pdf/2303.11320v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11316v1","updated":"2023-03-20T17:55:37Z","published":"2023-03-20T17:55:37Z","title":"Generative Semantic Segmentation","summary":"  We present Generative Semantic Segmentation (GSS), a generative learning\napproach for semantic segmentation. Uniquely, we cast semantic segmentation as\nan image-conditioned mask generation problem. This is achieved by replacing the\nconventional per-pixel discriminative learning with a latent prior learning\nprocess. Specifically, we model the variational posterior distribution of\nlatent variables given the segmentation mask. To that end, the segmentation\nmask is expressed with a special type of image (dubbed as maskige). This\nposterior distribution allows to generate segmentation masks unconditionally.\nTo achieve semantic segmentation on a given image, we further introduce a\nconditioning network. It is optimized by minimizing the divergence between the\nposterior distribution of maskige (i.e., segmentation masks) and the latent\nprior distribution of input training images. Extensive experiments on standard\nbenchmarks show that our GSS can perform competitively to prior art\nalternatives in the standard semantic segmentation setting, whilst achieving a\nnew state of the art in the more challenging cross-domain setting.\n","authors":["Jiaqi Chen","Jiachen Lu","Xiatian Zhu","Li Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.11316v1.pdf","comment":"To appear at CVPR2023, code at http://github.com/fudan-zvg/GSS"},{"id":"http://arxiv.org/abs/2303.11313v1","updated":"2023-03-20T17:52:24Z","published":"2023-03-20T17:52:24Z","title":"CLIP goes 3D: Leveraging Prompt Tuning for Language Grounded 3D\n  Recognition","summary":"  Vision-Language models like CLIP have been widely adopted for various tasks\ndue to their impressive zero-shot capabilities. However, CLIP is not suitable\nfor extracting 3D geometric features as it was trained on only images and text\nby natural language supervision. We work on addressing this limitation and\npropose a new framework termed CG3D (CLIP Goes 3D) where a 3D encoder is\nlearned to exhibit zero-shot capabilities. CG3D is trained using triplets of\npointclouds, corresponding rendered 2D images, and texts using natural language\nsupervision. To align the features in a multimodal embedding space, we utilize\ncontrastive loss on 3D features obtained from the 3D encoder, as well as visual\nand text features extracted from CLIP. We note that the natural images used to\ntrain CLIP and the rendered 2D images in CG3D have a distribution shift.\nAttempting to train the visual and text encoder to account for this shift\nresults in catastrophic forgetting and a notable decrease in performance. To\nsolve this, we employ prompt tuning and introduce trainable parameters in the\ninput space to shift CLIP towards the 3D pre-training dataset utilized in CG3D.\nWe extensively test our pre-trained CG3D framework and demonstrate its\nimpressive capabilities in zero-shot, open scene understanding, and retrieval\ntasks. Further, it also serves as strong starting weights for fine-tuning in\ndownstream 3D recognition tasks.\n","authors":["Deepti Hegde","Jeya Maria Jose Valanarasu","Vishal M. Patel"],"pdf_url":"https://arxiv.org/pdf/2303.11313v1.pdf","comment":"Website: https://jeya-maria-jose.github.io/cg3d-web/"},{"id":"http://arxiv.org/abs/2207.01614v2","updated":"2023-03-20T17:51:09Z","published":"2022-07-04T17:56:14Z","title":"Beyond mAP: Towards better evaluation of instance segmentation","summary":"  Correctness of instance segmentation constitutes counting the number of\nobjects, correctly localizing all predictions and classifying each localized\nprediction. Average Precision is the de-facto metric used to measure all these\nconstituents of segmentation. However, this metric does not penalize duplicate\npredictions in the high-recall range, and cannot distinguish instances that are\nlocalized correctly but categorized incorrectly. This weakness has\ninadvertently led to network designs that achieve significant gains in AP but\nalso introduce a large number of false positives. We therefore cannot rely on\nAP to choose a model that provides an optimal tradeoff between false positives\nand high recall. To resolve this dilemma, we review alternative metrics in the\nliterature and propose two new measures to explicitly measure the amount of\nboth spatial and categorical duplicate predictions. We also propose a Semantic\nSorting and NMS module to remove these duplicates based on a pixel occupancy\nmatching scheme. Experiments show that modern segmentation networks have\nsignificant gains in AP, but also contain a considerable amount of duplicates.\nOur Semantic Sorting and NMS can be added as a plug-and-play module to mitigate\nhedged predictions and preserve AP.\n","authors":["Rohit Jena","Lukas Zhornyak","Nehal Doiphode","Pratik Chaudhari","Vivek Buch","James Gee","Jianbo Shi"],"pdf_url":"https://arxiv.org/pdf/2207.01614v2.pdf","comment":"Accepted at CVPR 2023"},{"id":"http://arxiv.org/abs/2303.11307v1","updated":"2023-03-20T17:45:12Z","published":"2023-03-20T17:45:12Z","title":"DIME-Net: Neural Network-Based Dynamic Intrinsic Parameter Rectification\n  for Cameras with Optical Image Stabilization System","summary":"  Optical Image Stabilization (OIS) system in mobile devices reduces image\nblurring by steering lens to compensate for hand jitters. However, OIS changes\nintrinsic camera parameters (i.e. $\\mathrm{K}$ matrix) dynamically which\nhinders accurate camera pose estimation or 3D reconstruction. Here we propose a\nnovel neural network-based approach that estimates $\\mathrm{K}$ matrix in\nreal-time so that pose estimation or scene reconstruction can be run at camera\nnative resolution for the highest accuracy on mobile devices. Our network\ndesign takes gratified projection model discrepancy feature and 3D point\npositions as inputs and employs a Multi-Layer Perceptron (MLP) to approximate\n$f_{\\mathrm{K}}$ manifold. We also design a unique training scheme for this\nnetwork by introducing a Back propagated PnP (BPnP) layer so that reprojection\nerror can be adopted as the loss function. The training process utilizes\nprecise calibration patterns for capturing accurate $f_{\\mathrm{K}}$ manifold\nbut the trained network can be used anywhere. We name the proposed Dynamic\nIntrinsic Manifold Estimation network as DIME-Net and have it implemented and\ntested on three different mobile devices. In all cases, DIME-Net can reduce\nreprojection error by at least $64\\%$ indicating that our design is successful.\n","authors":["Shu-Hao Yeh","Shuangyu Xie","Di Wang","Wei Yan","Dezhen Song"],"pdf_url":"https://arxiv.org/pdf/2303.11307v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11306v1","updated":"2023-03-20T17:45:08Z","published":"2023-03-20T17:45:08Z","title":"Localizing Object-level Shape Variations with Text-to-Image Diffusion\n  Models","summary":"  Text-to-image models give rise to workflows which often begin with an\nexploration step, where users sift through a large collection of generated\nimages. The global nature of the text-to-image generation process prevents\nusers from narrowing their exploration to a particular object in the image. In\nthis paper, we present a technique to generate a collection of images that\ndepicts variations in the shape of a specific object, enabling an object-level\nshape exploration process. Creating plausible variations is challenging as it\nrequires control over the shape of the generated object while respecting its\nsemantics. A particular challenge when generating object variations is\naccurately localizing the manipulation applied over the object's shape. We\nintroduce a prompt-mixing technique that switches between prompts along the\ndenoising process to attain a variety of shape choices. To localize the\nimage-space operation, we present two techniques that use the self-attention\nlayers in conjunction with the cross-attention layers. Moreover, we show that\nthese localization techniques are general and effective beyond the scope of\ngenerating object variations. Extensive results and comparisons demonstrate the\neffectiveness of our method in generating object variations, and the competence\nof our localization techniques.\n","authors":["Or Patashnik","Daniel Garibi","Idan Azuri","Hadar Averbuch-Elor","Daniel Cohen-Or"],"pdf_url":"https://arxiv.org/pdf/2303.11306v1.pdf","comment":"Project page at https://orpatashnik.github.io/local-prompt-mixing/"},{"id":"http://arxiv.org/abs/2303.11305v1","updated":"2023-03-20T17:45:02Z","published":"2023-03-20T17:45:02Z","title":"SVDiff: Compact Parameter Space for Diffusion Fine-Tuning","summary":"  Diffusion models have achieved remarkable success in text-to-image\ngeneration, enabling the creation of high-quality images from text prompts or\nother modalities. However, existing methods for customizing these models are\nlimited by handling multiple personalized subjects and the risk of overfitting.\nMoreover, their large number of parameters is inefficient for model storage. In\nthis paper, we propose a novel approach to address these limitations in\nexisting text-to-image diffusion models for personalization. Our method\ninvolves fine-tuning the singular values of the weight matrices, leading to a\ncompact and efficient parameter space that reduces the risk of overfitting and\nlanguage-drifting. We also propose a Cut-Mix-Unmix data-augmentation technique\nto enhance the quality of multi-subject image generation and a simple\ntext-based image editing framework. Our proposed SVDiff method has a\nsignificantly smaller model size (1.7MB for StableDiffusion) compared to\nexisting methods (vanilla DreamBooth 3.66GB, Custom Diffusion 73MB), making it\nmore practical for real-world applications.\n","authors":["Ligong Han","Yinxiao Li","Han Zhang","Peyman Milanfar","Dimitris Metaxas","Feng Yang"],"pdf_url":"https://arxiv.org/pdf/2303.11305v1.pdf","comment":"20 pages, 21 figures"},{"id":"http://arxiv.org/abs/2303.11302v1","updated":"2023-03-20T17:41:11Z","published":"2023-03-20T17:41:11Z","title":"Learning Audio-Visual Source Localization via False Negative Aware\n  Contrastive Learning","summary":"  Self-supervised audio-visual source localization aims to locate sound-source\nobjects in video frames without extra annotations. Recent methods often\napproach this goal with the help of contrastive learning, which assumes only\nthe audio and visual contents from the same video are positive samples for each\nother. However, this assumption would suffer from false negative samples in\nreal-world training. For example, for an audio sample, treating the frames from\nthe same audio class as negative samples may mislead the model and therefore\nharm the learned representations e.g., the audio of a siren wailing may\nreasonably correspond to the ambulances in multiple images). Based on this\nobservation, we propose a new learning strategy named False Negative Aware\nContrastive (FNAC) to mitigate the problem of misleading the training with such\nfalse negative samples. Specifically, we utilize the intra-modal similarities\nto identify potentially similar samples and construct corresponding adjacency\nmatrices to guide contrastive learning. Further, we propose to strengthen the\nrole of true negative samples by explicitly leveraging the visual features of\nsound sources to facilitate the differentiation of authentic sounding source\nregions. FNAC achieves state-of-the-art performances on Flickr-SoundNet,\nVGG-Sound, and AVSBench, which demonstrates the effectiveness of our method in\nmitigating the false negative issue. The code is available at\n\\url{https://github.com/weixuansun/FNAC-AVL}.\n","authors":["Weixuan Sun","Jiayi Zhang","Jianyuan Wang","Zheyuan Liu","Yiran Zhong","Tianpeng Feng","Yandong Guo","Yanhao Zhang","Nick Barnes"],"pdf_url":"https://arxiv.org/pdf/2303.11302v1.pdf","comment":"CVPR2023"},{"id":"http://arxiv.org/abs/2303.11301v1","updated":"2023-03-20T17:40:44Z","published":"2023-03-20T17:40:44Z","title":"VoxelNeXt: Fully Sparse VoxelNet for 3D Object Detection and Tracking","summary":"  3D object detectors usually rely on hand-crafted proxies, e.g., anchors or\ncenters, and translate well-studied 2D frameworks to 3D. Thus, sparse voxel\nfeatures need to be densified and processed by dense prediction heads, which\ninevitably costs extra computation. In this paper, we instead propose VoxelNext\nfor fully sparse 3D object detection. Our core insight is to predict objects\ndirectly based on sparse voxel features, without relying on hand-crafted\nproxies. Our strong sparse convolutional network VoxelNeXt detects and tracks\n3D objects through voxel features entirely. It is an elegant and efficient\nframework, with no need for sparse-to-dense conversion or NMS post-processing.\nOur method achieves a better speed-accuracy trade-off than other mainframe\ndetectors on the nuScenes dataset. For the first time, we show that a fully\nsparse voxel-based representation works decently for LIDAR 3D object detection\nand tracking. Extensive experiments on nuScenes, Waymo, and Argoverse2\nbenchmarks validate the effectiveness of our approach. Without bells and\nwhistles, our model outperforms all existing LIDAR methods on the nuScenes\ntracking test benchmark.\n","authors":["Yukang Chen","Jianhui Liu","Xiangyu Zhang","Xiaojuan Qi","Jiaya Jia"],"pdf_url":"https://arxiv.org/pdf/2303.11301v1.pdf","comment":"In CVPR 2023, Code and models are available at\n  https://github.com/dvlab-research/VoxelNeXt"},{"id":"http://arxiv.org/abs/2303.11298v1","updated":"2023-03-20T17:38:24Z","published":"2023-03-20T17:38:24Z","title":"Reliability in Semantic Segmentation: Are We on the Right Track?","summary":"  Motivated by the increasing popularity of transformers in computer vision, in\nrecent times there has been a rapid development of novel architectures. While\nin-domain performance follows a constant, upward trend, properties like\nrobustness or uncertainty estimation are less explored -leaving doubts about\nadvances in model reliability. Studies along these axes exist, but they are\nmainly limited to classification models. In contrast, we carry out a study on\nsemantic segmentation, a relevant task for many real-world applications where\nmodel reliability is paramount. We analyze a broad variety of models, spanning\nfrom older ResNet-based architectures to novel transformers and assess their\nreliability based on four metrics: robustness, calibration, misclassification\ndetection and out-of-distribution (OOD) detection. We find that while recent\nmodels are significantly more robust, they are not overall more reliable in\nterms of uncertainty estimation. We further explore methods that can come to\nthe rescue and show that improving calibration can also help with other\nuncertainty metrics such as misclassification or OOD detection. This is the\nfirst study on modern segmentation models focused on both robustness and\nuncertainty estimation and we hope it will help practitioners and researchers\ninterested in this fundamental vision task. Code available at\nhttps://github.com/naver/relis.\n","authors":["Pau de Jorge","Riccardo Volpi","Philip Torr","Gregory Rogez"],"pdf_url":"https://arxiv.org/pdf/2303.11298v1.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2303.11296v1","updated":"2023-03-20T17:34:05Z","published":"2023-03-20T17:34:05Z","title":"Attribute-preserving Face Dataset Anonymization via Latent Code\n  Optimization","summary":"  This work addresses the problem of anonymizing the identity of faces in a\ndataset of images, such that the privacy of those depicted is not violated,\nwhile at the same time the dataset is useful for downstream task such as for\ntraining machine learning models. To the best of our knowledge, we are the\nfirst to explicitly address this issue and deal with two major drawbacks of the\nexisting state-of-the-art approaches, namely that they (i) require the costly\ntraining of additional, purpose-trained neural networks, and/or (ii) fail to\nretain the facial attributes of the original images in the anonymized\ncounterparts, the preservation of which is of paramount importance for their\nuse in downstream tasks. We accordingly present a task-agnostic anonymization\nprocedure that directly optimizes the images' latent representation in the\nlatent space of a pre-trained GAN. By optimizing the latent codes directly, we\nensure both that the identity is of a desired distance away from the original\n(with an identity obfuscation loss), whilst preserving the facial attributes\n(using a novel feature-matching loss in FaRL's deep feature space). We\ndemonstrate through a series of both qualitative and quantitative experiments\nthat our method is capable of anonymizing the identity of the images whilst --\ncrucially -- better-preserving the facial attributes. We make the code and the\npre-trained models publicly available at: https://github.com/chi0tzp/FALCO.\n","authors":["Simone Barattin","Christos Tzelepis","Ioannis Patras","Nicu Sebe"],"pdf_url":"https://arxiv.org/pdf/2303.11296v1.pdf","comment":"Accepted for publication in CVPR 2023"},{"id":"http://arxiv.org/abs/2211.13221v2","updated":"2023-03-20T17:29:45Z","published":"2022-11-23T18:58:39Z","title":"Latent Video Diffusion Models for High-Fidelity Long Video Generation","summary":"  AI-generated content has attracted lots of attention recently, but\nphoto-realistic video synthesis is still challenging. Although many attempts\nusing GANs and autoregressive models have been made in this area, the visual\nquality and length of generated videos are far from satisfactory. Diffusion\nmodels have shown remarkable results recently but require significant\ncomputational resources. To address this, we introduce lightweight video\ndiffusion models by leveraging a low-dimensional 3D latent space, significantly\noutperforming previous pixel-space video diffusion models under a limited\ncomputational budget. In addition, we propose hierarchical diffusion in the\nlatent space such that longer videos with more than one thousand frames can be\nproduced. To further overcome the performance degradation issue for long video\ngeneration, we propose conditional latent perturbation and unconditional\nguidance that effectively mitigate the accumulated errors during the extension\nof video length. Extensive experiments on small domain datasets of different\ncategories suggest that our framework generates more realistic and longer\nvideos than previous strong baselines. We additionally provide an extension to\nlarge-scale text-to-video generation to demonstrate the superiority of our\nwork. Our code and models will be made publicly available.\n","authors":["Yingqing He","Tianyu Yang","Yong Zhang","Ying Shan","Qifeng Chen"],"pdf_url":"https://arxiv.org/pdf/2211.13221v2.pdf","comment":"Project Page: https://yingqinghe.github.io/LVDM/ Github:\n  https://github.com/YingqingHe/LVDM"},{"id":"http://arxiv.org/abs/2303.11274v1","updated":"2023-03-20T17:08:48Z","published":"2023-03-20T17:08:48Z","title":"Cascading Hierarchical Networks with Multi-task Balanced Loss for\n  Fine-grained hashing","summary":"  With the explosive growth in the number of fine-grained images in the\nInternet era, it has become a challenging problem to perform fast and efficient\nretrieval from large-scale fine-grained images. Among the many retrieval\nmethods, hashing methods are widely used due to their high efficiency and small\nstorage space occupation. Fine-grained hashing is more challenging than\ntraditional hashing problems due to the difficulties such as low inter-class\nvariances and high intra-class variances caused by the characteristics of\nfine-grained images. To improve the retrieval accuracy of fine-grained hashing,\nwe propose a cascaded network to learn compact and highly semantic hash codes,\nand introduce an attention-guided data augmentation method. We refer to this\nnetwork as a cascaded hierarchical data augmentation network. We also propose a\nnovel approach to coordinately balance the loss of multi-task learning. We do\nextensive experiments on some common fine-grained visual classification\ndatasets. The experimental results demonstrate that our proposed method\noutperforms several state-of-art hashing methods and can effectively improve\nthe accuracy of fine-grained retrieval. The source code is publicly available:\nhttps://github.com/kaiba007/FG-CNET.\n","authors":["Xianxian Zeng","Yanjun Zheng"],"pdf_url":"https://arxiv.org/pdf/2303.11274v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08319v2","updated":"2023-03-20T16:54:34Z","published":"2023-03-15T02:14:56Z","title":"FAQ: Feature Aggregated Queries for Transformer-based Video Object\n  Detectors","summary":"  Video object detection needs to solve feature degradation situations that\nrarely happen in the image domain. One solution is to use the temporal\ninformation and fuse the features from the neighboring frames. With\nTransformerbased object detectors getting a better performance on the image\ndomain tasks, recent works began to extend those methods to video object\ndetection. However, those existing Transformer-based video object detectors\nstill follow the same pipeline as those used for classical object detectors,\nlike enhancing the object feature representations by aggregation. In this work,\nwe take a different perspective on video object detection. In detail, we\nimprove the qualities of queries for the Transformer-based models by\naggregation. To achieve this goal, we first propose a vanilla query aggregation\nmodule that weighted averages the queries according to the features of the\nneighboring frames. Then, we extend the vanilla module to a more practical\nversion, which generates and aggregates queries according to the features of\nthe input frames. Extensive experimental results validate the effectiveness of\nour proposed methods: On the challenging ImageNet VID benchmark, when\nintegrated with our proposed modules, the current state-of-the-art\nTransformer-based object detectors can be improved by more than 2.4% on mAP and\n4.2% on AP50.\n","authors":["Yiming Cui","Linjie Yang"],"pdf_url":"https://arxiv.org/pdf/2303.08319v2.pdf","comment":"12 pages, 4 figures"},{"id":"http://arxiv.org/abs/2303.11267v1","updated":"2023-03-20T16:50:29Z","published":"2023-03-20T16:50:29Z","title":"Rethinking the backbone architecture for tiny object detection","summary":"  Tiny object detection has become an active area of research because images\nwith tiny targets are common in several important real-world scenarios.\nHowever, existing tiny object detection methods use standard deep neural\nnetworks as their backbone architecture. We argue that such backbones are\ninappropriate for detecting tiny objects as they are designed for the\nclassification of larger objects, and do not have the spatial resolution to\nidentify small targets. Specifically, such backbones use max-pooling or a large\nstride at early stages in the architecture. This produces lower resolution\nfeature-maps that can be efficiently processed by subsequent layers. However,\nsuch low-resolution feature-maps do not contain information that can reliably\ndiscriminate tiny objects. To solve this problem we design 'bottom-heavy'\nversions of backbones that allocate more resources to processing\nhigher-resolution features without introducing any additional computational\nburden overall. We also investigate if pre-training these backbones on images\nof appropriate size, using CIFAR100 and ImageNet32, can further improve\nperformance on tiny object detection. Results on TinyPerson and WiderFace show\nthat detectors with our proposed backbones achieve better results than the\ncurrent state-of-the-art methods.\n","authors":["Jinlai Ning","Haoyan Guan","Michael Spratling"],"pdf_url":"https://arxiv.org/pdf/2303.11267v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11253v1","updated":"2023-03-20T16:40:37Z","published":"2023-03-20T16:40:37Z","title":"Zero-Shot Noise2Noise: Efficient Image Denoising without any Data","summary":"  Recently, self-supervised neural networks have shown excellent image\ndenoising performance. However, current dataset free methods are either\ncomputationally expensive, require a noise model, or have inadequate image\nquality. In this work we show that a simple 2-layer network, without any\ntraining data or knowledge of the noise distribution, can enable high-quality\nimage denoising at low computational cost. Our approach is motivated by\nNoise2Noise and Neighbor2Neighbor and works well for denoising pixel-wise\nindependent noise. Our experiments on artificial, real-world camera, and\nmicroscope noise show that our method termed ZS-N2N (Zero Shot Noise2Noise)\noften outperforms existing dataset-free methods at a reduced cost, making it\nsuitable for use cases with scarce data availability and limited compute\nresources. A demo of our implementation including our code and hyperparameters\ncan be found in the following colab notebook:\nhttps://colab.research.google.com/drive/1i82nyizTdszyHkaHBuKPbWnTzao8HF9b\n","authors":["Youssef Mansour","Reinhard Heckel"],"pdf_url":"https://arxiv.org/pdf/2303.11253v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.06051v2","updated":"2023-03-20T16:36:27Z","published":"2023-01-15T09:31:58Z","title":"DSVT: Dynamic Sparse Voxel Transformer with Rotated Sets","summary":"  Designing an efficient yet deployment-friendly 3D backbone to handle sparse\npoint clouds is a fundamental problem in 3D perception. Compared with the\ncustomized sparse convolution, the attention mechanism in Transformers is more\nappropriate for flexibly modeling long-range relationships and is easier to be\ndeployed in real-world applications. However, due to the sparse characteristics\nof point clouds, it is non-trivial to apply a standard transformer on sparse\npoints. In this paper, we present Dynamic Sparse Voxel Transformer (DSVT), a\nsingle-stride window-based voxel Transformer backbone for outdoor 3D\nperception. In order to efficiently process sparse points in parallel, we\npropose Dynamic Sparse Window Attention, which partitions a series of local\nregions in each window according to its sparsity and then computes the features\nof all regions in a fully parallel manner. To allow the cross-set connection,\nwe design a rotated set partitioning strategy that alternates between two\npartitioning configurations in consecutive self-attention layers. To support\neffective downsampling and better encode geometric information, we also propose\nan attention-style 3D pooling module on sparse points, which is powerful and\ndeployment-friendly without utilizing any customized CUDA operations. Our model\nachieves state-of-the-art performance with a broad range of 3D perception\ntasks. More importantly, DSVT can be easily deployed by TensorRT with real-time\ninference speed (27Hz). Code will be available at\n\\url{https://github.com/Haiyang-W/DSVT}.\n","authors":["Haiyang Wang","Chen Shi","Shaoshuai Shi","Meng Lei","Sen Wang","Di He","Bernt Schiele","Liwei Wang"],"pdf_url":"https://arxiv.org/pdf/2301.06051v2.pdf","comment":"Accepted by CVPR2023"},{"id":"http://arxiv.org/abs/2303.11251v1","updated":"2023-03-20T16:35:38Z","published":"2023-03-20T16:35:38Z","title":"Towards End-to-End Generative Modeling of Long Videos with\n  Memory-Efficient Bidirectional Transformers","summary":"  Autoregressive transformers have shown remarkable success in video\ngeneration. However, the transformers are prohibited from directly learning the\nlong-term dependency in videos due to the quadratic complexity of\nself-attention, and inherently suffering from slow inference time and error\npropagation due to the autoregressive process. In this paper, we propose\nMemory-efficient Bidirectional Transformer (MeBT) for end-to-end learning of\nlong-term dependency in videos and fast inference. Based on recent advances in\nbidirectional transformers, our method learns to decode the entire\nspatio-temporal volume of a video in parallel from partially observed patches.\nThe proposed transformer achieves a linear time complexity in both encoding and\ndecoding, by projecting observable context tokens into a fixed number of latent\ntokens and conditioning them to decode the masked tokens through the\ncross-attention. Empowered by linear complexity and bidirectional modeling, our\nmethod demonstrates significant improvement over the autoregressive\nTransformers for generating moderately long videos in both quality and speed.\n","authors":["Jaehoon Yoo","Semin Kim","Doyup Lee","Chiheon Kim","Seunghoon Hong"],"pdf_url":"https://arxiv.org/pdf/2303.11251v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11243v1","updated":"2023-03-20T16:28:15Z","published":"2023-03-20T16:28:15Z","title":"Augment and Criticize: Exploring Informative Samples for Semi-Supervised\n  Monocular 3D Object Detection","summary":"  In this paper, we improve the challenging monocular 3D object detection\nproblem with a general semi-supervised framework. Specifically, having observed\nthat the bottleneck of this task lies in lacking reliable and informative\nsamples to train the detector, we introduce a novel, simple, yet effective\n`Augment and Criticize' framework that explores abundant informative samples\nfrom unlabeled data for learning more robust detection models. In the `Augment'\nstage, we present the Augmentation-based Prediction aGgregation (APG), which\naggregates detections from various automatically learned augmented views to\nimprove the robustness of pseudo label generation. Since not all pseudo labels\nfrom APG are beneficially informative, the subsequent `Criticize' phase is\npresented. In particular, we introduce the Critical Retraining Strategy (CRS)\nthat, unlike simply filtering pseudo labels using a fixed threshold (e.g.,\nclassification score) as in 2D semi-supervised tasks, leverages a learnable\nnetwork to evaluate the contribution of unlabeled images at different training\ntimestamps. This way, the noisy samples prohibitive to model evolution could be\neffectively suppressed. To validate our framework, we apply it to MonoDLE and\nMonoFlex. The two new detectors, dubbed 3DSeMo_DLE and 3DSeMo_FLEX, achieve\nstate-of-the-art results with remarkable improvements for over 3.5% AP_3D/BEV\n(Easy) on KITTI, showing its effectiveness and generality. Code and models will\nbe released.\n","authors":["Zhenyu Li","Zhipeng Zhang","Heng Fan","Yuan He","Ke Wang","Xianming Liu","Junjun Jiang"],"pdf_url":"https://arxiv.org/pdf/2303.11243v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11242v1","updated":"2023-03-20T16:27:36Z","published":"2023-03-20T16:27:36Z","title":"Make Landscape Flatter in Differentially Private Federated Learning","summary":"  To defend the inference attacks and mitigate the sensitive information\nleakages in Federated Learning (FL), client-level Differentially Private FL\n(DPFL) is the de-facto standard for privacy protection by clipping local\nupdates and adding random noise. However, existing DPFL methods tend to make a\nsharper loss landscape and have poorer weight perturbation robustness,\nresulting in severe performance degradation. To alleviate these issues, we\npropose a novel DPFL algorithm named DP-FedSAM, which leverages gradient\nperturbation to mitigate the negative impact of DP. Specifically, DP-FedSAM\nintegrates Sharpness Aware Minimization (SAM) optimizer to generate local\nflatness models with better stability and weight perturbation robustness, which\nresults in the small norm of local updates and robustness to DP noise, thereby\nimproving the performance. From the theoretical perspective, we analyze in\ndetail how DP-FedSAM mitigates the performance degradation induced by DP.\nMeanwhile, we give rigorous privacy guarantees with R\\'enyi DP and present the\nsensitivity analysis of local updates. At last, we empirically confirm that our\nalgorithm achieves state-of-the-art (SOTA) performance compared with existing\nSOTA baselines in DPFL.\n","authors":["Yifan Shi","Yingqi Liu","Kang Wei","Li Shen","Xueqian Wang","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2303.11242v1.pdf","comment":"CVPR2023, 18 pages"},{"id":"http://arxiv.org/abs/2303.11239v1","updated":"2023-03-20T16:24:06Z","published":"2023-03-20T16:24:06Z","title":"Training Invertible Neural Networks as Autoencoders","summary":"  Autoencoders are able to learn useful data representations in an unsupervised\nmatter and have been widely used in various machine learning and computer\nvision tasks. In this work, we present methods to train Invertible Neural\nNetworks (INNs) as (variational) autoencoders which we call INN (variational)\nautoencoders. Our experiments on MNIST, CIFAR and CelebA show that for low\nbottleneck sizes our INN autoencoder achieves results similar to the classical\nautoencoder. However, for large bottleneck sizes our INN autoencoder\noutperforms its classical counterpart. Based on the empirical results, we\nhypothesize that INN autoencoders might not have any intrinsic information loss\nand thereby are not bounded to a maximal number of layers (depth) after which\nonly suboptimal results can be achieved.\n","authors":["The-Gia Leo Nguyen","Lynton Ardizzone","Ullrich Koethe"],"pdf_url":"https://arxiv.org/pdf/2303.11239v1.pdf","comment":"Conference Paper at GCPR2019"},{"id":"http://arxiv.org/abs/2303.10103v2","updated":"2023-03-20T16:21:34Z","published":"2023-03-17T16:26:20Z","title":"Image comparison and scaling via nonlinear elasticity","summary":"  A nonlinear elasticity model for comparing images is formulated and analyzed,\nin which optimal transformations between images are sought as minimizers of an\nintegral functional. The existence of minimizers in a suitable class of\nhomeomorphisms between image domains is established under natural hypotheses.\nWe investigate whether for linearly related images the minimization algorithm\ndelivers the linear transformation as the unique minimizer.\n","authors":["John M. Ball","Christopher L. Horner"],"pdf_url":"https://arxiv.org/pdf/2303.10103v2.pdf","comment":"SSVM2023 Proceedings to appear. New references added plus related\n  minor changes"},{"id":"http://arxiv.org/abs/2303.11235v1","updated":"2023-03-20T16:19:23Z","published":"2023-03-20T16:19:23Z","title":"FullFormer: Generating Shapes Inside Shapes","summary":"  Implicit generative models have been widely employed to model 3D data and\nhave recently proven to be successful in encoding and generating high-quality\n3D shapes. This work builds upon these models and alleviates current\nlimitations by presenting the first implicit generative model that facilitates\nthe generation of complex 3D shapes with rich internal geometric details. To\nachieve this, our model uses unsigned distance fields to represent nested 3D\nsurfaces allowing learning from non-watertight mesh data. We propose a\ntransformer-based autoregressive model for 3D shape generation that leverages\ncontext-rich tokens from vector quantized shape embeddings. The generated\ntokens are decoded into an unsigned distance field which is rendered into a\nnovel 3D shape exhibiting a rich internal structure. We demonstrate that our\nmodel achieves state-of-the-art point cloud generation results on popular\nclasses of 'Cars', 'Planes', and 'Chairs' of the ShapeNet dataset.\nAdditionally, we curate a dataset that exclusively comprises shapes with\nrealistic internal details from the `Cars' class of ShapeNet and demonstrate\nour method's efficacy in generating these shapes with internal geometry.\n","authors":["Tejaswini Medi","Jawad Tayyub","Muhammad Sarmad","Frank Lindseth","Margret Keuper"],"pdf_url":"https://arxiv.org/pdf/2303.11235v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2110.02273v2","updated":"2023-03-20T16:18:27Z","published":"2021-10-05T18:26:22Z","title":"Bilevel Imaging Learning Problems as Mathematical Programs with\n  Complementarity Constraints: Reformulation and Theory","summary":"  We investigate a family of bilevel imaging learning problems where the\nlower-level instance corresponds to a convex variational model involving first-\nand second-order nonsmooth sparsity-based regularizers. By using geometric\nproperties of the primal-dual reformulation of the lower-level problem and\nintroducing suitable auxiliar variables, we are able to reformulate the\noriginal bilevel problems as Mathematical Programs with Complementarity\nConstraints (MPCC). For the latter, we prove tight constraint qualification\nconditions (MPCC-RCPLD and partial MPCC-LICQ) and derive Mordukhovich (M-) and\nStrong (S-) stationarity conditions. The stationarity systems for the MPCC turn\nalso into stationarity conditions for the original formulation. Second-order\nsufficient optimality conditions are derived as well, together with a local\nuniqueness result for stationary points. The proposed reformulation may be\nextended to problems in function spaces, leading to MPCC's with constraints on\nthe gradient of the state. The MPCC reformulation also leads to the efficient\nuse of available large-scale nonlinear programming solvers, as shown in a\ncompanion paper, where different imaging applications are studied.\n","authors":["Juan Carlos De los Reyes"],"pdf_url":"https://arxiv.org/pdf/2110.02273v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11228v1","updated":"2023-03-20T16:09:25Z","published":"2023-03-20T16:09:25Z","title":"Bimodal SegNet: Instance Segmentation Fusing Events and RGB Frames for\n  Robotic Grasping","summary":"  Object segmentation for robotic grasping under dynamic conditions often faces\nchallenges such as occlusion, low light conditions, motion blur and object size\nvariance. To address these challenges, we propose a Deep Learning network that\nfuses two types of visual signals, event-based data and RGB frame data. The\nproposed Bimodal SegNet network has two distinct encoders, one for each signal\ninput and a spatial pyramidal pooling with atrous convolutions. Encoders\ncapture rich contextual information by pooling the concatenated features at\ndifferent resolutions while the decoder obtains sharp object boundaries. The\nevaluation of the proposed method undertakes five unique image degradation\nchallenges including occlusion, blur, brightness, trajectory and scale variance\non the Event-based Segmentation (ESD) Dataset. The evaluation results show a\n6-10\\% segmentation accuracy improvement over state-of-the-art methods in terms\nof mean intersection over the union and pixel accuracy. The model code is\navailable at https://github.com/sanket0707/Bimodal-SegNet.git\n","authors":["Sanket Kachole","Xiaoqian Huang","Fariborz Baghaei Naeini","Rajkumar Muthusamy","Dimitrios Makris","Yahya Zweiri"],"pdf_url":"https://arxiv.org/pdf/2303.11228v1.pdf","comment":"8 Pages"},{"id":"http://arxiv.org/abs/2303.11225v1","updated":"2023-03-20T16:07:02Z","published":"2023-03-20T16:07:02Z","title":"HiFace: High-Fidelity 3D Face Reconstruction by Learning Static and\n  Dynamic Details","summary":"  3D Morphable Models (3DMMs) demonstrate great potential for reconstructing\nfaithful and animatable 3D facial surfaces from a single image. The facial\nsurface is influenced by the coarse shape, as well as the static detail (e,g.,\nperson-specific appearance) and dynamic detail (e.g., expression-driven\nwrinkles). Previous work struggles to decouple the static and dynamic details\nthrough image-level supervision, leading to reconstructions that are not\nrealistic. In this paper, we aim at high-fidelity 3D face reconstruction and\npropose HiFace to explicitly model the static and dynamic details.\nSpecifically, the static detail is modeled as the linear combination of a\ndisplacement basis, while the dynamic detail is modeled as the linear\ninterpolation of two displacement maps with polarized expressions. We exploit\nseveral loss functions to jointly learn the coarse shape and fine details with\nboth synthetic and real-world datasets, which enable HiFace to reconstruct\nhigh-fidelity 3D shapes with animatable details. Extensive quantitative and\nqualitative experiments demonstrate that HiFace presents state-of-the-art\nreconstruction quality and faithfully recovers both the static and dynamic\ndetails. Our project page can be found at https://project-hiface.github.io\n","authors":["Zenghao Chai","Tianke Zhang","Tianyu He","Xu Tan","Tadas Baltrusaitis","HsiangTao Wu","Runnan Li","Sheng Zhao","Chun Yuan","Jiang Bian"],"pdf_url":"https://arxiv.org/pdf/2303.11225v1.pdf","comment":"Tech report"},{"id":"http://arxiv.org/abs/2303.11224v1","updated":"2023-03-20T16:00:20Z","published":"2023-03-20T16:00:20Z","title":"Cascaded Latent Diffusion Models for High-Resolution Chest X-ray\n  Synthesis","summary":"  While recent advances in large-scale foundational models show promising\nresults, their application to the medical domain has not yet been explored in\ndetail. In this paper, we progress into the realms of large-scale modeling in\nmedical synthesis by proposing Cheff - a foundational cascaded latent diffusion\nmodel, which generates highly-realistic chest radiographs providing\nstate-of-the-art quality on a 1-megapixel scale. We further propose MaCheX,\nwhich is a unified interface for public chest datasets and forms the largest\nopen collection of chest X-rays up to date. With Cheff conditioned on\nradiological reports, we further guide the synthesis process over text prompts\nand unveil the research area of report-to-chest-X-ray generation.\n","authors":["Tobias Weber","Michael Ingrisch","Bernd Bischl","David Rügamer"],"pdf_url":"https://arxiv.org/pdf/2303.11224v1.pdf","comment":"accepted at PAKDD 2023"},{"id":"http://arxiv.org/abs/2210.09276v3","updated":"2023-03-20T15:58:50Z","published":"2022-10-17T17:27:32Z","title":"Imagic: Text-Based Real Image Editing with Diffusion Models","summary":"  Text-conditioned image editing has recently attracted considerable interest.\nHowever, most methods are currently either limited to specific editing types\n(e.g., object overlay, style transfer), or apply to synthetically generated\nimages, or require multiple input images of a common object. In this paper we\ndemonstrate, for the very first time, the ability to apply complex (e.g.,\nnon-rigid) text-guided semantic edits to a single real image. For example, we\ncan change the posture and composition of one or multiple objects inside an\nimage, while preserving its original characteristics. Our method can make a\nstanding dog sit down or jump, cause a bird to spread its wings, etc. -- each\nwithin its single high-resolution natural image provided by the user. Contrary\nto previous work, our proposed method requires only a single input image and a\ntarget text (the desired edit). It operates on real images, and does not\nrequire any additional inputs (such as image masks or additional views of the\nobject). Our method, which we call \"Imagic\", leverages a pre-trained\ntext-to-image diffusion model for this task. It produces a text embedding that\naligns with both the input image and the target text, while fine-tuning the\ndiffusion model to capture the image-specific appearance. We demonstrate the\nquality and versatility of our method on numerous inputs from various domains,\nshowcasing a plethora of high quality complex semantic image edits, all within\na single unified framework.\n","authors":["Bahjat Kawar","Shiran Zada","Oran Lang","Omer Tov","Huiwen Chang","Tali Dekel","Inbar Mosseri","Michal Irani"],"pdf_url":"https://arxiv.org/pdf/2210.09276v3.pdf","comment":"Project page: https://imagic-editing.github.io/"},{"id":"http://arxiv.org/abs/2303.11219v1","updated":"2023-03-20T15:50:00Z","published":"2023-03-20T15:50:00Z","title":"NeTO:Neural Reconstruction of Transparent Objects with Self-Occlusion\n  Aware Refraction-Tracing","summary":"  We present a novel method, called NeTO, for capturing 3D geometry of solid\ntransparent objects from 2D images via volume rendering. Reconstructing\ntransparent objects is a very challenging task, which is ill-suited for\ngeneral-purpose reconstruction techniques due to the specular light transport\nphenomena. Although existing refraction-tracing based methods, designed\nspecially for this task, achieve impressive results, they still suffer from\nunstable optimization and loss of fine details, since the explicit surface\nrepresentation they adopted is difficult to be optimized, and the\nself-occlusion problem is ignored for refraction-tracing. In this paper, we\npropose to leverage implicit Signed Distance Function (SDF) as surface\nrepresentation, and optimize the SDF field via volume rendering with a\nself-occlusion aware refractive ray tracing. The implicit representation\nenables our method to be capable of reconstructing high-quality reconstruction\neven with a limited set of images, and the self-occlusion aware strategy makes\nit possible for our method to accurately reconstruct the self-occluded regions.\nExperiments show that our method achieves faithful reconstruction results and\noutperforms prior works by a large margin. Visit our project page at\n\\url{https://www.xxlong.site/NeTO/}\n","authors":["Zongcheng Li","Xiaoxiao Long","Yusen Wang","Tuo Cao","Wenping Wang","Fei Luo","Chunxia Xiao"],"pdf_url":"https://arxiv.org/pdf/2303.11219v1.pdf","comment":"www.xxlong.site/NeTO/"},{"id":"http://arxiv.org/abs/2303.11217v1","updated":"2023-03-20T15:49:18Z","published":"2023-03-20T15:49:18Z","title":"Inverse problem regularization with hierarchical variational\n  autoencoders","summary":"  In this paper, we propose to regularize ill-posed inverse problems using a\ndeep hierarchical variational autoencoder (HVAE) as an image prior. The\nproposed method synthesizes the advantages of i) denoiser-based Plug \\& Play\napproaches and ii) generative model based approaches to inverse problems.\nFirst, we exploit VAE properties to design an efficient algorithm that benefits\nfrom convergence guarantees of Plug-and-Play (PnP) methods. Second, our\napproach is not restricted to specialized datasets and the proposed PnP-HVAE\nmodel is able to solve image restoration problems on natural images of any\nsize. Our experiments show that the proposed PnP-HVAE method is competitive\nwith both SOTA denoiser-based PnP approaches, and other SOTA restoration\nmethods based on generative models.\n","authors":["Jean Prost","Antoine Houdard","Andrés Almansa","Nicolas Papadakis"],"pdf_url":"https://arxiv.org/pdf/2303.11217v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11215v1","updated":"2023-03-20T15:47:05Z","published":"2023-03-20T15:47:05Z","title":"Learning to Generate 3D Representations of Building Roofs Using\n  Single-View Aerial Imagery","summary":"  We present a novel pipeline for learning the conditional distribution of a\nbuilding roof mesh given pixels from an aerial image, under the assumption that\nroof geometry follows a set of regular patterns. Unlike alternative methods\nthat require multiple images of the same object, our approach enables\nestimating 3D roof meshes using only a single image for predictions. The\napproach employs the PolyGen, a deep generative transformer architecture for 3D\nmeshes. We apply this model in a new domain and investigate the sensitivity of\nthe image resolution. We propose a novel metric to evaluate the performance of\nthe inferred meshes, and our results show that the model is robust even at\nlower resolutions, while qualitatively producing realistic representations for\nout-of-distribution samples.\n","authors":["Maxim Khomiakov","Alejandro Valverde Mahou","Alba Reinders Sánchez","Jes Frellsen","Michael Riis Andersen"],"pdf_url":"https://arxiv.org/pdf/2303.11215v1.pdf","comment":"Copyright 2023 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works"},{"id":"http://arxiv.org/abs/2303.11214v1","updated":"2023-03-20T15:46:49Z","published":"2023-03-20T15:46:49Z","title":"Accurate Detection of Mediastinal Lesions with nnDetection","summary":"  The accurate detection of mediastinal lesions is one of the rarely explored\nmedical object detection problems. In this work, we applied a modified version\nof the self-configuring method nnDetection to the Mediastinal Lesion Analysis\n(MELA) Challenge 2022. By incorporating automatically generated pseudo masks,\ntraining high capacity models with large patch sizes in a multi GPU setup and\nan adapted augmentation scheme to reduce localization errors caused by\nrotations, our method achieved an excellent FROC score of 0.9922 at IoU 0.10\nand 0.9880 at IoU 0.3 in our cross-validation experiments. The submitted\nensemble ranked third in the competition with a FROC score of 0.9897 on the\nMELA challenge leaderboard.\n","authors":["Michael Baumgartner","Peter M. Full","Klaus H. Maier-Hein"],"pdf_url":"https://arxiv.org/pdf/2303.11214v1.pdf","comment":"Published in \"Lesion Segmentation in Surgical and Diagnostic\n  Applications\""},{"id":"http://arxiv.org/abs/2303.11203v1","updated":"2023-03-20T15:36:10Z","published":"2023-03-20T15:36:10Z","title":"Less is More: Reducing Task and Model Complexity for 3D Point Cloud\n  Semantic Segmentation","summary":"  Whilst the availability of 3D LiDAR point cloud data has significantly grown\nin recent years, annotation remains expensive and time-consuming, leading to a\ndemand for semi-supervised semantic segmentation methods with application\ndomains such as autonomous driving. Existing work very often employs relatively\nlarge segmentation backbone networks to improve segmentation accuracy, at the\nexpense of computational costs. In addition, many use uniform sampling to\nreduce ground truth data requirements for learning needed, often resulting in\nsub-optimal performance. To address these issues, we propose a new pipeline\nthat employs a smaller architecture, requiring fewer ground-truth annotations\nto achieve superior segmentation accuracy compared to contemporary approaches.\nThis is facilitated via a novel Sparse Depthwise Separable Convolution module\nthat significantly reduces the network parameter count while retaining overall\ntask performance. To effectively sub-sample our training data, we propose a new\nSpatio-Temporal Redundant Frame Downsampling (ST-RFD) method that leverages\nknowledge of sensor motion within the environment to extract a more diverse\nsubset of training data frame samples. To leverage the use of limited annotated\ndata samples, we further propose a soft pseudo-label method informed by LiDAR\nreflectivity. Our method outperforms contemporary semi-supervised work in terms\nof mIoU, using less labeled data, on the SemanticKITTI (59.5@5%) and\nScribbleKITTI (58.1@5%) benchmark datasets, based on a 2.3x reduction in model\nparameters and 641x fewer multiply-add operations whilst also demonstrating\nsignificant performance improvement on limited training data (i.e., Less is\nMore).\n","authors":["Li Li","Hubert P. H. Shum","Toby P. Breckon"],"pdf_url":"https://arxiv.org/pdf/2303.11203v1.pdf","comment":"Accepted by CVPR 2023; Code at https://github.com/l1997i/lim3d; 11\n  pages, 8 figures"},{"id":"http://arxiv.org/abs/2302.00291v2","updated":"2023-03-20T15:28:36Z","published":"2023-02-01T07:45:54Z","title":"Development of Real-time Rendering Technology for High-Precision Models\n  in Autonomous Driving","summary":"  Our autonomous driving simulation lab produces a high-precision 3D model\nsimulating the parking lot. However, the current model still has poor rendering\nquality in some aspects. In this work, we develop a system to improve the\nrendering of the model and evaluate the quality of the rendered model.\n","authors":["Zhang Wencheng","Wang Chengyi"],"pdf_url":"https://arxiv.org/pdf/2302.00291v2.pdf","comment":"3 pages, 6 figures"},{"id":"http://arxiv.org/abs/2303.01498v3","updated":"2023-03-20T15:25:09Z","published":"2023-03-02T18:58:15Z","title":"ABAW: Valence-Arousal Estimation, Expression Recognition, Action Unit\n  Detection & Emotional Reaction Intensity Estimation Challenges","summary":"  The fifth Affective Behavior Analysis in-the-wild (ABAW) Competition is part\nof the respective ABAW Workshop which will be held in conjunction with IEEE\nComputer Vision and Pattern Recognition Conference (CVPR), 2023. The 5th ABAW\nCompetition is a continuation of the Competitions held at ECCV 2022, IEEE CVPR\n2022, ICCV 2021, IEEE FG 2020 and CVPR 2017 Conferences, and is dedicated at\nautomatically analyzing affect. For this year's Competition, we feature two\ncorpora: i) an extended version of the Aff-Wild2 database and ii) the\nHume-Reaction dataset. The former database is an audiovisual one of around 600\nvideos of around 3M frames and is annotated with respect to:a) two continuous\naffect dimensions -valence (how positive/negative a person is) and arousal (how\nactive/passive a person is)-; b) basic expressions (e.g. happiness, sadness,\nneutral state); and c) atomic facial muscle actions (i.e., action units). The\nlatter dataset is an audiovisual one in which reactions of individuals to\nemotional stimuli have been annotated with respect to seven emotional\nexpression intensities. Thus the 5th ABAW Competition encompasses four\nChallenges: i) uni-task Valence-Arousal Estimation, ii) uni-task Expression\nClassification, iii) uni-task Action Unit Detection, and iv) Emotional Reaction\nIntensity Estimation. In this paper, we present these Challenges, along with\ntheir corpora, we outline the evaluation metrics, we present the baseline\nsystems and illustrate their obtained performance.\n","authors":["Dimitrios Kollias","Panagiotis Tzirakis","Alice Baird","Alan Cowen","Stefanos Zafeiriou"],"pdf_url":"https://arxiv.org/pdf/2303.01498v3.pdf","comment":"arXiv admin note: text overlap with arXiv:2202.10659"},{"id":"http://arxiv.org/abs/2212.12053v3","updated":"2023-03-20T15:22:59Z","published":"2022-12-22T22:05:16Z","title":"On Calibrating Semantic Segmentation Models: Analyses and An Algorithm","summary":"  We study the problem of semantic segmentation calibration. Lots of solutions\nhave been proposed to approach model miscalibration of confidence in image\nclassification. However, to date, confidence calibration research on semantic\nsegmentation is still limited. We provide a systematic study on the calibration\nof semantic segmentation models and propose a simple yet effective approach.\nFirst, we find that model capacity, crop size, multi-scale testing, and\nprediction correctness have impact on calibration. Among them, prediction\ncorrectness, especially misprediction, is more important to miscalibration due\nto over-confidence. Next, we propose a simple, unifying, and effective\napproach, namely selective scaling, by separating correct/incorrect prediction\nfor scaling and more focusing on misprediction logit smoothing. Then, we study\npopular existing calibration methods and compare them with selective scaling on\nsemantic segmentation calibration. We conduct extensive experiments with a\nvariety of benchmarks on both in-domain and domain-shift calibration, and show\nthat selective scaling consistently outperforms other methods.\n","authors":["Dongdong Wang","Boqing Gong","Liqiang Wang"],"pdf_url":"https://arxiv.org/pdf/2212.12053v3.pdf","comment":"Accepted to CVPR2023 (8 pages, 4 figures)"},{"id":"http://arxiv.org/abs/2303.11183v1","updated":"2023-03-20T15:10:41Z","published":"2023-03-20T15:10:41Z","title":"Architecture, Dataset and Model-Scale Agnostic Data-free Meta-Learning","summary":"  The goal of data-free meta-learning is to learn useful prior knowledge from a\ncollection of pre-trained models without accessing their training data.\nHowever, existing works only solve the problem in parameter space, which (i)\nignore the fruitful data knowledge contained in the pre-trained models; (ii)\ncan not scale to large-scale pre-trained models; (iii) can only meta-learn\npre-trained models with the same network architecture. To address those issues,\nwe propose a unified framework, dubbed PURER, which contains: (1) ePisode\ncUrriculum inveRsion (ECI) during data-free meta training; and (2) invErsion\ncalibRation following inner loop (ICFIL) during meta testing. During meta\ntraining, we propose ECI to perform pseudo episode training for learning to\nadapt fast to new unseen tasks. Specifically, we progressively synthesize a\nsequence of pseudo episodes by distilling the training data from each\npre-trained model. The ECI adaptively increases the difficulty level of pseudo\nepisodes according to the real-time feedback of the meta model. We formulate\nthe optimization process of meta training with ECI as an adversarial form in an\nend-to-end manner. During meta testing, we further propose a simple\nplug-and-play supplement-ICFIL-only used during meta testing to narrow the gap\nbetween meta training and meta testing task distribution. Extensive experiments\nin various real-world scenarios show the superior performance of ours.\n","authors":["Zixuan Hu","Li Shen","Zhenyi Wang","Tongliang Liu","Chun Yuan","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2303.11183v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11180v1","updated":"2023-03-20T15:05:24Z","published":"2023-03-20T15:05:24Z","title":"Self-Correctable and Adaptable Inference for Generalizable Human Pose\n  Estimation","summary":"  A central challenge in human pose estimation, as well as in many other\nmachine learning and prediction tasks, is the generalization problem. The\nlearned network does not have the capability to characterize the prediction\nerror, generate feedback information from the test sample, and correct the\nprediction error on the fly for each individual test sample, which results in\ndegraded performance in generalization. In this work, we introduce a\nself-correctable and adaptable inference (SCAI) method to address the\ngeneralization challenge of network prediction and use human pose estimation as\nan example to demonstrate its effectiveness and performance. We learn a\ncorrection network to correct the prediction result conditioned by a fitness\nfeedback error. This feedback error is generated by a learned fitness feedback\nnetwork which maps the prediction result to the original input domain and\ncompares it against the original input. Interestingly, we find that this\nself-referential feedback error is highly correlated with the actual prediction\nerror. This strong correlation suggests that we can use this error as feedback\nto guide the correction process. It can be also used as a loss function to\nquickly adapt and optimize the correction network during the inference process.\nOur extensive experimental results on human pose estimation demonstrate that\nthe proposed SCAI method is able to significantly improve the generalization\ncapability and performance of human pose estimation.\n","authors":["Zhehan Kan","Shuoshuo Chen","Ce Zhang","Yushun Tang","Zhihai He"],"pdf_url":"https://arxiv.org/pdf/2303.11180v1.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2303.11177v1","updated":"2023-03-20T15:00:52Z","published":"2023-03-20T15:00:52Z","title":"Integration of Radiomics and Tumor Biomarkers in Interpretable Machine\n  Learning Models","summary":"  Despite the unprecedented performance of deep neural networks (DNNs) in\ncomputer vision, their practical application in the diagnosis and prognosis of\ncancer using medical imaging has been limited. One of the critical challenges\nfor integrating diagnostic DNNs into radiological and oncological applications\nis their lack of interpretability, preventing clinicians from understanding the\nmodel predictions. Therefore, we study and propose the integration of\nexpert-derived radiomics and DNN-predicted biomarkers in interpretable\nclassifiers which we call ConRad, for computerized tomography (CT) scans of\nlung cancer. Importantly, the tumor biomarkers are predicted from a concept\nbottleneck model (CBM) such that once trained, our ConRad models do not require\nlabor-intensive and time-consuming biomarkers. In our evaluation and practical\napplication, the only input to ConRad is a segmented CT scan. The proposed\nmodel is compared to convolutional neural networks (CNNs) which act as a black\nbox classifier. We further investigated and evaluated all combinations of\nradiomics, predicted biomarkers and CNN features in five different classifiers.\nWe found the ConRad models using non-linear SVM and the logistic regression\nwith the Lasso outperform others in five-fold cross-validation, although we\nhighlight that interpretability of ConRad is its primary advantage. The Lasso\nis used for feature selection, which substantially reduces the number of\nnon-zero weights while increasing the accuracy. Overall, the proposed ConRad\nmodel combines CBM-derived biomarkers and radiomics features in an\ninterpretable ML model which perform excellently for the lung nodule malignancy\nclassification.\n","authors":["Lennart Brocki","Neo Christopher Chung"],"pdf_url":"https://arxiv.org/pdf/2303.11177v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.10941v2","updated":"2023-03-20T14:57:52Z","published":"2023-01-26T05:14:12Z","title":"GeCoNeRF: Few-shot Neural Radiance Fields via Geometric Consistency","summary":"  We present a novel framework to regularize Neural Radiance Field (NeRF) in a\nfew-shot setting with a geometry-aware consistency regularization. The proposed\napproach leverages a rendered depth map at unobserved viewpoint to warp sparse\ninput images to the unobserved viewpoint and impose them as pseudo ground\ntruths to facilitate learning of NeRF. By encouraging such geometry-aware\nconsistency at a feature-level instead of using pixel-level reconstruction\nloss, we regularize the NeRF at semantic and structural levels while allowing\nfor modeling view dependent radiance to account for color variations across\nviewpoints. We also propose an effective method to filter out erroneous warped\nsolutions, along with training strategies to stabilize training during\noptimization. We show that our model achieves competitive results compared to\nstate-of-the-art few-shot NeRF models. Project page is available at\nhttps://ku-cvlab.github.io/GeCoNeRF/.\n","authors":["Minseop Kwak","Jiuhn Song","Seungryong Kim"],"pdf_url":"https://arxiv.org/pdf/2301.10941v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2108.06600v3","updated":"2023-03-20T14:53:07Z","published":"2021-08-14T18:16:12Z","title":"A Self-Distillation Embedded Supervised Affinity Attention Model for\n  Few-Shot Segmentation","summary":"  Few-shot segmentation focuses on the generalization of models to segment\nunseen object with limited annotated samples. However, existing approaches\nstill face two main challenges. First, huge feature distinction between support\nand query images causes knowledge transferring barrier, which harms the\nsegmentation performance. Second, limited support prototypes cannot adequately\nrepresent features of support objects, hard to guide high-quality query\nsegmentation. To deal with the above two issues, we propose self-distillation\nembedded supervised affinity attention model to improve the performance of\nfew-shot segmentation task. Specifically, the self-distillation guided\nprototype module uses self-distillation to align the features of support and\nquery. The supervised affinity attention module generates high-quality query\nattention map to provide sufficient object information. Extensive experiments\nprove that our model significantly improves the performance compared to\nexisting methods. Comprehensive ablation experiments and visualization studies\nalso show the significant effect of our method on few-shot segmentation task.\nOn COCO-20i dataset, we achieve new state-of-the-art results. Training code and\npretrained models are available at https://github.com/cv516Buaa/SD-AANet.\n","authors":["Qi Zhao","Binghao Liu","Shuchang Lyu","Huojin Chen"],"pdf_url":"https://arxiv.org/pdf/2108.06600v3.pdf","comment":"14 pages, 12 figures"},{"id":"http://arxiv.org/abs/2303.11165v1","updated":"2023-03-20T14:50:27Z","published":"2023-03-20T14:50:27Z","title":"Computationally Budgeted Continual Learning: What Does Matter?","summary":"  Continual Learning (CL) aims to sequentially train models on streams of\nincoming data that vary in distribution by preserving previous knowledge while\nadapting to new data. Current CL literature focuses on restricted access to\npreviously seen data, while imposing no constraints on the computational budget\nfor training. This is unreasonable for applications in-the-wild, where systems\nare primarily constrained by computational and time budgets, not storage. We\nrevisit this problem with a large-scale benchmark and analyze the performance\nof traditional CL approaches in a compute-constrained setting, where effective\nmemory samples used in training can be implicitly restricted as a consequence\nof limited computation. We conduct experiments evaluating various CL sampling\nstrategies, distillation losses, and partial fine-tuning on two large-scale\ndatasets, namely ImageNet2K and Continual Google Landmarks V2 in data\nincremental, class incremental, and time incremental settings. Through\nextensive experiments amounting to a total of over 1500 GPU-hours, we find\nthat, under compute-constrained setting, traditional CL approaches, with no\nexception, fail to outperform a simple minimal baseline that samples uniformly\nfrom memory. Our conclusions are consistent in a different number of stream\ntime steps, e.g., 20 to 200, and under several computational budgets. This\nsuggests that most existing CL methods are particularly too computationally\nexpensive for realistic budgeted deployment. Code for this project is available\nat: https://github.com/drimpossible/BudgetCL.\n","authors":["Ameya Prabhu","Hasan Abed Al Kader Hammoud","Puneet Dokania","Philip H. S. Torr","Ser-Nam Lim","Bernard Ghanem","Adel Bibi"],"pdf_url":"https://arxiv.org/pdf/2303.11165v1.pdf","comment":"Appearing in CVPR 2023"},{"id":"http://arxiv.org/abs/2303.11162v1","updated":"2023-03-20T14:49:03Z","published":"2023-03-20T14:49:03Z","title":"Picture that Sketch: Photorealistic Image Generation from Abstract\n  Sketches","summary":"  Given an abstract, deformed, ordinary sketch from untrained amateurs like you\nand me, this paper turns it into a photorealistic image - just like those shown\nin Fig. 1(a), all non-cherry-picked. We differ significantly from prior art in\nthat we do not dictate an edgemap-like sketch to start with, but aim to work\nwith abstract free-hand human sketches. In doing so, we essentially democratise\nthe sketch-to-photo pipeline, \"picturing\" a sketch regardless of how good you\nsketch. Our contribution at the outset is a decoupled encoder-decoder training\nparadigm, where the decoder is a StyleGAN trained on photos only. This\nimportantly ensures that generated results are always photorealistic. The rest\nis then all centred around how best to deal with the abstraction gap between\nsketch and photo. For that, we propose an autoregressive sketch mapper trained\non sketch-photo pairs that maps a sketch to the StyleGAN latent space. We\nfurther introduce specific designs to tackle the abstract nature of human\nsketches, including a fine-grained discriminative loss on the back of a trained\nsketch-photo retrieval model, and a partial-aware sketch augmentation strategy.\nFinally, we showcase a few downstream tasks our generation model enables,\namongst them is showing how fine-grained sketch-based image retrieval, a\nwell-studied problem in the sketch community, can be reduced to an image\n(generated) to image retrieval task, surpassing state-of-the-arts. We put\nforward generated results in the supplementary for everyone to scrutinise.\n","authors":["Subhadeep Koley","Ayan Kumar Bhunia","Aneeshan Sain","Pinaki Nath Chowdhury","Tao Xiang","Yi-Zhe song"],"pdf_url":"https://arxiv.org/pdf/2303.11162v1.pdf","comment":"Accepted in CVPR 2023. Project page available at\n  https://subhadeepkoley.github.io/PictureThatSketch"},{"id":"http://arxiv.org/abs/2303.09917v2","updated":"2023-03-20T14:39:32Z","published":"2023-03-16T13:43:02Z","title":"Vision Transformer for Action Units Detection","summary":"  Facial Action Units detection (FAUs) represents a fine-grained classification\nproblem that involves identifying different units on the human face, as defined\nby the Facial Action Coding System. In this paper, we present a simple yet\nefficient Vision Transformer-based approach for addressing the task of Action\nUnits (AU) detection in the context of Affective Behavior Analysis in-the-wild\n(ABAW) competition. We employ the Video Vision Transformer(ViViT) Network to\ncapture the temporal facial change in the video. Besides, to reduce massive\nsize of the Vision Transformers model, we replace the ViViT feature extraction\nlayers with the CNN backbone (Regnet). Our model outperform the baseline model\nof ABAW 2023 challenge, with a notable 14% difference in result. Furthermore,\nthe achieved results are comparable to those of the top three teams in the\nprevious ABAW 2022 challenge.\n","authors":["Tu Vu","Van Thong Huynh","Soo Hyung Kim"],"pdf_url":"https://arxiv.org/pdf/2303.09917v2.pdf","comment":"Will be updated"},{"id":"http://arxiv.org/abs/2211.13203v3","updated":"2023-03-20T14:32:01Z","published":"2022-11-23T18:44:25Z","title":"Inversion-Based Style Transfer with Diffusion Models","summary":"  The artistic style within a painting is the means of expression, which\nincludes not only the painting material, colors, and brushstrokes, but also the\nhigh-level attributes including semantic elements, object shapes, etc. Previous\narbitrary example-guided artistic image generation methods often fail to\ncontrol shape changes or convey elements. The pre-trained text-to-image\nsynthesis diffusion probabilistic models have achieved remarkable quality, but\nit often requires extensive textual descriptions to accurately portray\nattributes of a particular painting. We believe that the uniqueness of an\nartwork lies precisely in the fact that it cannot be adequately explained with\nnormal language. Our key idea is to learn artistic style directly from a single\npainting and then guide the synthesis without providing complex textual\ndescriptions. Specifically, we assume style as a learnable textual description\nof a painting. We propose an inversion-based style transfer method (InST),\nwhich can efficiently and accurately learn the key information of an image,\nthus capturing and transferring the artistic style of a painting. We\ndemonstrate the quality and efficiency of our method on numerous paintings of\nvarious artists and styles. Code and models are available at\nhttps://github.com/zyxElsa/InST.\n","authors":["Yuxin Zhang","Nisha Huang","Fan Tang","Haibin Huang","Chongyang Ma","Weiming Dong","Changsheng Xu"],"pdf_url":"https://arxiv.org/pdf/2211.13203v3.pdf","comment":"accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2303.11137v1","updated":"2023-03-20T14:15:23Z","published":"2023-03-20T14:15:23Z","title":"AnimeDiffusion: Anime Face Line Drawing Colorization via Diffusion\n  Models","summary":"  It is a time-consuming and tedious work for manually colorizing anime line\ndrawing images, which is an essential stage in cartoon animation creation\npipeline. Reference-based line drawing colorization is a challenging task that\nrelies on the precise cross-domain long-range dependency modelling between the\nline drawing and reference image. Existing learning methods still utilize\ngenerative adversarial networks (GANs) as one key module of their model\narchitecture. In this paper, we propose a novel method called AnimeDiffusion\nusing diffusion models that performs anime face line drawing colorization\nautomatically. To the best of our knowledge, this is the first diffusion model\ntailored for anime content creation. In order to solve the huge training\nconsumption problem of diffusion models, we design a hybrid training strategy,\nfirst pre-training a diffusion model with classifier-free guidance and then\nfine-tuning it with image reconstruction guidance. We find that with a few\niterations of fine-tuning, the model shows wonderful colorization performance,\nas illustrated in Fig. 1. For training AnimeDiffusion, we conduct an anime face\nline drawing colorization benchmark dataset, which contains 31696 training data\nand 579 testing data. We hope this dataset can fill the gap of no available\nhigh resolution anime face dataset for colorization method evaluation. Through\nmultiple quantitative metrics evaluated on our dataset and a user study, we\ndemonstrate AnimeDiffusion outperforms state-of-the-art GANs-based models for\nanime face line drawing colorization. We also collaborate with professional\nartists to test and apply our AnimeDiffusion for their creation work. We\nrelease our code on https://github.com/xq-meng/AnimeDiffusion.\n","authors":["Yu Cao","Xiangqiao Meng","P. Y. Mok","Xueting Liu","Tong-Yee Lee","Ping Li"],"pdf_url":"https://arxiv.org/pdf/2303.11137v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11135v1","updated":"2023-03-20T14:12:55Z","published":"2023-03-20T14:12:55Z","title":"TWINS: A Fine-Tuning Framework for Improved Transferability of\n  Adversarial Robustness and Generalization","summary":"  Recent years have seen the ever-increasing importance of pre-trained models\nand their downstream training in deep learning research and applications. At\nthe same time, the defense for adversarial examples has been mainly\ninvestigated in the context of training from random initialization on simple\nclassification tasks. To better exploit the potential of pre-trained models in\nadversarial robustness, this paper focuses on the fine-tuning of an\nadversarially pre-trained model in various classification tasks. Existing\nresearch has shown that since the robust pre-trained model has already learned\na robust feature extractor, the crucial question is how to maintain the\nrobustness in the pre-trained model when learning the downstream task. We study\nthe model-based and data-based approaches for this goal and find that the two\ncommon approaches cannot achieve the objective of improving both generalization\nand adversarial robustness. Thus, we propose a novel statistics-based approach,\nTwo-WIng NormliSation (TWINS) fine-tuning framework, which consists of two\nneural networks where one of them keeps the population means and variances of\npre-training data in the batch normalization layers. Besides the robust\ninformation transfer, TWINS increases the effective learning rate without\nhurting the training stability since the relationship between a weight norm and\nits gradient norm in standard batch normalization layer is broken, resulting in\na faster escape from the sub-optimal initialization and alleviating the robust\noverfitting. Finally, TWINS is shown to be effective on a wide range of image\nclassification datasets in terms of both generalization and robustness. Our\ncode is available at https://github.com/ziquanliu/CVPR2023-TWINS.\n","authors":["Ziquan Liu","Yi Xu","Xiangyang Ji","Antoni B. Chan"],"pdf_url":"https://arxiv.org/pdf/2303.11135v1.pdf","comment":"CVPR2023"},{"id":"http://arxiv.org/abs/2303.11130v1","updated":"2023-03-20T14:06:32Z","published":"2023-03-20T14:06:32Z","title":"Deep learning automated quantification of lung disease in pulmonary\n  hypertension on CT pulmonary angiography: A preliminary clinical study with\n  external validation","summary":"  Purpose: Lung disease assessment in precapillary pulmonary hypertension (PH)\nis essential for appropriate patient management. This study aims to develop an\nartificial intelligence (AI) deep learning model for lung texture\nclassification in CT Pulmonary Angiography (CTPA), and evaluate its correlation\nwith clinical assessment methods.\n  Materials and Methods: In this retrospective study with external validation,\n122 patients with pre-capillary PH were used to train (n=83), validate (n=17)\nand test (n=10 internal test, n=12 external test) a patch based DenseNet-121\nclassification model. \"Normal\", \"Ground glass\", \"Ground glass with\nreticulation\", \"Honeycombing\", and \"Emphysema\" were classified as per the\nFleishner Society glossary of terms. Ground truth classes were segmented by two\nradiologists with patches extracted from the labelled regions. Proportion of\nlung volume for each texture was calculated by classifying patches throughout\nthe entire lung volume to generate a coarse texture classification mapping\nthroughout the lung parenchyma. AI output was assessed against diffusing\ncapacity of carbon monoxide (DLCO) and specialist radiologist reported disease\nseverity.\n  Results: Micro-average AUCs for the validation, internal test, and external\ntest were 0.92, 0.95, and 0.94, respectively. The model had consistent\nperformance across parenchymal textures, demonstrated strong correlation with\ndiffusing capacity of carbon monoxide (DLCO), and showed good correspondence\nwith disease severity reported by specialist radiologists.\n  Conclusion: The classification model demonstrates excellent performance on\nexternal validation. The clinical utility of its output has been demonstrated.\nThis objective, repeatable measure of disease severity can aid in patient\nmanagement in adjunct to radiological reporting.\n","authors":["Michael J. Sharkey","Krit Dwivedi","Samer Alabed","Andrew J. Swift"],"pdf_url":"https://arxiv.org/pdf/2303.11130v1.pdf","comment":"16 pages, 3 figures, 2 tables"},{"id":"http://arxiv.org/abs/2303.11127v1","updated":"2023-03-20T14:04:50Z","published":"2023-03-20T14:04:50Z","title":"MT-SNN: Enhance Spiking Neural Network with Multiple Thresholds","summary":"  Spiking neural networks (SNNs), as a biology-inspired method mimicking the\nspiking nature of brain neurons, is a promising energy-efficient alternative to\nthe traditional artificial neural networks (ANNs). The energy saving of SNNs is\nmainly from multiplication free property brought by binarized intermediate\nactivations. In this paper, we proposed a Multiple Threshold (MT) approach to\nalleviate the precision loss brought by the binarized activations, such that\nSNNs can reach higher accuracy at fewer steps. We evaluate the approach on\nCIFAR10, CIFAR100 and DVS-CIFAR10, and demonstrate that MT can promote SNNs\nextensively, especially at early steps. For example, With MT,\nParametric-Leaky-Integrate-Fire(PLIF) based VGG net can even outperform the ANN\ncounterpart with 1 step.\n","authors":["Xiaoting Wang","Yanxiang Zhang","Yongzhe Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.11127v1.pdf","comment":"10 pages, 5 figures, 3 tables"},{"id":"http://arxiv.org/abs/2303.11126v1","updated":"2023-03-20T14:04:40Z","published":"2023-03-20T14:04:40Z","title":"Robustifying Token Attention for Vision Transformers","summary":"  Despite the success of vision transformers (ViTs), they still suffer from\nsignificant drops in accuracy in the presence of common corruptions, such as\nnoise or blur. Interestingly, we observe that the attention mechanism of ViTs\ntends to rely on few important tokens, a phenomenon we call token overfocusing.\nMore critically, these tokens are not robust to corruptions, often leading to\nhighly diverging attention patterns. In this paper, we intend to alleviate this\noverfocusing issue and make attention more stable through two general\ntechniques: First, our Token-aware Average Pooling (TAP) module encourages the\nlocal neighborhood of each token to take part in the attention mechanism.\nSpecifically, TAP learns average pooling schemes for each token such that the\ninformation of potentially important tokens in the neighborhood can adaptively\nbe taken into account. Second, we force the output tokens to aggregate\ninformation from a diverse set of input tokens rather than focusing on just a\nfew by using our Attention Diversification Loss (ADL). We achieve this by\npenalizing high cosine similarity between the attention vectors of different\ntokens. In experiments, we apply our methods to a wide range of transformer\narchitectures and improve robustness significantly. For example, we improve\ncorruption robustness on ImageNet-C by 2.4% while simultaneously improving\naccuracy by 0.4% based on state-of-the-art robust architecture FAN. Also, when\nfinetuning on semantic segmentation tasks, we improve robustness on\nCityScapes-C by 2.4% and ACDC by 3.1%.\n","authors":["Yong Guo","David Stutz","Bernt Schiele"],"pdf_url":"https://arxiv.org/pdf/2303.11126v1.pdf","comment":"7 figures and 5 tables in the main paper"},{"id":"http://arxiv.org/abs/2303.11120v1","updated":"2023-03-20T14:01:01Z","published":"2023-03-20T14:01:01Z","title":"Positional Diffusion: Ordering Unordered Sets with Diffusion\n  Probabilistic Models","summary":"  Positional reasoning is the process of ordering unsorted parts contained in a\nset into a consistent structure. We present Positional Diffusion, a\nplug-and-play graph formulation with Diffusion Probabilistic Models to address\npositional reasoning. We use the forward process to map elements' positions in\na set to random positions in a continuous space. Positional Diffusion learns to\nreverse the noising process and recover the original positions through an\nAttention-based Graph Neural Network. We conduct extensive experiments with\nbenchmark datasets including two puzzle datasets, three sentence ordering\ndatasets, and one visual storytelling dataset, demonstrating that our method\noutperforms long-lasting research on puzzle solving with up to +18% compared to\nthe second-best deep learning method, and performs on par against the\nstate-of-the-art methods on sentence ordering and visual storytelling. Our work\nhighlights the suitability of diffusion models for ordering problems and\nproposes a novel formulation and method for solving various ordering tasks.\nProject website at https://iit-pavis.github.io/Positional_Diffusion/\n","authors":["Francesco Giuliari","Gianluca Scarpellini","Stuart James","Yiming Wang","Alessio Del Bue"],"pdf_url":"https://arxiv.org/pdf/2303.11120v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11114v1","updated":"2023-03-20T13:55:35Z","published":"2023-03-20T13:55:35Z","title":"SeiT: Storage-Efficient Vision Training with Tokens Using 1% of Pixel\n  Storage","summary":"  We need billion-scale images to achieve more generalizable and\nground-breaking vision models, as well as massive dataset storage to ship the\nimages (e.g., the LAION-4B dataset needs 240TB storage space). However, it has\nbecome challenging to deal with unlimited dataset storage with limited storage\ninfrastructure. A number of storage-efficient training methods have been\nproposed to tackle the problem, but they are rarely scalable or suffer from\nsevere damage to performance. In this paper, we propose a storage-efficient\ntraining strategy for vision classifiers for large-scale datasets (e.g.,\nImageNet) that only uses 1024 tokens per instance without using the raw level\npixels; our token storage only needs <1% of the original JPEG-compressed raw\npixels. We also propose token augmentations and a Stem-adaptor module to make\nour approach able to use the same architecture as pixel-based approaches with\nonly minimal modifications on the stem layer and the carefully tuned\noptimization settings. Our experimental results on ImageNet-1k show that our\nmethod significantly outperforms other storage-efficient training methods with\na large gap. We further show the effectiveness of our method in other practical\nscenarios, storage-efficient pre-training, and continual learning. Code is\navailable at https://github.com/naver-ai/seit\n","authors":["Song Park","Sanghyuk Chun","Byeongho Heo","Wonjae Kim","Sangdoo Yun"],"pdf_url":"https://arxiv.org/pdf/2303.11114v1.pdf","comment":"First two authors contributed equally; 15 pages, 1.1MB"},{"id":"http://arxiv.org/abs/2303.11108v1","updated":"2023-03-20T13:45:58Z","published":"2023-03-20T13:45:58Z","title":"I2Edit: Towards Multi-turn Interactive Image Editing via Dialogue","summary":"  Although there have been considerable research efforts on controllable facial\nimage editing, the desirable interactive setting where the users can interact\nwith the system to adjust their requirements dynamically hasn't been well\nexplored. This paper focuses on facial image editing via dialogue and\nintroduces a new benchmark dataset, Multi-turn Interactive Image Editing\n(I2Edit), for evaluating image editing quality and interaction ability in\nreal-world interactive facial editing scenarios. The dataset is constructed\nupon the CelebA-HQ dataset with images annotated with a multi-turn dialogue\nthat corresponds to the user editing requirements. I2Edit is challenging, as it\nneeds to 1) track the dynamically updated user requirements and edit the images\naccordingly, as well as 2) generate the appropriate natural language response\nto communicate with the user. To address these challenges, we propose a\nframework consisting of a dialogue module and an image editing module. The\nformer is for user edit requirements tracking and generating the corresponding\nindicative responses, while the latter edits the images conditioned on the\ntracked user edit requirements. In contrast to previous works that simply treat\nmulti-turn interaction as a sequence of single-turn interactions, we extract\nthe user edit requirements from the whole dialogue history instead of the\ncurrent single turn. The extracted global user edit requirements enable us to\ndirectly edit the input raw image to avoid error accumulation and attribute\nforgetting issues. Extensive quantitative and qualitative experiments on the\nI2Edit dataset demonstrate the advantage of our proposed framework over the\nprevious single-turn methods. We believe our new dataset could serve as a\nvaluable resource to push forward the exploration of real-world, complex\ninteractive image editing. Code and data will be made public.\n","authors":["Xing Cui","Zekun Li","Peipei Li","Yibo Hu","Hailin Shi","Zhaofeng He"],"pdf_url":"https://arxiv.org/pdf/2303.11108v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11101v1","updated":"2023-03-20T13:38:29Z","published":"2023-03-20T13:38:29Z","title":"Coreset Sampling from Open-Set for Fine-Grained Self-Supervised Learning","summary":"  Deep learning in general domains has constantly been extended to\ndomain-specific tasks requiring the recognition of fine-grained\ncharacteristics. However, real-world applications for fine-grained tasks suffer\nfrom two challenges: a high reliance on expert knowledge for annotation and\nnecessity of a versatile model for various downstream tasks in a specific\ndomain (e.g., prediction of categories, bounding boxes, or pixel-wise\nannotations). Fortunately, the recent self-supervised learning (SSL) is a\npromising approach to pretrain a model without annotations, serving as an\neffective initialization for any downstream tasks. Since SSL does not rely on\nthe presence of annotation, in general, it utilizes the large-scale unlabeled\ndataset, referred to as an open-set. In this sense, we introduce a novel\nOpen-Set Self-Supervised Learning problem under the assumption that a\nlarge-scale unlabeled open-set is available, as well as the fine-grained target\ndataset, during a pretraining phase. In our problem setup, it is crucial to\nconsider the distribution mismatch between the open-set and target dataset.\nHence, we propose SimCore algorithm to sample a coreset, the subset of an\nopen-set that has a minimum distance to the target dataset in the latent space.\nWe demonstrate that SimCore significantly improves representation learning\nperformance through extensive experimental settings, including eleven\nfine-grained datasets and seven open-sets in various downstream tasks.\n","authors":["Sungnyun Kim","Sangmin Bae","Se-Young Yun"],"pdf_url":"https://arxiv.org/pdf/2303.11101v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.11100v1","updated":"2023-03-20T13:34:28Z","published":"2023-03-20T13:34:28Z","title":"A Multi-Task Deep Learning Approach for Sensor-based Human Activity\n  Recognition and Segmentation","summary":"  Sensor-based human activity segmentation and recognition are two important\nand challenging problems in many real-world applications and they have drawn\nincreasing attention from the deep learning community in recent years. Most of\nthe existing deep learning works were designed based on pre-segmented sensor\nstreams and they have treated activity segmentation and recognition as two\nseparate tasks. In practice, performing data stream segmentation is very\nchallenging. We believe that both activity segmentation and recognition may\nconvey unique information which can complement each other to improve the\nperformance of the two tasks. In this paper, we firstly proposes a new\nmultitask deep neural network to solve the two tasks simultaneously. The\nproposed neural network adopts selective convolution and features multiscale\nwindows to segment activities of long or short time durations. First, multiple\nwindows of different scales are generated to center on each unit of the feature\nsequence. Then, the model is trained to predict, for each window, the activity\nclass and the offset to the true activity boundaries. Finally, overlapping\nwindows are filtered out by non-maximum suppression, and adjacent windows of\nthe same activity are concatenated to complete the segmentation task. Extensive\nexperiments were conducted on eight popular benchmarking datasets, and the\nresults show that our proposed method outperforms the state-of-the-art methods\nboth for activity recognition and segmentation.\n","authors":["Furong Duan","Tao Zhu","Jinqiang Wang","Liming Chen","Huansheng Ning","Yaping Wan"],"pdf_url":"https://arxiv.org/pdf/2303.11100v1.pdf","comment":"14 pages, 14 figures"},{"id":"http://arxiv.org/abs/2303.11098v1","updated":"2023-03-20T13:33:31Z","published":"2023-03-20T13:33:31Z","title":"A closer look at the training dynamics of knowledge distillation","summary":"  In this paper we revisit the efficacy of knowledge distillation as a function\nmatching and metric learning problem. In doing so we verify three important\ndesign decisions, namely the normalisation, soft maximum function, and\nprojection layers as key ingredients. We theoretically show that the projector\nimplicitly encodes information on past examples, enabling relational gradients\nfor the student. We then show that the normalisation of representations is\ntightly coupled with the training dynamics of this projector, which can have a\nlarge impact on the students performance. Finally, we show that a simple soft\nmaximum function can be used to address any significant capacity gap problems.\nExperimental results on various benchmark datasets demonstrate that using these\ninsights can lead to superior or comparable performance to state-of-the-art\nknowledge distillation techniques, despite being much more computationally\nefficient. In particular, we obtain these results across image classification\n(CIFAR100 and ImageNet), object detection (COCO2017), and on more difficult\ndistillation objectives, such as training data efficient transformers, whereby\nwe attain a 77.2% top-1 accuracy with DeiT-Ti on ImageNet.\n","authors":["Roy Miles","Krystian Mikolajczyk"],"pdf_url":"https://arxiv.org/pdf/2303.11098v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07564v2","updated":"2023-03-20T13:28:36Z","published":"2023-03-14T01:10:59Z","title":"Unsupervised Cumulative Domain Adaptation for Foggy Scene Optical Flow","summary":"  Optical flow has achieved great success under clean scenes, but suffers from\nrestricted performance under foggy scenes. To bridge the clean-to-foggy domain\ngap, the existing methods typically adopt the domain adaptation to transfer the\nmotion knowledge from clean to synthetic foggy domain. However, these methods\nunexpectedly neglect the synthetic-to-real domain gap, and thus are erroneous\nwhen applied to real-world scenes. To handle the practical optical flow under\nreal foggy scenes, in this work, we propose a novel unsupervised cumulative\ndomain adaptation optical flow (UCDA-Flow) framework: depth-association motion\nadaptation and correlation-alignment motion adaptation. Specifically, we\ndiscover that depth is a key ingredient to influence the optical flow: the\ndeeper depth, the inferior optical flow, which motivates us to design a\ndepth-association motion adaptation module to bridge the clean-to-foggy domain\ngap. Moreover, we figure out that the cost volume correlation shares similar\ndistribution of the synthetic and real foggy images, which enlightens us to\ndevise a correlation-alignment motion adaptation module to distill motion\nknowledge of the synthetic foggy domain to the real foggy domain. Note that\nsynthetic fog is designed as the intermediate domain. Under this unified\nframework, the proposed cumulative adaptation progressively transfers knowledge\nfrom clean scenes to real foggy scenes. Extensive experiments have been\nperformed to verify the superiority of the proposed method.\n","authors":["Hanyu Zhou","Yi Chang","Wending Yan","Luxin Yan"],"pdf_url":"https://arxiv.org/pdf/2303.07564v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11090v1","updated":"2023-03-20T13:22:56Z","published":"2023-03-20T13:22:56Z","title":"Scene Graph Based Fusion Network For Image-Text Retrieval","summary":"  A critical challenge to image-text retrieval is how to learn accurate\ncorrespondences between images and texts. Most existing methods mainly focus on\ncoarse-grained correspondences based on co-occurrences of semantic objects,\nwhile failing to distinguish the fine-grained local correspondences. In this\npaper, we propose a novel Scene Graph based Fusion Network (dubbed SGFN), which\nenhances the images'/texts' features through intra- and cross-modal fusion for\nimage-text retrieval. To be specific, we design an intra-modal hierarchical\nattention fusion to incorporate semantic contexts, such as objects, attributes,\nand relationships, into images'/texts' feature vectors via scene graphs, and a\ncross-modal attention fusion to combine the contextual semantics and local\nfusion via contextual vectors. Extensive experiments on public datasets\nFlickr30K and MSCOCO show that our SGFN performs better than quite a few SOTA\nimage-text retrieval methods.\n","authors":["Guoliang Wang","Yanlei Shang","Yong Chen"],"pdf_url":"https://arxiv.org/pdf/2303.11090v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11089v1","updated":"2023-03-20T13:22:04Z","published":"2023-03-20T13:22:04Z","title":"EmoTalk: Speech-driven emotional disentanglement for 3D face animation","summary":"  Speech-driven 3D face animation aims to generate realistic facial expressions\nthat match the speech content and emotion. However, existing methods often\nneglect emotional facial expressions or fail to disentangle them from speech\ncontent. To address this issue, this paper proposes an end-to-end neural\nnetwork to disentangle different emotions in speech so as to generate rich 3D\nfacial expressions. Specifically, we introduce the emotion disentangling\nencoder (EDE) to disentangle the emotion and content in the speech by\ncross-reconstructed speech signals with different emotion labels. Then an\nemotion-guided feature fusion decoder is employed to generate a 3D talking face\nwith enhanced emotion. The decoder is driven by the disentangled identity,\nemotional, and content embeddings so as to generate controllable personal and\nemotional styles. Finally, considering the scarcity of the 3D emotional talking\nface data, we resort to the supervision of facial blendshapes, which enables\nthe reconstruction of plausible 3D faces from 2D emotional data, and contribute\na large-scale 3D emotional talking face dataset (3D-ETF) to train the network.\nOur experiments and user studies demonstrate that our approach outperforms\nstate-of-the-art methods and exhibits more diverse facial movements. We\nrecommend watching the supplementary video:\nhttps://ziqiaopeng.github.io/emotalk\n","authors":["Ziqiao Peng","Haoyu Wu","Zhenbo Song","Hao Xu","Xiangyu Zhu","Hongyan Liu","Jun He","Zhaoxin Fan"],"pdf_url":"https://arxiv.org/pdf/2303.11089v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11086v1","updated":"2023-03-20T13:20:14Z","published":"2023-03-20T13:20:14Z","title":"Pluralistic Aging Diffusion Autoencoder","summary":"  Face aging is an ill-posed problem because multiple plausible aging patterns\nmay correspond to a given input. Most existing methods often produce one\ndeterministic estimation. This paper proposes a novel CLIP-driven Pluralistic\nAging Diffusion Autoencoder (PADA) to enhance the diversity of aging patterns.\nFirst, we employ diffusion models to generate diverse low-level aging details\nvia a sequential denoising reverse process. Second, we present Probabilistic\nAging Embedding (PAE) to capture diverse high-level aging patterns, which\nrepresents age information as probabilistic distributions in the common CLIP\nlatent space. A text-guided KL-divergence loss is designed to guide this\nlearning. Our method can achieve pluralistic face aging conditioned on\nopen-world aging texts and arbitrary unseen face images. Qualitative and\nquantitative experiments demonstrate that our method can generate more diverse\nand high-quality plausible aging results.\n","authors":["Peipei Li","Rui Wang","Huaibo Huang","Ran He","Zhaofeng He"],"pdf_url":"https://arxiv.org/pdf/2303.11086v1.pdf","comment":"8 pages, 8 figures"},{"id":"http://arxiv.org/abs/2303.09858v2","updated":"2023-03-20T13:00:49Z","published":"2023-03-17T09:37:41Z","title":"MedLocker: A Transferable Adversarial Watermarking for Preventing\n  Unauthorized Analysis of Medical Image Dataset","summary":"  The collection of medical image datasets is a demanding and laborious process\nthat requires significant resources. Furthermore, these medical datasets may\ncontain personally identifiable information, necessitating measures to ensure\nthat unauthorized access is prevented. Failure to do so could violate the\nintellectual property rights of the dataset owner and potentially compromise\nthe privacy of patients. As a result, safeguarding medical datasets and\npreventing unauthorized usage by AI diagnostic models is a pressing challenge.\nTo address this challenge, we propose a novel visible adversarial watermarking\nmethod for medical image copyright protection, called MedLocker. Our approach\ninvolves continuously optimizing the position and transparency of a watermark\nlogo, which reduces the performance of the target model, leading to incorrect\npredictions. Importantly, we ensure that our method minimizes the impact on\nclinical visualization by constraining watermark positions using semantical\nmasks (WSM), which are bounding boxes of lesion regions based on semantic\nsegmentation. To ensure the transferability of the watermark across different\nmodels, we verify the cross-model transferability of the watermark generated on\na single model. Additionally, we generate a unique watermark parameter list\neach time, which can be used as a certification to verify the authorization. We\nevaluate the performance of MedLocker on various mainstream backbones and\nvalidate the feasibility of adversarial watermarking for copyright protection\non two widely-used diabetic retinopathy detection datasets. Our results\ndemonstrate that MedLocker can effectively protect the copyright of medical\ndatasets and prevent unauthorized users from analyzing medical images with AI\ndiagnostic models.\n","authors":["Bangzheng Pu","Xingxing Wei","Shiji Zhao","Huazhu Fu"],"pdf_url":"https://arxiv.org/pdf/2303.09858v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11073v1","updated":"2023-03-20T12:59:32Z","published":"2023-03-20T12:59:32Z","title":"Discovering Interpretable Directions in the Semantic Latent Space of\n  Diffusion Models","summary":"  Denoising Diffusion Models (DDMs) have emerged as a strong competitor to\nGenerative Adversarial Networks (GANs). However, despite their widespread use\nin image synthesis and editing applications, their latent space is still not as\nwell understood. Recently, a semantic latent space for DDMs, coined\n`$h$-space', was shown to facilitate semantic image editing in a way\nreminiscent of GANs. The $h$-space is comprised of the bottleneck activations\nin the DDM's denoiser across all timesteps of the diffusion process. In this\npaper, we explore the properties of h-space and propose several novel methods\nfor finding meaningful semantic directions within it. We start by studying\nunsupervised methods for revealing interpretable semantic directions in\npretrained DDMs. Specifically, we show that global latent directions emerge as\nthe principal components in the latent space. Additionally, we provide a novel\nmethod for discovering image-specific semantic directions by spectral analysis\nof the Jacobian of the denoiser w.r.t. the latent code. Next, we extend the\nanalysis by finding directions in a supervised fashion in unconditional DDMs.\nWe demonstrate how such directions can be found by relying on either a labeled\ndata set of real images or by annotating generated samples with a\ndomain-specific attribute classifier. We further show how to semantically\ndisentangle the found direction by simple linear projection. Our approaches are\napplicable without requiring any architectural modifications, text-based\nguidance, CLIP-based optimization, or model fine-tuning.\n","authors":["René Haas","Inbar Huberman-Spiegelglas","Rotem Mulayoff","Tomer Michaeli"],"pdf_url":"https://arxiv.org/pdf/2303.11073v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.11436v3","updated":"2023-03-20T12:48:37Z","published":"2022-11-21T13:23:52Z","title":"N-Gram in Swin Transformers for Efficient Lightweight Image\n  Super-Resolution","summary":"  While some studies have proven that Swin Transformer (Swin) with window\nself-attention (WSA) is suitable for single image super-resolution (SR), the\nplain WSA ignores the broad regions when reconstructing high-resolution images\ndue to a limited receptive field. In addition, many deep learning SR methods\nsuffer from intensive computations. To address these problems, we introduce the\nN-Gram context to the low-level vision with Transformers for the first time. We\ndefine N-Gram as neighboring local windows in Swin, which differs from text\nanalysis that views N-Gram as consecutive characters or words. N-Grams interact\nwith each other by sliding-WSA, expanding the regions seen to restore degraded\npixels. Using the N-Gram context, we propose NGswin, an efficient SR network\nwith SCDP bottleneck taking multi-scale outputs of the hierarchical encoder.\nExperimental results show that NGswin achieves competitive performance while\nmaintaining an efficient structure when compared with previous leading methods.\nMoreover, we also improve other Swin-based SR methods with the N-Gram context,\nthereby building an enhanced model: SwinIR-NG. Our improved SwinIR-NG\noutperforms the current best lightweight SR approaches and establishes\nstate-of-the-art results. Codes are available at\nhttps://github.com/rami0205/NGramSwin.\n","authors":["Haram Choi","Jeongmin Lee","Jihoon Yang"],"pdf_url":"https://arxiv.org/pdf/2211.11436v3.pdf","comment":"CVPR 2023 camera-ready. Codes are available at\n  https://github.com/rami0205/NGramSwin"},{"id":"http://arxiv.org/abs/2303.11066v1","updated":"2023-03-20T12:44:11Z","published":"2023-03-20T12:44:11Z","title":"Boosting Semi-Supervised Learning by Exploiting All Unlabeled Data","summary":"  Semi-supervised learning (SSL) has attracted enormous attention due to its\nvast potential of mitigating the dependence on large labeled datasets. The\nlatest methods (e.g., FixMatch) use a combination of consistency regularization\nand pseudo-labeling to achieve remarkable successes. However, these methods all\nsuffer from the waste of complicated examples since all pseudo-labels have to\nbe selected by a high threshold to filter out noisy ones. Hence, the examples\nwith ambiguous predictions will not contribute to the training phase. For\nbetter leveraging all unlabeled examples, we propose two novel techniques:\nEntropy Meaning Loss (EML) and Adaptive Negative Learning (ANL). EML\nincorporates the prediction distribution of non-target classes into the\noptimization objective to avoid competition with target class, and thus\ngenerating more high-confidence predictions for selecting pseudo-label. ANL\nintroduces the additional negative pseudo-label for all unlabeled data to\nleverage low-confidence examples. It adaptively allocates this label by\ndynamically evaluating the top-k performance of the model. EML and ANL do not\nintroduce any additional parameter and hyperparameter. We integrate these\ntechniques with FixMatch, and develop a simple yet powerful framework called\nFullMatch. Extensive experiments on several common SSL benchmarks\n(CIFAR-10/100, SVHN, STL-10 and ImageNet) demonstrate that FullMatch exceeds\nFixMatch by a large margin. Integrated with FlexMatch (an advanced\nFixMatch-based framework), we achieve state-of-the-art performance. Source code\nis at https://github.com/megvii-research/FullMatch.\n","authors":["Yuhao Chen","Xin Tan","Borui Zhao","Zhaowei Chen","Renjie Song","Jiajun Liang","Xuequan Lu"],"pdf_url":"https://arxiv.org/pdf/2303.11066v1.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2112.12925v2","updated":"2023-03-20T12:30:36Z","published":"2021-12-24T03:25:40Z","title":"Not All Voxels Are Equal: Semantic Scene Completion from the Point-Voxel\n  Perspective","summary":"  We revisit Semantic Scene Completion (SSC), a useful task to predict the\nsemantic and occupancy representation of 3D scenes, in this paper. A number of\nmethods for this task are always based on voxelized scene representations for\nkeeping local scene structure. However, due to the existence of visible empty\nvoxels, these methods always suffer from heavy computation redundancy when the\nnetwork goes deeper, and thus limit the completion quality. To address this\ndilemma, we propose our novel point-voxel aggregation network for this task.\nFirstly, we transfer the voxelized scenes to point clouds by removing these\nvisible empty voxels and adopt a deep point stream to capture semantic\ninformation from the scene efficiently. Meanwhile, a light-weight voxel stream\ncontaining only two 3D convolution layers preserves local structures of the\nvoxelized scenes. Furthermore, we design an anisotropic voxel aggregation\noperator to fuse the structure details from the voxel stream into the point\nstream, and a semantic-aware propagation module to enhance the up-sampling\nprocess in the point stream by semantic labels. We demonstrate that our model\nsurpasses state-of-the-arts on two benchmarks by a large margin, with only\ndepth images as the input.\n","authors":["Xiaokang Chen","Jiaxiang Tang","Jingbo Wang","Gang Zeng"],"pdf_url":"https://arxiv.org/pdf/2112.12925v2.pdf","comment":"Accepted to AAAI 2022"},{"id":"http://arxiv.org/abs/2303.06869v3","updated":"2023-03-20T12:24:58Z","published":"2023-03-13T05:37:40Z","title":"Adaptive Data-Free Quantization","summary":"  Data-free quantization (DFQ) recovers the performance of quantized network\n(Q) without the original data, but generates the fake sample via a generator\n(G) by learning from full-precision network (P), which, however, is totally\nindependent of Q, overlooking the adaptability of the knowledge from generated\nsamples, i.e., informative or not to the learning process of Q, resulting into\nthe overflow of generalization error. Building on this, several critical\nquestions -- how to measure the sample adaptability to Q under varied bit-width\nscenarios? whether the largest adaptability is the best? how to generate the\nsamples with adaptive adaptability to improve Q's generalization? To answer the\nabove questions, in this paper, we propose an Adaptive Data-Free Quantization\n(AdaDFQ) method, which revisits DFQ from a zero-sum game perspective upon the\nsample adaptability between two players -- a generator and a quantized network.\nFollowing this viewpoint, we further define the disagreement and agreement\nsamples to form two boundaries, where the margin is optimized to adaptively\nregulate the adaptability of generated samples to Q, so as to address the\nover-and-under fitting issues. Our AdaDFQ reveals: 1) the largest adaptability\nis NOT the best for sample generation to benefit Q's generalization; 2) the\nknowledge of the generated sample should not be informative to Q only, but also\nrelated to the category and distribution information of the training data for\nP. The theoretical and empirical analysis validate the advantages of AdaDFQ\nover the state-of-the-arts. Our code is available at\nhttps://github.com/hfutqian/AdaDFQ.\n","authors":["Biao Qian","Yang Wang","Richang Hong","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2303.06869v3.pdf","comment":"9 pages, 6 figures, Refined camera ready version for CVPR 2023"},{"id":"http://arxiv.org/abs/2303.09158v2","updated":"2023-03-20T12:17:53Z","published":"2023-03-16T08:47:36Z","title":"Facial Affect Recognition based on Transformer Encoder and Audiovisual\n  Fusion for the ABAW5 Challenge","summary":"  In this paper, we present our solutions for the 5th Workshop and Competition\non Affective Behavior Analysis in-the-wild (ABAW), which includes four\nsub-challenges of Valence-Arousal (VA) Estimation, Expression (Expr)\nClassification, Action Unit (AU) Detection and Emotional Reaction Intensity\n(ERI) Estimation. The 5th ABAW competition focuses on facial affect recognition\nutilizing different modalities and datasets. In our work, we extract powerful\naudio and visual features using a large number of sota models. These features\nare fused by Transformer Encoder and TEMMA. Besides, to avoid the possible\nimpact of large dimensional differences between various features, we design an\nAffine Module to align different features to the same dimension. Extensive\nexperiments demonstrate that the superiority of the proposed method. For the VA\nEstimation sub-challenge, our method obtains the mean Concordance Correlation\nCoefficient (CCC) of 0.6066. For the Expression Classification sub-challenge,\nthe average F1 Score is 0.4055. For the AU Detection sub-challenge, the average\nF1 Score is 0.5296. For the Emotional Reaction Intensity Estimation\nsub-challenge, the average pearson's correlations coefficient on the validation\nset is 0.3968. All of the results of four sub-challenges outperform the\nbaseline with a large margin.\n","authors":["Ziyang Zhang","Liuwei An","Zishun Cui","Ao xu","Tengteng Dong","Yueqi Jiang","Jingyi Shi","Xin Liu","Xiao Sun","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2303.09158v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11057v1","updated":"2023-03-20T12:14:13Z","published":"2023-03-20T12:14:13Z","title":"Learning Foresightful Dense Visual Affordance for Deformable Object\n  Manipulation","summary":"  Understanding and manipulating deformable objects (e.g., ropes and fabrics)\nis an essential yet challenging task with broad applications. Difficulties come\nfrom complex states and dynamics, diverse configurations and high-dimensional\naction space of deformable objects. Besides, the manipulation tasks usually\nrequire multiple steps to accomplish, and greedy policies may easily lead to\nlocal optimal states. Existing studies usually tackle this problem using\nreinforcement learning or imitating expert demonstrations, with limitations in\nmodeling complex states or requiring hand-crafted expert policies. In this\npaper, we study deformable object manipulation using dense visual affordance,\nwith generalization towards diverse states, and propose a novel kind of\nforesightful dense affordance, which avoids local optima by estimating states'\nvalues for long-term manipulation. We propose a framework for learning this\nrepresentation, with novel designs such as multi-stage stable learning and\nefficient self-supervised data collection without experts. Experiments\ndemonstrate the superiority of our proposed foresightful dense affordance.\nProject page: https://hyperplane-lab.github.io/DeformableAffordance\n","authors":["Ruihai Wu","Chuanruo Ning","Hao Dong"],"pdf_url":"https://arxiv.org/pdf/2303.11057v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11055v1","updated":"2023-03-20T12:08:58Z","published":"2023-03-20T12:08:58Z","title":"Parameter-Free Channel Attention for Image Classification and\n  Super-Resolution","summary":"  The channel attention mechanism is a useful technique widely employed in deep\nconvolutional neural networks to boost the performance for image processing\ntasks, eg, image classification and image super-resolution. It is usually\ndesigned as a parameterized sub-network and embedded into the convolutional\nlayers of the network to learn more powerful feature representations. However,\ncurrent channel attention induces more parameters and therefore leads to higher\ncomputational costs. To deal with this issue, in this work, we propose a\nParameter-Free Channel Attention (PFCA) module to boost the performance of\npopular image classification and image super-resolution networks, but\ncompletely sweep out the parameter growth of channel attention. Experiments on\nCIFAR-100, ImageNet, and DIV2K validate that our PFCA module improves the\nperformance of ResNet on image classification and improves the performance of\nMSRResNet on image super-resolution tasks, respectively, while bringing little\ngrowth of parameters and FLOPs.\n","authors":["Yuxuan Shi","Lingxiao Yang","Wangpeng An","Xiantong Zhen","Liuqing Wang"],"pdf_url":"https://arxiv.org/pdf/2303.11055v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11052v1","updated":"2023-03-20T12:06:14Z","published":"2023-03-20T12:06:14Z","title":"ContraNeRF: Generalizable Neural Radiance Fields for Synthetic-to-real\n  Novel View Synthesis via Contrastive Learning","summary":"  Although many recent works have investigated generalizable NeRF-based novel\nview synthesis for unseen scenes, they seldom consider the synthetic-to-real\ngeneralization, which is desired in many practical applications. In this work,\nwe first investigate the effects of synthetic data in synthetic-to-real novel\nview synthesis and surprisingly observe that models trained with synthetic data\ntend to produce sharper but less accurate volume densities. For pixels where\nthe volume densities are correct, fine-grained details will be obtained.\nOtherwise, severe artifacts will be produced. To maintain the advantages of\nusing synthetic data while avoiding its negative effects, we propose to\nintroduce geometry-aware contrastive learning to learn multi-view consistent\nfeatures with geometric constraints. Meanwhile, we adopt cross-view attention\nto further enhance the geometry perception of features by querying features\nacross input views. Experiments demonstrate that under the synthetic-to-real\nsetting, our method can render images with higher quality and better\nfine-grained details, outperforming existing generalizable novel view synthesis\nmethods in terms of PSNR, SSIM, and LPIPS. When trained on real data, our\nmethod also achieves state-of-the-art results.\n","authors":["Hao Yang","Lanqing Hong","Aoxue Li","Tianyang Hu","Zhenguo Li","Gim Hee Lee","Liwei Wang"],"pdf_url":"https://arxiv.org/pdf/2303.11052v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11048v1","updated":"2023-03-20T11:59:23Z","published":"2023-03-20T11:59:23Z","title":"Revisiting Transformer for Point Cloud-based 3D Scene Graph Generation","summary":"  In this paper, we propose the semantic graph Transformer (SGT) for the 3D\nscene graph generation. The task aims to parse a cloud point-based scene into a\nsemantic structural graph, with the core challenge of modeling the complex\nglobal structure. Existing methods based on graph convolutional networks (GCNs)\nsuffer from the over-smoothing dilemma and could only propagate information\nfrom limited neighboring nodes. In contrast, our SGT uses Transformer layers as\nthe base building block to allow global information passing, with two types of\nproposed Transformer layers tailored for the 3D scene graph generation task.\nSpecifically, we introduce the graph embedding layer to best utilize the global\ninformation in graph edges while maintaining comparable computation costs.\nAdditionally, we propose the semantic injection layer to leverage categorical\ntext labels and visual object knowledge. We benchmark our SGT on the\nestablished 3DSSG benchmark and achieve a 35.9% absolute improvement in\nrelationship prediction's R@50 and an 80.40% boost on the subset with complex\nscenes over the state-of-the-art. Our analyses further show SGT's superiority\nin the long-tailed and zero-shot scenarios. We will release the code and model.\n","authors":["Changsheng Lv","Mengshi Qi","Xia Li","Zhengyuan Yang","Huadong Ma"],"pdf_url":"https://arxiv.org/pdf/2303.11048v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11041v1","updated":"2023-03-20T11:47:02Z","published":"2023-03-20T11:47:02Z","title":"From Sparse to Precise: A Practical Editing Approach for Intracardiac\n  Echocardiography Segmentation","summary":"  Accurate and safe catheter ablation procedures for patients with atrial\nfibrillation require precise segmentation of cardiac structures in Intracardiac\nEchocardiography (ICE) imaging. Prior studies have suggested methods that\nemploy 3D geometry information from the ICE transducer to create a sparse ICE\nvolume by placing 2D frames in a 3D grid, enabling training of 3D segmentation\nmodels. However, the resulting 3D masks from these models can be inaccurate and\nmay lead to serious clinical complications due to the sparse sampling in ICE\ndata, frames misalignment, and cardiac motion. To address this issue, we\npropose an interactive editing framework that allows users to edit segmentation\noutput by drawing scribbles on a 2D frame. The user interaction is mapped to\nthe 3D grid and utilized to execute an editing step that modifies the\nsegmentation in the vicinity of the interaction while preserving the previous\nsegmentation away from the interaction. Furthermore, our framework accommodates\nmultiple edits to the segmentation output in a sequential manner without\ncompromising previous edits. This paper presents a novel loss function and a\nnovel evaluation metric specifically designed for editing. Results from\ncross-validation and testing indicate that our proposed loss function\noutperforms standard losses and training strategies in terms of segmentation\nquality and following user input. Additionally, we show quantitatively and\nqualitatively that subsequent edits do not compromise previous edits when using\nour method, as opposed to standard segmentation losses. Overall, our approach\nenhances the accuracy of the segmentation while avoiding undesired changes away\nfrom user interactions and without compromising the quality of previously\nedited regions, leading to better patient outcomes.\n","authors":["Ahmed H. Shahin","Yan Zhuang","Noha El-Zehiry"],"pdf_url":"https://arxiv.org/pdf/2303.11041v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11040v1","updated":"2023-03-20T11:45:54Z","published":"2023-03-20T11:45:54Z","title":"Benchmarking Robustness of 3D Object Detection to Common Corruptions in\n  Autonomous Driving","summary":"  3D object detection is an important task in autonomous driving to perceive\nthe surroundings. Despite the excellent performance, the existing 3D detectors\nlack the robustness to real-world corruptions caused by adverse weathers,\nsensor noises, etc., provoking concerns about the safety and reliability of\nautonomous driving systems. To comprehensively and rigorously benchmark the\ncorruption robustness of 3D detectors, in this paper we design 27 types of\ncommon corruptions for both LiDAR and camera inputs considering real-world\ndriving scenarios. By synthesizing these corruptions on public datasets, we\nestablish three corruption robustness benchmarks -- KITTI-C, nuScenes-C, and\nWaymo-C. Then, we conduct large-scale experiments on 24 diverse 3D object\ndetection models to evaluate their corruption robustness. Based on the\nevaluation results, we draw several important findings, including: 1)\nmotion-level corruptions are the most threatening ones that lead to significant\nperformance drop of all models; 2) LiDAR-camera fusion models demonstrate\nbetter robustness; 3) camera-only models are extremely vulnerable to image\ncorruptions, showing the indispensability of LiDAR point clouds. We release the\nbenchmarks and codes at https://github.com/kkkcx/3D_Corruptions_AD. We hope\nthat our benchmarks and findings can provide insights for future research on\ndeveloping robust 3D object detection models.\n","authors":["Yinpeng Dong","Caixin Kang","Jinlai Zhang","Zijian Zhu","Yikai Wang","Xiao Yang","Hang Su","Xingxing Wei","Jun Zhu"],"pdf_url":"https://arxiv.org/pdf/2303.11040v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.11034v1","updated":"2023-03-20T11:36:09Z","published":"2023-03-20T11:36:09Z","title":"Internal Structure Attention Network for Fingerprint Presentation Attack\n  Detection from Optical Coherence Tomography","summary":"  As a non-invasive optical imaging technique, optical coherence tomography\n(OCT) has proven promising for automatic fingerprint recognition system (AFRS)\napplications. Diverse approaches have been proposed for OCT-based fingerprint\npresentation attack detection (PAD). However, considering the complexity and\nvariety of PA samples, it is extremely challenging to increase the\ngeneralization ability with the limited PA dataset. To solve the challenge,\nthis paper presents a novel supervised learning-based PAD method, denoted as\nISAPAD, which applies prior knowledge to guide network training and enhance the\ngeneralization ability. The proposed dual-branch architecture can not only\nlearns global features from the OCT image, but also concentrate on layered\nstructure feature which comes from the internal structure attention module\n(ISAM). The simple yet effective ISAM enables the proposed network to obtain\nlayered segmentation features belonging only to Bonafide from noisy OCT volume\ndata directly. Combined with effective training strategies and PAD score\ngeneration rules, ISAPAD obtains optimal PAD performance in limited training\ndata. Domain generalization experiments and visualization analysis validate the\neffectiveness of the proposed method for OCT PAD.\n","authors":["Haohao Sun","Yilong Zhang","Peng Chen","Haixia Wang","Ronghua Liang"],"pdf_url":"https://arxiv.org/pdf/2303.11034v1.pdf","comment":"12 pages, 14 figures"},{"id":"http://arxiv.org/abs/2211.11674v2","updated":"2023-03-20T11:33:18Z","published":"2022-11-21T17:42:42Z","title":"Shape, Pose, and Appearance from a Single Image via Bootstrapped\n  Radiance Field Inversion","summary":"  Neural Radiance Fields (NeRF) coupled with GANs represent a promising\ndirection in the area of 3D reconstruction from a single view, owing to their\nability to efficiently model arbitrary topologies. Recent work in this area,\nhowever, has mostly focused on synthetic datasets where exact ground-truth\nposes are known, and has overlooked pose estimation, which is important for\ncertain downstream applications such as augmented reality (AR) and robotics. We\nintroduce a principled end-to-end reconstruction framework for natural images,\nwhere accurate ground-truth poses are not available. Our approach recovers an\nSDF-parameterized 3D shape, pose, and appearance from a single image of an\nobject, without exploiting multiple views during training. More specifically,\nwe leverage an unconditional 3D-aware generator, to which we apply a hybrid\ninversion scheme where a model produces a first guess of the solution which is\nthen refined via optimization. Our framework can de-render an image in as few\nas 10 steps, enabling its use in practical scenarios. We demonstrate\nstate-of-the-art results on a variety of real and synthetic benchmarks.\n","authors":["Dario Pavllo","David Joseph Tan","Marie-Julie Rakotosaona","Federico Tombari"],"pdf_url":"https://arxiv.org/pdf/2211.11674v2.pdf","comment":"CVPR 2023. Code and models are available at\n  https://github.com/google-research/nerf-from-image"},{"id":"http://arxiv.org/abs/2211.10307v2","updated":"2023-03-20T11:30:49Z","published":"2022-11-18T15:46:24Z","title":"SeaTurtleID: A novel long-span dataset highlighting the importance of\n  timestamps in wildlife re-identification","summary":"  This paper introduces SeaTurtleID, the first public large-scale, long-span\ndataset with sea turtle photographs captured in the wild. The dataset is\nsuitable for benchmarking re-identification methods and evaluating several\nother computer vision tasks. The dataset consists of 7774 high-resolution\nphotographs of 400 unique individuals collected within 12 years in 1081\nencounters. Each photograph is accompanied by rich metadata, e.g., identity\nlabel, head segmentation mask, and encounter timestamp. The 12-year span of the\ndataset makes it the longest-spanned public wild animal dataset with\ntimestamps. By exploiting this unique property, we show that timestamps are\nnecessary for an unbiased evaluation of animal re-identification methods\nbecause they allow time-aware splits of the dataset into reference and query\nsets. We show that time-unaware (random) splits can lead to performance\noverestimation of more than 100% compared to the time-aware splits for both\nfeature- and CNN-based re-identification methods. We also argue that time-aware\nsplits correspond to more realistic re-identification pipelines than the\ntime-unaware ones. We recommend that animal re-identification methods should\nonly be tested on datasets with timestamps using time-aware splits, and we\nencourage dataset curators to include such information in the associated\nmetadata.\n","authors":["Kostas Papafitsoros","Lukáš Adam","Vojtěch Čermák","Lukáš Picek"],"pdf_url":"https://arxiv.org/pdf/2211.10307v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09807v2","updated":"2023-03-20T10:57:45Z","published":"2023-03-17T07:26:16Z","title":"TKN: Transformer-based Keypoint Prediction Network For Real-time Video\n  Prediction","summary":"  Video prediction is a complex time-series forecasting task with great\npotential in many use cases. However, conventional methods overemphasize\naccuracy while ignoring the slow prediction speed caused by complicated model\nstructures that learn too much redundant information with excessive GPU memory\nconsumption. Furthermore, conventional methods mostly predict frames\nsequentially (frame-by-frame) and thus are hard to accelerate. Consequently,\nvaluable use cases such as real-time danger prediction and warning cannot\nachieve fast enough inference speed to be applicable in reality. Therefore, we\npropose a transformer-based keypoint prediction neural network (TKN), an\nunsupervised learning method that boost the prediction process via constrained\ninformation extraction and parallel prediction scheme. TKN is the first\nreal-time video prediction solution to our best knowledge, while significantly\nreducing computation costs and maintaining other performance. Extensive\nexperiments on KTH and Human3.6 datasets demonstrate that TKN predicts 11 times\nfaster than existing methods while reducing memory consumption by 17.4% and\nachieving state-of-the-art prediction performance on average.\n","authors":["Haoran Li","Pengyuan Zhou","Yihang Lin","Yanbin Hao","Haiyong Xie","Yong Liao"],"pdf_url":"https://arxiv.org/pdf/2303.09807v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11019v1","updated":"2023-03-20T10:57:28Z","published":"2023-03-20T10:57:28Z","title":"A Dual-branch Self-supervised Representation Learning Framework for\n  Tumour Segmentation in Whole Slide Images","summary":"  Supervised deep learning methods have achieved considerable success in\nmedical image analysis, owing to the availability of large-scale and\nwell-annotated datasets. However, creating such datasets for whole slide images\n(WSIs) in histopathology is a challenging task due to their gigapixel size. In\nrecent years, self-supervised learning (SSL) has emerged as an alternative\nsolution to reduce the annotation overheads in WSIs, as it does not require\nlabels for training. These SSL approaches, however, are not designed for\nhandling multi-resolution WSIs, which limits their performance in learning\ndiscriminative image features. In this paper, we propose a Dual-branch SSL\nFramework for WSI tumour segmentation (DSF-WSI) that can effectively learn\nimage features from multi-resolution WSIs. Our DSF-WSI connected two branches\nand jointly learnt low and high resolution WSIs in a self-supervised manner.\nMoreover, we introduced a novel Context-Target Fusion Module (CTFM) and a\nmasked jigsaw pretext task to align the learnt multi-resolution features.\nFurthermore, we designed a Dense SimSiam Learning (DSL) strategy to maximise\nthe similarity of different views of WSIs, enabling the learnt representations\nto be more efficient and discriminative. We evaluated our method using two\npublic datasets on breast and liver cancer segmentation tasks. The experiment\nresults demonstrated that our DSF-WSI can effectively extract robust and\nefficient representations, which we validated through subsequent fine-tuning\nand semi-supervised settings. Our proposed method achieved better accuracy than\nother state-of-the-art approaches. Code is available at\nhttps://github.com/Dylan-H-Wang/dsf-wsi.\n","authors":["Hao Wang","Euijoon Ahn","Jinman Kim"],"pdf_url":"https://arxiv.org/pdf/2303.11019v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08131v3","updated":"2023-03-20T10:52:40Z","published":"2023-03-14T17:58:34Z","title":"A Simple Framework for Open-Vocabulary Segmentation and Detection","summary":"  We present OpenSeeD, a simple Open-vocabulary Segmentation and Detection\nframework that jointly learns from different segmentation and detection\ndatasets. To bridge the gap of vocabulary and annotation granularity, we first\nintroduce a pre-trained text encoder to encode all the visual concepts in two\ntasks and learn a common semantic space for them. This gives us reasonably good\nresults compared with the counterparts trained on segmentation task only. To\nfurther reconcile them, we locate two discrepancies: $i$) task discrepancy --\nsegmentation requires extracting masks for both foreground objects and\nbackground stuff, while detection merely cares about the former; $ii$) data\ndiscrepancy -- box and mask annotations are with different spatial granularity,\nand thus not directly interchangeable. To address these issues, we propose a\ndecoupled decoding to reduce the interference between foreground/background and\na conditioned mask decoding to assist in generating masks for given boxes. To\nthis end, we develop a simple encoder-decoder model encompassing all three\ntechniques and train it jointly on COCO and Objects365. After pre-training, our\nmodel exhibits competitive or stronger zero-shot transferability for both\nsegmentation and detection. Specifically, OpenSeeD beats the state-of-the-art\nmethod for open-vocabulary instance and panoptic segmentation across 5\ndatasets, and outperforms previous work for open-vocabulary detection on LVIS\nand ODinW under similar settings. When transferred to specific tasks, our model\nachieves new SoTA for panoptic segmentation on COCO and ADE20K, and instance\nsegmentation on ADE20K and Cityscapes.\n  Finally, we note that OpenSeeD is the first to explore the potential of joint\ntraining on segmentation and detection, and hope it can be received as a strong\nbaseline for developing a single model for both tasks in open world.\n","authors":["Hao Zhang","Feng Li","Xueyan Zou","Shilong Liu","Chunyuan Li","Jianfeng Gao","Jianwei Yang","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.08131v3.pdf","comment":"A Simple Framework for Open-Vocabulary Segmentation and Detection"},{"id":"http://arxiv.org/abs/2302.08453v2","updated":"2023-03-20T10:52:26Z","published":"2023-02-16T17:56:08Z","title":"T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for\n  Text-to-Image Diffusion Models","summary":"  The incredible generative ability of large-scale text-to-image (T2I) models\nhas demonstrated strong power of learning complex structures and meaningful\nsemantics. However, relying solely on text prompts cannot fully take advantage\nof the knowledge learned by the model, especially when flexible and accurate\ncontrolling (e.g., color and structure) is needed. In this paper, we aim to\n``dig out\" the capabilities that T2I models have implicitly learned, and then\nexplicitly use them to control the generation more granularly. Specifically, we\npropose to learn simple and lightweight T2I-Adapters to align internal\nknowledge in T2I models with external control signals, while freezing the\noriginal large T2I models. In this way, we can train various adapters according\nto different conditions, achieving rich control and editing effects in the\ncolor and structure of the generation results. Further, the proposed\nT2I-Adapters have attractive properties of practical value, such as\ncomposability and generalization ability. Extensive experiments demonstrate\nthat our T2I-Adapter has promising generation quality and a wide range of\napplications.\n","authors":["Chong Mou","Xintao Wang","Liangbin Xie","Yanze Wu","Jian Zhang","Zhongang Qi","Ying Shan","Xiaohu Qie"],"pdf_url":"https://arxiv.org/pdf/2302.08453v2.pdf","comment":"Tech Report. GitHub: https://github.com/TencentARC/T2I-Adapter"},{"id":"http://arxiv.org/abs/2303.11011v1","updated":"2023-03-20T10:44:32Z","published":"2023-03-20T10:44:32Z","title":"Learning Optical Flow from Event Camera with Rendered Dataset","summary":"  We study the problem of estimating optical flow from event cameras. One\nimportant issue is how to build a high-quality event-flow dataset with accurate\nevent values and flow labels. Previous datasets are created by either capturing\nreal scenes by event cameras or synthesizing from images with pasted foreground\nobjects. The former case can produce real event values but with calculated flow\nlabels, which are sparse and inaccurate. The later case can generate dense flow\nlabels but the interpolated events are prone to errors. In this work, we\npropose to render a physically correct event-flow dataset using computer\ngraphics models. In particular, we first create indoor and outdoor 3D scenes by\nBlender with rich scene content variations. Second, diverse camera motions are\nincluded for the virtual capturing, producing images and accurate flow labels.\nThird, we render high-framerate videos between images for accurate events. The\nrendered dataset can adjust the density of events, based on which we further\nintroduce an adaptive density module (ADM). Experiments show that our proposed\ndataset can facilitate event-flow learning, whereas previous approaches when\ntrained on our dataset can improve their performances constantly by a\nrelatively large margin. In addition, event-flow pipelines when equipped with\nour ADM can further improve performances.\n","authors":["Xinglong Luo","Kunming Luo","Ao Luo","Zhengning Wang","Ping Tan","Shuaicheng Liu"],"pdf_url":"https://arxiv.org/pdf/2303.11011v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11003v1","updated":"2023-03-20T10:31:35Z","published":"2023-03-20T10:31:35Z","title":"Tubelet-Contrastive Self-Supervision for Video-Efficient Generalization","summary":"  We propose a self-supervised method for learning motion-focused video\nrepresentations. Existing approaches minimize distances between temporally\naugmented videos, which maintain high spatial similarity. We instead propose to\nlearn similarities between videos with identical local motion dynamics but an\notherwise different appearance. We do so by adding synthetic motion\ntrajectories to videos which we refer to as tubelets. By simulating different\ntubelet motions and applying transformations, such as scaling and rotation, we\nintroduce motion patterns beyond what is present in the pretraining data. This\nallows us to learn a video representation that is remarkably data-efficient:\nour approach maintains performance when using only 25% of the pretraining\nvideos. Experiments on 10 diverse downstream settings demonstrate our\ncompetitive performance and generalizability to new domains and fine-grained\nactions.\n","authors":["Fida Mohammad Thoker","Hazel Doughty","Cees Snoek"],"pdf_url":"https://arxiv.org/pdf/2303.11003v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10999v1","updated":"2023-03-20T10:29:35Z","published":"2023-03-20T10:29:35Z","title":"Induced Feature Selection by Structured Pruning","summary":"  The advent of sparsity inducing techniques in neural networks has been of a\ngreat help in the last few years. Indeed, those methods allowed to find lighter\nand faster networks, able to perform more efficiently in resource-constrained\nenvironment such as mobile devices or highly requested servers. Such a sparsity\nis generally imposed on the weights of neural networks, reducing the footprint\nof the architecture. In this work, we go one step further by imposing sparsity\njointly on the weights and on the input data. This can be achieved following a\nthree-step process: 1) impose a certain structured sparsity on the weights of\nthe network; 2) track back input features corresponding to zeroed blocks of\nweight; 3) remove useless weights and input features and retrain the network.\nPerforming pruning both on the network and on input data not only allows for\nextreme reduction in terms of parameters and operations but can also serve as\nan interpretation process. Indeed, with the help of data pruning, we now have\ninformation about which input feature is useful for the network to keep its\nperformance. Experiments conducted on a variety of architectures and datasets:\nMLP validated on MNIST, CIFAR10/100 and ConvNets (VGG16 and ResNet18),\nvalidated on CIFAR10/100 and CALTECH101 respectively, show that it is possible\nto achieve additional gains in terms of total parameters and in FLOPs by\nperforming pruning on input data, while also increasing accuracy.\n","authors":["Nathan Hubens","Victor Delvigne","Matei Mancas","Bernard Gosselin","Marius Preda","Titus Zaharia"],"pdf_url":"https://arxiv.org/pdf/2303.10999v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10991v1","updated":"2023-03-20T10:19:50Z","published":"2023-03-20T10:19:50Z","title":"Versatile Depth Estimator Based on Common Relative Depth Estimation and\n  Camera-Specific Relative-to-Metric Depth Conversion","summary":"  A typical monocular depth estimator is trained for a single camera, so its\nperformance drops severely on images taken with different cameras. To address\nthis issue, we propose a versatile depth estimator (VDE), composed of a common\nrelative depth estimator (CRDE) and multiple relative-to-metric converters\n(R2MCs). The CRDE extracts relative depth information, and each R2MC converts\nthe relative information to predict metric depths for a specific camera. The\nproposed VDE can cope with diverse scenes, including both indoor and outdoor\nscenes, with only a 1.12\\% parameter increase per camera. Experimental results\ndemonstrate that VDE supports multiple cameras effectively and efficiently and\nalso achieves state-of-the-art performance in the conventional single-camera\nscenario.\n","authors":["Jinyoung Jun","Jae-Han Lee","Chang-Su Kim"],"pdf_url":"https://arxiv.org/pdf/2303.10991v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.02669v3","updated":"2023-03-20T10:19:37Z","published":"2022-02-06T01:20:50Z","title":"SRPCN: Structure Retrieval based Point Completion Network","summary":"  Given partial objects and some complete ones as references, point cloud\ncompletion aims to recover authentic shapes. However, existing methods pay\nlittle attention to general shapes, which leads to the poor authenticity of\ncompletion results. Besides, the missing patterns are diverse in reality, but\nexisting methods can only handle fixed ones, which means a poor generalization\nability. Considering that a partial point cloud is a subset of the\ncorresponding complete one, we regard them as different samples of the same\ndistribution and propose Structure Retrieval based Point Completion Network\n(SRPCN). It first uses k-means clustering to extract structure points and\ndisperses them into distributions, and then KL Divergence is used as a metric\nto find the complete structure point cloud that best matches the input in a\ndatabase. Finally, a PCN-like decoder network is adopted to generate the final\nresults based on the retrieved structure point clouds. As structure plays an\nimportant role in describing the general shape of an object and the proposed\nstructure retrieval method is robust to missing patterns, experiments show that\nour method can generate more authentic results and has a stronger\ngeneralization ability.\n","authors":["Kaiyi Zhang","Ximing Yang","Yuan Wu","Cheng Jin"],"pdf_url":"https://arxiv.org/pdf/2202.02669v3.pdf","comment":"I think the proposed method has some defects"},{"id":"http://arxiv.org/abs/2301.04525v2","updated":"2023-03-20T10:18:28Z","published":"2023-01-11T15:44:42Z","title":"Clustering disease trajectories in contrastive feature space for\n  biomarker discovery in age-related macular degeneration","summary":"  Age-related macular degeneration (AMD) is the leading cause of blindness in\nthe elderly. Current grading systems based on imaging biomarkers only coarsely\ngroup disease stages into broad categories and are unable to predict future\ndisease progression. It is widely believed that this is due to their focus on a\nsingle point in time, disregarding the dynamic nature of the disease. In this\nwork, we present the first method to automatically discover biomarkers that\ncapture temporal dynamics of disease progression. Our method represents patient\ntime series as trajectories in a latent feature space built with contrastive\nlearning. Then, individual trajectories are partitioned into atomic\nsub-sequences that encode transitions between disease states. These are\nclustered using a newly introduced distance metric. In quantitative experiments\nwe found our method yields temporal biomarkers that are predictive of\nconversion to late AMD. Furthermore, these clusters were highly interpretable\nto ophthalmologists who confirmed that many of the clusters represent dynamics\nthat have previously been linked to the progression of AMD, even though they\nare currently not included in any clinical grading system.\n","authors":["Robbie Holland","Oliver Leingang","Christopher Holmes","Philipp Anders","Rebecca Kaye","Sophie Riedl","Johannes C. Paetzold","Ivan Ezhov","Hrvoje Bogunović","Ursula Schmidt-Erfurth","Lars Fritsche","Hendrik P. N. Scholl","Sobha Sivaprasad","Andrew J. Lotery","Daniel Rueckert","Martin J. Menten"],"pdf_url":"https://arxiv.org/pdf/2301.04525v2.pdf","comment":"Submitted to MICCAI2023"},{"id":"http://arxiv.org/abs/2303.10976v1","updated":"2023-03-20T09:56:35Z","published":"2023-03-20T09:56:35Z","title":"Attention Disturbance and Dual-Path Constraint Network for Occluded\n  Person Re-Identification","summary":"  Occluded person re-identification (Re-ID) aims to address the potential\nocclusion problem when matching occluded or holistic pedestrians from different\ncamera views. Many methods use the background as artificial occlusion and rely\non attention networks to exclude noisy interference. However, the significant\ndiscrepancy between simple background occlusion and realistic occlusion can\nnegatively impact the generalization of the network.To address this issue, we\npropose a novel transformer-based Attention Disturbance and Dual-Path\nConstraint Network (ADP) to enhance the generalization of attention networks.\nFirstly, to imitate real-world obstacles, we introduce an Attention Disturbance\nMask (ADM) module that generates an offensive noise, which can distract\nattention like a realistic occluder, as a more complex form of\nocclusion.Secondly, to fully exploit these complex occluded images, we develop\na Dual-Path Constraint Module (DPC) that can obtain preferable supervision\ninformation from holistic images through dual-path interaction. With our\nproposed method, the network can effectively circumvent a wide variety of\nocclusions using the basic ViT baseline. Comprehensive experimental evaluations\nconducted on person re-ID benchmarks demonstrate the superiority of ADP over\nstate-of-the-art methods.\n","authors":["Jiaer Xia","Lei Tan","Pingyang Dai","Mingbo Zhao","Yongjian Wu","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2303.10976v1.pdf","comment":"10 pages, 4 figures"},{"id":"http://arxiv.org/abs/2303.10975v1","updated":"2023-03-20T09:56:17Z","published":"2023-03-20T09:56:17Z","title":"VIMI: Vehicle-Infrastructure Multi-view Intermediate Fusion for\n  Camera-based 3D Object Detection","summary":"  In autonomous driving, Vehicle-Infrastructure Cooperative 3D Object Detection\n(VIC3D) makes use of multi-view cameras from both vehicles and traffic\ninfrastructure, providing a global vantage point with rich semantic context of\nroad conditions beyond a single vehicle viewpoint. Two major challenges prevail\nin VIC3D: 1) inherent calibration noise when fusing multi-view images, caused\nby time asynchrony across cameras; 2) information loss when projecting 2D\nfeatures into 3D space. To address these issues, We propose a novel 3D object\ndetection framework, Vehicles-Infrastructure Multi-view Intermediate fusion\n(VIMI). First, to fully exploit the holistic perspectives from both vehicles\nand infrastructure, we propose a Multi-scale Cross Attention (MCA) module that\nfuses infrastructure and vehicle features on selective multi-scales to correct\nthe calibration noise introduced by camera asynchrony. Then, we design a\nCamera-aware Channel Masking (CCM) module that uses camera parameters as priors\nto augment the fused features. We further introduce a Feature Compression (FC)\nmodule with channel and spatial compression blocks to reduce the size of\ntransmitted features for enhanced efficiency. Experiments show that VIMI\nachieves 15.61% overall AP_3D and 21.44% AP_BEV on the new VIC3D dataset,\nDAIR-V2X-C, significantly outperforming state-of-the-art early fusion and late\nfusion methods with comparable transmission cost.\n","authors":["Zhe Wang","Siqi Fan","Xiaoliang Huo","Tongda Xu","Yan Wang","Jingjing Liu","Yilun Chen","Ya-Qin Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.10975v1.pdf","comment":"8 pages, 9 figures"},{"id":"http://arxiv.org/abs/2303.10972v1","updated":"2023-03-20T09:50:07Z","published":"2023-03-20T09:50:07Z","title":"Semantic segmentation of surgical hyperspectral images under geometric\n  domain shifts","summary":"  Robust semantic segmentation of intraoperative image data could pave the way\nfor automatic surgical scene understanding and autonomous robotic surgery.\nGeometric domain shifts, however, although common in real-world open surgeries\ndue to variations in surgical procedures or situs occlusions, remain a topic\nlargely unaddressed in the field. To address this gap in the literature, we (1)\npresent the first analysis of state-of-the-art (SOA) semantic segmentation\nnetworks in the presence of geometric out-of-distribution (OOD) data, and (2)\naddress generalizability with a dedicated augmentation technique termed \"Organ\nTransplantation\" that we adapted from the general computer vision community.\nAccording to a comprehensive validation on six different OOD data sets\ncomprising 600 RGB and hyperspectral imaging (HSI) cubes from 33 pigs\nsemantically annotated with 19 classes, we demonstrate a large performance drop\nof SOA organ segmentation networks applied to geometric OOD data. Surprisingly,\nthis holds true not only for conventional RGB data (drop of Dice similarity\ncoefficient (DSC) by 46 %) but also for HSI data (drop by 45 %), despite the\nlatter's rich information content per pixel. Using our augmentation scheme\nimproves on the SOA DSC by up to 67 % (RGB) and 90 % (HSI) and renders\nperformance on par with in-distribution performance on real OOD test data. The\nsimplicity and effectiveness of our augmentation scheme makes it a valuable\nnetwork-independent tool for addressing geometric domain shifts in semantic\nscene segmentation of intraoperative data. Our code and pre-trained models will\nbe made publicly available.\n","authors":["Jan Sellner","Silvia Seidlitz","Alexander Studier-Fischer","Alessandro Motta","Berkin Özdemir","Beat Peter Müller-Stich","Felix Nickel","Lena Maier-Hein"],"pdf_url":"https://arxiv.org/pdf/2303.10972v1.pdf","comment":"The first two authors (Jan Sellner and Silvia Seidlitz) contributed\n  equally to this paper"},{"id":"http://arxiv.org/abs/2303.10971v1","updated":"2023-03-20T09:47:02Z","published":"2023-03-20T09:47:02Z","title":"Self-Supervised Learning for Multimodal Non-Rigid 3D Shape Matching","summary":"  The matching of 3D shapes has been extensively studied for shapes represented\nas surface meshes, as well as for shapes represented as point clouds. While\npoint clouds are a common representation of raw real-world 3D data (e.g. from\nlaser scanners), meshes encode rich and expressive topological information, but\ntheir creation typically requires some form of (often manual) curation. In\nturn, methods that purely rely on point clouds are unable to meet the matching\nquality of mesh-based methods that utilise the additional topological\nstructure. In this work we close this gap by introducing a self-supervised\nmultimodal learning strategy that combines mesh-based functional map\nregularisation with a contrastive loss that couples mesh and point cloud data.\nOur shape matching approach allows to obtain intramodal correspondences for\ntriangle meshes, complete point clouds, and partially observed point clouds, as\nwell as correspondences across these data modalities. We demonstrate that our\nmethod achieves state-of-the-art results on several challenging benchmark\ndatasets even in comparison to recent supervised methods, and that our method\nreaches previously unseen cross-dataset generalisation ability.\n","authors":["Dongliang Cao","Florian Bernard"],"pdf_url":"https://arxiv.org/pdf/2303.10971v1.pdf","comment":"accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2211.11432v3","updated":"2023-03-20T09:44:58Z","published":"2022-11-21T13:19:08Z","title":"MATE: Masked Autoencoders are Online 3D Test-Time Learners","summary":"  Our MATE is the first Test-Time-Training (TTT) method designed for 3D data,\nwhich makes deep networks trained for point cloud classification robust to\ndistribution shifts occurring in test data. Like existing TTT methods from the\n2D image domain, MATE also leverages test data for adaptation. Its test-time\nobjective is that of a Masked Autoencoder: a large portion of each test point\ncloud is removed before it is fed to the network, tasked with reconstructing\nthe full point cloud. Once the network is updated, it is used to classify the\npoint cloud. We test MATE on several 3D object classification datasets and show\nthat it significantly improves robustness of deep networks to several types of\ncorruptions commonly occurring in 3D point clouds. We show that MATE is very\nefficient in terms of the fraction of points it needs for the adaptation. It\ncan effectively adapt given as few as 5% of tokens of each test sample, making\nit extremely lightweight. Our experiments show that MATE also achieves\ncompetitive performance by adapting sparsely on the test data, which further\nreduces its computational overhead, making it ideal for real-time applications.\n","authors":["M. Jehanzeb Mirza","Inkyu Shin","Wei Lin","Andreas Schriebl","Kunyang Sun","Jaesung Choe","Horst Possegger","Mateusz Kozinski","In So Kweon","Kun-Jin Yoon","Horst Bischof"],"pdf_url":"https://arxiv.org/pdf/2211.11432v3.pdf","comment":"Code is available at this repository:\n  https://github.com/jmiemirza/MATE"},{"id":"http://arxiv.org/abs/2303.10967v1","updated":"2023-03-20T09:41:50Z","published":"2023-03-20T09:41:50Z","title":"Real-time Semantic Scene Completion Via Feature Aggregation and\n  Conditioned Prediction","summary":"  Semantic Scene Completion (SSC) aims to simultaneously predict the volumetric\noccupancy and semantic category of a 3D scene. In this paper, we propose a\nreal-time semantic scene completion method with a feature aggregation strategy\nand conditioned prediction module. Feature aggregation fuses feature with\ndifferent receptive fields and gathers context to improve scene completion\nperformance. And the conditioned prediction module adopts a two-step prediction\nscheme that takes volumetric occupancy as a condition to enhance semantic\ncompletion prediction. We conduct experiments on three recognized benchmarks\nNYU, NYUCAD, and SUNCG. Our method achieves competitive performance at a speed\nof 110 FPS on one GTX 1080 Ti GPU.\n","authors":["Xiaokang Chen","Yajie Xing","Gang Zeng"],"pdf_url":"https://arxiv.org/pdf/2303.10967v1.pdf","comment":"Accepted by ICIP"},{"id":"http://arxiv.org/abs/2212.04245v2","updated":"2023-03-20T09:41:47Z","published":"2022-12-07T12:44:41Z","title":"Domain generalization of 3D semantic segmentation in autonomous driving","summary":"  Using deep learning, 3D autonomous driving semantic segmentation has become a\nwell-studied subject, with methods that can reach very high performance.\nNonetheless, because of the limited size of the training datasets, these models\ncannot see every type of object and scene found in real-world applications. The\nability to be reliable in these various unknown environments is called domain\ngeneralization.\n  Despite its importance, domain generalization is relatively unexplored in the\ncase of 3D autonomous driving semantic segmentation. To fill this gap, this\npaper presents the first benchmark for this application by testing\nstate-of-the-art methods and discussing the difficulty of tackling Laser\nImaging Detection and Ranging (LiDAR) domain shifts.\n  We also propose the first method designed to address this domain\ngeneralization, which we call 3DLabelProp. This method relies on leveraging the\ngeometry and sequentiality of the LiDAR data to enhance its generalization\nperformances by working on partially accumulated point clouds. It reaches a\nmean Intersection over Union (mIoU) of 50.4% on SemanticPOSS and of 55.2% on\nPandaSet solid-state LiDAR while being trained only on SemanticKITTI, making it\nthe state-of-the-art method for generalization (+5% and +33% better,\nrespectively, than the second best method).\n  The code for this method will be available on GitHub.\n","authors":["Jules Sanchez","Jean-Emmanuel Deschaud","Francois Goulette"],"pdf_url":"https://arxiv.org/pdf/2212.04245v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10962v1","updated":"2023-03-20T09:38:09Z","published":"2023-03-20T09:38:09Z","title":"Neural Implicit Vision-Language Feature Fields","summary":"  Recently, groundbreaking results have been presented on open-vocabulary\nsemantic image segmentation. Such methods segment each pixel in an image into\narbitrary categories provided at run-time in the form of text prompts, as\nopposed to a fixed set of classes defined at training time. In this work, we\npresent a zero-shot volumetric open-vocabulary semantic scene segmentation\nmethod. Our method builds on the insight that we can fuse image features from a\nvision-language model into a neural implicit representation. We show that the\nresulting feature field can be segmented into different classes by assigning\npoints to natural language text prompts. The implicit volumetric representation\nenables us to segment the scene both in 3D and 2D by rendering feature maps\nfrom any given viewpoint of the scene. We show that our method works on noisy\nreal-world data and can run in real-time on live sensor data dynamically\nadjusting to text prompts. We also present quantitative comparisons on the\nScanNet dataset.\n","authors":["Kenneth Blomqvist","Francesco Milano","Jen Jen Chung","Lionel Ott","Roland Siegwart"],"pdf_url":"https://arxiv.org/pdf/2303.10962v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10961v1","updated":"2023-03-20T09:37:41Z","published":"2023-03-20T09:37:41Z","title":"LFACon: Introducing Anglewise Attention to No-Reference Quality\n  Assessment in Light Field Space","summary":"  Light field imaging can capture both the intensity information and the\ndirection information of light rays. It naturally enables a\nsix-degrees-of-freedom viewing experience and deep user engagement in virtual\nreality. Compared to 2D image assessment, light field image quality assessment\n(LFIQA) needs to consider not only the image quality in the spatial domain but\nalso the quality consistency in the angular domain. However, there is a lack of\nmetrics to effectively reflect the angular consistency and thus the angular\nquality of a light field image (LFI). Furthermore, the existing LFIQA metrics\nsuffer from high computational costs due to the excessive data volume of LFIs.\nIn this paper, we propose a novel concept of \"anglewise attention\" by\nintroducing a multihead self-attention mechanism to the angular domain of an\nLFI. This mechanism better reflects the LFI quality. In particular, we propose\nthree new attention kernels, including anglewise self-attention, anglewise grid\nattention, and anglewise central attention. These attention kernels can realize\nangular self-attention, extract multiangled features globally or selectively,\nand reduce the computational cost of feature extraction. By effectively\nincorporating the proposed kernels, we further propose our light field\nattentional convolutional neural network (LFACon) as an LFIQA metric. Our\nexperimental results show that the proposed LFACon metric significantly\noutperforms the state-of-the-art LFIQA metrics. For the majority of distortion\ntypes, LFACon attains the best performance with lower complexity and less\ncomputational time.\n","authors":["Qiang Qu","Xiaoming Chen","Yuk Ying Chung","Weidong Cai"],"pdf_url":"https://arxiv.org/pdf/2303.10961v1.pdf","comment":"Accepted for IEEE VR 2023 (TVCG Special Issues) (Early Access)"},{"id":"http://arxiv.org/abs/2303.10960v1","updated":"2023-03-20T09:33:47Z","published":"2023-03-20T09:33:47Z","title":"Improved Benthic Classification using Resolution Scaling and SymmNet\n  Unsupervised Domain Adaptation","summary":"  Autonomous Underwater Vehicles (AUVs) conduct regular visual surveys of\nmarine environments to characterise and monitor the composition and diversity\nof the benthos. The use of machine learning classifiers for this task is\nlimited by the low numbers of annotations available and the many fine-grained\nclasses involved. In addition to these challenges, there are domain shifts\nbetween image sets acquired during different AUV surveys due to changes in\ncamera systems, imaging altitude, illumination and water column properties\nleading to a drop in classification performance for images from a different\nsurvey where some or all these elements may have changed. This paper proposes a\nframework to improve the performance of a benthic morphospecies classifier when\nused to classify images from a different survey compared to the training data.\nWe adapt the SymmNet state-of-the-art Unsupervised Domain Adaptation method\nwith an efficient bilinear pooling layer and image scaling to normalise spatial\nresolution, and show improved classification accuracy. We test our approach on\ntwo datasets with images from AUV surveys with different imaging payloads and\nlocations. The results show that generic domain adaptation can be enhanced to\nproduce a significant increase in accuracy for images from an AUV survey that\ndiffers from the training images.\n","authors":["Heather Doig","Oscar Pizarro","Stefan B. Williams"],"pdf_url":"https://arxiv.org/pdf/2303.10960v1.pdf","comment":"7 pages, 6 figures. Accepted to IEEE International Conference on\n  Robotics and Automation (ICRA) 2023, London UK"},{"id":"http://arxiv.org/abs/2303.10959v1","updated":"2023-03-20T09:33:05Z","published":"2023-03-20T09:33:05Z","title":"Long-Term Indoor Localization with Metric-Semantic Mapping using a Floor\n  Plan Prior","summary":"  Object-based maps are relevant for scene understanding since they integrate\ngeometric and semantic information of the environment, allowing autonomous\nrobots to robustly localize and interact with on objects. In this paper, we\naddress the task of constructing a metric-semantic map for the purpose of\nlong-term object-based localization. We exploit 3D object detections from\nmonocular RGB frames for both, the object-based map construction, and for\nglobally localizing in the constructed map. To tailor the approach to a target\nenvironment, we propose an efficient way of generating 3D annotations to\nfinetune the 3D object detection model. We evaluate our map construction in an\noffice building, and test our long-term localization approach on challenging\nsequences recorded in the same environment over nine months. The experiments\nsuggest that our approach is suitable for constructing metric-semantic maps,\nand that our localization approach is robust to long-term changes. Both, the\nmapping algorithm and the localization pipeline can run online on an onboard\ncomputer. We will release an open-source C++/ROS implementation of our\napproach.\n","authors":["Nicky Zimmerman","Matteo Sodano","Elias Marks","Jens Behley","Cyrill Stachniss"],"pdf_url":"https://arxiv.org/pdf/2303.10959v1.pdf","comment":"7 pages, submitted to IROS 2023"},{"id":"http://arxiv.org/abs/2211.00288v2","updated":"2023-03-20T09:20:03Z","published":"2022-11-01T05:48:18Z","title":"Self-supervised Character-to-Character Distillation for Text Recognition","summary":"  When handling complicated text images (e.g., irregular structures, low\nresolution, heavy occlusion, and uneven illumination), existing supervised text\nrecognition methods are data-hungry. Although these methods employ large-scale\nsynthetic text images to reduce the dependence on annotated real images, the\ndomain gap still limits the recognition performance. Therefore, exploring the\nrobust text feature representations on unlabeled real images by self-supervised\nlearning is a good solution. However, existing self-supervised text recognition\nmethods conduct sequence-to-sequence representation learning by roughly\nsplitting the visual features along the horizontal axis, which limits the\nflexibility of the augmentations, as large geometric-based augmentations may\nlead to sequence-to-sequence feature inconsistency. Motivated by this, we\npropose a novel self-supervised Character-to-Character Distillation method,\nCCD, which enables versatile augmentations to facilitate general text\nrepresentation learning. Specifically, we delineate the character structures of\nunlabeled real images by designing a self-supervised character segmentation\nmodule. Following this, CCD easily enriches the diversity of local characters\nwhile keeping their pairwise alignment under flexible augmentations, using the\ntransformation matrix between two augmented views from images. Experiments\ndemonstrate that CCD achieves state-of-the-art results, with average\nperformance gains of 1.38% in text recognition, 1.7% in text segmentation, 0.24\ndB (PSNR) and 0.0321 (SSIM) in text super-resolution. Code will be released\nsoon.\n","authors":["Tongkun Guan","Wei Shen","Xue Yang","Qi Feng","Zekun Jiang"],"pdf_url":"https://arxiv.org/pdf/2211.00288v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10951v1","updated":"2023-03-20T09:18:52Z","published":"2023-03-20T09:18:52Z","title":"Tracker Meets Night: A Transformer Enhancer for UAV Tracking","summary":"  Most previous progress in object tracking is realized in daytime scenes with\nfavorable illumination. State-of-the-arts can hardly carry on their superiority\nat night so far, thereby considerably blocking the broadening of visual\ntracking-related unmanned aerial vehicle (UAV) applications. To realize\nreliable UAV tracking at night, a spatial-channel Transformer-based low-light\nenhancer (namely SCT), which is trained in a novel task-inspired manner, is\nproposed and plugged prior to tracking approaches. To achieve semantic-level\nlow-light enhancement targeting the high-level task, the novel spatial-channel\nattention module is proposed to model global information while preserving local\ncontext. In the enhancement process, SCT denoises and illuminates nighttime\nimages simultaneously through a robust non-linear curve projection. Moreover,\nto provide a comprehensive evaluation, we construct a challenging nighttime\ntracking benchmark, namely DarkTrack2021, which contains 110 challenging\nsequences with over 100 K frames in total. Evaluations on both the public\nUAVDark135 benchmark and the newly constructed DarkTrack2021 benchmark show\nthat the task-inspired design enables SCT with significant performance gains\nfor nighttime UAV tracking compared with other top-ranked low-light enhancers.\nReal-world tests on a typical UAV platform further verify the practicability of\nthe proposed approach. The DarkTrack2021 benchmark and the code of the proposed\napproach are publicly available at https://github.com/vision4robotics/SCT.\n","authors":["Junjie Ye","Changhong Fu","Ziang Cao","Shan An","Guangze Zheng","Bowen Li"],"pdf_url":"https://arxiv.org/pdf/2303.10951v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.05871v2","updated":"2023-03-20T09:12:21Z","published":"2023-01-14T09:43:23Z","title":"Dyna-DepthFormer: Multi-frame Transformer for Self-Supervised Depth\n  Estimation in Dynamic Scenes","summary":"  Self-supervised methods have showed promising results on depth estimation\ntask. However, previous methods estimate the target depth map and camera\nego-motion simultaneously, underusing multi-frame correlation information and\nignoring the motion of dynamic objects. In this paper, we propose a novel\nDyna-Depthformer framework, which predicts scene depth and 3D motion field\njointly and aggregates multi-frame information with transformer. Our\ncontributions are two-fold. First, we leverage multi-view correlation through a\nseries of self- and cross-attention layers in order to obtain enhanced depth\nfeature representation. Specifically, we use the perspective transformation to\nacquire the initial reference point, and use deformable attention to reduce the\ncomputational cost. Second, we propose a warping-based Motion Network to\nestimate the motion field of dynamic objects without using semantic prior. To\nimprove the motion field predictions, we propose an iterative optimization\nstrategy, together with a sparsity-regularized loss. The entire pipeline\nachieves end-to-end self-supervised training by constructing a minimum\nreprojection loss. Extensive experiments on the KITTI and Cityscapes benchmarks\ndemonstrate the effectiveness of our method and show that our method\noutperforms state-of-the-art algorithms.\n","authors":["Songchun Zhang","Chunhui Zhao"],"pdf_url":"https://arxiv.org/pdf/2301.05871v2.pdf","comment":"ICRA 2023"},{"id":"http://arxiv.org/abs/2211.12860v3","updated":"2023-03-20T09:11:57Z","published":"2022-11-22T16:19:52Z","title":"DETRs with Collaborative Hybrid Assignments Training","summary":"  In this paper, we provide the observation that too few queries assigned as\npositive samples in DETR with one-to-one set matching leads to sparse\nsupervisions on the encoder's output which considerably hurt the discriminative\nfeature learning of the encoder and vice visa for attention learning in the\ndecoder. To alleviate this, we present a novel collaborative hybrid assignments\ntraining scheme, namely Co-DETR, to learn more efficient and effective\nDETR-based detectors from versatile label assignment manners. This new training\nscheme can easily enhance the encoder's learning ability in end-to-end\ndetectors by training the multiple parallel auxiliary heads supervised by\none-to-many label assignments such as ATSS, FCOS, and Faster RCNN. In addition,\nwe conduct extra customized positive queries by extracting the positive\ncoordinates from these auxiliary heads to improve the training efficiency of\npositive samples in the decoder. In inference, these auxiliary heads are\ndiscarded and thus our method introduces no additional parameters and\ncomputational cost to the original detector while requiring no hand-crafted\nnon-maximum suppression (NMS). We conduct extensive experiments to evaluate the\neffectiveness of the proposed approach on DETR variants, including DAB-DETR,\nDeformable-DETR, and DINO-Deformable-DETR. Specifically, we improve the basic\nDeformable-DETR by 5.8% in 12-epoch training and 3.2% in 36-epoch training. The\nstate-of-the-art DINO-Deformable-DETR with Swin-L can still be improved from\n58.5% to 59.5%. Surprisingly, incorporated with the large-scale backbone\nMixMIM-g with 1-Billion parameters, we achieve the 64.5% mAP on MS COCO\ntest-dev, achieving superior performance with much fewer extra data sizes.\nCodes will be available at https://github.com/Sense-X/Co-DETR.\n","authors":["Zhuofan Zong","Guanglu Song","Yu Liu"],"pdf_url":"https://arxiv.org/pdf/2211.12860v3.pdf","comment":"Tech report. Codes will be available at\n  https://github.com/Sense-X/Co-DETR"},{"id":"http://arxiv.org/abs/2303.09941v2","updated":"2023-03-20T09:07:49Z","published":"2023-03-17T12:55:22Z","title":"Leaping Into Memories: Space-Time Deep Feature Synthesis","summary":"  The success of deep learning models has led to their adaptation and adoption\nby prominent video understanding methods. The majority of these approaches\nencode features in a joint space-time modality for which the inner workings and\nlearned representations are difficult to visually interpret. We propose LEArned\nPreconscious Synthesis (LEAPS), an architecture-agnostic method for\nsynthesizing videos from the internal spatiotemporal representations of models.\nUsing a stimulus video and a target class, we prime a fixed space-time model\nand iteratively optimize a video initialized with random noise. We incorporate\nadditional regularizers to improve the feature diversity of the synthesized\nvideos as well as the cross-frame temporal coherence of motions. We\nquantitatively and qualitatively evaluate the applicability of LEAPS by\ninverting a range of spatiotemporal convolutional and attention-based\narchitectures trained on Kinetics-400, which to the best of our knowledge has\nnot been previously accomplished.\n","authors":["Alexandros Stergiou","Nikos Deligiannis"],"pdf_url":"https://arxiv.org/pdf/2303.09941v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.13901v2","updated":"2023-03-20T09:07:21Z","published":"2022-11-25T05:20:04Z","title":"Learning Detailed Radiance Manifolds for High-Fidelity and 3D-Consistent\n  Portrait Synthesis from Monocular Image","summary":"  A key challenge for novel view synthesis of monocular portrait images is 3D\nconsistency under continuous pose variations. Most existing methods rely on 2D\ngenerative models which often leads to obvious 3D inconsistency artifacts. We\npresent a 3D-consistent novel view synthesis approach for monocular portrait\nimages based on a recent proposed 3D-aware GAN, namely Generative Radiance\nManifolds (GRAM), which has shown strong 3D consistency at multiview image\ngeneration of virtual subjects via the radiance manifolds representation.\nHowever, simply learning an encoder to map a real image into the latent space\nof GRAM can only reconstruct coarse radiance manifolds without faithful fine\ndetails, while improving the reconstruction fidelity via instance-specific\noptimization is time-consuming. We introduce a novel detail manifolds\nreconstructor to learn 3D-consistent fine details on the radiance manifolds\nfrom monocular images, and combine them with the coarse radiance manifolds for\nhigh-fidelity reconstruction. The 3D priors derived from the coarse radiance\nmanifolds are used to regulate the learned details to ensure reasonable\nsynthesized results at novel views. Trained on in-the-wild 2D images, our\nmethod achieves high-fidelity and 3D-consistent portrait synthesis largely\noutperforming the prior art.\n","authors":["Yu Deng","Baoyuan Wang","Heung-Yeung Shum"],"pdf_url":"https://arxiv.org/pdf/2211.13901v2.pdf","comment":"CVPR 2023 camera-ready version. Project page:\n  https://yudeng.github.io/GRAMInverter/"},{"id":"http://arxiv.org/abs/2303.10945v1","updated":"2023-03-20T09:01:23Z","published":"2023-03-20T09:01:23Z","title":"Open-World Pose Transfer via Sequential Test-Time Adaption","summary":"  Pose transfer aims to transfer a given person into a specified posture, has\nrecently attracted considerable attention. A typical pose transfer framework\nusually employs representative datasets to train a discriminative model, which\nis often violated by out-of-distribution (OOD) instances. Recently, test-time\nadaption (TTA) offers a feasible solution for OOD data by using a pre-trained\nmodel that learns essential features with self-supervision. However, those\nmethods implicitly make an assumption that all test distributions have a\nunified signal that can be learned directly. In open-world conditions, the pose\ntransfer task raises various independent signals: OOD appearance and skeleton,\nwhich need to be extracted and distributed in speciality. To address this\npoint, we develop a SEquential Test-time Adaption (SETA). In the test-time\nphrase, SETA extracts and distributes external appearance texture by augmenting\nOOD data for self-supervised training. To make non-Euclidean similarity among\ndifferent postures explicit, SETA uses the image representations derived from a\nperson re-identification (Re-ID) model for similarity computation. By\naddressing implicit posture representation in the test-time sequentially, SETA\ngreatly improves the generalization performance of current pose transfer\nmodels. In our experiment, we first show that pose transfer can be applied to\nopen-world applications, including Tiktok reenactment and celebrity motion\nsynthesis.\n","authors":["Junyang Chen","Xiaoyu Xian","Zhijing Yang","Tianshui Chen","Yongyi Lu","Yukai Shi","Jinshan Pan","Liang Lin"],"pdf_url":"https://arxiv.org/pdf/2303.10945v1.pdf","comment":"We call for a solid pose transfer model that can handle open-world\n  instances beyond a specific dataset"},{"id":"http://arxiv.org/abs/2201.03597v2","updated":"2023-03-20T08:58:19Z","published":"2022-01-10T19:04:28Z","title":"Cross-Modality Sub-Image Retrieval using Contrastive Multimodal Image\n  Representations","summary":"  In tissue characterization and cancer diagnostics, multimodal imaging has\nemerged as a powerful technique. Thanks to computational advances, large\ndatasets can be exploited to discover patterns in pathologies and improve\ndiagnosis. However, this requires efficient and scalable image retrieval\nmethods. Cross-modality image retrieval is particularly challenging, since\nimages of similar (or even the same) content captured by different modalities\nmight share few common structures. We propose a new application-independent\ncontent-based image retrieval (CBIR) system for reverse (sub-)image search\nacross modalities, which combines deep learning to generate representations\n(embedding the different modalities in a common space) with classical feature\nextraction and bag-of-words models for efficient and reliable retrieval. We\nillustrate its advantages through a replacement study, exploring a number of\nfeature extractors and learned representations, as well as through comparison\nto recent (cross-modality) CBIR methods. For the task of (sub-)image retrieval\non a (publicly available) dataset of brightfield and second harmonic generation\nmicroscopy images, the results show that our approach is superior to all tested\nalternatives. We discuss the shortcomings of the compared methods and observe\nthe importance of equivariance and invariance properties of the learned\nrepresentations and feature extractors in the CBIR pipeline. Code is available\nat: \\url{https://github.com/MIDA-group/CrossModal_ImgRetrieval}.\n","authors":["Eva Breznik","Elisabeth Wetzer","Joakim Lindblad","Nataša Sladoje"],"pdf_url":"https://arxiv.org/pdf/2201.03597v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10944v1","updated":"2023-03-20T08:57:45Z","published":"2023-03-20T08:57:45Z","title":"Location-Free Scene Graph Generation","summary":"  Scene Graph Generation (SGG) is a challenging visual understanding task. It\ncombines the detection of entities and relationships between them in a scene.\nBoth previous works and existing evaluation metrics rely on bounding box\nlabels, even though many downstream scene graph applications do not need\nlocation information. The need for localization labels significantly increases\nthe annotation cost and hampers the creation of more and larger scene graph\ndatasets. We suggest breaking the dependency of scene graphs on bounding box\nlabels by proposing location-free scene graph generation (LF-SGG). This new\ntask aims at predicting instances of entities, as well as their relationships,\nwithout spatial localization. To objectively evaluate the task, the predicted\nand ground truth scene graphs need to be compared. We solve this NP-hard\nproblem through an efficient algorithm using branching. Additionally, we design\nthe first LF-SGG method, Pix2SG, using autoregressive sequence modeling. Our\nproposed method is evaluated on Visual Genome and 4D-OR. Although using\nsignificantly fewer labels during training, we achieve 74.12\\% of the\nlocation-supervised SOTA performance on Visual Genome and even outperform the\nbest method on 4D-OR.\n","authors":["Ege Özsoy","Felix Holm","Tobias Czempiel","Nassir Navab","Benjamin Busam"],"pdf_url":"https://arxiv.org/pdf/2303.10944v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2303.10941v1","updated":"2023-03-20T08:52:34Z","published":"2023-03-20T08:52:34Z","title":"HMC: Hierarchical Mesh Coarsening for Skeleton-free Motion Retargeting","summary":"  We present a simple yet effective method for skeleton-free motion\nretargeting. Previous methods transfer motion between high-resolution meshes,\nfailing to preserve the inherent local-part motions in the mesh. Addressing\nthis issue, our proposed method learns the correspondence in a coarse-to-fine\nfashion by integrating the retargeting process with a mesh-coarsening pipeline.\nFirst, we propose a mesh-coarsening module that coarsens the mesh\nrepresentations for better motion transfer. This module improves the ability to\nhandle small-part motion and preserves the local motion interdependence between\nneighboring mesh vertices. Furthermore, we leverage a hierarchical refinement\nprocedure to complement missing mesh details by gradually improving the\nlow-resolution mesh output with a higher-resolution one. We evaluate our method\non several well-known 3D character datasets, and it yields an average\nimprovement of 25% on point-wise mesh euclidean distance (PMD) against the\nstart-of-art method. Moreover, our qualitative results show that our method is\nsignificantly helpful in preserving the moving consistency of different body\nparts on the target character due to disentangling body-part structures and\nmesh details in a hierarchical way.\n","authors":["Haoyu Wang","Shaoli Huang","Fang Zhao","Chun Yuan","Ying Shan"],"pdf_url":"https://arxiv.org/pdf/2303.10941v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10049v2","updated":"2023-03-20T08:52:12Z","published":"2023-03-17T15:23:15Z","title":"Uncertainty-informed Mutual Learning for Joint Medical Image\n  Classification and Segmentation","summary":"  Classification and segmentation are crucial in medical image analysis as they\nenable accurate diagnosis and disease monitoring. However, current methods\noften prioritize the mutual learning features and shared model parameters,\nwhile neglecting the reliability of features and performances. In this paper,\nwe propose a novel Uncertainty-informed Mutual Learning (UML) framework for\nreliable and interpretable medical image analysis. Our UML introduces\nreliability to joint classification and segmentation tasks, leveraging mutual\nlearning with uncertainty to improve performance. To achieve this, we first use\nevidential deep learning to provide image-level and pixel-wise confidences.\nThen, an Uncertainty Navigator Decoder is constructed for better using mutual\nfeatures and generating segmentation results. Besides, an Uncertainty\nInstructor is proposed to screen reliable masks for classification. Overall,\nUML could produce confidence estimation in features and performance for each\nlink (classification and segmentation). The experiments on the public datasets\ndemonstrate that our UML outperforms existing methods in terms of both accuracy\nand robustness. Our UML has the potential to explore the development of more\nreliable and explainable medical image analysis models. We will release the\ncodes for reproduction after acceptance.\n","authors":["Kai Ren","Ke Zou","Xianjie Liu","Yidi Chen","Xuedong Yuan","Xiaojing Shen","Meng Wang","Huazhu Fu"],"pdf_url":"https://arxiv.org/pdf/2303.10049v2.pdf","comment":"10 pages, 3 figures, 3 tables"},{"id":"http://arxiv.org/abs/2211.05405v4","updated":"2023-03-20T08:29:29Z","published":"2022-11-10T08:19:44Z","title":"VieCap4H-VLSP 2021: ObjectAoA-Enhancing performance of Object Relation\n  Transformer with Attention on Attention for Vietnamese image captioning","summary":"  Image captioning is currently a challenging task that requires the ability to\nboth understand visual information and use human language to describe this\nvisual information in the image. In this paper, we propose an efficient way to\nimprove the image understanding ability of transformer-based method by\nextending Object Relation Transformer architecture with Attention on Attention\nmechanism. Experiments on the VieCap4H dataset show that our proposed method\nsignificantly outperforms its original structure on both the public test and\nprivate test of the Image Captioning shared task held by VLSP.\n","authors":["Nghia Hieu Nguyen","Duong T. D. Vo","Minh-Quan Ha"],"pdf_url":"https://arxiv.org/pdf/2211.05405v4.pdf","comment":"Accepted for publishing at the VNU Journal of Science: Computer\n  Science and Communication Engineering"},{"id":"http://arxiv.org/abs/2301.09489v3","updated":"2023-03-20T08:29:04Z","published":"2023-01-23T15:32:27Z","title":"Contracting Skeletal Kinematic Embeddings for Anomaly Detection","summary":"  Detecting the anomaly of human behavior is paramount to timely recognizing\nendangering situations, such as street fights or elderly falls. However,\nanomaly detection is complex, since anomalous events are rare and because it is\nan open set recognition task, i.e., what is anomalous at inference has not been\nobserved at training. We propose COSKAD, a novel model which encodes skeletal\nhuman motion by an efficient graph convolutional network and learns to COntract\nSKeletal kinematic embeddings onto a latent hypersphere of minimum volume for\nAnomaly Detection. We propose and analyze three latent space designs for\nCOSKAD: the commonly-adopted Euclidean, and the new spherical-radial and\nhyperbolic volumes. All three variants outperform the state-of-the-art,\nincluding video-based techniques, on the ShangaiTechCampus, the Avenue, and on\nthe most recent UBnormal dataset, for which we contribute novel skeleton\nannotations and the selection of human-related videos. The source code and\ndataset will be released upon acceptance.\n","authors":["Alessandro Flaborea","Guido D'Amely","Stefano D'Arrigo","Marco Aurelio Sterpa","Alessio Sampieri","Fabio Galasso"],"pdf_url":"https://arxiv.org/pdf/2301.09489v3.pdf","comment":"Submitted to Pattern Recognition Journal"},{"id":"http://arxiv.org/abs/2303.10937v1","updated":"2023-03-20T08:26:29Z","published":"2023-03-20T08:26:29Z","title":"Boosting Weakly Supervised Object Detection using Fusion and Priors from\n  Hallucinated Depth","summary":"  Despite recent attention and exploration of depth for various tasks, it is\nstill an unexplored modality for weakly-supervised object detection (WSOD). We\npropose an amplifier method for enhancing the performance of WSOD by\nintegrating depth information. Our approach can be applied to any WSOD method\nbased on multiple-instance learning, without necessitating additional\nannotations or inducing large computational expenses. Our proposed method\nemploys a monocular depth estimation technique to obtain hallucinated depth\ninformation, which is then incorporated into a Siamese WSOD network using\ncontrastive loss and fusion. By analyzing the relationship between language\ncontext and depth, we calculate depth priors to identify the bounding box\nproposals that may contain an object of interest. These depth priors are then\nutilized to update the list of pseudo ground-truth boxes, or adjust the\nconfidence of per-box predictions. Our proposed method is evaluated on six\ndatasets (COCO, PASCAL VOC, Conceptual Captions, Clipart1k, Watercolor2k, and\nComic2k) by implementing it on top of two state-of-the-art WSOD methods, and we\ndemonstrate a substantial enhancement in performance.\n","authors":["Cagri Gungor","Adriana Kovashka"],"pdf_url":"https://arxiv.org/pdf/2303.10937v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09232v2","updated":"2023-03-20T08:25:38Z","published":"2023-03-16T11:15:55Z","title":"Generative Adversarial Network for Personalized Art Therapy in Melanoma\n  Disease Management","summary":"  Melanoma is the most lethal type of skin cancer. Patients are vulnerable to\nmental health illnesses which can reduce the effectiveness of the cancer\ntreatment and the patients adherence to drug plans. It is crucial to preserve\nthe mental health of patients while they are receiving treatment. However,\ncurrent art therapy approaches are not personal and unique to the patient. We\naim to provide a well-trained image style transfer model that can quickly\ngenerate unique art from personal dermoscopic melanoma images as an additional\ntool for art therapy in disease management of melanoma. Visual art appreciation\nas a common form of art therapy in disease management that measurably reduces\nthe degree of psychological distress. We developed a network based on the\ncycle-consistent generative adversarial network for style transfer that\ngenerates personalized and unique artworks from dermoscopic melanoma images. We\ndeveloped a model that converts melanoma images into unique flower-themed\nartworks that relate to the shape of the lesion and are therefore personal to\nthe patient. Further, we altered the initial framework and made comparisons and\nevaluations of the results. With this, we increased the options in the toolbox\nfor art therapy in disease management of melanoma. The development of an\neasy-to-use user interface ensures the availability of the approach to\nstakeholders. The transformation of melanoma into flower-themed artworks is\nachieved by the proposed model and the graphical user interface. This\ncontribution opens a new field of GANs in art therapy and could lead to more\npersonalized disease management.\n","authors":["Lennart Jütte","Ning Wang","Bernhard Roth"],"pdf_url":"https://arxiv.org/pdf/2303.09232v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.09288v3","updated":"2023-03-20T08:24:20Z","published":"2022-11-17T01:43:53Z","title":"Longitudinal thermal imaging for scalable non-residential HVAC and\n  occupant behaviour characterization","summary":"  This work presents a study on the characterization of the air-conditioning\n(AC) usage pattern of non-residential buildings from thermal images collected\nfrom an urban-scale infrared (IR) observatory. To achieve this first, an image\nprocessing scheme, for cleaning and extraction of the temperature time series\nfrom the thermal images is implemented. To test the accuracy of the thermal\nmeasurements using IR camera, the extracted temperature is compared against the\nground truth surface temperature measurements. It is observed that the\ndetrended thermal measurements match well with the ground truth surface\ntemperature measurements. Subsequently, the operational pattern of the\nwater-cooled systems and window AC units are extracted from the analysis of the\nthermal signature. It is observed that for the water-cooled system, the\ndifference between the rate of change of the window and wall can be used to\nextract the operational pattern. While, in the case of the window AC units,\nwavelet transform of the AC unit temperature is used to extract the frequency\nand time domain information of the AC unit operation. The results of the\nanalysis are compared against the indoor temperature sensors installed in the\noffice spaces of the building. It is realized that the accuracy in the\nprediction of the operational pattern is highest between 8 pm to 10 am, and it\nreduces during the day because of solar radiation and high daytime temperature.\nSubsequently, a characterization study is conducted for eight window/split AC\nunits from the thermal image collected during the nighttime. This forms one of\nthe first studies on the operational behavior of HVAC systems for\nnon-residential buildings using the longitudinal thermal imaging technique. The\noutput from this study can be used to better understand the operational and\noccupant behavior, without requiring to deploy a large array of sensors in the\nbuilding space.\n","authors":["Vasantha Ramani","Miguel Martin","Pandarasamy Arjunan","Adrian Chong","Kameshwar Poolla","Clayton Miller"],"pdf_url":"https://arxiv.org/pdf/2211.09288v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10936v1","updated":"2023-03-20T08:20:04Z","published":"2023-03-20T08:20:04Z","title":"Learning to Explore Informative Trajectories and Samples for Embodied\n  Perception","summary":"  We are witnessing significant progress on perception models, specifically\nthose trained on large-scale internet images. However, efficiently generalizing\nthese perception models to unseen embodied tasks is insufficiently studied,\nwhich will help various relevant applications (e.g., home robots). Unlike\nstatic perception methods trained on pre-collected images, the embodied agent\ncan move around in the environment and obtain images of objects from any\nviewpoints. Therefore, efficiently learning the exploration policy and\ncollection method to gather informative training samples is the key to this\ntask. To do this, we first build a 3D semantic distribution map to train the\nexploration policy self-supervised by introducing the semantic distribution\ndisagreement and the semantic distribution uncertainty rewards. Note that the\nmap is generated from multi-view observations and can weaken the impact of\nmisidentification from an unfamiliar viewpoint. Our agent is then encouraged to\nexplore the objects with different semantic distributions across viewpoints, or\nuncertain semantic distributions. With the explored informative trajectories,\nwe propose to select hard samples on trajectories based on the semantic\ndistribution uncertainty to reduce unnecessary observations that can be\ncorrectly identified. Experiments show that the perception model fine-tuned\nwith our method outperforms the baselines trained with other exploration\npolicies. Further, we demonstrate the robustness of our method in real-robot\nexperiments.\n","authors":["Ya Jing","Tao Kong"],"pdf_url":"https://arxiv.org/pdf/2303.10936v1.pdf","comment":"To be published in IEEE International Conference on Robotics and\n  Automation (ICRA), 2023"},{"id":"http://arxiv.org/abs/2207.11209v3","updated":"2023-03-20T08:05:08Z","published":"2022-07-22T17:19:00Z","title":"Divide and Conquer: 3D Point Cloud Instance Segmentation With Point-Wise\n  Binarization","summary":"  Instance segmentation on point clouds is crucially important for 3D scene\nunderstanding. Most SOTAs adopt distance clustering, which is typically\neffective but does not perform well in segmenting adjacent objects with the\nsame semantic label (especially when they share neighboring points). Due to the\nuneven distribution of offset points, these existing methods can hardly cluster\nall instance points. To this end, we design a novel divide-and-conquer strategy\nnamed PBNet that binarizes each point and clusters them separately to segment\ninstances. Our binary clustering divides offset instance points into two\ncategories: high and low density points (HPs vs. LPs). Adjacent objects can be\nclearly separated by removing LPs, and then be completed and refined by\nassigning LPs via a neighbor voting method. To suppress potential\nover-segmentation, we propose to construct local scenes with the weight mask\nfor each instance. As a plug-in, the proposed binary clustering can replace the\ntraditional distance clustering and lead to consistent performance gains on\nmany mainstream baselines. A series of experiments on ScanNetV2 and S3DIS\ndatasets indicate the superiority of our model. In particular, PBNet ranks\nfirst on the ScanNetV2 official benchmark challenge, achieving the highest mAP.\n","authors":["Weiguang Zhao","Yuyao Yan","Chaolong Yang","Jianan Ye","Xi Yang","Kaizhu Huang"],"pdf_url":"https://arxiv.org/pdf/2207.11209v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.03376v3","updated":"2023-03-20T07:52:57Z","published":"2022-02-07T17:47:46Z","title":"Message Passing Neural PDE Solvers","summary":"  The numerical solution of partial differential equations (PDEs) is difficult,\nhaving led to a century of research so far. Recently, there have been pushes to\nbuild neural--numerical hybrid solvers, which piggy-backs the modern trend\ntowards fully end-to-end learned systems. Most works so far can only generalize\nover a subset of properties to which a generic solver would be faced,\nincluding: resolution, topology, geometry, boundary conditions, domain\ndiscretization regularity, dimensionality, etc. In this work, we build a\nsolver, satisfying these properties, where all the components are based on\nneural message passing, replacing all heuristically designed components in the\ncomputation graph with backprop-optimized neural function approximators. We\nshow that neural message passing solvers representationally contain some\nclassical methods, such as finite differences, finite volumes, and WENO\nschemes. In order to encourage stability in training autoregressive models, we\nput forward a method that is based on the principle of zero-stability, posing\nstability as a domain adaptation problem. We validate our method on various\nfluid-like flow problems, demonstrating fast, stable, and accurate performance\nacross different domain topologies, equation parameters, discretizations, etc.,\nin 1D and 2D.\n","authors":["Johannes Brandstetter","Daniel Worrall","Max Welling"],"pdf_url":"https://arxiv.org/pdf/2202.03376v3.pdf","comment":"Published at ICLR 2022 (Spotlight paper), Github:\n  https://github.com/brandstetter-johannes/MP-Neural-PDE-Solvers"},{"id":"http://arxiv.org/abs/2207.01331v2","updated":"2023-03-20T07:50:13Z","published":"2022-07-04T11:24:10Z","title":"Improving Nighttime Driving-Scene Segmentation via Dual Image-adaptive\n  Learnable Filters","summary":"  Semantic segmentation on driving-scene images is vital for autonomous\ndriving. Although encouraging performance has been achieved on daytime images,\nthe performance on nighttime images are less satisfactory due to the\ninsufficient exposure and the lack of labeled data. To address these issues, we\npresent an add-on module called dual image-adaptive learnable filters\n(DIAL-Filters) to improve the semantic segmentation in nighttime driving\nconditions, aiming at exploiting the intrinsic features of driving-scene images\nunder different illuminations. DIAL-Filters consist of two parts, including an\nimage-adaptive processing module (IAPM) and a learnable guided filter (LGF).\nWith DIAL-Filters, we design both unsupervised and supervised frameworks for\nnighttime driving-scene segmentation, which can be trained in an end-to-end\nmanner. Specifically, the IAPM module consists of a small convolutional neural\nnetwork with a set of differentiable image filters, where each image can be\nadaptively enhanced for better segmentation with respect to the different\nilluminations. The LGF is employed to enhance the output of segmentation\nnetwork to get the final segmentation result. The DIAL-Filters are light-weight\nand efficient and they can be readily applied for both daytime and nighttime\nimages. Our experiments show that DAIL-Filters can significantly improve the\nsupervised segmentation performance on ACDC_Night and NightCity datasets, while\nit demonstrates the state-of-the-art performance on unsupervised nighttime\nsemantic segmentation on Dark Zurich and Nighttime Driving testbeds.\n","authors":["Wenyu Liu","Wentong Li","Jianke Zhu","Miaomiao Cui","Xuansong Xie","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2207.01331v2.pdf","comment":"Accepted by IEEE TCSVT(2023)"},{"id":"http://arxiv.org/abs/2303.10923v1","updated":"2023-03-20T07:47:36Z","published":"2023-03-20T07:47:36Z","title":"Dynamic Object Removal for Effective Slam","summary":"  This research paper focuses on the problem of dynamic objects and their\nimpact on effective motion planning and localization. The paper proposes a\ntwo-step process to address this challenge, which involves finding the dynamic\nobjects in the scene using a Flow-based method and then using a deep Video\ninpainting algorithm to remove them. The study aims to test the validity of\nthis approach by comparing it with baseline results using two state-of-the-art\nSLAM algorithms, ORB-SLAM2 and LSD, and understanding the impact of dynamic\nobjects and the corresponding trade-offs. The proposed approach does not\nrequire any significant modifications to the baseline SLAM algorithms, and\ntherefore, the computational effort required remains unchanged. The paper\npresents a detailed analysis of the results obtained and concludes that the\nproposed method is effective in removing dynamic objects from the scene,\nleading to improved SLAM performance.\n","authors":["Phani Krishna Uppala","Abhishek Bamotra","Raj Kolamuri"],"pdf_url":"https://arxiv.org/pdf/2303.10923v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.13386v2","updated":"2023-03-20T07:33:09Z","published":"2022-04-28T10:01:36Z","title":"Self-supervised Contrastive Learning for Audio-Visual Action Recognition","summary":"  The underlying correlation between audio and visual modalities can be\nutilized to learn supervised information for unlabeled videos. In this paper,\nwe propose an end-to-end self-supervised framework named Audio-Visual\nContrastive Learning (AVCL), to learn discriminative audio-visual\nrepresentations for action recognition. Specifically, we design an attention\nbased multi-modal fusion module (AMFM) to fuse audio and visual modalities. To\nalign heterogeneous audio-visual modalities, we construct a novel\nco-correlation guided representation alignment module (CGRA). To learn\nsupervised information from unlabeled videos, we propose a novel\nself-supervised contrastive learning module (SelfCL). Furthermore, we build a\nnew audio-visual action recognition dataset named Kinetics-Sounds100.\nExperimental results on Kinetics-Sounds32 and Kinetics-Sounds100 datasets\ndemonstrate the superiority of our AVCL over the state-of-the-art methods on\nlarge-scale action recognition benchmark.\n","authors":["Yang Liu","Ying Tan","Haoyuan Lan"],"pdf_url":"https://arxiv.org/pdf/2204.13386v2.pdf","comment":"6 pages, 2 figures"},{"id":"http://arxiv.org/abs/2303.10916v1","updated":"2023-03-20T07:16:58Z","published":"2023-03-20T07:16:58Z","title":"Learning Behavior Recognition in Smart Classroom with Multiple Students\n  Based on YOLOv5","summary":"  Deep learning-based computer vision technology has grown stronger in recent\nyears, and cross-fertilization using computer vision technology has been a\npopular direction in recent years. The use of computer vision technology to\nidentify students' learning behavior in the classroom can reduce the workload\nof traditional teachers in supervising students in the classroom, and ensure\ngreater accuracy and comprehensiveness. However, existing student learning\nbehavior detection systems are unable to track and detect multiple targets\nprecisely, and the accuracy of learning behavior recognition is not high enough\nto meet the existing needs for the accurate recognition of student behavior in\nthe classroom. To solve this problem, we propose a YOLOv5s network structure\nbased on you only look once (YOLO) algorithm to recognize and analyze students'\nclassroom behavior in this paper. Firstly, the input images taken in the smart\nclassroom are pre-processed. Then, the pre-processed image is fed into the\ndesigned YOLOv5 networks to extract deep features through convolutional layers,\nand the Squeeze-and-Excitation (SE) attention detection mechanism is applied to\nreduce the weight of background information in the recognition process.\nFinally, the extracted features are classified by the Feature Pyramid Networks\n(FPN) and Path Aggregation Network (PAN) structures. Multiple groups of\nexperiments were performed to compare with traditional learning behavior\nrecognition methods to validate the effectiveness of the proposed method. When\ncompared with YOLOv4, the proposed method is able to improve the mAP\nperformance by 11%.\n","authors":["Zhifeng Wang","Jialong Yao","Chunyan Zeng","Wanxuan Wu","Hongmin Xu","Yang Yang"],"pdf_url":"https://arxiv.org/pdf/2303.10916v1.pdf","comment":"8 pages, 10 figures"},{"id":"http://arxiv.org/abs/2303.09554v2","updated":"2023-03-20T07:02:38Z","published":"2023-03-16T17:59:22Z","title":"PartNeRF: Generating Part-Aware Editable 3D Shapes without 3D\n  Supervision","summary":"  Impressive progress in generative models and implicit representations gave\nrise to methods that can generate 3D shapes of high quality. However, being\nable to locally control and edit shapes is another essential property that can\nunlock several content creation applications. Local control can be achieved\nwith part-aware models, but existing methods require 3D supervision and cannot\nproduce textures. In this work, we devise PartNeRF, a novel part-aware\ngenerative model for editable 3D shape synthesis that does not require any\nexplicit 3D supervision. Our model generates objects as a set of locally\ndefined NeRFs, augmented with an affine transformation. This enables several\nediting operations such as applying transformations on parts, mixing parts from\ndifferent objects etc. To ensure distinct, manipulable parts we enforce a hard\nassignment of rays to parts that makes sure that the color of each ray is only\ndetermined by a single NeRF. As a result, altering one part does not affect the\nappearance of the others. Evaluations on various ShapeNet categories\ndemonstrate the ability of our model to generate editable 3D objects of\nimproved fidelity, compared to previous part-based generative approaches that\nrequire 3D supervision or models relying on NeRFs.\n","authors":["Konstantinos Tertikas","Paschalidou Despoina","Boxiao Pan","Jeong Joon Park","Mikaela Angelina Uy","Ioannis Emiris","Yannis Avrithis","Leonidas Guibas"],"pdf_url":"https://arxiv.org/pdf/2303.09554v2.pdf","comment":"To appear in CVPR 2023, Project Page:\n  https://ktertikas.github.io/part_nerf"},{"id":"http://arxiv.org/abs/2303.08536v2","updated":"2023-03-20T07:01:45Z","published":"2023-03-15T11:29:36Z","title":"Watch or Listen: Robust Audio-Visual Speech Recognition with Visual\n  Corruption Modeling and Reliability Scoring","summary":"  This paper deals with Audio-Visual Speech Recognition (AVSR) under multimodal\ninput corruption situations where audio inputs and visual inputs are both\ncorrupted, which is not well addressed in previous research directions.\nPrevious studies have focused on how to complement the corrupted audio inputs\nwith the clean visual inputs with the assumption of the availability of clean\nvisual inputs. However, in real life, clean visual inputs are not always\naccessible and can even be corrupted by occluded lip regions or noises. Thus,\nwe firstly analyze that the previous AVSR models are not indeed robust to the\ncorruption of multimodal input streams, the audio and the visual inputs,\ncompared to uni-modal models. Then, we design multimodal input corruption\nmodeling to develop robust AVSR models. Lastly, we propose a novel AVSR\nframework, namely Audio-Visual Reliability Scoring module (AV-RelScore), that\nis robust to the corrupted multimodal inputs. The AV-RelScore can determine\nwhich input modal stream is reliable or not for the prediction and also can\nexploit the more reliable streams in prediction. The effectiveness of the\nproposed method is evaluated with comprehensive experiments on popular\nbenchmark databases, LRS2 and LRS3. We also show that the reliability scores\nobtained by AV-RelScore well reflect the degree of corruption and make the\nproposed model focus on the reliable multimodal representations.\n","authors":["Joanna Hong","Minsu Kim","Jeongsoo Choi","Yong Man Ro"],"pdf_url":"https://arxiv.org/pdf/2303.08536v2.pdf","comment":"Accepted at CVPR 2023. Implementation available:\n  https://github.com/joannahong/AV-RelScore"},{"id":"http://arxiv.org/abs/2303.05499v4","updated":"2023-03-20T06:57:39Z","published":"2023-03-09T18:52:16Z","title":"Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set\n  Object Detection","summary":"  In this paper, we present an open-set object detector, called Grounding DINO,\nby marrying Transformer-based detector DINO with grounded pre-training, which\ncan detect arbitrary objects with human inputs such as category names or\nreferring expressions. The key solution of open-set object detection is\nintroducing language to a closed-set detector for open-set concept\ngeneralization. To effectively fuse language and vision modalities, we\nconceptually divide a closed-set detector into three phases and propose a tight\nfusion solution, which includes a feature enhancer, a language-guided query\nselection, and a cross-modality decoder for cross-modality fusion. While\nprevious works mainly evaluate open-set object detection on novel categories,\nwe propose to also perform evaluations on referring expression comprehension\nfor objects specified with attributes. Grounding DINO performs remarkably well\non all three settings, including benchmarks on COCO, LVIS, ODinW, and\nRefCOCO/+/g. Grounding DINO achieves a $52.5$ AP on the COCO detection\nzero-shot transfer benchmark, i.e., without any training data from COCO. It\nsets a new record on the ODinW zero-shot benchmark with a mean $26.1$ AP. Code\nwill be available at \\url{https://github.com/IDEA-Research/GroundingDINO}.\n","authors":["Shilong Liu","Zhaoyang Zeng","Tianhe Ren","Feng Li","Hao Zhang","Jie Yang","Chunyuan Li","Jianwei Yang","Hang Su","Jun Zhu","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.05499v4.pdf","comment":"Code will be available at\n  https://github.com/IDEA-Research/GroundingDINO"},{"id":"http://arxiv.org/abs/2303.10904v1","updated":"2023-03-20T06:47:59Z","published":"2023-03-20T06:47:59Z","title":"Actionlet-Dependent Contrastive Learning for Unsupervised Skeleton-Based\n  Action Recognition","summary":"  The self-supervised pretraining paradigm has achieved great success in\nskeleton-based action recognition. However, these methods treat the motion and\nstatic parts equally, and lack an adaptive design for different parts, which\nhas a negative impact on the accuracy of action recognition. To realize the\nadaptive action modeling of both parts, we propose an Actionlet-Dependent\nContrastive Learning method (ActCLR). The actionlet, defined as the\ndiscriminative subset of the human skeleton, effectively decomposes motion\nregions for better action modeling. In detail, by contrasting with the static\nanchor without motion, we extract the motion region of the skeleton data, which\nserves as the actionlet, in an unsupervised manner. Then, centering on\nactionlet, a motion-adaptive data transformation method is built. Different\ndata transformations are applied to actionlet and non-actionlet regions to\nintroduce more diversity while maintaining their own characteristics.\nMeanwhile, we propose a semantic-aware feature pooling method to build feature\nrepresentations among motion and static regions in a distinguished manner.\nExtensive experiments on NTU RGB+D and PKUMMD show that the proposed method\nachieves remarkable action recognition performance. More visualization and\nquantitative experiments demonstrate the effectiveness of our method. Our\nproject website is available at https://langlandslin.github.io/projects/ActCLR/\n","authors":["Lilang Lin","Jiahang Zhang","Jiaying Liu"],"pdf_url":"https://arxiv.org/pdf/2303.10904v1.pdf","comment":"Accepted to CVPR2023. The project page is at\n  https://langlandslin.github.io/projects/ActCLR/"},{"id":"http://arxiv.org/abs/2303.10902v1","updated":"2023-03-20T06:44:49Z","published":"2023-03-20T06:44:49Z","title":"Feature Alignment and Uniformity for Test Time Adaptation","summary":"  Test time adaptation (TTA) aims to adapt deep neural networks when receiving\nout of distribution test domain samples. In this setting, the model can only\naccess online unlabeled test samples and pre-trained models on the training\ndomains. We first address TTA as a feature revision problem due to the domain\ngap between source domains and target domains. After that, we follow the two\nmeasurements alignment and uniformity to discuss the test time feature\nrevision. For test time feature uniformity, we propose a test time\nself-distillation strategy to guarantee the consistency of uniformity between\nrepresentations of the current batch and all the previous batches. For test\ntime feature alignment, we propose a memorized spatial local clustering\nstrategy to align the representations among the neighborhood samples for the\nupcoming batch. To deal with the common noisy label problem, we propound the\nentropy and consistency filters to select and drop the possible noisy labels.\nTo prove the scalability and efficacy of our method, we conduct experiments on\nfour domain generalization benchmarks and four medical image segmentation tasks\nwith various backbones. Experiment results show that our method not only\nimproves baseline stably but also outperforms existing state-of-the-art test\ntime adaptation methods.\n","authors":["Shuai Wang","Daoan Zhang","Zipei Yan","Jianguo Zhang","Rui Li"],"pdf_url":"https://arxiv.org/pdf/2303.10902v1.pdf","comment":"CVPR2023"},{"id":"http://arxiv.org/abs/2303.10898v1","updated":"2023-03-20T06:35:46Z","published":"2023-03-20T06:35:46Z","title":"A Tiny Machine Learning Model for Point Cloud Object Classification","summary":"  The design of a tiny machine learning model, which can be deployed in mobile\nand edge devices, for point cloud object classification is investigated in this\nwork. To achieve this objective, we replace the multi-scale representation of a\npoint cloud object with a single-scale representation for complexity reduction,\nand exploit rich 3D geometric information of a point cloud object for\nperformance improvement. The proposed solution is named Green-PointHop due to\nits low computational complexity. We evaluate the performance of Green-PointHop\non ModelNet40 and ScanObjectNN two datasets. Green-PointHop has a model size of\n64K parameters. It demands 2.3M floating-point operations (FLOPs) to classify a\nModelNet40 object of 1024 down-sampled points. Its classification performance\ngaps against the state-of-the-art DGCNN method are 3% and 7% for ModelNet40 and\nScanObjectNN, respectively. On the other hand, the model size and inference\ncomplexity of DGCNN are 42X and 1203X of those of Green-PointHop, respectively.\n","authors":["Min Zhang","Jintang Xue","Pranav Kadam","Hardik Prajapati","Shan Liu","C. -C. Jay Kuo"],"pdf_url":"https://arxiv.org/pdf/2303.10898v1.pdf","comment":"13 pages, 4 figures"},{"id":"http://arxiv.org/abs/2303.10896v1","updated":"2023-03-20T06:32:55Z","published":"2023-03-20T06:32:55Z","title":"Graphics Capsule: Learning Hierarchical 3D Face Representations from 2D\n  Images","summary":"  The function of constructing the hierarchy of objects is important to the\nvisual process of the human brain. Previous studies have successfully adopted\ncapsule networks to decompose the digits and faces into parts in an\nunsupervised manner to investigate the similar perception mechanism of neural\nnetworks. However, their descriptions are restricted to the 2D space, limiting\ntheir capacities to imitate the intrinsic 3D perception ability of humans. In\nthis paper, we propose an Inverse Graphics Capsule Network (IGC-Net) to learn\nthe hierarchical 3D face representations from large-scale unlabeled images. The\ncore of IGC-Net is a new type of capsule, named graphics capsule, which\nrepresents 3D primitives with interpretable parameters in computer graphics\n(CG), including depth, albedo, and 3D pose. Specifically, IGC-Net first\ndecomposes the objects into a set of semantic-consistent part-level\ndescriptions and then assembles them into object-level descriptions to build\nthe hierarchy. The learned graphics capsules reveal how the neural networks,\noriented at visual perception, understand faces as a hierarchy of 3D models.\nBesides, the discovered parts can be deployed to the unsupervised face\nsegmentation task to evaluate the semantic consistency of our method. Moreover,\nthe part-level descriptions with explicit physical meanings provide insight\ninto the face analysis that originally runs in a black box, such as the\nimportance of shape and texture for face recognition. Experiments on CelebA,\nBP4D, and Multi-PIE demonstrate the characteristics of our IGC-Net.\n","authors":["Chang Yu","Xiangyu Zhu","Xiaomei Zhang","Zhaoxiang Zhang","Zhen Lei"],"pdf_url":"https://arxiv.org/pdf/2303.10896v1.pdf","comment":"Accepted by CVPR2023"},{"id":"http://arxiv.org/abs/2303.10895v1","updated":"2023-03-20T06:32:48Z","published":"2023-03-20T06:32:48Z","title":"Leapfrog Diffusion Model for Stochastic Trajectory Prediction","summary":"  To model the indeterminacy of human behaviors, stochastic trajectory\nprediction requires a sophisticated multi-modal distribution of future\ntrajectories. Emerging diffusion models have revealed their tremendous\nrepresentation capacities in numerous generation tasks, showing potential for\nstochastic trajectory prediction. However, expensive time consumption prevents\ndiffusion models from real-time prediction, since a large number of denoising\nsteps are required to assure sufficient representation ability. To resolve the\ndilemma, we present LEapfrog Diffusion model (LED), a novel diffusion-based\ntrajectory prediction model, which provides real-time, precise, and diverse\npredictions. The core of the proposed LED is to leverage a trainable leapfrog\ninitializer to directly learn an expressive multi-modal distribution of future\ntrajectories, which skips a large number of denoising steps, significantly\naccelerating inference speed. Moreover, the leapfrog initializer is trained to\nappropriately allocate correlated samples to provide a diversity of predicted\nfuture trajectories, significantly improving prediction performances. Extensive\nexperiments on four real-world datasets, including NBA/NFL/SDD/ETH-UCY, show\nthat LED consistently improves performance and achieves 23.7%/21.9% ADE/FDE\nimprovement on NFL. The proposed LED also speeds up the inference\n19.3/30.8/24.3/25.1 times compared to the standard diffusion model on\nNBA/NFL/SDD/ETH-UCY, satisfying real-time inference needs. Code is available at\nhttps://github.com/MediaBrain-SJTU/LED.\n","authors":["Weibo Mao","Chenxin Xu","Qi Zhu","Siheng Chen","Yanfeng Wang"],"pdf_url":"https://arxiv.org/pdf/2303.10895v1.pdf","comment":"Accepted by CVPR2023"},{"id":"http://arxiv.org/abs/2303.10894v1","updated":"2023-03-20T06:26:49Z","published":"2023-03-20T06:26:49Z","title":"M$^{2}$SNet: Multi-scale in Multi-scale Subtraction Network for Medical\n  Image Segmentation","summary":"  Accurate medical image segmentation is critical for early medical diagnosis.\nMost existing methods are based on U-shape structure and use element-wise\naddition or concatenation to fuse different level features progressively in\ndecoder. However, both the two operations easily generate plenty of redundant\ninformation, which will weaken the complementarity between different level\nfeatures, resulting in inaccurate localization and blurred edges of lesions. To\naddress this challenge, we propose a general multi-scale in multi-scale\nsubtraction network (M$^{2}$SNet) to finish diverse segmentation from medical\nimage. Specifically, we first design a basic subtraction unit (SU) to produce\nthe difference features between adjacent levels in encoder. Next, we expand the\nsingle-scale SU to the intra-layer multi-scale SU, which can provide the\ndecoder with both pixel-level and structure-level difference information. Then,\nwe pyramidally equip the multi-scale SUs at different levels with varying\nreceptive fields, thereby achieving the inter-layer multi-scale feature\naggregation and obtaining rich multi-scale difference information. In addition,\nwe build a training-free network ``LossNet'' to comprehensively supervise the\ntask-aware features from bottom layer to top layer, which drives our\nmulti-scale subtraction network to capture the detailed and structural cues\nsimultaneously. Without bells and whistles, our method performs favorably\nagainst most state-of-the-art methods under different evaluation metrics on\neleven datasets of four different medical image segmentation tasks of diverse\nimage modalities, including color colonoscopy imaging, ultrasound imaging,\ncomputed tomography (CT), and optical coherence tomography (OCT). The source\ncode can be available at \\url{https://github.com/Xiaoqi-Zhao-DLUT/MSNet}.\n","authors":["Xiaoqi Zhao","Hongpeng Jia","Youwei Pang","Long Lv","Feng Tian","Lihe Zhang","Weibing Sun","Huchuan Lu"],"pdf_url":"https://arxiv.org/pdf/2303.10894v1.pdf","comment":"Submitted to IEEE TMI"},{"id":"http://arxiv.org/abs/2111.04019v2","updated":"2023-03-20T06:26:10Z","published":"2021-11-07T07:29:24Z","title":"Multi-Fake Evolutionary Generative Adversarial Networks for Imbalance\n  Hyperspectral Image Classification","summary":"  This paper presents a novel multi-fake evolutionary generative adversarial\nnetwork(MFEGAN) for handling imbalance hyperspectral image classification. It\nis an end-to-end approach in which different generative objective losses are\nconsidered in the generator network to improve the classification performance\nof the discriminator network. Thus, the same discriminator network has been\nused as a standard classifier by embedding the classifier network on top of the\ndiscriminating function. The effectiveness of the proposed method has been\nvalidated through two hyperspectral spatial-spectral data sets. The same\ngenerative and discriminator architectures have been utilized with two\ndifferent GAN objectives for a fair performance comparison with the proposed\nmethod. It is observed from the experimental validations that the proposed\nmethod outperforms the state-of-the-art methods with better classification\nperformance.\n","authors":["Tanmoy Dam","Nidhi Swami","Sreenatha G. Anavatti","Hussein A. Abbass"],"pdf_url":"https://arxiv.org/pdf/2111.04019v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10891v1","updated":"2023-03-20T06:16:22Z","published":"2023-03-20T06:16:22Z","title":"Offline-Online Class-incremental Continual Learning via Dual-prototype\n  Self-augment and Refinement","summary":"  This paper investigates a new, practical, but challenging problem named\nOffline-Online Class-incremental Continual Learning (O$^2$CL), which aims to\npreserve the discernibility of pre-trained (i.e., offline) base classes without\nbuffering data examples, and efficiently learn novel classes continuously in a\nsingle-pass (i.e., online) data stream. The challenges of this task are mainly\ntwo-fold: 1) Both base and novel classes suffer from severe catastrophic\nforgetting as no previous samples are available for replay. 2) As the online\ndata can only be observed once, there is no way to fully re-train the whole\nmodel, e.g., re-calibrate the decision boundaries via prototype alignment or\nfeature distillation. In this paper, we propose a novel Dual-prototype\nSelf-augment and Refinement method (DSR) for O$^2$CL problem, which consists of\ntwo strategies: 1) Dual class prototypes: Inner and hyper-dimensional\nprototypes are exploited to utilize the pre-trained information and obtain\nrobust quasi-orthogonal representations rather than example buffers for both\nprivacy preservation and memory reduction. 2) Self-augment and refinement:\nInstead of updating the whole network, we jointly optimize the extra projection\nmodule with the self-augment inner prototypes from base and novel classes,\ngradually refining the hyper-dimensional prototypes to obtain accurate decision\nboundaries for learned classes. Extensive experiments demonstrate the\neffectiveness and superiority of the proposed DSR in O$^2$CL.\n","authors":["Fushuo Huo","Wenchao Xu","Jingcai Guo","Haozhao Wang","Yunfeng Fan","Song Guo"],"pdf_url":"https://arxiv.org/pdf/2303.10891v1.pdf","comment":"Fushuo Huo, Wenchao Xu, Jingcai Guo, Haozhao Wang, and Yunfeng Fan,\n  Song Guo"},{"id":"http://arxiv.org/abs/2303.03757v2","updated":"2023-03-20T06:05:27Z","published":"2023-03-07T09:33:49Z","title":"Deep Learning for Inertial Positioning: A Survey","summary":"  Inertial sensors are widely utilized in smartphones, drones, robots, and IoT\ndevices, playing a crucial role in enabling ubiquitous and reliable\nlocalization. Inertial sensor-based positioning is essential in various\napplications, including personal navigation, location-based security, and\nhuman-device interaction. However, low-cost MEMS inertial sensors' measurements\nare inevitably corrupted by various error sources, leading to unbounded drifts\nwhen integrated doubly in traditional inertial navigation algorithms,\nsubjecting inertial positioning to the problem of error drifts. In recent\nyears, with the rapid increase in sensor data and computational power, deep\nlearning techniques have been developed, sparking significant research into\naddressing the problem of inertial positioning. Relevant literature in this\nfield spans across mobile computing, robotics, and machine learning. In this\narticle, we provide a comprehensive review of deep learning-based inertial\npositioning and its applications in tracking pedestrians, drones, vehicles, and\nrobots. We connect efforts from different fields and discuss how deep learning\ncan be applied to address issues such as sensor calibration, positioning error\ndrift reduction, and multi-sensor fusion. This article aims to attract readers\nfrom various backgrounds, including researchers and practitioners interested in\nthe potential of deep learning-based techniques to solve inertial positioning\nproblems. Our review demonstrates the exciting possibilities that deep learning\nbrings to the table and provides a roadmap for future research in this field.\n","authors":["Changhao Chen","Xianfei Pan"],"pdf_url":"https://arxiv.org/pdf/2303.03757v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10883v1","updated":"2023-03-20T06:01:53Z","published":"2023-03-20T06:01:53Z","title":"Explicit Visual Prompting for Low-Level Structure Segmentations","summary":"  We consider the generic problem of detecting low-level structures in images,\nwhich includes segmenting the manipulated parts, identifying out-of-focus\npixels, separating shadow regions, and detecting concealed objects. Whereas\neach such topic has been typically addressed with a domain-specific solution,\nwe show that a unified approach performs well across all of them. We take\ninspiration from the widely-used pre-training and then prompt tuning protocols\nin NLP and propose a new visual prompting model, named Explicit Visual\nPrompting (EVP). Different from the previous visual prompting which is\ntypically a dataset-level implicit embedding, our key insight is to enforce the\ntunable parameters focusing on the explicit visual content from each individual\nimage, i.e., the features from frozen patch embeddings and the input's\nhigh-frequency components. The proposed EVP significantly outperforms other\nparameter-efficient tuning protocols under the same amount of tunable\nparameters (5.7% extra trainable parameters of each task). EVP also achieves\nstate-of-the-art performances on diverse low-level structure segmentation tasks\ncompared to task-specific solutions. Our code is available at:\nhttps://github.com/NiFangBaAGe/Explict-Visual-Prompt.\n","authors":["Weihuang Liu","Xi Shen","Chi-Man Pun","Xiaodong Cun"],"pdf_url":"https://arxiv.org/pdf/2303.10883v1.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2303.10882v1","updated":"2023-03-20T05:49:14Z","published":"2023-03-20T05:49:14Z","title":"Efficient Map Sparsification Based on 2D and 3D Discretized Grids","summary":"  Localization in a pre-built map is a basic technique for robot autonomous\nnavigation. Existing mapping and localization methods commonly work well in\nsmall-scale environments. As a map grows larger, however, more memory is\nrequired and localization becomes inefficient. To solve these problems, map\nsparsification becomes a practical necessity to acquire a subset of the\noriginal map for localization. Previous map sparsification methods add a\nquadratic term in mixed-integer programming to enforce a uniform distribution\nof selected landmarks, which requires high memory capacity and heavy\ncomputation. In this paper, we formulate map sparsification in an efficient\nlinear form and select uniformly distributed landmarks based on 2D discretized\ngrids. Furthermore, to reduce the influence of different spatial distributions\nbetween the mapping and query sequences, which is not considered in previous\nmethods, we also introduce a space constraint term based on 3D discretized\ngrids. The exhaustive experiments in different datasets demonstrate the\nsuperiority of the proposed methods in both efficiency and localization\nperformance. The relevant codes will be released at\nhttps://github.com/fishmarch/SLAM_Map_Compression.\n","authors":["Xiaoyu Zhang","Yun-Hui Liu"],"pdf_url":"https://arxiv.org/pdf/2303.10882v1.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2303.09101v2","updated":"2023-03-20T05:47:14Z","published":"2023-03-16T06:14:18Z","title":"Contrastive Semi-supervised Learning for Underwater Image Restoration\n  via Reliable Bank","summary":"  Despite the remarkable achievement of recent underwater image restoration\ntechniques, the lack of labeled data has become a major hurdle for further\nprogress. In this work, we propose a mean-teacher based Semi-supervised\nUnderwater Image Restoration (Semi-UIR) framework to incorporate the unlabeled\ndata into network training. However, the naive mean-teacher method suffers from\ntwo main problems: (1) The consistency loss used in training might become\nineffective when the teacher's prediction is wrong. (2) Using L1 distance may\ncause the network to overfit wrong labels, resulting in confirmation bias. To\naddress the above problems, we first introduce a reliable bank to store the\n\"best-ever\" outputs as pseudo ground truth. To assess the quality of outputs,\nwe conduct an empirical analysis based on the monotonicity property to select\nthe most trustworthy NR-IQA method. Besides, in view of the confirmation bias\nproblem, we incorporate contrastive regularization to prevent the overfitting\non wrong labels. Experimental results on both full-reference and non-reference\nunderwater benchmarks demonstrate that our algorithm has obvious improvement\nover SOTA methods quantitatively and qualitatively. Code has been released at\nhttps://github.com/Huang-ShiRui/Semi-UIR.\n","authors":["Shirui Huang","Keyan Wang","Huan Liu","Jun Chen","Yunsong Li"],"pdf_url":"https://arxiv.org/pdf/2303.09101v2.pdf","comment":"CVPR2023"},{"id":"http://arxiv.org/abs/2303.10876v1","updated":"2023-03-20T05:23:46Z","published":"2023-03-20T05:23:46Z","title":"EqMotion: Equivariant Multi-agent Motion Prediction with Invariant\n  Interaction Reasoning","summary":"  Learning to predict agent motions with relationship reasoning is important\nfor many applications. In motion prediction tasks, maintaining motion\nequivariance under Euclidean geometric transformations and invariance of agent\ninteraction is a critical and fundamental principle. However, such equivariance\nand invariance properties are overlooked by most existing methods. To fill this\ngap, we propose EqMotion, an efficient equivariant motion prediction model with\ninvariant interaction reasoning. To achieve motion equivariance, we propose an\nequivariant geometric feature learning module to learn a Euclidean\ntransformable feature through dedicated designs of equivariant operations. To\nreason agent's interactions, we propose an invariant interaction reasoning\nmodule to achieve a more stable interaction modeling. To further promote more\ncomprehensive motion features, we propose an invariant pattern feature learning\nmodule to learn an invariant pattern feature, which cooperates with the\nequivariant geometric feature to enhance network expressiveness. We conduct\nexperiments for the proposed model on four distinct scenarios: particle\ndynamics, molecule dynamics, human skeleton motion prediction and pedestrian\ntrajectory prediction. Experimental results show that our method is not only\ngenerally applicable, but also achieves state-of-the-art prediction\nperformances on all the four tasks, improving by 24.0/30.1/8.6/9.2%. Code is\navailable at https://github.com/MediaBrain-SJTU/EqMotion.\n","authors":["Chenxin Xu","Robby T. Tan","Yuhong Tan","Siheng Chen","Yu Guang Wang","Xinchao Wang","Yanfeng Wang"],"pdf_url":"https://arxiv.org/pdf/2303.10876v1.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2211.15046v3","updated":"2023-03-20T05:18:49Z","published":"2022-11-28T04:08:55Z","title":"PCT-CycleGAN: Paired Complementary Temporal Cycle-Consistent Adversarial\n  Networks for Radar-Based Precipitation Nowcasting","summary":"  The precipitation nowcasting methods have been elaborated over the centuries\nbecause rain has a crucial impact on human life. Not only quantitative\nprecipitation forecast (QPF) models and convolutional long short-term memory\n(ConvLSTM), but also various sophisticated methods such as the latest MetNet-2\nare emerging. In this paper, we propose a paired complementary temporal\ncycle-consistent adversarial networks (PCT-CycleGAN) for radar-based\nprecipitation nowcasting, inspired by cycle-consistent adversarial networks\n(CycleGAN), which shows strong performance in image-to-image translation.\nPCT-CycleGAN generates temporal causality using two generator networks with\nforward and backward temporal dynamics in paired complementary cycles. Each\ngenerator network learns a huge number of one-to-one mappings about\ntime-dependent radar-based precipitation data to approximate a mapping function\nrepresenting the temporal dynamics in each direction. To create robust temporal\ncausality between paired complementary cycles, novel connection loss is\nproposed. The generator network learning forward temporal dynamics in\nPCT-CycleGAN generates radar-based precipitation data 10 minutes from the\ncurrent time. Also, it provides a reliable prediction of up to 2 hours with\niterative forecasting. The superiority of PCT-CycleGAN is demonstrated through\nqualitative and quantitative comparisons with several previous methods.\n","authors":["Jaeho Choi","Yura Kim","Kwang-Ho Kim","Sung-Hwa Jung","Ikhyun Cho"],"pdf_url":"https://arxiv.org/pdf/2211.15046v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10863v1","updated":"2023-03-20T04:54:26Z","published":"2023-03-20T04:54:26Z","title":"Decomposed Prototype Learning for Few-Shot Scene Graph Generation","summary":"  Today's scene graph generation (SGG) models typically require abundant manual\nannotations to learn new predicate types. Thus, it is difficult to apply them\nto real-world applications with a long-tailed distribution of predicates. In\nthis paper, we focus on a new promising task of SGG: few-shot SGG (FSSGG).\nFSSGG encourages models to be able to quickly transfer previous knowledge and\nrecognize novel predicates well with only a few examples. Although many\nadvanced approaches have achieved great success on few-shot learning (FSL)\ntasks, straightforwardly extending them into FSSGG is not applicable due to two\nintrinsic characteristics of predicate concepts: 1) Each predicate category\ncommonly has multiple semantic meanings under different contexts. 2) The visual\nappearance of relation triplets with the same predicate differs greatly under\ndifferent subject-object pairs. Both issues make it hard to model conventional\nlatent representations for predicate categories with state-of-the-art FSL\nmethods. To this end, we propose a novel Decomposed Prototype Learning (DPL).\nSpecifically, we first construct a decomposable prototype space to capture\nintrinsic visual patterns of subjects and objects for predicates, and enhance\ntheir feature representations with these decomposed prototypes. Then, we devise\nan intelligent metric learner to assign adaptive weights to each support sample\nby considering the relevance of their subject-object pairs. We further re-split\nthe VG dataset and compare DPL with various FSL methods to benchmark this task.\nExtensive results show that DPL achieves excellent performance in both base and\nnovel categories.\n","authors":["Xingchen Li","Long Chen","Guikun Chen","Yinfu Feng","Yi Yang","Jun Xiao"],"pdf_url":"https://arxiv.org/pdf/2303.10863v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.09004v3","updated":"2023-03-20T04:49:10Z","published":"2022-09-19T13:28:32Z","title":"EcoFormer: Energy-Saving Attention with Linear Complexity","summary":"  Transformer is a transformative framework that models sequential data and has\nachieved remarkable performance on a wide range of tasks, but with high\ncomputational and energy cost. To improve its efficiency, a popular choice is\nto compress the models via binarization which constrains the floating-point\nvalues into binary ones to save resource consumption owing to cheap bitwise\noperations significantly. However, existing binarization methods only aim at\nminimizing the information loss for the input distribution statistically, while\nignoring the pairwise similarity modeling at the core of the attention. To this\nend, we propose a new binarization paradigm customized to high-dimensional\nsoftmax attention via kernelized hashing, called EcoFormer, to map the original\nqueries and keys into low-dimensional binary codes in Hamming space. The\nkernelized hash functions are learned to match the ground-truth similarity\nrelations extracted from the attention map in a self-supervised way. Based on\nthe equivalence between the inner product of binary codes and the Hamming\ndistance as well as the associative property of matrix multiplication, we can\napproximate the attention in linear complexity by expressing it as a\ndot-product of binary codes. Moreover, the compact binary representations of\nqueries and keys enable us to replace most of the expensive multiply-accumulate\noperations in attention with simple accumulations to save considerable on-chip\nenergy footprint on edge devices. Extensive experiments on both vision and\nlanguage tasks show that EcoFormer consistently achieves comparable performance\nwith standard attentions while consuming much fewer resources. For example,\nbased on PVTv2-B0 and ImageNet-1K, Ecoformer achieves a 73% on-chip energy\nfootprint reduction with only a 0.33% performance drop compared to the standard\nattention. Code is available at https://github.com/ziplab/EcoFormer.\n","authors":["Jing Liu","Zizheng Pan","Haoyu He","Jianfei Cai","Bohan Zhuang"],"pdf_url":"https://arxiv.org/pdf/2209.09004v3.pdf","comment":"NeurIPS 2022 camera ready; First two authors contributed equally"},{"id":"http://arxiv.org/abs/2210.15929v2","updated":"2023-03-20T04:36:45Z","published":"2022-10-28T06:20:55Z","title":"Being Comes from Not-being: Open-vocabulary Text-to-Motion Generation\n  with Wordless Training","summary":"  Text-to-motion generation is an emerging and challenging problem, which aims\nto synthesize motion with the same semantics as the input text. However, due to\nthe lack of diverse labeled training data, most approaches either limit to\nspecific types of text annotations or require online optimizations to cater to\nthe texts during inference at the cost of efficiency and stability. In this\npaper, we investigate offline open-vocabulary text-to-motion generation in a\nzero-shot learning manner that neither requires paired training data nor extra\nonline optimization to adapt for unseen texts. Inspired by the prompt learning\nin NLP, we pretrain a motion generator that learns to reconstruct the full\nmotion from the masked motion. During inference, instead of changing the motion\ngenerator, our method reformulates the input text into a masked motion as the\nprompt for the motion generator to ``reconstruct'' the motion. In constructing\nthe prompt, the unmasked poses of the prompt are synthesized by a text-to-pose\ngenerator. To supervise the optimization of the text-to-pose generator, we\npropose the first text-pose alignment model for measuring the alignment between\ntexts and 3D poses. And to prevent the pose generator from overfitting to\nlimited training texts, we further propose a novel wordless training mechanism\nthat optimizes the text-to-pose generator without any training texts. The\ncomprehensive experimental results show that our method obtains a significant\nimprovement against the baseline methods. The code is available.\n","authors":["Junfan Lin","Jianlong Chang","Lingbo Liu","Guanbin Li","Liang Lin","Qi Tian","Chang-wen Chen"],"pdf_url":"https://arxiv.org/pdf/2210.15929v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10856v1","updated":"2023-03-20T04:30:18Z","published":"2023-03-20T04:30:18Z","title":"Revisiting Realistic Test-Time Training: Sequential Inference and\n  Adaptation by Anchored Clustering Regularized Self-Training","summary":"  Deploying models on target domain data subject to distribution shift requires\nadaptation. Test-time training (TTT) emerges as a solution to this adaptation\nunder a realistic scenario where access to full source domain data is not\navailable, and instant inference on the target domain is required. Despite many\nefforts into TTT, there is a confusion over the experimental settings, thus\nleading to unfair comparisons. In this work, we first revisit TTT assumptions\nand categorize TTT protocols by two key factors. Among the multiple protocols,\nwe adopt a realistic sequential test-time training (sTTT) protocol, under which\nwe develop a test-time anchored clustering (TTAC) approach to enable stronger\ntest-time feature learning. TTAC discovers clusters in both source and target\ndomains and matches the target clusters to the source ones to improve\nadaptation. When source domain information is strictly absent (i.e.\nsource-free) we further develop an efficient method to infer source domain\ndistributions for anchored clustering. Finally, self-training~(ST) has\ndemonstrated great success in learning from unlabeled data and we empirically\nfigure out that applying ST alone to TTT is prone to confirmation bias.\nTherefore, a more effective TTT approach is introduced by regularizing\nself-training with anchored clustering, and the improved model is referred to\nas TTAC++. We demonstrate that, under all TTT protocols, TTAC++ consistently\noutperforms the state-of-the-art methods on five TTT datasets, including\ncorrupted target domain, selected hard samples, synthetic-to-real adaptation\nand adversarially attacked target domain. We hope this work will provide a fair\nbenchmarking of TTT methods, and future research should be compared within\nrespective protocols.\n","authors":["Yongyi Su","Xun Xu","Tianrui Li","Kui Jia"],"pdf_url":"https://arxiv.org/pdf/2303.10856v1.pdf","comment":"Test-time training, Self-training. arXiv admin note: substantial text\n  overlap with arXiv:2206.02721"},{"id":"http://arxiv.org/abs/2201.09548v2","updated":"2023-03-20T04:28:10Z","published":"2022-01-24T09:44:11Z","title":"Consistent 3D Hand Reconstruction in Video via self-supervised Learning","summary":"  We present a method for reconstructing accurate and consistent 3D hands from\na monocular video. We observe that detected 2D hand keypoints and the image\ntexture provide important cues about the geometry and texture of the 3D hand,\nwhich can reduce or even eliminate the requirement on 3D hand annotation. Thus\nwe propose ${\\rm {S}^{2}HAND}$, a self-supervised 3D hand reconstruction model,\nthat can jointly estimate pose, shape, texture, and the camera viewpoint from a\nsingle RGB input through the supervision of easily accessible 2D detected\nkeypoints. We leverage the continuous hand motion information contained in the\nunlabeled video data and propose ${\\rm {S}^{2}HAND(V)}$, which uses a set of\nweights shared ${\\rm {S}^{2}HAND}$ to process each frame and exploits\nadditional motion, texture, and shape consistency constrains to promote more\naccurate hand poses and more consistent shapes and textures. Experiments on\nbenchmark datasets demonstrate that our self-supervised approach produces\ncomparable hand reconstruction performance compared with the recent\nfull-supervised methods in single-frame as input setup, and notably improves\nthe reconstruction accuracy and consistency when using video training data.\n","authors":["Zhigang Tu","Zhisheng Huang","Yujin Chen","Di Kang","Linchao Bao","Bisheng Yang","Junsong Yuan"],"pdf_url":"https://arxiv.org/pdf/2201.09548v2.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2103.11703"},{"id":"http://arxiv.org/abs/2303.10849v1","updated":"2023-03-20T03:58:03Z","published":"2023-03-20T03:58:03Z","title":"Facial Affective Analysis based on MAE and Multi-modal Information for\n  5th ABAW Competition","summary":"  Human affective behavior analysis focuses on analyzing human expressions or\nother behaviors, which helps improve the understanding of human psychology.\nCVPR 2023 Competition on Affective Behavior Analysis in-the-wild (ABAW) makes\ngreat efforts to provide the diversity data for the recognition of the commonly\nused emotion representations, including Action Units~(AU), basic expression\ncategories and Valence-Arousal~(VA). In this paper, we introduce our submission\nto the CVPR 2023: ABAW5 for AU detection, expression classification, VA\nestimation and emotional reaction intensity (ERI) estimation. First of all, we\nintroduce the vision information from an MAE model, which has been pre-trained\non a large-scale face image dataset in a self-supervised manner. Then the MAE\nencoder part is finetuned on the ABAW challenges on the single frame of\nAff-wild2 dataset. We also exploit the multi-modal and temporal information\nfrom the videos and design a transformer-based framework to fusion the\nmulti-modal features. Moreover, we construct a novel two-branch collaboration\ntraining strategy to further enhance the model generalization by randomly\ninterpolating the logits space. The extensive quantitative experiments, as well\nas ablation studies on the Aff-Wild2 dataset and Hume-Reaction dataset prove\nthe effectiveness of our proposed method.\n","authors":["Wei Zhang","Bowen Ma","Feng Qiu","Yu Ding"],"pdf_url":"https://arxiv.org/pdf/2303.10849v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2212.09598v2","updated":"2023-03-20T16:56:39Z","published":"2022-12-19T16:34:19Z","title":"Query-as-context Pre-training for Dense Passage Retrieval","summary":"  Recently, methods have been developed to improve the performance of dense\npassage retrieval by using context-supervised pre-training. These methods\nsimply consider two passages from the same document to be relevant, without\ntaking into account the possibility of weakly correlated pairs. Thus, this\npaper proposes query-as-context pre-training, a simple yet effective\npre-training technique to alleviate the issue. Query-as-context pre-training\nassumes that the query derived from a passage is more likely to be relevant to\nthat passage and forms a passage-query pair. These passage-query pairs are then\nused in contrastive or generative context-supervised pre-training. The\npre-trained models are evaluated on large-scale passage retrieval benchmarks\nand out-of-domain zero-shot benchmarks. Experimental results show that\nquery-as-context pre-training brings considerable gains and meanwhile speeds up\ntraining, demonstrating its effectiveness and efficiency. Our code will be\navailable at https://github.com/caskcsg/ir/tree/main/cotmae-qc .\n","authors":["Xing Wu","Guangyuan Ma","Wanhui Qian","Zijia Lin","Fuzheng Zhang","Songlin Hu"],"pdf_url":"https://arxiv.org/pdf/2212.09598v2.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2303.11009v1","updated":"2023-03-20T10:39:25Z","published":"2023-03-20T10:39:25Z","title":"Learning Multi-Stage Multi-Grained Semantic Embeddings for E-Commerce\n  Search","summary":"  Retrieving relevant items that match users' queries from billion-scale corpus\nforms the core of industrial e-commerce search systems, in which\nembedding-based retrieval (EBR) methods are prevailing. These methods adopt a\ntwo-tower framework to learn embedding vectors for query and item separately\nand thus leverage efficient approximate nearest neighbor (ANN) search to\nretrieve relevant items. However, existing EBR methods usually ignore\ninconsistent user behaviors in industrial multi-stage search systems, resulting\nin insufficient retrieval efficiency with a low commercial return. To tackle\nthis challenge, we propose to improve EBR methods by learning Multi-level\nMulti-Grained Semantic Embeddings(MMSE). We propose the multi-stage information\nmining to exploit the ordered, clicked, unclicked and random sampled items in\npractical user behavior data, and then capture query-item similarity via a\npost-fusion strategy. We then propose multi-grained learning objectives that\nintegrate the retrieval loss with global comparison ability and the ranking\nloss with local comparison ability to generate semantic embeddings. Both\nexperiments on a real-world billion-scale dataset and online A/B tests verify\nthe effectiveness of MMSE in achieving significant performance improvements on\nmetrics such as offline recall and online conversion rate (CVR).\n","authors":["Binbin Wang","Mingming Li","Zhixiong Zeng","Jingwei Zhuo","Songlin Wang","Sulong Xu","Bo Long","Weipeng Yan"],"pdf_url":"https://arxiv.org/pdf/2303.11009v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11005v1","updated":"2023-03-20T10:33:06Z","published":"2023-03-20T10:33:06Z","title":"Controllable Ancient Chinese Lyrics Generation Based on Phrase Prototype\n  Retrieving","summary":"  Generating lyrics and poems is one of the essential downstream tasks in the\nNatural Language Processing (NLP) field. Current methods have performed well in\nsome lyrics generation scenarios but need further improvements in tasks\nrequiring fine control. We propose a novel method for generating ancient\nChinese lyrics (Song Ci), a type of ancient lyrics that involves precise\ncontrol of song structure. The system is equipped with a phrase retriever and a\nphrase connector. Based on an input prompt, the phrase retriever picks phrases\nfrom a database to construct a phrase pool. The phrase connector then selects a\nseries of phrases from the phrase pool that minimizes a multi-term loss\nfunction that considers rhyme, song structure, and fluency. Experimental\nresults show that our method can generate high-quality ancient Chinese lyrics\nwhile performing well on topic and song structure control. We also expect our\napproach to be generalized to other lyrics-generating tasks.\n","authors":["Li Yi"],"pdf_url":"https://arxiv.org/pdf/2303.11005v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10871v1","updated":"2023-03-20T05:12:43Z","published":"2023-03-20T05:12:43Z","title":"NASA Science Mission Directorate Knowledge Graph Discovery","summary":"  The size of the National Aeronautics and Space Administration (NASA) Science\nMission Directorate (SMD) is growing exponentially, allowing researchers to\nmake discoveries. However, making discoveries is challenging and time-consuming\ndue to the size of the data catalogs, and as many concepts and data are\nindirectly connected. This paper proposes a pipeline to generate knowledge\ngraphs (KGs) representing different NASA SMD domains. These KGs can be used as\nthe basis for dataset search engines, saving researchers time and supporting\nthem in finding new connections. We collected textual data and used several\nmodern natural language processing (NLP) methods to create the nodes and the\nedges of the KGs. We explore the cross-domain connections, discuss our\nchallenges, and provide future directions to inspire researchers working on\nsimilar challenges.\n","authors":["Roelien C. Timmer","Fech Scen Khoo","Megan Mark","Marcella Scoczynski Ribeiro Martins","Anamaria Berea","Gregory Renard","Kaylin Bugbee"],"pdf_url":"https://arxiv.org/pdf/2303.10871v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10816v1","updated":"2023-03-20T01:20:02Z","published":"2023-03-20T01:20:02Z","title":"IMF: Interactive Multimodal Fusion Model for Link Prediction","summary":"  Link prediction aims to identify potential missing triples in knowledge\ngraphs. To get better results, some recent studies have introduced multimodal\ninformation to link prediction. However, these methods utilize multimodal\ninformation separately and neglect the complicated interaction between\ndifferent modalities. In this paper, we aim at better modeling the\ninter-modality information and thus introduce a novel Interactive Multimodal\nFusion (IMF) model to integrate knowledge from different modalities. To this\nend, we propose a two-stage multimodal fusion framework to preserve\nmodality-specific knowledge as well as take advantage of the complementarity\nbetween different modalities. Instead of directly projecting different\nmodalities into a unified space, our multimodal fusion module limits the\nrepresentations of different modalities independent while leverages bilinear\npooling for fusion and incorporates contrastive learning as additional\nconstraints. Furthermore, the decision fusion module delivers the learned\nweighted average over the predictions of all modalities to better incorporate\nthe complementarity of different modalities. Our approach has been demonstrated\nto be effective through empirical evaluations on several real-world datasets.\nThe implementation code is available online at\nhttps://github.com/HestiaSky/IMF-Pytorch.\n","authors":["Xinhang Li","Xiangyu Zhao","Jiaxing Xu","Yong Zhang","Chunxiao Xing"],"pdf_url":"https://arxiv.org/pdf/2303.10816v1.pdf","comment":"9 pages, 7 figures, 4 tables, WWW'2023"},{"id":"http://arxiv.org/abs/2207.11187v2","updated":"2023-03-20T19:15:04Z","published":"2022-07-18T18:08:34Z","title":"TaDaa: real time Ticket Assignment Deep learning Auto Advisor for\n  customer support, help desk, and issue ticketing systems","summary":"  This paper proposes TaDaa: Ticket Assignment Deep learning Auto Advisor,\nwhich leverages the latest Transformers models and machine learning techniques\nquickly assign issues within an organization, like customer support, help desk\nand alike issue ticketing systems. The project provides functionality to 1)\nassign an issue to the correct group, 2) assign an issue to the best resolver,\nand 3) provide the most relevant previously solved tickets to resolvers. We\nleverage one ticketing system sample dataset, with over 3k+ groups and over\n10k+ resolvers to obtain a 95.2% top 3 accuracy on group suggestions and a\n79.0% top 5 accuracy on resolver suggestions. We hope this research will\ngreatly improve average issue resolution time on customer support, help desk,\nand issue ticketing systems.\n","authors":["Leon Feng","Jnana Senapati","Bill Liu"],"pdf_url":"https://arxiv.org/pdf/2207.11187v2.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2303.11330v1","updated":"2023-03-20T17:59:58Z","published":"2023-03-20T17:59:58Z","title":"Legs as Manipulator: Pushing Quadrupedal Agility Beyond Locomotion","summary":"  Locomotion has seen dramatic progress for walking or running across\nchallenging terrains. However, robotic quadrupeds are still far behind their\nbiological counterparts, such as dogs, which display a variety of agile skills\nand can use the legs beyond locomotion to perform several basic manipulation\ntasks like interacting with objects and climbing. In this paper, we take a step\ntowards bridging this gap by training quadruped robots not only to walk but\nalso to use the front legs to climb walls, press buttons, and perform object\ninteraction in the real world. To handle this challenging optimization, we\ndecouple the skill learning broadly into locomotion, which involves anything\nthat involves movement whether via walking or climbing a wall, and\nmanipulation, which involves using one leg to interact while balancing on the\nother three legs. These skills are trained in simulation using curriculum and\ntransferred to the real world using our proposed sim2real variant that builds\nupon recent locomotion success. Finally, we combine these skills into a robust\nlong-term plan by learning a behavior tree that encodes a high-level task\nhierarchy from one clean expert demonstration. We evaluate our method in both\nsimulation and real-world showing successful executions of both short as well\nas long-range tasks and how robustness helps confront external perturbations.\nVideos at https://robot-skills.github.io\n","authors":["Xuxin Cheng","Ashish Kumar","Deepak Pathak"],"pdf_url":"https://arxiv.org/pdf/2303.11330v1.pdf","comment":"Accepted at ICRA 2023. Videos at https://robot-skills.github.io"},{"id":"http://arxiv.org/abs/2303.11327v1","updated":"2023-03-20T17:59:49Z","published":"2023-03-20T17:59:49Z","title":"3D Concept Learning and Reasoning from Multi-View Images","summary":"  Humans are able to accurately reason in 3D by gathering multi-view\nobservations of the surrounding world. Inspired by this insight, we introduce a\nnew large-scale benchmark for 3D multi-view visual question answering\n(3DMV-VQA). This dataset is collected by an embodied agent actively moving and\ncapturing RGB images in an environment using the Habitat simulator. In total,\nit consists of approximately 5k scenes, 600k images, paired with 50k questions.\nWe evaluate various state-of-the-art models for visual reasoning on our\nbenchmark and find that they all perform poorly. We suggest that a principled\napproach for 3D reasoning from multi-view images should be to infer a compact\n3D representation of the world from the multi-view images, which is further\ngrounded on open-vocabulary semantic concepts, and then to execute reasoning on\nthese 3D representations. As the first step towards this approach, we propose a\nnovel 3D concept learning and reasoning (3D-CLR) framework that seamlessly\ncombines these components via neural fields, 2D pre-trained vision-language\nmodels, and neural reasoning operators. Experimental results suggest that our\nframework outperforms baseline models by a large margin, but the challenge\nremains largely unsolved. We further perform an in-depth analysis of the\nchallenges and highlight potential future directions.\n","authors":["Yining Hong","Chunru Lin","Yilun Du","Zhenfang Chen","Joshua B. Tenenbaum","Chuang Gan"],"pdf_url":"https://arxiv.org/pdf/2303.11327v1.pdf","comment":"CVPR 2023. Project page: https://vis-www.cs.umass.edu/3d-clr/"},{"id":"http://arxiv.org/abs/2303.11323v1","updated":"2023-03-20T17:57:15Z","published":"2023-03-20T17:57:15Z","title":"Tangent Bundle Convolutional Learning: from Manifolds to Cellular\n  Sheaves and Back","summary":"  In this work we introduce a convolution operation over the tangent bundle of\nRiemann manifolds in terms of exponentials of the Connection Laplacian\noperator. We define tangent bundle filters and tangent bundle neural networks\n(TNNs) based on this convolution operation, which are novel continuous\narchitectures operating on tangent bundle signals, i.e. vector fields over the\nmanifolds. Tangent bundle filters admit a spectral representation that\ngeneralizes the ones of scalar manifold filters, graph filters and standard\nconvolutional filters in continuous time. We then introduce a discretization\nprocedure, both in the space and time domains, to make TNNs implementable,\nshowing that their discrete counterpart is a novel principled variant of the\nvery recently introduced sheaf neural networks. We formally prove that this\ndiscretized architecture converges to the underlying continuous TNN. Finally,\nwe numerically evaluate the effectiveness of the proposed architecture on\nvarious learning tasks, both on synthetic and real data.\n","authors":["Claudio Battiloro","Zhiyang Wang","Hans Riess","Paolo Di Lorenzo","Alejandro Ribeiro"],"pdf_url":"https://arxiv.org/pdf/2303.11323v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2210.15058"},{"id":"http://arxiv.org/abs/2207.01614v2","updated":"2023-03-20T17:51:09Z","published":"2022-07-04T17:56:14Z","title":"Beyond mAP: Towards better evaluation of instance segmentation","summary":"  Correctness of instance segmentation constitutes counting the number of\nobjects, correctly localizing all predictions and classifying each localized\nprediction. Average Precision is the de-facto metric used to measure all these\nconstituents of segmentation. However, this metric does not penalize duplicate\npredictions in the high-recall range, and cannot distinguish instances that are\nlocalized correctly but categorized incorrectly. This weakness has\ninadvertently led to network designs that achieve significant gains in AP but\nalso introduce a large number of false positives. We therefore cannot rely on\nAP to choose a model that provides an optimal tradeoff between false positives\nand high recall. To resolve this dilemma, we review alternative metrics in the\nliterature and propose two new measures to explicitly measure the amount of\nboth spatial and categorical duplicate predictions. We also propose a Semantic\nSorting and NMS module to remove these duplicates based on a pixel occupancy\nmatching scheme. Experiments show that modern segmentation networks have\nsignificant gains in AP, but also contain a considerable amount of duplicates.\nOur Semantic Sorting and NMS can be added as a plug-and-play module to mitigate\nhedged predictions and preserve AP.\n","authors":["Rohit Jena","Lukas Zhornyak","Nehal Doiphode","Pratik Chaudhari","Vivek Buch","James Gee","Jianbo Shi"],"pdf_url":"https://arxiv.org/pdf/2207.01614v2.pdf","comment":"Accepted at CVPR 2023"},{"id":"http://arxiv.org/abs/2303.11306v1","updated":"2023-03-20T17:45:08Z","published":"2023-03-20T17:45:08Z","title":"Localizing Object-level Shape Variations with Text-to-Image Diffusion\n  Models","summary":"  Text-to-image models give rise to workflows which often begin with an\nexploration step, where users sift through a large collection of generated\nimages. The global nature of the text-to-image generation process prevents\nusers from narrowing their exploration to a particular object in the image. In\nthis paper, we present a technique to generate a collection of images that\ndepicts variations in the shape of a specific object, enabling an object-level\nshape exploration process. Creating plausible variations is challenging as it\nrequires control over the shape of the generated object while respecting its\nsemantics. A particular challenge when generating object variations is\naccurately localizing the manipulation applied over the object's shape. We\nintroduce a prompt-mixing technique that switches between prompts along the\ndenoising process to attain a variety of shape choices. To localize the\nimage-space operation, we present two techniques that use the self-attention\nlayers in conjunction with the cross-attention layers. Moreover, we show that\nthese localization techniques are general and effective beyond the scope of\ngenerating object variations. Extensive results and comparisons demonstrate the\neffectiveness of our method in generating object variations, and the competence\nof our localization techniques.\n","authors":["Or Patashnik","Daniel Garibi","Idan Azuri","Hadar Averbuch-Elor","Daniel Cohen-Or"],"pdf_url":"https://arxiv.org/pdf/2303.11306v1.pdf","comment":"Project page at https://orpatashnik.github.io/local-prompt-mixing/"},{"id":"http://arxiv.org/abs/2201.04207v4","updated":"2023-03-20T17:32:54Z","published":"2022-01-11T21:31:18Z","title":"Fighting Money Laundering with Statistics and Machine Learning: An\n  Introduction and Review","summary":"  Money laundering is a profound global problem. Nonetheless, there is little\nscientific literature on statistical and machine learning methods for\nanti-money laundering. In this paper, we focus on anti-money laundering in\nbanks and provide an introduction and review of the literature. We propose a\nunifying terminology with two central elements: (i) client risk profiling and\n(ii) suspicious behavior flagging. We find that client risk profiling is\ncharacterized by diagnostics, i.e., efforts to find and explain risk factors.\nOn the other hand, suspicious behavior flagging is characterized by\nnon-disclosed features and hand-crafted risk indices. Finally, we discuss\ndirections for future research. One major challenge is the need for more public\ndata sets. This may potentially be addressed by synthetic data generation.\nOther possible research directions include semi-supervised and deep learning,\ninterpretability, and fairness of the results.\n","authors":["Rasmus Jensen","Alexandros Iosifidis"],"pdf_url":"https://arxiv.org/pdf/2201.04207v4.pdf","comment":"Accepted for publication in IEEE Access, vol. 11, pp. 8889-8903,\n  doi:10.1109/ACCESS.2023.3239549"},{"id":"http://arxiv.org/abs/2208.09099v3","updated":"2023-03-20T17:24:38Z","published":"2022-08-19T00:18:19Z","title":"Scalable Multi-Agent Lab Framework for Lab Optimization","summary":"  Autonomous materials research systems allow scientists to fail smarter, learn\nfaster, and spend less resources in their studies. As these systems grow in\nnumber, capability, and complexity, a new challenge arises - how will they work\ntogether across large facilities? We explore one solution to this question - a\nmulti-agent laboratory control frame-work. We demonstrate this framework with\nan autonomous material science lab in mind - where information from diverse\nresearch campaigns can be combined to ad-dress the scientific question at hand.\nThis framework can 1) account for realistic resource limits such as equipment\nuse, 2) allow for machine learning agents with diverse learning capabilities\nand goals capable of running re-search campaigns, and 3) facilitate multi-agent\ncollaborations and teams. The framework is dubbed the MULTI-agent auTonomous\nfAcilities - a Scalable frameworK aka MULTITASK. MULTITASK makes possible\nfacility-wide simulations, including agent-instrument and agent-agent\ninteractions. Through MULTITASK's modularity, real-world facilities can come\non-line in phases, with simulated instruments gradually replaced by real-world\ninstruments. We hope MULTITASK opens new areas of study in large-scale\nautonomous and semi-autonomous research campaigns and facilities.\n","authors":["A. Gilad Kusne","Austin McDannald"],"pdf_url":"https://arxiv.org/pdf/2208.09099v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11288v1","updated":"2023-03-20T17:23:15Z","published":"2023-03-20T17:23:15Z","title":"Rethinking SO(3)-equivariance with Bilinear Tensor Networks","summary":"  Many datasets in scientific and engineering applications are comprised of\nobjects which have specific geometric structure. A common example is data which\ninhabits a representation of the group SO$(3)$ of 3D rotations: scalars,\nvectors, tensors, \\textit{etc}. One way for a neural network to exploit prior\nknowledge of this structure is to enforce SO$(3)$-equivariance throughout its\nlayers, and several such architectures have been proposed. While general\nmethods for handling arbitrary SO$(3)$ representations exist, they\ncomputationally intensive and complicated to implement. We show that by\njudicious symmetry breaking, we can efficiently increase the expressiveness of\na network operating only on vector and order-2 tensor representations of\nSO$(2)$. We demonstrate the method on an important problem from High Energy\nPhysics known as \\textit{b-tagging}, where particle jets originating from\nb-meson decays must be discriminated from an overwhelming QCD background. In\nthis task, we find that augmenting a standard architecture with our method\nresults in a \\ensuremath{2.3\\times} improvement in rejection score.\n","authors":["Chase Shimmin","Zhelun Li","Ema Smith"],"pdf_url":"https://arxiv.org/pdf/2303.11288v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.11241v4","updated":"2023-03-20T17:16:33Z","published":"2022-06-22T17:42:24Z","title":"Concentration inequalities and optimal number of layers for stochastic\n  deep neural networks","summary":"  We state concentration inequalities for the output of the hidden layers of a\nstochastic deep neural network (SDNN), as well as for the output of the whole\nSDNN. These results allow us to introduce an expected classifier (EC), and to\ngive probabilistic upper bound for the classification error of the EC. We also\nstate the optimal number of layers for the SDNN via an optimal stopping\nprocedure. We apply our analysis to a stochastic version of a feedforward\nneural network with ReLU activation function.\n","authors":["Michele Caprio","Sayan Mukherjee"],"pdf_url":"https://arxiv.org/pdf/2206.11241v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11278v1","updated":"2023-03-20T17:13:50Z","published":"2023-03-20T17:13:50Z","title":"Constructing Bayesian Pseudo-Coresets using Contrastive Divergence","summary":"  Bayesian Pseudo-Coreset (BPC) and Dataset Condensation are two parallel\nstreams of work that construct a synthetic set such that, a model trained\nindependently on this synthetic set, yields the same performance as training on\nthe original training set. While dataset condensation methods use non-bayesian,\nheuristic ways to construct such a synthetic set, BPC methods take a bayesian\napproach and formulate the problem as divergence minimization between\nposteriors associated with original data and synthetic data. However, BPC\nmethods generally rely on distributional assumptions on these posteriors which\nmakes them less flexible and hinders their performance. In this work, we\npropose to solve these issues by modeling the posterior associated with\nsynthetic data by an energy-based distribution. We derive a\ncontrastive-divergence-like loss function to learn the synthetic set and show a\nsimple and efficient way to estimate this loss. Further, we perform rigorous\nexperiments pertaining to the proposed method. Our experiments on multiple\ndatasets show that the proposed method not only outperforms previous BPC\nmethods but also gives performance comparable to dataset condensation\ncounterparts.\n","authors":["Piyush Tiwary","Kumar Shubham","Vivek Kashyap","Prathosh A. P"],"pdf_url":"https://arxiv.org/pdf/2303.11278v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11277v1","updated":"2023-03-20T17:12:42Z","published":"2023-03-20T17:12:42Z","title":"Model Stitching: Looking For Functional Similarity Between\n  Representations","summary":"  Model stitching (Lenc & Vedaldi 2015) is a compelling methodology to compare\ndifferent neural network representations, because it allows us to measure to\nwhat degree they may be interchanged. We expand on a previous work from Bansal,\nNakkiran & Barak which used model stitching to compare representations of the\nsame shapes learned by differently seeded and/or trained neural networks of the\nsame architecture. Our contribution enables us to compare the representations\nlearned by layers with different shapes from neural networks with different\narchitectures. We subsequently reveal unexpected behavior of model stitching.\nNamely, we find that stitching, based on convolutions, for small ResNets, can\nreach high accuracy if those layers come later in the first (sender) network\nthan in the second (receiver), even if those layers are far apart.\n","authors":["Adriano Hernandez","Rumen Dangovski","Peter Y. Lu","Marin Soljacic"],"pdf_url":"https://arxiv.org/pdf/2303.11277v1.pdf","comment":"5 pages, 2 figures"},{"id":"http://arxiv.org/abs/2209.09419v4","updated":"2023-03-20T17:06:12Z","published":"2022-09-20T02:31:42Z","title":"Multi-armed Bandit Learning on a Graph","summary":"  The multi-armed bandit(MAB) problem is a simple yet powerful framework that\nhas been extensively studied in the context of decision-making under\nuncertainty. In many real-world applications, such as robotic applications,\nselecting an arm corresponds to a physical action that constrains the choices\nof the next available arms (actions). Motivated by this, we study an extension\nof MAB called the graph bandit, where an agent travels over a graph to maximize\nthe reward collected from different nodes. The graph defines the agent's\nfreedom in selecting the next available nodes at each step. We assume the graph\nstructure is fully available, but the reward distributions are unknown. Built\nupon an offline graph-based planning algorithm and the principle of optimism,\nwe design a learning algorithm, G-UCB, that balances long-term\nexploration-exploitation using the principle of optimism. We show that our\nproposed algorithm achieves $O(\\sqrt{|S|T\\log(T)}+D|S|\\log T)$ learning regret,\nwhere $|S|$ is the number of nodes and $D$ is the diameter of the graph, which\nmatches the theoretical lower bound $\\Omega(\\sqrt{|S|T})$ up to logarithmic\nfactors. To our knowledge, this result is among the first tight regret bounds\nin non-episodic, un-discounted learning problems with known deterministic\ntransitions. Numerical experiments confirm that our algorithm outperforms\nseveral benchmarks.\n","authors":["Tianpeng Zhang","Kasper Johansson","Na Li"],"pdf_url":"https://arxiv.org/pdf/2209.09419v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11272v1","updated":"2023-03-20T17:04:59Z","published":"2023-03-20T17:04:59Z","title":"Agent-based Simulation for Online Mental Health Matching","summary":"  Online mental health communities (OMHCs) are an effective and accessible\nchannel to give and receive social support for individuals with mental and\nemotional issues. However, a key challenge on these platforms is finding\nsuitable partners to interact with given that mechanisms to match users are\ncurrently underdeveloped. In this paper, we collaborate with one of the world's\nlargest OMHC to develop an agent-based simulation framework and explore the\ntrade-offs in different matching algorithms. The simulation framework allows us\nto compare current mechanisms and new algorithmic matching policies on the\nplatform, and observe their differing effects on a variety of outcome metrics.\nOur findings include that usage of the deferred-acceptance algorithm can\nsignificantly better the experiences of support-seekers in one-on-one chats\nwhile maintaining low waiting time. We note key design considerations that\nagent-based modeling reveals in the OMHC context, including the potential\nbenefits of algorithmic matching on marginalized communities.\n","authors":["Yuhan Liu","Anna Fang","Glen Moriarty","Robert Kraut","Haiyi Zhu"],"pdf_url":"https://arxiv.org/pdf/2303.11272v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11265v1","updated":"2023-03-20T16:49:40Z","published":"2023-03-20T16:49:40Z","title":"Convergence Guarantees of Overparametrized Wide Deep Inverse Prior","summary":"  Neural networks have become a prominent approach to solve inverse problems in\nrecent years. Amongst the different existing methods, the Deep Image/Inverse\nPriors (DIPs) technique is an unsupervised approach that optimizes a highly\noverparametrized neural network to transform a random input into an object\nwhose image under the forward model matches the observation. However, the level\nof overparametrization necessary for such methods remains an open problem. In\nthis work, we aim to investigate this question for a two-layers neural network\nwith a smooth activation function. We provide overparametrization bounds under\nwhich such network trained via continuous-time gradient descent will converge\nexponentially fast with high probability which allows to derive recovery\nprediction bounds. This work is thus a first step towards a theoretical\nunderstanding of overparametrized DIP networks, and more broadly it\nparticipates to the theoretical understanding of neural networks in inverse\nproblem settings.\n","authors":["Nathan Buskulic","Yvain Quéau","Jalal Fadili"],"pdf_url":"https://arxiv.org/pdf/2303.11265v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11257v1","updated":"2023-03-20T16:42:25Z","published":"2023-03-20T16:42:25Z","title":"Unit Scaling: Out-of-the-Box Low-Precision Training","summary":"  We present unit scaling, a paradigm for designing deep learning models that\nsimplifies the use of low-precision number formats. Training in FP16 or the\nrecently proposed FP8 formats offers substantial efficiency gains, but can lack\nsufficient range for out-of-the-box training. Unit scaling addresses this by\nintroducing a principled approach to model numerics: seeking unit variance of\nall weights, activations and gradients at initialisation. Unlike alternative\nmethods, this approach neither requires multiple training runs to find a\nsuitable scale nor has significant computational overhead. We demonstrate the\nefficacy of unit scaling across a range of models and optimisers. We further\nshow that existing models can be adapted to be unit-scaled, training BERT-Large\nin FP16 and then FP8 with no degradation in accuracy.\n","authors":["Charlie Blake","Douglas Orr","Carlo Luschi"],"pdf_url":"https://arxiv.org/pdf/2303.11257v1.pdf","comment":"29 pages, 11 figures"},{"id":"http://arxiv.org/abs/2303.11249v1","updated":"2023-03-20T16:34:39Z","published":"2023-03-20T16:34:39Z","title":"What Makes Data Suitable for a Locally Connected Neural Network? A\n  Necessary and Sufficient Condition Based on Quantum Entanglement","summary":"  The question of what makes a data distribution suitable for deep learning is\na fundamental open problem. Focusing on locally connected neural networks (a\nprevalent family of architectures that includes convolutional and recurrent\nneural networks as well as local self-attention models), we address this\nproblem by adopting theoretical tools from quantum physics. Our main\ntheoretical result states that a certain locally connected neural network is\ncapable of accurate prediction over a data distribution if and only if the data\ndistribution admits low quantum entanglement under certain canonical partitions\nof features. As a practical application of this result, we derive a\npreprocessing method for enhancing the suitability of a data distribution to\nlocally connected neural networks. Experiments with widespread models over\nvarious datasets demonstrate our findings. We hope that our use of quantum\nentanglement will encourage further adoption of tools from physics for formally\nreasoning about the relation between deep learning and real-world data.\n","authors":["Yotam Alexander","Nimrod De La Vega","Noam Razin","Nadav Cohen"],"pdf_url":"https://arxiv.org/pdf/2303.11249v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11247v1","updated":"2023-03-20T16:33:17Z","published":"2023-03-20T16:33:17Z","title":"Memorization Capacity of Neural Networks with Conditional Computation","summary":"  Many empirical studies have demonstrated the performance benefits of\nconditional computation in neural networks, including reduced inference time\nand power consumption. We study the fundamental limits of neural conditional\ncomputation from the perspective of memorization capacity. For Rectified Linear\nUnit (ReLU) networks without conditional computation, it is known that\nmemorizing a collection of $n$ input-output relationships can be accomplished\nvia a neural network with $O(\\sqrt{n})$ neurons. Calculating the output of this\nneural network can be accomplished using $O(\\sqrt{n})$ elementary arithmetic\noperations of additions, multiplications and comparisons for each input. Using\na conditional ReLU network, we show that the same task can be accomplished\nusing only $O(\\log n)$ operations per input. This represents an almost\nexponential improvement as compared to networks without conditional\ncomputation. We also show that the $\\Theta(\\log n)$ rate is the best possible.\nOur achievability result utilizes a general methodology to synthesize a\nconditional network out of an unconditional network in a\ncomputationally-efficient manner, bridging the gap between unconditional and\nconditional architectures.\n","authors":["Erdem Koyuncu"],"pdf_url":"https://arxiv.org/pdf/2303.11247v1.pdf","comment":"To be presented at International Conference on Learning\n  Representations (ICLR), 2023"},{"id":"http://arxiv.org/abs/2303.11242v1","updated":"2023-03-20T16:27:36Z","published":"2023-03-20T16:27:36Z","title":"Make Landscape Flatter in Differentially Private Federated Learning","summary":"  To defend the inference attacks and mitigate the sensitive information\nleakages in Federated Learning (FL), client-level Differentially Private FL\n(DPFL) is the de-facto standard for privacy protection by clipping local\nupdates and adding random noise. However, existing DPFL methods tend to make a\nsharper loss landscape and have poorer weight perturbation robustness,\nresulting in severe performance degradation. To alleviate these issues, we\npropose a novel DPFL algorithm named DP-FedSAM, which leverages gradient\nperturbation to mitigate the negative impact of DP. Specifically, DP-FedSAM\nintegrates Sharpness Aware Minimization (SAM) optimizer to generate local\nflatness models with better stability and weight perturbation robustness, which\nresults in the small norm of local updates and robustness to DP noise, thereby\nimproving the performance. From the theoretical perspective, we analyze in\ndetail how DP-FedSAM mitigates the performance degradation induced by DP.\nMeanwhile, we give rigorous privacy guarantees with R\\'enyi DP and present the\nsensitivity analysis of local updates. At last, we empirically confirm that our\nalgorithm achieves state-of-the-art (SOTA) performance compared with existing\nSOTA baselines in DPFL.\n","authors":["Yifan Shi","Yingqi Liu","Kang Wei","Li Shen","Xueqian Wang","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2303.11242v1.pdf","comment":"CVPR2023, 18 pages"},{"id":"http://arxiv.org/abs/2303.11239v1","updated":"2023-03-20T16:24:06Z","published":"2023-03-20T16:24:06Z","title":"Training Invertible Neural Networks as Autoencoders","summary":"  Autoencoders are able to learn useful data representations in an unsupervised\nmatter and have been widely used in various machine learning and computer\nvision tasks. In this work, we present methods to train Invertible Neural\nNetworks (INNs) as (variational) autoencoders which we call INN (variational)\nautoencoders. Our experiments on MNIST, CIFAR and CelebA show that for low\nbottleneck sizes our INN autoencoder achieves results similar to the classical\nautoencoder. However, for large bottleneck sizes our INN autoencoder\noutperforms its classical counterpart. Based on the empirical results, we\nhypothesize that INN autoencoders might not have any intrinsic information loss\nand thereby are not bounded to a maximal number of layers (depth) after which\nonly suboptimal results can be achieved.\n","authors":["The-Gia Leo Nguyen","Lynton Ardizzone","Ullrich Koethe"],"pdf_url":"https://arxiv.org/pdf/2303.11239v1.pdf","comment":"Conference Paper at GCPR2019"},{"id":"http://arxiv.org/abs/2110.02273v2","updated":"2023-03-20T16:18:27Z","published":"2021-10-05T18:26:22Z","title":"Bilevel Imaging Learning Problems as Mathematical Programs with\n  Complementarity Constraints: Reformulation and Theory","summary":"  We investigate a family of bilevel imaging learning problems where the\nlower-level instance corresponds to a convex variational model involving first-\nand second-order nonsmooth sparsity-based regularizers. By using geometric\nproperties of the primal-dual reformulation of the lower-level problem and\nintroducing suitable auxiliar variables, we are able to reformulate the\noriginal bilevel problems as Mathematical Programs with Complementarity\nConstraints (MPCC). For the latter, we prove tight constraint qualification\nconditions (MPCC-RCPLD and partial MPCC-LICQ) and derive Mordukhovich (M-) and\nStrong (S-) stationarity conditions. The stationarity systems for the MPCC turn\nalso into stationarity conditions for the original formulation. Second-order\nsufficient optimality conditions are derived as well, together with a local\nuniqueness result for stationary points. The proposed reformulation may be\nextended to problems in function spaces, leading to MPCC's with constraints on\nthe gradient of the state. The MPCC reformulation also leads to the efficient\nuse of available large-scale nonlinear programming solvers, as shown in a\ncompanion paper, where different imaging applications are studied.\n","authors":["Juan Carlos De los Reyes"],"pdf_url":"https://arxiv.org/pdf/2110.02273v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.02642v2","updated":"2023-03-20T16:08:30Z","published":"2022-11-01T14:12:58Z","title":"A Meta-GNN approach to personalized seizure detection and classification","summary":"  In this paper, we propose a personalized seizure detection and classification\nframework that quickly adapts to a specific patient from limited seizure\nsamples. We achieve this by combining two novel paradigms that have recently\nseen much success in a wide variety of real-world applications: graph neural\nnetworks (GNN), and meta-learning. We train a Meta-GNN based classifier that\nlearns a global model from a set of training patients such that this global\nmodel can eventually be adapted to a new unseen patient using very limited\nsamples. We apply our approach on the TUSZ-dataset, one of the largest and\npublicly available benchmark datasets for epilepsy. We show that our method\noutperforms the baselines by reaching 82.7% on accuracy and 82.08% on F1 score\nafter only 20 iterations on new unseen patients.\n","authors":["Abdellah Rahmani","Arun Venkitaraman","Pascal Frossard"],"pdf_url":"https://arxiv.org/pdf/2211.02642v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11224v1","updated":"2023-03-20T16:00:20Z","published":"2023-03-20T16:00:20Z","title":"Cascaded Latent Diffusion Models for High-Resolution Chest X-ray\n  Synthesis","summary":"  While recent advances in large-scale foundational models show promising\nresults, their application to the medical domain has not yet been explored in\ndetail. In this paper, we progress into the realms of large-scale modeling in\nmedical synthesis by proposing Cheff - a foundational cascaded latent diffusion\nmodel, which generates highly-realistic chest radiographs providing\nstate-of-the-art quality on a 1-megapixel scale. We further propose MaCheX,\nwhich is a unified interface for public chest datasets and forms the largest\nopen collection of chest X-rays up to date. With Cheff conditioned on\nradiological reports, we further guide the synthesis process over text prompts\nand unveil the research area of report-to-chest-X-ray generation.\n","authors":["Tobias Weber","Michael Ingrisch","Bernd Bischl","David Rügamer"],"pdf_url":"https://arxiv.org/pdf/2303.11224v1.pdf","comment":"accepted at PAKDD 2023"},{"id":"http://arxiv.org/abs/2302.07419v3","updated":"2023-03-20T16:00:11Z","published":"2023-02-15T01:09:03Z","title":"Spatially heterogeneous learning by a deep student machine","summary":"  Despite the spectacular successes, deep neural networks (DNN) with a huge\nnumber of adjustable parameters remain largely black boxes. To shed light on\nthe hidden layers of DNN, we study supervised learning by a DNN of width $N$\nand depth $L$ consisting of perceptrons with $c$ inputs by a statistical\nmechanics approach called the teacher-student setting. We consider an ensemble\nof student machines that exactly reproduce $M$ sets of $N$ dimensional\ninput/output relations provided by a teacher machine. We analyze the ensemble\ntheoretically using a replica method (H. Yoshino (2020)) and numerically\nperforming greedy Monte Carlo simulations. The replica theory which works on\nhigh dimensional data $N \\gg 1$ becomes exact in 'dense limit' $N \\gg c \\gg 1$\nand $M \\gg 1$ with fixed $\\alpha=M/c$. Both the theory and the simulation\nsuggest learning by the DNN is quite heterogeneous in the network space:\nconfigurations of the machines are more correlated within the layers closer to\nthe input/output boundaries while the central region remains much less\ncorrelated due to over-parametrization. Deep enough systems relax faster thanks\nto the less correlated central region. Remarkably both the theory and\nsimulation suggest generalization-ability of the student machines does not\nvanish even in the deep limit $L \\gg 1$ where the system becomes strongly\nover-parametrized. We also consider the impact of effective dimension $D(\\leq\nN)$ of data by incorporating the hidden manifold model (S. Goldt et al (2020))\ninto our model. The replica theory implies that the loop corrections to the\ndense limit, which reflect correlations between different nodes in the network,\nbecome enhanced by either decreasing the width $\\ N$ or decreasing the\neffective dimension $D$ of the data. Simulation suggests both leads to\nsignificant improvements in generalization-ability.\n","authors":["Hajime Yoshino"],"pdf_url":"https://arxiv.org/pdf/2302.07419v3.pdf","comment":"34 page, 18 figures (revised version with normalized squared\n  overlaps)"},{"id":"http://arxiv.org/abs/2303.11217v1","updated":"2023-03-20T15:49:18Z","published":"2023-03-20T15:49:18Z","title":"Inverse problem regularization with hierarchical variational\n  autoencoders","summary":"  In this paper, we propose to regularize ill-posed inverse problems using a\ndeep hierarchical variational autoencoder (HVAE) as an image prior. The\nproposed method synthesizes the advantages of i) denoiser-based Plug \\& Play\napproaches and ii) generative model based approaches to inverse problems.\nFirst, we exploit VAE properties to design an efficient algorithm that benefits\nfrom convergence guarantees of Plug-and-Play (PnP) methods. Second, our\napproach is not restricted to specialized datasets and the proposed PnP-HVAE\nmodel is able to solve image restoration problems on natural images of any\nsize. Our experiments show that the proposed PnP-HVAE method is competitive\nwith both SOTA denoiser-based PnP approaches, and other SOTA restoration\nmethods based on generative models.\n","authors":["Jean Prost","Antoine Houdard","Andrés Almansa","Nicolas Papadakis"],"pdf_url":"https://arxiv.org/pdf/2303.11217v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2108.00413v4","updated":"2023-03-20T15:48:12Z","published":"2021-08-01T09:42:29Z","title":"Data-Driven Constitutive Relation Reveals Scaling Law for Hydrodynamic\n  Transport Coefficients","summary":"  Finding extended hydrodynamics equations valid from the dense gas region to\nthe rarefied gas region remains a great challenge. The key to success is to\nobtain accurate constitutive relations for stress and heat flux. Data-driven\nmodels offer a new phenomenological approach to learning constitutive relations\nfrom data. Such models enable complex constitutive relations that extend\nNewton's law of viscosity and Fourier's law of heat conduction by regression on\nhigher derivatives. However, the choices of derivatives in these models are\nad-hoc without a clear physical explanation. We investigated data-driven models\ntheoretically on a linear system. We argue that these models are equivalent to\nnon-linear length scale scaling laws of transport coefficients. The equivalence\nto scaling laws justified the physical plausibility and revealed the limitation\nof data-driven models. Our argument also points out that modeling the scaling\nlaw could avoid practical difficulties in data-driven models like derivative\nestimation and variable selection on noisy data. We further proposed a\nconstitutive relation model based on scaling law and tested it on the\ncalculation of Rayleigh scattering spectra. The result shows our data-driven\nmodel has a clear advantage over the Chapman-Enskog expansion and moment\nmethods.\n","authors":["Candi Zheng","Yang Wang","Shiyi Chen"],"pdf_url":"https://arxiv.org/pdf/2108.00413v4.pdf","comment":"10 pages, 4 figures"},{"id":"http://arxiv.org/abs/2303.11215v1","updated":"2023-03-20T15:47:05Z","published":"2023-03-20T15:47:05Z","title":"Learning to Generate 3D Representations of Building Roofs Using\n  Single-View Aerial Imagery","summary":"  We present a novel pipeline for learning the conditional distribution of a\nbuilding roof mesh given pixels from an aerial image, under the assumption that\nroof geometry follows a set of regular patterns. Unlike alternative methods\nthat require multiple images of the same object, our approach enables\nestimating 3D roof meshes using only a single image for predictions. The\napproach employs the PolyGen, a deep generative transformer architecture for 3D\nmeshes. We apply this model in a new domain and investigate the sensitivity of\nthe image resolution. We propose a novel metric to evaluate the performance of\nthe inferred meshes, and our results show that the model is robust even at\nlower resolutions, while qualitatively producing realistic representations for\nout-of-distribution samples.\n","authors":["Maxim Khomiakov","Alejandro Valverde Mahou","Alba Reinders Sánchez","Jes Frellsen","Michael Riis Andersen"],"pdf_url":"https://arxiv.org/pdf/2303.11215v1.pdf","comment":"Copyright 2023 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works"},{"id":"http://arxiv.org/abs/2303.11207v1","updated":"2023-03-20T15:40:28Z","published":"2023-03-20T15:40:28Z","title":"Investigating Topological Order using Recurrent Neural Networks","summary":"  Recurrent neural networks (RNNs), originally developed for natural language\nprocessing, hold great promise for accurately describing strongly correlated\nquantum many-body systems. Here, we employ 2D RNNs to investigate two\nprototypical quantum many-body Hamiltonians exhibiting topological order.\nSpecifically, we demonstrate that RNN wave functions can effectively capture\nthe topological order of the toric code and a Bose-Hubbard spin liquid on the\nkagome lattice by estimating their topological entanglement entropies. We also\nfind that RNNs favor coherent superpositions of minimally-entangled states over\nminimally-entangled states themselves. Overall, our findings demonstrate that\nRNN wave functions constitute a powerful tool to study phases of matter beyond\nLandau's symmetry-breaking paradigm.\n","authors":["Mohamed Hibat-Allah","Roger G. Melko","Juan Carrasquilla"],"pdf_url":"https://arxiv.org/pdf/2303.11207v1.pdf","comment":"14 pages, 7 figures, 1 table"},{"id":"http://arxiv.org/abs/2302.10681v2","updated":"2023-03-20T15:40:26Z","published":"2023-02-21T14:03:22Z","title":"FrankenSplit: Saliency Guided Neural Feature Compression with Shallow\n  Variational Bottleneck Injection","summary":"  The rise of mobile AI accelerators allows latency-sensitive applications to\nexecute lightweight Deep Neural Networks (DNNs) on the client side. However,\ncritical applications require powerful models that edge devices cannot host and\nmust therefore offload requests, where the high-dimensional data will compete\nfor limited bandwidth. This work proposes shifting away from focusing on\nexecuting shallow layers of partitioned DNNs. Instead, it advocates\nconcentrating the local resources on variational compression optimized for\nmachine interpretability. We introduce a novel framework for resource-conscious\ncompression models and extensively evaluate our method in an environment\nreflecting the asymmetric resource distribution between edge devices and\nservers. Our method achieves 60\\% lower bitrate than a state-of-the-art SC\nmethod without decreasing accuracy and is up to 16x faster than offloading with\nexisting codec standards.\n","authors":["Alireza Furutanpey","Philipp Raith","Schahram Dustdar"],"pdf_url":"https://arxiv.org/pdf/2302.10681v2.pdf","comment":"14 pages, 8 figures"},{"id":"http://arxiv.org/abs/2303.01498v3","updated":"2023-03-20T15:25:09Z","published":"2023-03-02T18:58:15Z","title":"ABAW: Valence-Arousal Estimation, Expression Recognition, Action Unit\n  Detection & Emotional Reaction Intensity Estimation Challenges","summary":"  The fifth Affective Behavior Analysis in-the-wild (ABAW) Competition is part\nof the respective ABAW Workshop which will be held in conjunction with IEEE\nComputer Vision and Pattern Recognition Conference (CVPR), 2023. The 5th ABAW\nCompetition is a continuation of the Competitions held at ECCV 2022, IEEE CVPR\n2022, ICCV 2021, IEEE FG 2020 and CVPR 2017 Conferences, and is dedicated at\nautomatically analyzing affect. For this year's Competition, we feature two\ncorpora: i) an extended version of the Aff-Wild2 database and ii) the\nHume-Reaction dataset. The former database is an audiovisual one of around 600\nvideos of around 3M frames and is annotated with respect to:a) two continuous\naffect dimensions -valence (how positive/negative a person is) and arousal (how\nactive/passive a person is)-; b) basic expressions (e.g. happiness, sadness,\nneutral state); and c) atomic facial muscle actions (i.e., action units). The\nlatter dataset is an audiovisual one in which reactions of individuals to\nemotional stimuli have been annotated with respect to seven emotional\nexpression intensities. Thus the 5th ABAW Competition encompasses four\nChallenges: i) uni-task Valence-Arousal Estimation, ii) uni-task Expression\nClassification, iii) uni-task Action Unit Detection, and iv) Emotional Reaction\nIntensity Estimation. In this paper, we present these Challenges, along with\ntheir corpora, we outline the evaluation metrics, we present the baseline\nsystems and illustrate their obtained performance.\n","authors":["Dimitrios Kollias","Panagiotis Tzirakis","Alice Baird","Alan Cowen","Stefanos Zafeiriou"],"pdf_url":"https://arxiv.org/pdf/2303.01498v3.pdf","comment":"arXiv admin note: text overlap with arXiv:2202.10659"},{"id":"http://arxiv.org/abs/2105.11977v3","updated":"2023-03-20T15:24:01Z","published":"2021-05-25T14:28:58Z","title":"Towards Teachable Autotelic Agents","summary":"  Autonomous discovery and direct instruction are two distinct sources of\nlearning in children but education sciences demonstrate that mixed approaches\nsuch as assisted discovery or guided play result in improved skill acquisition.\nIn the field of Artificial Intelligence, these extremes respectively map to\nautonomous agents learning from their own signals and interactive learning\nagents fully taught by their teachers. In between should stand teachable\nautotelic agents (TAA): agents that learn from both internal and teaching\nsignals to benefit from the higher efficiency of assisted discovery. Designing\nsuch agents will enable real-world non-expert users to orient the learning\ntrajectories of agents towards their expectations. More fundamentally, this may\nalso be a key step to build agents with human-level intelligence. This paper\npresents a roadmap towards the design of teachable autonomous agents. Building\non developmental psychology and education sciences, we start by identifying key\nfeatures enabling assisted discovery processes in child-tutor interactions.\nThis leads to the production of a checklist of features that future TAA will\nneed to demonstrate. The checklist allows us to precisely pinpoint the various\nlimitations of current reinforcement learning agents and to identify the\npromising first steps towards TAA. It also shows the way forward by\nhighlighting key research directions towards the design or autonomous agents\nthat can be taught by ordinary people via natural pedagogy.\n","authors":["Olivier Sigaud","Ahmed Akakzia","Hugo Caselles-Dupré","Cédric Colas","Pierre-Yves Oudeyer","Mohamed Chetouani"],"pdf_url":"https://arxiv.org/pdf/2105.11977v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.12053v3","updated":"2023-03-20T15:22:59Z","published":"2022-12-22T22:05:16Z","title":"On Calibrating Semantic Segmentation Models: Analyses and An Algorithm","summary":"  We study the problem of semantic segmentation calibration. Lots of solutions\nhave been proposed to approach model miscalibration of confidence in image\nclassification. However, to date, confidence calibration research on semantic\nsegmentation is still limited. We provide a systematic study on the calibration\nof semantic segmentation models and propose a simple yet effective approach.\nFirst, we find that model capacity, crop size, multi-scale testing, and\nprediction correctness have impact on calibration. Among them, prediction\ncorrectness, especially misprediction, is more important to miscalibration due\nto over-confidence. Next, we propose a simple, unifying, and effective\napproach, namely selective scaling, by separating correct/incorrect prediction\nfor scaling and more focusing on misprediction logit smoothing. Then, we study\npopular existing calibration methods and compare them with selective scaling on\nsemantic segmentation calibration. We conduct extensive experiments with a\nvariety of benchmarks on both in-domain and domain-shift calibration, and show\nthat selective scaling consistently outperforms other methods.\n","authors":["Dongdong Wang","Boqing Gong","Liqiang Wang"],"pdf_url":"https://arxiv.org/pdf/2212.12053v3.pdf","comment":"Accepted to CVPR2023 (8 pages, 4 figures)"},{"id":"http://arxiv.org/abs/2303.11191v1","updated":"2023-03-20T15:22:10Z","published":"2023-03-20T15:22:10Z","title":"A Survey of Demonstration Learning","summary":"  With the fast improvement of machine learning, reinforcement learning (RL)\nhas been used to automate human tasks in different areas. However, training\nsuch agents is difficult and restricted to expert users. Moreover, it is mostly\nlimited to simulation environments due to the high cost and safety concerns of\ninteractions in the real world. Demonstration Learning is a paradigm in which\nan agent learns to perform a task by imitating the behavior of an expert shown\nin demonstrations. It is a relatively recent area in machine learning, but it\nis gaining significant traction due to having tremendous potential for learning\ncomplex behaviors from demonstrations. Learning from demonstration accelerates\nthe learning process by improving sample efficiency, while also reducing the\neffort of the programmer. Due to learning without interacting with the\nenvironment, demonstration learning would allow the automation of a wide range\nof real world applications such as robotics and healthcare. This paper provides\na survey of demonstration learning, where we formally introduce the\ndemonstration problem along with its main challenges and provide a\ncomprehensive overview of the process of learning from demonstrations from the\ncreation of the demonstration data set, to learning methods from\ndemonstrations, and optimization by combining demonstration learning with\ndifferent machine learning methods. We also review the existing benchmarks and\nidentify their strengths and limitations. Additionally, we discuss the\nadvantages and disadvantages of the paradigm as well as its main applications.\nLastly, we discuss our perspective on open problems and research directions for\nthis rapidly growing field.\n","authors":["André Correia","Luís A. Alexandre"],"pdf_url":"https://arxiv.org/pdf/2303.11191v1.pdf","comment":"35 pages, 9 figures"},{"id":"http://arxiv.org/abs/2303.11187v1","updated":"2023-03-20T15:17:31Z","published":"2023-03-20T15:17:31Z","title":"A Unified Framework of Policy Learning for Contextual Bandit with\n  Confounding Bias and Missing Observations","summary":"  We study the offline contextual bandit problem, where we aim to acquire an\noptimal policy using observational data. However, this data usually contains\ntwo deficiencies: (i) some variables that confound actions are not observed,\nand (ii) missing observations exist in the collected data. Unobserved\nconfounders lead to a confounding bias and missing observations cause bias and\ninefficiency problems. To overcome these challenges and learn the optimal\npolicy from the observed dataset, we present a new algorithm called\nCausal-Adjusted Pessimistic (CAP) policy learning, which forms the reward\nfunction as the solution of an integral equation system, builds a confidence\nset, and greedily takes action with pessimism. With mild assumptions on the\ndata, we develop an upper bound to the suboptimality of CAP for the offline\ncontextual bandit problem.\n","authors":["Siyu Chen","Yitan Wang","Zhaoran Wang","Zhuoran Yang"],"pdf_url":"https://arxiv.org/pdf/2303.11187v1.pdf","comment":"76 page, 5 figures"},{"id":"http://arxiv.org/abs/2303.11183v1","updated":"2023-03-20T15:10:41Z","published":"2023-03-20T15:10:41Z","title":"Architecture, Dataset and Model-Scale Agnostic Data-free Meta-Learning","summary":"  The goal of data-free meta-learning is to learn useful prior knowledge from a\ncollection of pre-trained models without accessing their training data.\nHowever, existing works only solve the problem in parameter space, which (i)\nignore the fruitful data knowledge contained in the pre-trained models; (ii)\ncan not scale to large-scale pre-trained models; (iii) can only meta-learn\npre-trained models with the same network architecture. To address those issues,\nwe propose a unified framework, dubbed PURER, which contains: (1) ePisode\ncUrriculum inveRsion (ECI) during data-free meta training; and (2) invErsion\ncalibRation following inner loop (ICFIL) during meta testing. During meta\ntraining, we propose ECI to perform pseudo episode training for learning to\nadapt fast to new unseen tasks. Specifically, we progressively synthesize a\nsequence of pseudo episodes by distilling the training data from each\npre-trained model. The ECI adaptively increases the difficulty level of pseudo\nepisodes according to the real-time feedback of the meta model. We formulate\nthe optimization process of meta training with ECI as an adversarial form in an\nend-to-end manner. During meta testing, we further propose a simple\nplug-and-play supplement-ICFIL-only used during meta testing to narrow the gap\nbetween meta training and meta testing task distribution. Extensive experiments\nin various real-world scenarios show the superior performance of ours.\n","authors":["Zixuan Hu","Li Shen","Zhenyi Wang","Tongliang Liu","Chun Yuan","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2303.11183v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.11771v2","updated":"2023-03-20T15:05:48Z","published":"2022-12-22T15:06:24Z","title":"Few-shot human motion prediction for heterogeneous sensors","summary":"  Human motion prediction is a complex task as it involves forecasting\nvariables over time on a graph of connected sensors. This is especially true in\nthe case of few-shot learning, where we strive to forecast motion sequences for\npreviously unseen actions based on only a few examples. Despite this, almost\nall related approaches for few-shot motion prediction do not incorporate the\nunderlying graph, while it is a common component in classical motion\nprediction. Furthermore, state-of-the-art methods for few-shot motion\nprediction are restricted to motion tasks with a fixed output space meaning\nthese tasks are all limited to the same sensor graph. In this work, we propose\nto extend recent works on few-shot time-series forecasting with heterogeneous\nattributes with graph neural networks to introduce the first few-shot motion\napproach that explicitly incorporates the spatial graph while also generalizing\nacross motion tasks with heterogeneous sensors. In our experiments on motion\ntasks with heterogeneous sensors, we demonstrate significant performance\nimprovements with lifts from 10.4% up to 39.3% compared to best\nstate-of-the-art models. Moreover, we show that our model can perform on par\nwith the best approach so far when evaluating on tasks with a fixed output\nspace while maintaining two magnitudes fewer parameters.\n","authors":["Rafael Rego Drumond","Lukas Brinkmeyer","Lars Schmidt-Thieme"],"pdf_url":"https://arxiv.org/pdf/2212.11771v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11177v1","updated":"2023-03-20T15:00:52Z","published":"2023-03-20T15:00:52Z","title":"Integration of Radiomics and Tumor Biomarkers in Interpretable Machine\n  Learning Models","summary":"  Despite the unprecedented performance of deep neural networks (DNNs) in\ncomputer vision, their practical application in the diagnosis and prognosis of\ncancer using medical imaging has been limited. One of the critical challenges\nfor integrating diagnostic DNNs into radiological and oncological applications\nis their lack of interpretability, preventing clinicians from understanding the\nmodel predictions. Therefore, we study and propose the integration of\nexpert-derived radiomics and DNN-predicted biomarkers in interpretable\nclassifiers which we call ConRad, for computerized tomography (CT) scans of\nlung cancer. Importantly, the tumor biomarkers are predicted from a concept\nbottleneck model (CBM) such that once trained, our ConRad models do not require\nlabor-intensive and time-consuming biomarkers. In our evaluation and practical\napplication, the only input to ConRad is a segmented CT scan. The proposed\nmodel is compared to convolutional neural networks (CNNs) which act as a black\nbox classifier. We further investigated and evaluated all combinations of\nradiomics, predicted biomarkers and CNN features in five different classifiers.\nWe found the ConRad models using non-linear SVM and the logistic regression\nwith the Lasso outperform others in five-fold cross-validation, although we\nhighlight that interpretability of ConRad is its primary advantage. The Lasso\nis used for feature selection, which substantially reduces the number of\nnon-zero weights while increasing the accuracy. Overall, the proposed ConRad\nmodel combines CBM-derived biomarkers and radiomics features in an\ninterpretable ML model which perform excellently for the lung nodule malignancy\nclassification.\n","authors":["Lennart Brocki","Neo Christopher Chung"],"pdf_url":"https://arxiv.org/pdf/2303.11177v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.03104v3","updated":"2023-03-20T14:51:38Z","published":"2022-02-07T12:33:14Z","title":"SimGRACE: A Simple Framework for Graph Contrastive Learning without Data\n  Augmentation","summary":"  Graph contrastive learning (GCL) has emerged as a dominant technique for\ngraph representation learning which maximizes the mutual information between\npaired graph augmentations that share the same semantics. Unfortunately, it is\ndifficult to preserve semantics well during augmentations in view of the\ndiverse nature of graph data. Currently, data augmentations in GCL that are\ndesigned to preserve semantics broadly fall into three unsatisfactory ways.\nFirst, the augmentations can be manually picked per dataset by\ntrial-and-errors. Second, the augmentations can be selected via cumbersome\nsearch. Third, the augmentations can be obtained by introducing expensive\ndomain-specific knowledge as guidance. All of these limit the efficiency and\nmore general applicability of existing GCL methods. To circumvent these crucial\nissues, we propose a \\underline{Sim}ple framework for \\underline{GRA}ph\n\\underline{C}ontrastive l\\underline{E}arning, \\textbf{SimGRACE} for brevity,\nwhich does not require data augmentations. Specifically, we take original graph\nas input and GNN model with its perturbed version as two encoders to obtain two\ncorrelated views for contrast. SimGRACE is inspired by the observation that\ngraph data can preserve their semantics well during encoder perturbations while\nnot requiring manual trial-and-errors, cumbersome search or expensive domain\nknowledge for augmentations selection. Also, we explain why SimGRACE can\nsucceed. Furthermore, we devise adversarial training scheme, dubbed\n\\textbf{AT-SimGRACE}, to enhance the robustness of graph contrastive learning\nand theoretically explain the reasons. Albeit simple, we show that SimGRACE can\nyield competitive or better performance compared with state-of-the-art methods\nin terms of generalizability, transferability and robustness, while enjoying\nunprecedented degree of flexibility and efficiency.\n","authors":["Jun Xia","Lirong Wu","Jintao Chen","Bozhen Hu","Stan Z. Li"],"pdf_url":"https://arxiv.org/pdf/2202.03104v3.pdf","comment":"Accepted by The Web Conference 2022 (WWW 2022)"},{"id":"http://arxiv.org/abs/2303.11166v1","updated":"2023-03-20T14:51:10Z","published":"2023-03-20T14:51:10Z","title":"Imitating Graph-Based Planning with Goal-Conditioned Policies","summary":"  Recently, graph-based planning algorithms have gained much attention to solve\ngoal-conditioned reinforcement learning (RL) tasks: they provide a sequence of\nsubgoals to reach the target-goal, and the agents learn to execute\nsubgoal-conditioned policies. However, the sample-efficiency of such RL schemes\nstill remains a challenge, particularly for long-horizon tasks. To address this\nissue, we present a simple yet effective self-imitation scheme which distills a\nsubgoal-conditioned policy into the target-goal-conditioned policy. Our\nintuition here is that to reach a target-goal, an agent should pass through a\nsubgoal, so target-goal- and subgoal- conditioned policies should be similar to\neach other. We also propose a novel scheme of stochastically skipping executed\nsubgoals in a planned path, which further improves performance. Unlike prior\nmethods that only utilize graph-based planning in an execution phase, our\nmethod transfers knowledge from a planner along with a graph into policy\nlearning. We empirically show that our method can significantly boost the\nsample-efficiency of the existing goal-conditioned RL methods under various\nlong-horizon control tasks.\n","authors":["Junsu Kim","Younggyo Seo","Sungsoo Ahn","Kyunghwan Son","Jinwoo Shin"],"pdf_url":"https://arxiv.org/pdf/2303.11166v1.pdf","comment":"Accepted to ICLR 2023"},{"id":"http://arxiv.org/abs/2303.11165v1","updated":"2023-03-20T14:50:27Z","published":"2023-03-20T14:50:27Z","title":"Computationally Budgeted Continual Learning: What Does Matter?","summary":"  Continual Learning (CL) aims to sequentially train models on streams of\nincoming data that vary in distribution by preserving previous knowledge while\nadapting to new data. Current CL literature focuses on restricted access to\npreviously seen data, while imposing no constraints on the computational budget\nfor training. This is unreasonable for applications in-the-wild, where systems\nare primarily constrained by computational and time budgets, not storage. We\nrevisit this problem with a large-scale benchmark and analyze the performance\nof traditional CL approaches in a compute-constrained setting, where effective\nmemory samples used in training can be implicitly restricted as a consequence\nof limited computation. We conduct experiments evaluating various CL sampling\nstrategies, distillation losses, and partial fine-tuning on two large-scale\ndatasets, namely ImageNet2K and Continual Google Landmarks V2 in data\nincremental, class incremental, and time incremental settings. Through\nextensive experiments amounting to a total of over 1500 GPU-hours, we find\nthat, under compute-constrained setting, traditional CL approaches, with no\nexception, fail to outperform a simple minimal baseline that samples uniformly\nfrom memory. Our conclusions are consistent in a different number of stream\ntime steps, e.g., 20 to 200, and under several computational budgets. This\nsuggests that most existing CL methods are particularly too computationally\nexpensive for realistic budgeted deployment. Code for this project is available\nat: https://github.com/drimpossible/BudgetCL.\n","authors":["Ameya Prabhu","Hasan Abed Al Kader Hammoud","Puneet Dokania","Philip H. S. Torr","Ser-Nam Lim","Bernard Ghanem","Adel Bibi"],"pdf_url":"https://arxiv.org/pdf/2303.11165v1.pdf","comment":"Appearing in CVPR 2023"},{"id":"http://arxiv.org/abs/2303.11143v1","updated":"2023-03-20T14:22:04Z","published":"2023-03-20T14:22:04Z","title":"Adversarial Attacks against Binary Similarity Systems","summary":"  In recent years, binary analysis gained traction as a fundamental approach to\ninspect software and guarantee its security. Due to the exponential increase of\ndevices running software, much research is now moving towards new autonomous\nsolutions based on deep learning models, as they have been showing\nstate-of-the-art performances in solving binary analysis problems. One of the\nhot topics in this context is binary similarity, which consists in determining\nif two functions in assembly code are compiled from the same source code.\nHowever, it is unclear how deep learning models for binary similarity behave in\nan adversarial context. In this paper, we study the resilience of binary\nsimilarity models against adversarial examples, showing that they are\nsusceptible to both targeted and untargeted attacks (w.r.t. similarity goals)\nperformed by black-box and white-box attackers. In more detail, we extensively\ntest three current state-of-the-art solutions for binary similarity against two\nblack-box greedy attacks, including a new technique that we call Spatial\nGreedy, and one white-box attack in which we repurpose a gradient-guided\nstrategy used in attacks to image classifiers.\n","authors":["Gianluca Capozzi","Daniele Cono D'Elia","Giuseppe Antonio Di Luna","Leonardo Querzoni"],"pdf_url":"https://arxiv.org/pdf/2303.11143v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.16997v5","updated":"2023-03-20T14:18:13Z","published":"2022-10-31T00:53:17Z","title":"Convergence Rates of Stochastic Zeroth-order Gradient Descent for Ł\n  ojasiewicz Functions","summary":"  We prove convergence rates of Stochastic Zeroth-order Gradient Descent (SZGD)\nalgorithms for Lojasiewicz functions. The SZGD algorithm iterates as\n\\begin{align*}\n  \\mathbf{x}_{t+1} = \\mathbf{x}_t - \\eta_t \\widehat{\\nabla} f (\\mathbf{x}_t),\n\\qquad t = 0,1,2,3,\\cdots , \\end{align*} where $f$ is the objective function\nthat satisfies the \\L ojasiewicz inequality with \\L ojasiewicz exponent\n$\\theta$, $\\eta_t$ is the step size (learning rate), and $ \\widehat{\\nabla} f\n(\\mathbf{x}_t) $ is the approximate gradient estimated using zeroth-order\ninformation only.\n  Our results show that $ \\{ f (\\mathbf{x}_t) - f (\\mathbf{x}_\\infty) \\}_{t \\in\n\\mathbb{N} } $ can converge faster than $ \\{ \\| \\mathbf{x}_t -\n\\mathbf{x}_\\infty \\| \\}_{t \\in \\mathbb{N} }$, regardless of whether the\nobjective $f$ is smooth or nonsmooth.\n","authors":["Tianyu Wang","Yasong Feng"],"pdf_url":"https://arxiv.org/pdf/2210.16997v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11138v1","updated":"2023-03-20T14:15:52Z","published":"2023-03-20T14:15:52Z","title":"Fault Detection via Occupation Kernel Principal Component Analysis","summary":"  The reliable operation of automatic systems is heavily dependent on the\nability to detect faults in the underlying dynamical system. While traditional\nmodel-based methods have been widely used for fault detection, data-driven\napproaches have garnered increasing attention due to their ease of deployment\nand minimal need for expert knowledge. In this paper, we present a novel\nprincipal component analysis (PCA) method that uses occupation kernels.\nOccupation kernels result in feature maps that are tailored to the measured\ndata, have inherent noise-robustness due to the use of integration, and can\nutilize irregularly sampled system trajectories of variable lengths for PCA.\nThe occupation kernel PCA method is used to develop a reconstruction error\napproach to fault detection and its efficacy is validated using numerical\nsimulations.\n","authors":["Zachary Morrison","Benjamin P. Russo","Yingzhao Lian","Rushikesh Kamalapurkar"],"pdf_url":"https://arxiv.org/pdf/2303.11138v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11135v1","updated":"2023-03-20T14:12:55Z","published":"2023-03-20T14:12:55Z","title":"TWINS: A Fine-Tuning Framework for Improved Transferability of\n  Adversarial Robustness and Generalization","summary":"  Recent years have seen the ever-increasing importance of pre-trained models\nand their downstream training in deep learning research and applications. At\nthe same time, the defense for adversarial examples has been mainly\ninvestigated in the context of training from random initialization on simple\nclassification tasks. To better exploit the potential of pre-trained models in\nadversarial robustness, this paper focuses on the fine-tuning of an\nadversarially pre-trained model in various classification tasks. Existing\nresearch has shown that since the robust pre-trained model has already learned\na robust feature extractor, the crucial question is how to maintain the\nrobustness in the pre-trained model when learning the downstream task. We study\nthe model-based and data-based approaches for this goal and find that the two\ncommon approaches cannot achieve the objective of improving both generalization\nand adversarial robustness. Thus, we propose a novel statistics-based approach,\nTwo-WIng NormliSation (TWINS) fine-tuning framework, which consists of two\nneural networks where one of them keeps the population means and variances of\npre-training data in the batch normalization layers. Besides the robust\ninformation transfer, TWINS increases the effective learning rate without\nhurting the training stability since the relationship between a weight norm and\nits gradient norm in standard batch normalization layer is broken, resulting in\na faster escape from the sub-optimal initialization and alleviating the robust\noverfitting. Finally, TWINS is shown to be effective on a wide range of image\nclassification datasets in terms of both generalization and robustness. Our\ncode is available at https://github.com/ziquanliu/CVPR2023-TWINS.\n","authors":["Ziquan Liu","Yi Xu","Xiangyang Ji","Antoni B. Chan"],"pdf_url":"https://arxiv.org/pdf/2303.11135v1.pdf","comment":"CVPR2023"},{"id":"http://arxiv.org/abs/2303.11131v1","updated":"2023-03-20T14:07:13Z","published":"2023-03-20T14:07:13Z","title":"Cocktail HuBERT: Generalized Self-Supervised Pre-training for Mixture\n  and Single-Source Speech","summary":"  Self-supervised learning leverages unlabeled data effectively, improving\nlabel efficiency and generalization to domains without labeled data. While\nrecent work has studied generalization to more acoustic/linguistic domains,\nlanguages, and modalities, these investigations are limited to single-source\nspeech with one primary speaker in the recording. This paper presents Cocktail\nHuBERT, a self-supervised learning framework that generalizes to mixture speech\nusing a masked pseudo source separation objective. This objective encourages\nthe model to identify the number of sources, separate and understand the\ncontext, and infer the content of masked regions represented as discovered\nunits. Cocktail HuBERT outperforms state-of-the-art results with 69% lower WER\non multi-speaker ASR, 31% lower DER on diarization, and is competitive on\nsingle- and multi-speaker tasks from SUPERB.\n","authors":["Maryam Fazel-Zarandi","Wei-Ning Hsu"],"pdf_url":"https://arxiv.org/pdf/2303.11131v1.pdf","comment":"ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.11130v1","updated":"2023-03-20T14:06:32Z","published":"2023-03-20T14:06:32Z","title":"Deep learning automated quantification of lung disease in pulmonary\n  hypertension on CT pulmonary angiography: A preliminary clinical study with\n  external validation","summary":"  Purpose: Lung disease assessment in precapillary pulmonary hypertension (PH)\nis essential for appropriate patient management. This study aims to develop an\nartificial intelligence (AI) deep learning model for lung texture\nclassification in CT Pulmonary Angiography (CTPA), and evaluate its correlation\nwith clinical assessment methods.\n  Materials and Methods: In this retrospective study with external validation,\n122 patients with pre-capillary PH were used to train (n=83), validate (n=17)\nand test (n=10 internal test, n=12 external test) a patch based DenseNet-121\nclassification model. \"Normal\", \"Ground glass\", \"Ground glass with\nreticulation\", \"Honeycombing\", and \"Emphysema\" were classified as per the\nFleishner Society glossary of terms. Ground truth classes were segmented by two\nradiologists with patches extracted from the labelled regions. Proportion of\nlung volume for each texture was calculated by classifying patches throughout\nthe entire lung volume to generate a coarse texture classification mapping\nthroughout the lung parenchyma. AI output was assessed against diffusing\ncapacity of carbon monoxide (DLCO) and specialist radiologist reported disease\nseverity.\n  Results: Micro-average AUCs for the validation, internal test, and external\ntest were 0.92, 0.95, and 0.94, respectively. The model had consistent\nperformance across parenchymal textures, demonstrated strong correlation with\ndiffusing capacity of carbon monoxide (DLCO), and showed good correspondence\nwith disease severity reported by specialist radiologists.\n  Conclusion: The classification model demonstrates excellent performance on\nexternal validation. The clinical utility of its output has been demonstrated.\nThis objective, repeatable measure of disease severity can aid in patient\nmanagement in adjunct to radiological reporting.\n","authors":["Michael J. Sharkey","Krit Dwivedi","Samer Alabed","Andrew J. Swift"],"pdf_url":"https://arxiv.org/pdf/2303.11130v1.pdf","comment":"16 pages, 3 figures, 2 tables"},{"id":"http://arxiv.org/abs/2210.13690v3","updated":"2023-03-20T13:57:50Z","published":"2022-10-25T01:20:24Z","title":"Highly Efficient Real-Time Streaming and Fully On-Device Speaker\n  Diarization with Multi-Stage Clustering","summary":"  While recent research advances in speaker diarization mostly focus on\nimproving the quality of diarization results, there is also an increasing\ninterest in improving the efficiency of diarization systems. In this paper, we\ndemonstrate that a multi-stage clustering strategy that uses different\nclustering algorithms for input of different lengths can address multi-faceted\nchallenges of on-device speaker diarization applications. Specifically, a\nfallback clusterer is used to handle short-form inputs; a main clusterer is\nused to handle medium-length inputs; and a pre-clusterer is used to compress\nlong-form inputs before they are processed by the main clusterer. Both the main\nclusterer and the pre-clusterer can be configured with an upper bound of the\ncomputational complexity to adapt to devices with different resource\nconstraints. This multi-stage clustering strategy is critical for streaming\non-device speaker diarization systems, where the budgets of CPU, memory and\nbattery are tight.\n","authors":["Quan Wang","Yiling Huang","Han Lu","Guanlong Zhao","Ignacio Lopez Moreno"],"pdf_url":"https://arxiv.org/pdf/2210.13690v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.12812v2","updated":"2023-03-20T13:56:49Z","published":"2022-10-23T18:27:04Z","title":"Symmetric (Optimistic) Natural Policy Gradient for Multi-agent Learning\n  with Parameter Convergence","summary":"  Multi-agent interactions are increasingly important in the context of\nreinforcement learning, and the theoretical foundations of policy gradient\nmethods have attracted surging research interest. We investigate the global\nconvergence of natural policy gradient (NPG) algorithms in multi-agent\nlearning. We first show that vanilla NPG may not have parameter convergence,\ni.e., the convergence of the vector that parameterizes the policy, even when\nthe costs are regularized (which enabled strong convergence guarantees in the\npolicy space in the literature). This non-convergence of parameters leads to\nstability issues in learning, which becomes especially relevant in the function\napproximation setting, where we can only operate on low-dimensional parameters,\ninstead of the high-dimensional policy. We then propose variants of the NPG\nalgorithm, for several standard multi-agent learning scenarios: two-player\nzero-sum matrix and Markov games, and multi-player monotone games, with global\nlast-iterate parameter convergence guarantees. We also generalize the results\nto certain function approximation settings. Note that in our algorithms, the\nagents take symmetric roles. Our results might also be of independent interest\nfor solving nonconvex-nonconcave minimax optimization problems with certain\nstructures. Simulations are also provided to corroborate our theoretical\nfindings.\n","authors":["Sarath Pattathil","Kaiqing Zhang","Asuman Ozdaglar"],"pdf_url":"https://arxiv.org/pdf/2210.12812v2.pdf","comment":"Initially submitted for publication in January 2022"},{"id":"http://arxiv.org/abs/2203.11854v2","updated":"2023-03-20T13:51:38Z","published":"2022-03-22T16:31:44Z","title":"Sionna: An Open-Source Library for Next-Generation Physical Layer\n  Research","summary":"  Sionna is a GPU-accelerated open-source library for link-level simulations\nbased on TensorFlow. It enables the rapid prototyping of complex communication\nsystem architectures and provides native support for the integration of neural\nnetworks. Sionna implements a wide breadth of carefully tested state-of-the-art\nalgorithms that can be used for benchmarking and end-to-end performance\nevaluation. This allows researchers to focus on their research, making it more\nimpactful and reproducible, while saving time implementing components outside\ntheir area of expertise. This white paper provides a brief introduction to\nSionna, explains its design principles and features, as well as future\nextensions, such as integrated ray tracing and custom CUDA kernels. We believe\nthat Sionna is a valuable tool for research on next-generation communication\nsystems, such as 6G, and we welcome contributions from our community.\n","authors":["Jakob Hoydis","Sebastian Cammerer","Fayçal Ait Aoudia","Avinash Vem","Nikolaus Binder","Guillermo Marcus","Alexander Keller"],"pdf_url":"https://arxiv.org/pdf/2203.11854v2.pdf","comment":"5 pages, 1 figure, 4 code listings"},{"id":"http://arxiv.org/abs/2210.00173v3","updated":"2023-03-20T13:44:46Z","published":"2022-10-01T02:57:37Z","title":"Predictive Inference with Feature Conformal Prediction","summary":"  Conformal prediction is a distribution-free technique for establishing valid\nprediction intervals. Although conventionally people conduct conformal\nprediction in the output space, this is not the only possibility. In this\npaper, we propose feature conformal prediction, which extends the scope of\nconformal prediction to semantic feature spaces by leveraging the inductive\nbias of deep representation learning. From a theoretical perspective, we\ndemonstrate that feature conformal prediction provably outperforms regular\nconformal prediction under mild assumptions. Our approach could be combined\nwith not only vanilla conformal prediction, but also other adaptive conformal\nprediction methods. Apart from experiments on existing predictive inference\nbenchmarks, we also demonstrate the state-of-the-art performance of the\nproposed methods on large-scale tasks such as ImageNet classification and\nCityscapes image segmentation.\n","authors":["Jiaye Teng","Chuan Wen","Dinghuai Zhang","Yoshua Bengio","Yang Gao","Yang Yuan"],"pdf_url":"https://arxiv.org/pdf/2210.00173v3.pdf","comment":"Published as a conference paper at ICLR 2023"},{"id":"http://arxiv.org/abs/2211.07866v2","updated":"2023-03-20T13:43:44Z","published":"2022-11-15T03:17:11Z","title":"Efficient Estimation for Longitudinal Network via Adaptive Merging","summary":"  Longitudinal network consists of a sequence of temporal edges among multiple\nnodes, where the temporal edges are observed in real time. It has become\nubiquitous with the rise of online social platform and e-commerce, but largely\nunder-investigated in literature. In this paper, we propose an efficient\nestimation framework for longitudinal network, leveraging strengths of adaptive\nnetwork merging, tensor decomposition and point process. It merges neighboring\nsparse networks so as to enlarge the number of observed edges and reduce\nestimation variance, whereas the estimation bias introduced by network merging\nis controlled by exploiting local temporal structures for adaptive network\nneighborhood. A projected gradient descent algorithm is proposed to facilitate\nestimation, where the upper bound of the estimation error in each iteration is\nestablished. A thorough analysis is conducted to quantify the asymptotic\nbehavior of the proposed method, which shows that it can significantly reduce\nthe estimation error and also provides guideline for network merging under\nvarious scenarios. We further demonstrate the advantage of the proposed method\nthrough extensive numerical experiments on synthetic datasets and a militarized\ninterstate dispute dataset.\n","authors":["Haoran Zhang","Junhui Wang"],"pdf_url":"https://arxiv.org/pdf/2211.07866v2.pdf","comment":"26 pages and 2 figures; the appendix including technical proof will\n  be uploaded later"},{"id":"http://arxiv.org/abs/2301.03364v3","updated":"2023-03-20T13:41:37Z","published":"2022-12-20T15:04:20Z","title":"Towards an AI-enabled Connected Industry: AGV Communication and Sensor\n  Measurement Datasets","summary":"  This paper presents two wireless measurement campaigns in industrial\ntestbeds: industrial Vehicle-to-vehicle (iV2V) and industrial\nVehicle-to-infrastructure plus Sensor (iV2I+). Detailed information about the\ntwo captured datasets is provided as well. iV2V covers sidelink communication\nscenarios between Automated Guided Vehicles (AGVs), while iV2I+ is conducted at\nan industrial setting where an autonomous cleaning robot is connected to a\nprivate cellular network. The combination of different communication\ntechnologies, together with a common measurement methodology, provides insights\nthat can be exploited by Machine Learning (ML) for tasks such as\nfingerprinting, line-of-sight detection, prediction of quality of service or\nlink selection. Moreover, the datasets are labelled and pre-filtered for fast\non-boarding and applicability. The corresponding testbeds and measurements are\nalso presented in detail for both datasets.\n","authors":["Rodrigo Hernangómez","Alexandros Palaios","Cara Watermann","Daniel Schäufele","Philipp Geuer","Rafail Ismayilov","Mohammad Parvini","Anton Krause","Martin Kasparick","Thomas Neugebauer","Oscar D. Ramos-Cantor","Hugues Tchouankem","Jose Leon Calvo","Bo Chen","Gerhard Fettweis","Sławomir Stańczak"],"pdf_url":"https://arxiv.org/pdf/2301.03364v3.pdf","comment":"7 pages, 3 figures. Submitted to a special issue magazine. Datasets\n  available at\n  https://ieee-dataport.org/open-access/ai4mobile-industrial-wireless-datasets-iv2v-and-iv2i"},{"id":"http://arxiv.org/abs/2303.11103v1","updated":"2023-03-20T13:40:11Z","published":"2023-03-20T13:40:11Z","title":"Sionna RT: Differentiable Ray Tracing for Radio Propagation Modeling","summary":"  Sionna is a GPU-accelerated open-source library for link-level simulations\nbased on TensorFlow. Its latest release (v0.14) integrates a differentiable ray\ntracer (RT) for the simulation of radio wave propagation. This unique feature\nallows for the computation of gradients of the channel impulse response and\nother related quantities with respect to many system and environment\nparameters, such as material properties, antenna patterns, array geometries, as\nwell as transmitter and receiver orientations and positions. In this paper, we\noutline the key components of Sionna RT and showcase example applications such\nas learning radio materials and optimizing transmitter orientations by gradient\ndescent. While classic ray tracing is a crucial tool for 6G research topics\nlike reconfigurable intelligent surfaces, integrated sensing and\ncommunications, as well as user localization, differentiable ray tracing is a\nkey enabler for many novel and exciting research directions, for example,\ndigital twins.\n","authors":["Jakob Hoydis","Fayçal Aït Aoudia","Sebastian Cammerer","Merlin Nimier-David","Nikolaus Binder","Guillermo Marcus","Alexander Keller"],"pdf_url":"https://arxiv.org/pdf/2303.11103v1.pdf","comment":"5 pages, 5 figures"},{"id":"http://arxiv.org/abs/2303.11079v1","updated":"2023-03-20T13:38:58Z","published":"2023-03-20T13:38:58Z","title":"Differentially Private Algorithms for Synthetic Power System Datasets","summary":"  While power systems research relies on the availability of real-world network\ndatasets, data owners (e.g., system operators) are hesitant to share data due\nto security and privacy risks. To control these risks, we develop\nprivacy-preserving algorithms for the synthetic generation of optimization and\nmachine learning datasets. Taking a real-world dataset as input, the algorithms\noutput its noisy, synthetic version, which preserves the accuracy of the real\ndata on a specific downstream model or even a large population of those. We\ncontrol the privacy loss using Laplace and Exponential mechanisms of\ndifferential privacy and preserve data accuracy using a post-processing convex\noptimization. We apply the algorithms to generate synthetic network parameters\nand wind power data.\n","authors":["Vladimir Dvorkin","Audun Botterud"],"pdf_url":"https://arxiv.org/pdf/2303.11079v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.12261v2","updated":"2023-03-20T13:34:48Z","published":"2022-07-06T13:56:48Z","title":"GraphCFC: A Directed Graph based Cross-modal Feature Complementation\n  Approach for Multimodal Conversational Emotion Recognition","summary":"  Emotion Recognition in Conversation (ERC) plays a significant part in\nHuman-Computer Interaction (HCI) systems since it can provide empathetic\nservices. Multimodal ERC can mitigate the drawbacks of uni-modal approaches.\nRecently, Graph Neural Networks (GNNs) have been widely used in a variety of\nfields due to their superior performance in relation modeling. In multimodal\nERC, GNNs are capable of extracting both long-distance contextual information\nand inter-modal interactive information. Unfortunately, since existing methods\nsuch as MMGCN directly fuse multiple modalities, redundant information may be\ngenerated and diverse information may be lost. In this work, we present a\ndirected Graph based Cross-modal Feature Complementation (GraphCFC) module that\ncan efficiently model contextual and interactive information. GraphCFC\nalleviates the problem of heterogeneity gap in multimodal fusion by utilizing\nmultiple subspace extractors and Pair-wise Cross-modal Complementary (PairCC)\nstrategy. We extract various types of edges from the constructed graph for\nencoding, thus enabling GNNs to extract crucial contextual and interactive\ninformation more accurately when performing message passing. Furthermore, we\ndesign a GNN structure called GAT-MLP, which can provide a new unified network\nframework for multimodal learning. The experimental results on two benchmark\ndatasets show that our GraphCFC outperforms the state-of-the-art (SOTA)\napproaches.\n","authors":["Jiang Li","Xiaoping Wang","Guoqing Lv","Zhigang Zeng"],"pdf_url":"https://arxiv.org/pdf/2207.12261v2.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2303.11073v1","updated":"2023-03-20T12:59:32Z","published":"2023-03-20T12:59:32Z","title":"Discovering Interpretable Directions in the Semantic Latent Space of\n  Diffusion Models","summary":"  Denoising Diffusion Models (DDMs) have emerged as a strong competitor to\nGenerative Adversarial Networks (GANs). However, despite their widespread use\nin image synthesis and editing applications, their latent space is still not as\nwell understood. Recently, a semantic latent space for DDMs, coined\n`$h$-space', was shown to facilitate semantic image editing in a way\nreminiscent of GANs. The $h$-space is comprised of the bottleneck activations\nin the DDM's denoiser across all timesteps of the diffusion process. In this\npaper, we explore the properties of h-space and propose several novel methods\nfor finding meaningful semantic directions within it. We start by studying\nunsupervised methods for revealing interpretable semantic directions in\npretrained DDMs. Specifically, we show that global latent directions emerge as\nthe principal components in the latent space. Additionally, we provide a novel\nmethod for discovering image-specific semantic directions by spectral analysis\nof the Jacobian of the denoiser w.r.t. the latent code. Next, we extend the\nanalysis by finding directions in a supervised fashion in unconditional DDMs.\nWe demonstrate how such directions can be found by relying on either a labeled\ndata set of real images or by annotating generated samples with a\ndomain-specific attribute classifier. We further show how to semantically\ndisentangle the found direction by simple linear projection. Our approaches are\napplicable without requiring any architectural modifications, text-based\nguidance, CLIP-based optimization, or model fine-tuning.\n","authors":["René Haas","Inbar Huberman-Spiegelglas","Rotem Mulayoff","Tomer Michaeli"],"pdf_url":"https://arxiv.org/pdf/2303.11073v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2106.05087v5","updated":"2023-03-20T12:43:45Z","published":"2021-06-09T14:06:53Z","title":"Who Is the Strongest Enemy? Towards Optimal and Efficient Evasion\n  Attacks in Deep RL","summary":"  Evaluating the worst-case performance of a reinforcement learning (RL) agent\nunder the strongest/optimal adversarial perturbations on state observations\n(within some constraints) is crucial for understanding the robustness of RL\nagents. However, finding the optimal adversary is challenging, in terms of both\nwhether we can find the optimal attack and how efficiently we can find it.\nExisting works on adversarial RL either use heuristics-based methods that may\nnot find the strongest adversary, or directly train an RL-based adversary by\ntreating the agent as a part of the environment, which can find the optimal\nadversary but may become intractable in a large state space. This paper\nintroduces a novel attacking method to find the optimal attacks through\ncollaboration between a designed function named \"actor\" and an RL-based learner\nnamed \"director\". The actor crafts state perturbations for a given policy\nperturbation direction, and the director learns to propose the best policy\nperturbation directions. Our proposed algorithm, PA-AD, is theoretically\noptimal and significantly more efficient than prior RL-based works in\nenvironments with large state spaces. Empirical results show that our proposed\nPA-AD universally outperforms state-of-the-art attacking methods in various\nAtari and MuJoCo environments. By applying PA-AD to adversarial training, we\nachieve state-of-the-art empirical robustness in multiple tasks under strong\nadversaries. The codebase is released at\nhttps://github.com/umd-huang-lab/paad_adv_rl.\n","authors":["Yanchao Sun","Ruijie Zheng","Yongyuan Liang","Furong Huang"],"pdf_url":"https://arxiv.org/pdf/2106.05087v5.pdf","comment":"In the 10th International Conference on Learning Representations\n  (ICLR 2022)"},{"id":"http://arxiv.org/abs/2303.11060v1","updated":"2023-03-20T12:23:31Z","published":"2023-03-20T12:23:31Z","title":"Quantile and moment neural networks for learning functionals of\n  distributions","summary":"  We study news neural networks to approximate function of distributions in a\nprobability space. Two classes of neural networks based on quantile and moment\napproximation are proposed to learn these functions and are theoretically\nsupported by universal approximation theorems. By mixing the quantile and\nmoment features in other new networks, we develop schemes that outperform\nexisting networks on numerical test cases involving univariate distributions.\nFor bivariate distributions, the moment neural network outperforms all other\nnetworks.\n","authors":["Xavier Warin"],"pdf_url":"https://arxiv.org/pdf/2303.11060v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11042v1","updated":"2023-03-20T11:48:36Z","published":"2023-03-20T11:48:36Z","title":"Hospitalization Length of Stay Prediction using Patient Event Sequences","summary":"  Predicting patients hospital length of stay (LOS) is essential for improving\nresource allocation and supporting decision-making in healthcare organizations.\nThis paper proposes a novel approach for predicting LOS by modeling patient\ninformation as sequences of events. Specifically, we present a\ntransformer-based model, termed Medic-BERT (M-BERT), for LOS prediction using\nthe unique features describing patients medical event sequences. We performed\nempirical experiments on a cohort of more than 45k emergency care patients from\na large Danish hospital. Experimental results show that M-BERT can achieve high\naccuracy on a variety of LOS problems and outperforms traditional\nnonsequence-based machine learning approaches.\n","authors":["Emil Riis Hansen","Thomas Dyhre Nielsen","Thomas Mulvad","Mads Nibe Strausholm","Tomer Sagi","Katja Hose"],"pdf_url":"https://arxiv.org/pdf/2303.11042v1.pdf","comment":"11 pages, 5 figures"},{"id":"http://arxiv.org/abs/2302.13007v3","updated":"2023-03-20T11:39:47Z","published":"2023-02-25T06:58:16Z","title":"AugGPT: Leveraging ChatGPT for Text Data Augmentation","summary":"  Text data augmentation is an effective strategy for overcoming the challenge\nof limited sample sizes in many natural language processing (NLP) tasks. This\nchallenge is especially prominent in the few-shot learning scenario, where the\ndata in the target domain is generally much scarcer and of lowered quality. A\nnatural and widely-used strategy to mitigate such challenges is to perform data\naugmentation to better capture the data invariance and increase the sample\nsize. However, current text data augmentation methods either can't ensure the\ncorrect labeling of the generated data (lacking faithfulness) or can't ensure\nsufficient diversity in the generated data (lacking compactness), or both.\nInspired by the recent success of large language models, especially the\ndevelopment of ChatGPT, which demonstrated improved language comprehension\nabilities, in this work, we propose a text data augmentation approach based on\nChatGPT (named AugGPT). AugGPT rephrases each sentence in the training samples\ninto multiple conceptually similar but semantically different samples. The\naugmented samples can then be used in downstream model training. Experiment\nresults on few-shot learning text classification tasks show the superior\nperformance of the proposed AugGPT approach over state-of-the-art text data\naugmentation methods in terms of testing accuracy and distribution of the\naugmented samples.\n","authors":["Haixing Dai","Zhengliang Liu","Wenxiong Liao","Xiaoke Huang","Yihan Cao","Zihao Wu","Lin Zhao","Shaochen Xu","Wei Liu","Ninghao Liu","Sheng Li","Dajiang Zhu","Hongmin Cai","Lichao Sun","Quanzheng Li","Dinggang Shen","Tianming Liu","Xiang Li"],"pdf_url":"https://arxiv.org/pdf/2302.13007v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.11674v2","updated":"2023-03-20T11:33:18Z","published":"2022-11-21T17:42:42Z","title":"Shape, Pose, and Appearance from a Single Image via Bootstrapped\n  Radiance Field Inversion","summary":"  Neural Radiance Fields (NeRF) coupled with GANs represent a promising\ndirection in the area of 3D reconstruction from a single view, owing to their\nability to efficiently model arbitrary topologies. Recent work in this area,\nhowever, has mostly focused on synthetic datasets where exact ground-truth\nposes are known, and has overlooked pose estimation, which is important for\ncertain downstream applications such as augmented reality (AR) and robotics. We\nintroduce a principled end-to-end reconstruction framework for natural images,\nwhere accurate ground-truth poses are not available. Our approach recovers an\nSDF-parameterized 3D shape, pose, and appearance from a single image of an\nobject, without exploiting multiple views during training. More specifically,\nwe leverage an unconditional 3D-aware generator, to which we apply a hybrid\ninversion scheme where a model produces a first guess of the solution which is\nthen refined via optimization. Our framework can de-render an image in as few\nas 10 steps, enabling its use in practical scenarios. We demonstrate\nstate-of-the-art results on a variety of real and synthetic benchmarks.\n","authors":["Dario Pavllo","David Joseph Tan","Marie-Julie Rakotosaona","Federico Tombari"],"pdf_url":"https://arxiv.org/pdf/2211.11674v2.pdf","comment":"CVPR 2023. Code and models are available at\n  https://github.com/google-research/nerf-from-image"},{"id":"http://arxiv.org/abs/2303.11028v1","updated":"2023-03-20T11:18:22Z","published":"2023-03-20T11:18:22Z","title":"MAQA: A Quantum Framework for Supervised Learning","summary":"  Quantum Machine Learning has the potential to improve traditional machine\nlearning methods and overcome some of the main limitations imposed by the\nclassical computing paradigm. However, the practical advantages of using\nquantum resources to solve pattern recognition tasks are still to be\ndemonstrated.\n  This work proposes a universal, efficient framework that can reproduce the\noutput of a plethora of classical supervised machine learning algorithms\nexploiting quantum computation's advantages. The proposed framework is named\nMultiple Aggregator Quantum Algorithm (MAQA) due to its capability to combine\nmultiple and diverse functions to solve typical supervised learning problems.\nIn its general formulation, MAQA can be potentially adopted as the quantum\ncounterpart of all those models falling into the scheme of aggregation of\nmultiple functions, such as ensemble algorithms and neural networks. From a\ncomputational point of view, the proposed framework allows generating an\nexponentially large number of different transformations of the input at the\ncost of increasing the depth of the corresponding quantum circuit linearly.\nThus, MAQA produces a model with substantial descriptive power to broaden the\nhorizon of possible applications of quantum machine learning with a\ncomputational advantage over classical methods. As a second meaningful\naddition, we discuss the adoption of the proposed framework as hybrid\nquantum-classical and fault-tolerant quantum algorithm.\n","authors":["Antonio Macaluso","Matthias Klusch","Stefano Lodi","Claudio Sartori"],"pdf_url":"https://arxiv.org/pdf/2303.11028v1.pdf","comment":"1 Figure"},{"id":"http://arxiv.org/abs/2302.08453v2","updated":"2023-03-20T10:52:26Z","published":"2023-02-16T17:56:08Z","title":"T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for\n  Text-to-Image Diffusion Models","summary":"  The incredible generative ability of large-scale text-to-image (T2I) models\nhas demonstrated strong power of learning complex structures and meaningful\nsemantics. However, relying solely on text prompts cannot fully take advantage\nof the knowledge learned by the model, especially when flexible and accurate\ncontrolling (e.g., color and structure) is needed. In this paper, we aim to\n``dig out\" the capabilities that T2I models have implicitly learned, and then\nexplicitly use them to control the generation more granularly. Specifically, we\npropose to learn simple and lightweight T2I-Adapters to align internal\nknowledge in T2I models with external control signals, while freezing the\noriginal large T2I models. In this way, we can train various adapters according\nto different conditions, achieving rich control and editing effects in the\ncolor and structure of the generation results. Further, the proposed\nT2I-Adapters have attractive properties of practical value, such as\ncomposability and generalization ability. Extensive experiments demonstrate\nthat our T2I-Adapter has promising generation quality and a wide range of\napplications.\n","authors":["Chong Mou","Xintao Wang","Liangbin Xie","Yanze Wu","Jian Zhang","Zhongang Qi","Ying Shan","Xiaohu Qie"],"pdf_url":"https://arxiv.org/pdf/2302.08453v2.pdf","comment":"Tech Report. GitHub: https://github.com/TencentARC/T2I-Adapter"},{"id":"http://arxiv.org/abs/2303.11005v1","updated":"2023-03-20T10:33:06Z","published":"2023-03-20T10:33:06Z","title":"Controllable Ancient Chinese Lyrics Generation Based on Phrase Prototype\n  Retrieving","summary":"  Generating lyrics and poems is one of the essential downstream tasks in the\nNatural Language Processing (NLP) field. Current methods have performed well in\nsome lyrics generation scenarios but need further improvements in tasks\nrequiring fine control. We propose a novel method for generating ancient\nChinese lyrics (Song Ci), a type of ancient lyrics that involves precise\ncontrol of song structure. The system is equipped with a phrase retriever and a\nphrase connector. Based on an input prompt, the phrase retriever picks phrases\nfrom a database to construct a phrase pool. The phrase connector then selects a\nseries of phrases from the phrase pool that minimizes a multi-term loss\nfunction that considers rhyme, song structure, and fluency. Experimental\nresults show that our method can generate high-quality ancient Chinese lyrics\nwhile performing well on topic and song structure control. We also expect our\napproach to be generalized to other lyrics-generating tasks.\n","authors":["Li Yi"],"pdf_url":"https://arxiv.org/pdf/2303.11005v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11000v1","updated":"2023-03-20T10:29:42Z","published":"2023-03-20T10:29:42Z","title":"Late Meta-learning Fusion Using Representation Learning for Time Series\n  Forecasting","summary":"  Meta-learning, decision fusion, hybrid models, and representation learning\nare topics of investigation with significant traction in time-series\nforecasting research. Of these two specific areas have shown state-of-the-art\nresults in forecasting: hybrid meta-learning models such as Exponential\nSmoothing - Recurrent Neural Network (ES-RNN) and Neural Basis Expansion\nAnalysis (N-BEATS) and feature-based stacking ensembles such as Feature-based\nFORecast Model Averaging (FFORMA). However, a unified taxonomy for model fusion\nand an empirical comparison of these hybrid and feature-based stacking ensemble\napproaches is still missing. This study presents a unified taxonomy\nencompassing these topic areas. Furthermore, the study empirically evaluates\nseveral model fusion approaches and a novel combination of hybrid and feature\nstacking algorithms called Deep-learning FORecast Model Averaging (DeFORMA).\nThe taxonomy contextualises the considered methods. Furthermore, the empirical\nanalysis of the results shows that the proposed model, DeFORMA, can achieve\nstate-of-the-art results in the M4 data set. DeFORMA, increases the mean\nOverall Weighted Average (OWA) in the daily, weekly and yearly subsets with\ncompetitive results in the hourly, monthly and quarterly subsets. The taxonomy\nand empirical results lead us to argue that significant progress is still to be\nmade by continuing to explore the intersection of these research areas.\n","authors":["Terence L. van Zyl"],"pdf_url":"https://arxiv.org/pdf/2303.11000v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10999v1","updated":"2023-03-20T10:29:35Z","published":"2023-03-20T10:29:35Z","title":"Induced Feature Selection by Structured Pruning","summary":"  The advent of sparsity inducing techniques in neural networks has been of a\ngreat help in the last few years. Indeed, those methods allowed to find lighter\nand faster networks, able to perform more efficiently in resource-constrained\nenvironment such as mobile devices or highly requested servers. Such a sparsity\nis generally imposed on the weights of neural networks, reducing the footprint\nof the architecture. In this work, we go one step further by imposing sparsity\njointly on the weights and on the input data. This can be achieved following a\nthree-step process: 1) impose a certain structured sparsity on the weights of\nthe network; 2) track back input features corresponding to zeroed blocks of\nweight; 3) remove useless weights and input features and retrain the network.\nPerforming pruning both on the network and on input data not only allows for\nextreme reduction in terms of parameters and operations but can also serve as\nan interpretation process. Indeed, with the help of data pruning, we now have\ninformation about which input feature is useful for the network to keep its\nperformance. Experiments conducted on a variety of architectures and datasets:\nMLP validated on MNIST, CIFAR10/100 and ConvNets (VGG16 and ResNet18),\nvalidated on CIFAR10/100 and CALTECH101 respectively, show that it is possible\nto achieve additional gains in terms of total parameters and in FLOPs by\nperforming pruning on input data, while also increasing accuracy.\n","authors":["Nathan Hubens","Victor Delvigne","Matei Mancas","Bernard Gosselin","Marius Preda","Titus Zaharia"],"pdf_url":"https://arxiv.org/pdf/2303.10999v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10993v1","updated":"2023-03-20T10:21:29Z","published":"2023-03-20T10:21:29Z","title":"A Survey on Oversmoothing in Graph Neural Networks","summary":"  Node features of graph neural networks (GNNs) tend to become more similar\nwith the increase of the network depth. This effect is known as over-smoothing,\nwhich we axiomatically define as the exponential convergence of suitable\nsimilarity measures on the node features. Our definition unifies previous\napproaches and gives rise to new quantitative measures of over-smoothing.\nMoreover, we empirically demonstrate this behavior for several over-smoothing\nmeasures on different graphs (small-, medium-, and large-scale). We also review\nseveral approaches for mitigating over-smoothing and empirically test their\neffectiveness on real-world graph datasets. Through illustrative examples, we\ndemonstrate that mitigating over-smoothing is a necessary but not sufficient\ncondition for building deep GNNs that are expressive on a wide range of graph\nlearning tasks. Finally, we extend our definition of over-smoothing to the\nrapidly emerging field of continuous-time GNNs.\n","authors":["T. Konstantin Rusch","Michael M. Bronstein","Siddhartha Mishra"],"pdf_url":"https://arxiv.org/pdf/2303.10993v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10972v1","updated":"2023-03-20T09:50:07Z","published":"2023-03-20T09:50:07Z","title":"Semantic segmentation of surgical hyperspectral images under geometric\n  domain shifts","summary":"  Robust semantic segmentation of intraoperative image data could pave the way\nfor automatic surgical scene understanding and autonomous robotic surgery.\nGeometric domain shifts, however, although common in real-world open surgeries\ndue to variations in surgical procedures or situs occlusions, remain a topic\nlargely unaddressed in the field. To address this gap in the literature, we (1)\npresent the first analysis of state-of-the-art (SOA) semantic segmentation\nnetworks in the presence of geometric out-of-distribution (OOD) data, and (2)\naddress generalizability with a dedicated augmentation technique termed \"Organ\nTransplantation\" that we adapted from the general computer vision community.\nAccording to a comprehensive validation on six different OOD data sets\ncomprising 600 RGB and hyperspectral imaging (HSI) cubes from 33 pigs\nsemantically annotated with 19 classes, we demonstrate a large performance drop\nof SOA organ segmentation networks applied to geometric OOD data. Surprisingly,\nthis holds true not only for conventional RGB data (drop of Dice similarity\ncoefficient (DSC) by 46 %) but also for HSI data (drop by 45 %), despite the\nlatter's rich information content per pixel. Using our augmentation scheme\nimproves on the SOA DSC by up to 67 % (RGB) and 90 % (HSI) and renders\nperformance on par with in-distribution performance on real OOD test data. The\nsimplicity and effectiveness of our augmentation scheme makes it a valuable\nnetwork-independent tool for addressing geometric domain shifts in semantic\nscene segmentation of intraoperative data. Our code and pre-trained models will\nbe made publicly available.\n","authors":["Jan Sellner","Silvia Seidlitz","Alexander Studier-Fischer","Alessandro Motta","Berkin Özdemir","Beat Peter Müller-Stich","Felix Nickel","Lena Maier-Hein"],"pdf_url":"https://arxiv.org/pdf/2303.10972v1.pdf","comment":"The first two authors (Jan Sellner and Silvia Seidlitz) contributed\n  equally to this paper"},{"id":"http://arxiv.org/abs/2303.10954v1","updated":"2023-03-20T09:27:58Z","published":"2023-03-20T09:27:58Z","title":"Uncertainty-aware deep learning for digital twin-driven monitoring:\n  Application to fault detection in power lines","summary":"  Deep neural networks (DNNs) are often coupled with physics-based models or\ndata-driven surrogate models to perform fault detection and health monitoring\nof systems in the low data regime. These models serve as digital twins to\ngenerate large quantities of data to train DNNs which would otherwise be\ndifficult to obtain from the real-life system. However, such models can exhibit\nparametric uncertainty that propagates to the generated data. In addition, DNNs\nexhibit uncertainty in the parameters learnt during training. In such a\nscenario, the performance of the DNN model will be influenced by the\nuncertainty in the physics-based model as well as the parameters of the DNN. In\nthis article, we quantify the impact of both these sources of uncertainty on\nthe performance of the DNN. We perform explicit propagation of uncertainty in\ninput data through all layers of the DNN, as well as implicit prediction of\noutput uncertainty to capture the former. Furthermore, we adopt Monte Carlo\ndropout to capture uncertainty in DNN parameters. We demonstrate the approach\nfor fault detection of power lines with a physics-based model, two types of\ninput data and three different neural network architectures. We compare the\nperformance of such uncertainty-aware probabilistic models with their\ndeterministic counterparts. The results show that the probabilistic models\nprovide important information regarding the confidence of predictions, while\nalso delivering an improvement in performance over deterministic models.\n","authors":["Laya Das","Blazhe Gjorgiev","Giovanni Sansavini"],"pdf_url":"https://arxiv.org/pdf/2303.10954v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.08645v3","updated":"2023-03-20T08:38:19Z","published":"2022-07-18T14:45:55Z","title":"Active Exploration for Inverse Reinforcement Learning","summary":"  Inverse Reinforcement Learning (IRL) is a powerful paradigm for inferring a\nreward function from expert demonstrations. Many IRL algorithms require a known\ntransition model and sometimes even a known expert policy, or they at least\nrequire access to a generative model. However, these assumptions are too strong\nfor many real-world applications, where the environment can be accessed only\nthrough sequential interaction. We propose a novel IRL algorithm: Active\nexploration for Inverse Reinforcement Learning (AceIRL), which actively\nexplores an unknown environment and expert policy to quickly learn the expert's\nreward function and identify a good policy. AceIRL uses previous observations\nto construct confidence intervals that capture plausible reward functions and\nfind exploration policies that focus on the most informative regions of the\nenvironment. AceIRL is the first approach to active IRL with sample-complexity\nbounds that does not require a generative model of the environment. AceIRL\nmatches the sample complexity of active IRL with a generative model in the\nworst case. Additionally, we establish a problem-dependent bound that relates\nthe sample complexity of AceIRL to the suboptimality gap of a given IRL\nproblem. We empirically evaluate AceIRL in simulations and find that it\nsignificantly outperforms more naive exploration strategies.\n","authors":["David Lindner","Andreas Krause","Giorgia Ramponi"],"pdf_url":"https://arxiv.org/pdf/2207.08645v3.pdf","comment":"Presented at Conference on Neural Information Processing Systems\n  (NeurIPS), 2022"},{"id":"http://arxiv.org/abs/2303.09232v2","updated":"2023-03-20T08:25:38Z","published":"2023-03-16T11:15:55Z","title":"Generative Adversarial Network for Personalized Art Therapy in Melanoma\n  Disease Management","summary":"  Melanoma is the most lethal type of skin cancer. Patients are vulnerable to\nmental health illnesses which can reduce the effectiveness of the cancer\ntreatment and the patients adherence to drug plans. It is crucial to preserve\nthe mental health of patients while they are receiving treatment. However,\ncurrent art therapy approaches are not personal and unique to the patient. We\naim to provide a well-trained image style transfer model that can quickly\ngenerate unique art from personal dermoscopic melanoma images as an additional\ntool for art therapy in disease management of melanoma. Visual art appreciation\nas a common form of art therapy in disease management that measurably reduces\nthe degree of psychological distress. We developed a network based on the\ncycle-consistent generative adversarial network for style transfer that\ngenerates personalized and unique artworks from dermoscopic melanoma images. We\ndeveloped a model that converts melanoma images into unique flower-themed\nartworks that relate to the shape of the lesion and are therefore personal to\nthe patient. Further, we altered the initial framework and made comparisons and\nevaluations of the results. With this, we increased the options in the toolbox\nfor art therapy in disease management of melanoma. The development of an\neasy-to-use user interface ensures the availability of the approach to\nstakeholders. The transformation of melanoma into flower-themed artworks is\nachieved by the proposed model and the graphical user interface. This\ncontribution opens a new field of GANs in art therapy and could lead to more\npersonalized disease management.\n","authors":["Lennart Jütte","Ning Wang","Bernhard Roth"],"pdf_url":"https://arxiv.org/pdf/2303.09232v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10934v1","updated":"2023-03-20T08:17:05Z","published":"2023-03-20T08:17:05Z","title":"EMC2-Net: Joint Equalization and Modulation Classification based on\n  Constellation Network","summary":"  Modulation classification (MC) is the first step performed at the receiver\nside unless the modulation type is explicitly indicated by the transmitter.\nMachine learning techniques have been widely used for MC recently. In this\npaper, we propose a novel MC technique dubbed as Joint Equalization and\nModulation Classification based on Constellation Network (EMC2-Net). Unlike\nprior works that considered the constellation points as an image, the proposed\nEMC2-Net directly uses a set of 2D constellation points to perform MC. In order\nto obtain clear and concrete constellation despite multipath fading channels,\nthe proposed EMC2-Net consists of equalizer and classifier having separate and\nexplainable roles via novel three-phase training and noise-curriculum\npretraining. Numerical results with linear modulation types under different\nchannel models show that the proposed EMC2-Net achieves the performance of\nstate-of-the-art MC techniques with significantly less complexity.\n","authors":["Hyun Ryu","Junil Choi"],"pdf_url":"https://arxiv.org/pdf/2303.10934v1.pdf","comment":"Accepted to ICASSP 2023 (5 pages, 2 figures, 1 table)"},{"id":"http://arxiv.org/abs/2303.10931v1","updated":"2023-03-20T08:09:13Z","published":"2023-03-20T08:09:13Z","title":"Approaching an unknown communication system by latent space exploration\n  and causal inference","summary":"  This paper proposes a methodology for discovering meaningful properties in\ndata by exploring the latent space of unsupervised deep generative models. We\ncombine manipulation of individual latent variables to extreme values outside\nthe training range with methods inspired by causal inference into an approach\nwe call causal disentanglement with extreme values (CDEV) and show that this\napproach yields insights for model interpretability. Using this technique, we\ncan infer what properties of unknown data the model encodes as meaningful. We\napply the methodology to test what is meaningful in the communication system of\nsperm whales, one of the most intriguing and understudied animal communication\nsystems. We train a network that has been shown to learn meaningful\nrepresentations of speech and test whether we can leverage such unsupervised\nlearning to decipher the properties of another vocal communication system for\nwhich we have no ground truth. The proposed technique suggests that sperm\nwhales encode information using the number of clicks in a sequence, the\nregularity of their timing, and audio properties such as the spectral mean and\nthe acoustic regularity of the sequences. Some of these findings are consistent\nwith existing hypotheses, while others are proposed for the first time. We also\nargue that our models uncover rules that govern the structure of communication\nunits in the sperm whale communication system and apply them while generating\ninnovative data not shown during training. This paper suggests that an\ninterpretation of the outputs of deep neural networks with causal methodology\ncan be a viable strategy for approaching data about which little is known and\npresents another case of how deep learning can limit the hypothesis space.\nFinally, the proposed approach combining latent space manipulation and causal\ninference can be extended to other architectures and arbitrary datasets.\n","authors":["Gašper Beguš","Andrej Leban","Shane Gero"],"pdf_url":"https://arxiv.org/pdf/2303.10931v1.pdf","comment":"25 pages, 23 figures"},{"id":"http://arxiv.org/abs/2202.03376v3","updated":"2023-03-20T07:52:57Z","published":"2022-02-07T17:47:46Z","title":"Message Passing Neural PDE Solvers","summary":"  The numerical solution of partial differential equations (PDEs) is difficult,\nhaving led to a century of research so far. Recently, there have been pushes to\nbuild neural--numerical hybrid solvers, which piggy-backs the modern trend\ntowards fully end-to-end learned systems. Most works so far can only generalize\nover a subset of properties to which a generic solver would be faced,\nincluding: resolution, topology, geometry, boundary conditions, domain\ndiscretization regularity, dimensionality, etc. In this work, we build a\nsolver, satisfying these properties, where all the components are based on\nneural message passing, replacing all heuristically designed components in the\ncomputation graph with backprop-optimized neural function approximators. We\nshow that neural message passing solvers representationally contain some\nclassical methods, such as finite differences, finite volumes, and WENO\nschemes. In order to encourage stability in training autoregressive models, we\nput forward a method that is based on the principle of zero-stability, posing\nstability as a domain adaptation problem. We validate our method on various\nfluid-like flow problems, demonstrating fast, stable, and accurate performance\nacross different domain topologies, equation parameters, discretizations, etc.,\nin 1D and 2D.\n","authors":["Johannes Brandstetter","Daniel Worrall","Max Welling"],"pdf_url":"https://arxiv.org/pdf/2202.03376v3.pdf","comment":"Published at ICLR 2022 (Spotlight paper), Github:\n  https://github.com/brandstetter-johannes/MP-Neural-PDE-Solvers"},{"id":"http://arxiv.org/abs/2303.10912v1","updated":"2023-03-20T07:09:26Z","published":"2023-03-20T07:09:26Z","title":"Exploring Representation Learning for Small-Footprint Keyword Spotting","summary":"  In this paper, we investigate representation learning for low-resource\nkeyword spotting (KWS). The main challenges of KWS are limited labeled data and\nlimited available device resources. To address those challenges, we explore\nrepresentation learning for KWS by self-supervised contrastive learning and\nself-training with pretrained model. First, local-global contrastive siamese\nnetworks (LGCSiam) are designed to learn similar utterance-level\nrepresentations for similar audio samplers by proposed local-global contrastive\nloss without requiring ground-truth. Second, a self-supervised pretrained\nWav2Vec 2.0 model is applied as a constraint module (WVC) to force the KWS\nmodel to learn frame-level acoustic representations. By the LGCSiam and WVC\nmodules, the proposed small-footprint KWS model can be pretrained with\nunlabeled data. Experiments on speech commands dataset show that the\nself-training WVC module and the self-supervised LGCSiam module significantly\nimprove accuracy, especially in the case of training on a small labeled\ndataset.\n","authors":["Fan Cui","Liyong Guo","Quandong Wang","Peng Gao","Yujun Wang"],"pdf_url":"https://arxiv.org/pdf/2303.10912v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.06983v3","updated":"2023-03-20T07:01:49Z","published":"2022-07-14T15:06:37Z","title":"Multitrack Music Transformer","summary":"  Existing approaches for generating multitrack music with transformer models\nhave been limited in terms of the number of instruments, the length of the\nmusic segments and slow inference. This is partly due to the memory\nrequirements of the lengthy input sequences necessitated by existing\nrepresentations. In this work, we propose a new multitrack music representation\nthat allows a diverse set of instruments while keeping a short sequence length.\nOur proposed Multitrack Music Transformer (MMT) achieves comparable performance\nwith state-of-the-art systems, landing in between two recently proposed models\nin a subjective listening test, while achieving substantial speedups and memory\nreductions over both, making the method attractive for real time improvisation\nor near real time creative applications. Further, we propose a new measure for\nanalyzing musical self-attention and show that the trained model attends more\nto notes that form a consonant interval with the current note and to notes that\nare 4N beats away from the current step.\n","authors":["Hao-Wen Dong","Ke Chen","Shlomo Dubnov","Julian McAuley","Taylor Berg-Kirkpatrick"],"pdf_url":"https://arxiv.org/pdf/2207.06983v3.pdf","comment":"Accepted by ICASSP 2023. Audio samples available at\n  https://salu133445.github.io/mmt/ . Source code available at\n  https://github.com/salu133445/mmt"},{"id":"http://arxiv.org/abs/2303.08536v2","updated":"2023-03-20T07:01:45Z","published":"2023-03-15T11:29:36Z","title":"Watch or Listen: Robust Audio-Visual Speech Recognition with Visual\n  Corruption Modeling and Reliability Scoring","summary":"  This paper deals with Audio-Visual Speech Recognition (AVSR) under multimodal\ninput corruption situations where audio inputs and visual inputs are both\ncorrupted, which is not well addressed in previous research directions.\nPrevious studies have focused on how to complement the corrupted audio inputs\nwith the clean visual inputs with the assumption of the availability of clean\nvisual inputs. However, in real life, clean visual inputs are not always\naccessible and can even be corrupted by occluded lip regions or noises. Thus,\nwe firstly analyze that the previous AVSR models are not indeed robust to the\ncorruption of multimodal input streams, the audio and the visual inputs,\ncompared to uni-modal models. Then, we design multimodal input corruption\nmodeling to develop robust AVSR models. Lastly, we propose a novel AVSR\nframework, namely Audio-Visual Reliability Scoring module (AV-RelScore), that\nis robust to the corrupted multimodal inputs. The AV-RelScore can determine\nwhich input modal stream is reliable or not for the prediction and also can\nexploit the more reliable streams in prediction. The effectiveness of the\nproposed method is evaluated with comprehensive experiments on popular\nbenchmark databases, LRS2 and LRS3. We also show that the reliability scores\nobtained by AV-RelScore well reflect the degree of corruption and make the\nproposed model focus on the reliable multimodal representations.\n","authors":["Joanna Hong","Minsu Kim","Jeongsoo Choi","Yong Man Ro"],"pdf_url":"https://arxiv.org/pdf/2303.08536v2.pdf","comment":"Accepted at CVPR 2023. Implementation available:\n  https://github.com/joannahong/AV-RelScore"},{"id":"http://arxiv.org/abs/2007.00691v3","updated":"2023-03-20T06:57:52Z","published":"2020-07-01T18:32:05Z","title":"Falsification-Based Robust Adversarial Reinforcement Learning","summary":"  Reinforcement learning (RL) has achieved enormous progress in solving various\nsequential decision-making problems, such as control tasks in robotics. Since\npolicies are overfitted to training environments, RL methods have often failed\nto be generalized to safety-critical test scenarios. Robust adversarial RL\n(RARL) was previously proposed to train an adversarial network that applies\ndisturbances to a system, which improves the robustness in test scenarios.\nHowever, an issue of neural network-based adversaries is that integrating\nsystem requirements without handcrafting sophisticated reward signals are\ndifficult. Safety falsification methods allow one to find a set of initial\nconditions and an input sequence, such that the system violates a given\nproperty formulated in temporal logic. In this paper, we propose\nfalsification-based RARL (FRARL): this is the first generic framework for\nintegrating temporal logic falsification in adversarial learning to improve\npolicy robustness. By applying our falsification method, we do not need to\nconstruct an extra reward function for the adversary. Moreover, we evaluate our\napproach on a braking assistance system and an adaptive cruise control system\nof autonomous vehicles. Our experimental results demonstrate that policies\ntrained with a falsification-based adversary generalize better and show less\nviolation of the safety specification in test scenarios than those trained\nwithout an adversary or with an adversarial network.\n","authors":["Xiao Wang","Saasha Nair","Matthias Althoff"],"pdf_url":"https://arxiv.org/pdf/2007.00691v3.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2303.10909v1","updated":"2023-03-20T06:57:10Z","published":"2023-03-20T06:57:10Z","title":"Graph Neural Rough Differential Equations for Traffic Forecasting","summary":"  Traffic forecasting is one of the most popular spatio-temporal tasks in the\nfield of machine learning. A prevalent approach in the field is to combine\ngraph convolutional networks and recurrent neural networks for the\nspatio-temporal processing. There has been fierce competition and many novel\nmethods have been proposed. In this paper, we present the method of\nspatio-temporal graph neural rough differential equation (STG-NRDE). Neural\nrough differential equations (NRDEs) are a breakthrough concept for processing\ntime-series data. Their main concept is to use the log-signature transform to\nconvert a time-series sample into a relatively shorter series of feature\nvectors. We extend the concept and design two NRDEs: one for the temporal\nprocessing and the other for the spatial processing. After that, we combine\nthem into a single framework. We conduct experiments with 6 benchmark datasets\nand 21 baselines. STG-NRDE shows the best accuracy in all cases, outperforming\nall those 21 baselines by non-trivial margins.\n","authors":["Jeongwhan Choi","Noseong Park"],"pdf_url":"https://arxiv.org/pdf/2303.10909v1.pdf","comment":"Under review by ACM TIST. arXiv admin note: substantial text overlap\n  with arXiv:2112.03558"},{"id":"http://arxiv.org/abs/2205.02910v2","updated":"2023-03-20T06:49:13Z","published":"2022-05-05T20:29:13Z","title":"GANs as Gradient Flows that Converge","summary":"  This paper approaches the unsupervised learning problem by gradient descent\nin the space of probability density functions. A main result shows that along\nthe gradient flow induced by a distribution-dependent ordinary differential\nequation (ODE), the unknown data distribution emerges as the long-time limit.\nThat is, one can uncover the data distribution by simulating the\ndistribution-dependent ODE. Intriguingly, the simulation of the ODE is shown\nequivalent to the training of generative adversarial networks (GANs). This\nequivalence provides a new \"cooperative\" view of GANs and, more importantly,\nsheds new light on the divergence of GANs. In particular, it reveals that the\nGAN algorithm implicitly minimizes the mean squared error (MSE) between two\nsets of samples, and this MSE fitting alone can cause GANs to diverge. To\nconstruct a solution to the distribution-dependent ODE, we first show that the\nassociated nonlinear Fokker-Planck equation has a unique weak solution, by the\nCrandall-Liggett theorem for differential equations in Banach spaces. Based on\nthis solution to the Fokker-Planck equation, we construct a unique solution to\nthe ODE, using Trevisan's superposition principle. The convergence of the\ninduced gradient flow to the data distribution is obtained by analyzing the\nFokker-Planck equation.\n","authors":["Yu-Jui Huang","Yuchong Zhang"],"pdf_url":"https://arxiv.org/pdf/2205.02910v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.15452v2","updated":"2023-03-20T06:46:22Z","published":"2022-09-30T13:00:33Z","title":"Safe Exploration Method for Reinforcement Learning under Existence of\n  Disturbance","summary":"  Recent rapid developments in reinforcement learning algorithms have been\ngiving us novel possibilities in many fields. However, due to their exploring\nproperty, we have to take the risk into consideration when we apply those\nalgorithms to safety-critical problems especially in real environments. In this\nstudy, we deal with a safe exploration problem in reinforcement learning under\nthe existence of disturbance. We define the safety during learning as\nsatisfaction of the constraint conditions explicitly defined in terms of the\nstate and propose a safe exploration method that uses partial prior knowledge\nof a controlled object and disturbance. The proposed method assures the\nsatisfaction of the explicit state constraints with a pre-specified probability\neven if the controlled object is exposed to a stochastic disturbance following\na normal distribution. As theoretical results, we introduce sufficient\nconditions to construct conservative inputs not containing an exploring aspect\nused in the proposed method and prove that the safety in the above explained\nsense is guaranteed with the proposed method. Furthermore, we illustrate the\nvalidity and effectiveness of the proposed method through numerical simulations\nof an inverted pendulum and a four-bar parallel link robot manipulator.\n","authors":["Yoshihiro Okawa","Tomotake Sasaki","Hitoshi Yanami","Toru Namerikawa"],"pdf_url":"https://arxiv.org/pdf/2209.15452v2.pdf","comment":"Accepted by the European Conference on Machine Learning and\n  Principles and Practice of Knowledge Discovery in Databases (ECMLPKDD) 2022.\n  The Version of Record is available at\n  https://doi.org/10.1007/978-3-031-26412-2_9"},{"id":"http://arxiv.org/abs/2303.10898v1","updated":"2023-03-20T06:35:46Z","published":"2023-03-20T06:35:46Z","title":"A Tiny Machine Learning Model for Point Cloud Object Classification","summary":"  The design of a tiny machine learning model, which can be deployed in mobile\nand edge devices, for point cloud object classification is investigated in this\nwork. To achieve this objective, we replace the multi-scale representation of a\npoint cloud object with a single-scale representation for complexity reduction,\nand exploit rich 3D geometric information of a point cloud object for\nperformance improvement. The proposed solution is named Green-PointHop due to\nits low computational complexity. We evaluate the performance of Green-PointHop\non ModelNet40 and ScanObjectNN two datasets. Green-PointHop has a model size of\n64K parameters. It demands 2.3M floating-point operations (FLOPs) to classify a\nModelNet40 object of 1024 down-sampled points. Its classification performance\ngaps against the state-of-the-art DGCNN method are 3% and 7% for ModelNet40 and\nScanObjectNN, respectively. On the other hand, the model size and inference\ncomplexity of DGCNN are 42X and 1203X of those of Green-PointHop, respectively.\n","authors":["Min Zhang","Jintang Xue","Pranav Kadam","Hardik Prajapati","Shan Liu","C. -C. Jay Kuo"],"pdf_url":"https://arxiv.org/pdf/2303.10898v1.pdf","comment":"13 pages, 4 figures"},{"id":"http://arxiv.org/abs/2303.10881v1","updated":"2023-03-20T05:47:35Z","published":"2023-03-20T05:47:35Z","title":"Machine Learning Automated Approach for Enormous Synchrotron X-Ray\n  Diffraction Data Interpretation","summary":"  Manual analysis of XRD data is usually laborious and time consuming. The deep\nneural network (DNN) based models trained by synthetic XRD patterns are proved\nto be an automatic, accurate, and high throughput method to analysis common XRD\ndata collected from solid sample in ambient environment. However, it remains\nunknown that whether synthetic XRD based models are capable to solve u-XRD\nmapping data for in-situ experiments involving liquid phase exhibiting lower\nquality with significant artifacts. In this study, we collected u-XRD mapping\ndata from an LaCl3-calcite hydrothermal fluid system and trained two categories\nof models to solve the experimental XRD patterns. The models trained by\nsynthetic XRD patterns show low accuracy (as low as 64%) when solving\nexperimental u-XRD mapping data. The accuracy of the DNN models was\nsignificantly improved (90% or above) when training them with the dataset\ncontaining both synthetic and small number of labeled experimental u-XRD\npatterns. This study highlighted the importance of labeled experimental\npatterns on the training of DNN models to solve u-XRD mapping data from in-situ\nexperiments involving liquid phase.\n","authors":["Xiaodong Zhao","YiXuan Luo","Juejing Liu","Wenjun Liu","Kevin M. Rosso","Xiaofeng Guo","Tong Geng","Ang Li","Xin Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.10881v1.pdf","comment":"See link below for supporting information\n  https://docs.google.com/document/d/1m2SyaBDej4BhkWCA38GRXJe5Jd7Di7cp/edit?usp=sharing&ouid=108731997922646321851&rtpof=true&sd=true"},{"id":"http://arxiv.org/abs/2303.10880v1","updated":"2023-03-20T05:38:30Z","published":"2023-03-20T05:38:30Z","title":"Rotating without Seeing: Towards In-hand Dexterity through Touch","summary":"  Tactile information plays a critical role in human dexterity. It reveals\nuseful contact information that may not be inferred directly from vision. In\nfact, humans can even perform in-hand dexterous manipulation without using\nvision. Can we enable the same ability for the multi-finger robot hand? In this\npaper, we propose to perform in-hand object rotation using only touching\nwithout seeing the object. Instead of relying on precise tactile sensing in a\nsmall region, we introduce a new system design using dense binary force sensors\n(touch or no touch) overlaying one side of the whole robot hand (palm, finger\nlinks, fingertips). Such a design is low-cost, giving a larger coverage of the\nobject, and minimizing the Sim2Real gap at the same time. We train an in-hand\nrotation policy using Reinforcement Learning on diverse objects in simulation.\nRelying on touch-only sensing, we can directly deploy the policy in a real\nrobot hand and rotate novel objects that are not presented in training.\nExtensive ablations are performed on how tactile information help in-hand\nmanipulation. Our project is available at https://touchdexterity.github.io.\n","authors":["Zhao-Heng Yin","Binghao Huang","Yuzhe Qin","Qifeng Chen","Xiaolong Wang"],"pdf_url":"https://arxiv.org/pdf/2303.10880v1.pdf","comment":"Project page: https://touchdexterity.github.io"},{"id":"http://arxiv.org/abs/2211.15046v3","updated":"2023-03-20T05:18:49Z","published":"2022-11-28T04:08:55Z","title":"PCT-CycleGAN: Paired Complementary Temporal Cycle-Consistent Adversarial\n  Networks for Radar-Based Precipitation Nowcasting","summary":"  The precipitation nowcasting methods have been elaborated over the centuries\nbecause rain has a crucial impact on human life. Not only quantitative\nprecipitation forecast (QPF) models and convolutional long short-term memory\n(ConvLSTM), but also various sophisticated methods such as the latest MetNet-2\nare emerging. In this paper, we propose a paired complementary temporal\ncycle-consistent adversarial networks (PCT-CycleGAN) for radar-based\nprecipitation nowcasting, inspired by cycle-consistent adversarial networks\n(CycleGAN), which shows strong performance in image-to-image translation.\nPCT-CycleGAN generates temporal causality using two generator networks with\nforward and backward temporal dynamics in paired complementary cycles. Each\ngenerator network learns a huge number of one-to-one mappings about\ntime-dependent radar-based precipitation data to approximate a mapping function\nrepresenting the temporal dynamics in each direction. To create robust temporal\ncausality between paired complementary cycles, novel connection loss is\nproposed. The generator network learning forward temporal dynamics in\nPCT-CycleGAN generates radar-based precipitation data 10 minutes from the\ncurrent time. Also, it provides a reliable prediction of up to 2 hours with\niterative forecasting. The superiority of PCT-CycleGAN is demonstrated through\nqualitative and quantitative comparisons with several previous methods.\n","authors":["Jaeho Choi","Yura Kim","Kwang-Ho Kim","Sung-Hwa Jung","Ikhyun Cho"],"pdf_url":"https://arxiv.org/pdf/2211.15046v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10875v1","updated":"2023-03-20T05:18:31Z","published":"2023-03-20T05:18:31Z","title":"Hardware-Aware Graph Neural Network Automated Design for Edge Computing\n  Platforms","summary":"  Graph neural networks (GNNs) have emerged as a popular strategy for handling\nnon-Euclidean data due to their state-of-the-art performance. However, most of\nthe current GNN model designs mainly focus on task accuracy, lacking in\nconsidering hardware resources limitation and real-time requirements of edge\napplication scenarios. Comprehensive profiling of typical GNN models indicates\nthat their execution characteristics are significantly affected across\ndifferent computing platforms, which demands hardware awareness for efficient\nGNN designs. In this work, HGNAS is proposed as the first Hardware-aware Graph\nNeural Architecture Search framework targeting resource constraint edge\ndevices. By decoupling the GNN paradigm, HGNAS constructs a fine-grained design\nspace and leverages an efficient multi-stage search strategy to explore optimal\narchitectures within a few GPU hours. Moreover, HGNAS achieves hardware\nawareness during the GNN architecture design by leveraging a hardware\nperformance predictor, which could balance the GNN model accuracy and\nefficiency corresponding to the characteristics of targeted devices.\nExperimental results show that HGNAS can achieve about $10.6\\times$ speedup and\n$88.2\\%$ peak memory reduction with a negligible accuracy loss compared to\nDGCNN on various edge devices, including Nvidia RTX3080, Jetson TX2, Intel\ni7-8700K and Raspberry Pi 3B+.\n","authors":["Ao Zhou","Jianlei Yang","Yingjie Qi","Yumeng Shi","Tong Qiao","Weisheng Zhao","Chunming Hu"],"pdf_url":"https://arxiv.org/pdf/2303.10875v1.pdf","comment":"Accepted by DAC'23"},{"id":"http://arxiv.org/abs/2209.09004v3","updated":"2023-03-20T04:49:10Z","published":"2022-09-19T13:28:32Z","title":"EcoFormer: Energy-Saving Attention with Linear Complexity","summary":"  Transformer is a transformative framework that models sequential data and has\nachieved remarkable performance on a wide range of tasks, but with high\ncomputational and energy cost. To improve its efficiency, a popular choice is\nto compress the models via binarization which constrains the floating-point\nvalues into binary ones to save resource consumption owing to cheap bitwise\noperations significantly. However, existing binarization methods only aim at\nminimizing the information loss for the input distribution statistically, while\nignoring the pairwise similarity modeling at the core of the attention. To this\nend, we propose a new binarization paradigm customized to high-dimensional\nsoftmax attention via kernelized hashing, called EcoFormer, to map the original\nqueries and keys into low-dimensional binary codes in Hamming space. The\nkernelized hash functions are learned to match the ground-truth similarity\nrelations extracted from the attention map in a self-supervised way. Based on\nthe equivalence between the inner product of binary codes and the Hamming\ndistance as well as the associative property of matrix multiplication, we can\napproximate the attention in linear complexity by expressing it as a\ndot-product of binary codes. Moreover, the compact binary representations of\nqueries and keys enable us to replace most of the expensive multiply-accumulate\noperations in attention with simple accumulations to save considerable on-chip\nenergy footprint on edge devices. Extensive experiments on both vision and\nlanguage tasks show that EcoFormer consistently achieves comparable performance\nwith standard attentions while consuming much fewer resources. For example,\nbased on PVTv2-B0 and ImageNet-1K, Ecoformer achieves a 73% on-chip energy\nfootprint reduction with only a 0.33% performance drop compared to the standard\nattention. Code is available at https://github.com/ziplab/EcoFormer.\n","authors":["Jing Liu","Zizheng Pan","Haoyu He","Jianfei Cai","Bohan Zhuang"],"pdf_url":"https://arxiv.org/pdf/2209.09004v3.pdf","comment":"NeurIPS 2022 camera ready; First two authors contributed equally"},{"id":"http://arxiv.org/abs/2303.10859v1","updated":"2023-03-20T04:39:39Z","published":"2023-03-20T04:39:39Z","title":"Improved Sample Complexity for Reward-free Reinforcement Learning under\n  Low-rank MDPs","summary":"  In reward-free reinforcement learning (RL), an agent explores the environment\nfirst without any reward information, in order to achieve certain learning\ngoals afterwards for any given reward. In this paper we focus on reward-free RL\nunder low-rank MDP models, in which both the representation and linear weight\nvectors are unknown. Although various algorithms have been proposed for\nreward-free low-rank MDPs, the corresponding sample complexity is still far\nfrom being satisfactory. In this work, we first provide the first known sample\ncomplexity lower bound that holds for any algorithm under low-rank MDPs. This\nlower bound implies it is strictly harder to find a near-optimal policy under\nlow-rank MDPs than under linear MDPs. We then propose a novel model-based\nalgorithm, coined RAFFLE, and show it can both find an $\\epsilon$-optimal\npolicy and achieve an $\\epsilon$-accurate system identification via reward-free\nexploration, with a sample complexity significantly improving the previous\nresults. Such a sample complexity matches our lower bound in the dependence on\n$\\epsilon$, as well as on $K$ in the large $d$ regime, where $d$ and $K$\nrespectively denote the representation dimension and action space cardinality.\nFinally, we provide a planning algorithm (without further interaction with true\nenvironment) for RAFFLE to learn a near-accurate representation, which is the\nfirst known representation learning guarantee under the same setting.\n","authors":["Yuan Cheng","Ruiquan Huang","Jing Yang","Yingbin Liang"],"pdf_url":"https://arxiv.org/pdf/2303.10859v1.pdf","comment":"Accepted by ICLR 2023"},{"id":"http://arxiv.org/abs/2303.10856v1","updated":"2023-03-20T04:30:18Z","published":"2023-03-20T04:30:18Z","title":"Revisiting Realistic Test-Time Training: Sequential Inference and\n  Adaptation by Anchored Clustering Regularized Self-Training","summary":"  Deploying models on target domain data subject to distribution shift requires\nadaptation. Test-time training (TTT) emerges as a solution to this adaptation\nunder a realistic scenario where access to full source domain data is not\navailable, and instant inference on the target domain is required. Despite many\nefforts into TTT, there is a confusion over the experimental settings, thus\nleading to unfair comparisons. In this work, we first revisit TTT assumptions\nand categorize TTT protocols by two key factors. Among the multiple protocols,\nwe adopt a realistic sequential test-time training (sTTT) protocol, under which\nwe develop a test-time anchored clustering (TTAC) approach to enable stronger\ntest-time feature learning. TTAC discovers clusters in both source and target\ndomains and matches the target clusters to the source ones to improve\nadaptation. When source domain information is strictly absent (i.e.\nsource-free) we further develop an efficient method to infer source domain\ndistributions for anchored clustering. Finally, self-training~(ST) has\ndemonstrated great success in learning from unlabeled data and we empirically\nfigure out that applying ST alone to TTT is prone to confirmation bias.\nTherefore, a more effective TTT approach is introduced by regularizing\nself-training with anchored clustering, and the improved model is referred to\nas TTAC++. We demonstrate that, under all TTT protocols, TTAC++ consistently\noutperforms the state-of-the-art methods on five TTT datasets, including\ncorrupted target domain, selected hard samples, synthetic-to-real adaptation\nand adversarially attacked target domain. We hope this work will provide a fair\nbenchmarking of TTT methods, and future research should be compared within\nrespective protocols.\n","authors":["Yongyi Su","Xun Xu","Tianrui Li","Kui Jia"],"pdf_url":"https://arxiv.org/pdf/2303.10856v1.pdf","comment":"Test-time training, Self-training. arXiv admin note: substantial text\n  overlap with arXiv:2206.02721"},{"id":"http://arxiv.org/abs/2302.09465v2","updated":"2023-03-20T04:07:09Z","published":"2023-02-19T03:19:40Z","title":"Stochastic Generative Flow Networks","summary":"  Generative Flow Networks (or GFlowNets for short) are a family of\nprobabilistic agents that learn to sample complex combinatorial structures\nthrough the lens of \"inference as control\". They have shown great potential in\ngenerating high-quality and diverse candidates from a given energy landscape.\nHowever, existing GFlowNets can be applied only to deterministic environments,\nand fail in more general tasks with stochastic dynamics, which can limit their\napplicability. To overcome this challenge, this paper introduces Stochastic\nGFlowNets, a new algorithm that extends GFlowNets to stochastic environments.\nBy decomposing state transitions into two steps, Stochastic GFlowNets isolate\nenvironmental stochasticity and learn a dynamics model to capture it. Extensive\nexperimental results demonstrate that Stochastic GFlowNets offer significant\nadvantages over standard GFlowNets as well as MCMC- and RL-based approaches, on\na variety of standard benchmarks with stochastic dynamics.\n","authors":["Ling Pan","Dinghuai Zhang","Moksh Jain","Longbo Huang","Yoshua Bengio"],"pdf_url":"https://arxiv.org/pdf/2302.09465v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.10817v3","updated":"2023-03-20T03:39:51Z","published":"2022-04-22T16:53:39Z","title":"Reward Reports for Reinforcement Learning","summary":"  Building systems that are good for society in the face of complex societal\neffects requires a dynamic approach. Recent approaches to machine learning (ML)\ndocumentation have demonstrated the promise of discursive frameworks for\ndeliberation about these complexities. However, these developments have been\ngrounded in a static ML paradigm, leaving the role of feedback and\npost-deployment performance unexamined. Meanwhile, recent work in reinforcement\nlearning has shown that the effects of feedback and optimization objectives on\nsystem behavior can be wide-ranging and unpredictable. In this paper we sketch\na framework for documenting deployed and iteratively updated learning systems,\nwhich we call Reward Reports. Taking inspiration from various contributions to\nthe technical literature on reinforcement learning, we outline Reward Reports\nas living documents that track updates to design choices and assumptions behind\nwhat a particular automated system is optimizing for. They are intended to\ntrack dynamic phenomena arising from system deployment, rather than merely\nstatic properties of models or data. After presenting the elements of a Reward\nReport, we discuss a concrete example: Meta's BlenderBot 3 chatbot. Several\nothers for game-playing (DeepMind's MuZero), content recommendation\n(MovieLens), and traffic control (Project Flow) are included in the appendix.\n","authors":["Thomas Krendl Gilbert","Nathan Lambert","Sarah Dean","Tom Zick","Aaron Snoswell"],"pdf_url":"https://arxiv.org/pdf/2204.10817v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2110.02398v5","updated":"2023-03-20T03:15:14Z","published":"2021-10-05T23:07:12Z","title":"Approximate Newton policy gradient algorithms","summary":"  Policy gradient algorithms have been widely applied to Markov decision\nprocesses and reinforcement learning problems in recent years. Regularization\nwith various entropy functions is often used to encourage exploration and\nimprove stability. This paper proposes an approximate Newton method for the\npolicy gradient algorithm with entropy regularization. In the case of Shannon\nentropy, the resulting algorithm reproduces the natural policy gradient\nalgorithm. For other entropy functions, this method results in brand-new policy\ngradient algorithms. We prove that all these algorithms enjoy Newton-type\nquadratic convergence and that the corresponding gradient flow converges\nglobally to the optimal solution. We use synthetic and industrial-scale\nexamples to demonstrate that the proposed approximate Newton method typically\nconverges in single-digit iterations, often orders of magnitude faster than\nother state-of-the-art algorithms.\n","authors":["Haoya Li","Samarth Gupta","Hsiangfu Yu","Lexing Ying","Inderjit Dhillon"],"pdf_url":"https://arxiv.org/pdf/2110.02398v5.pdf","comment":"22 pages, 15 figures"},{"id":"http://arxiv.org/abs/2303.10840v1","updated":"2023-03-20T03:08:22Z","published":"2023-03-20T03:08:22Z","title":"Ref-NeuS: Ambiguity-Reduced Neural Implicit Surface Learning for\n  Multi-View Reconstruction with Reflection","summary":"  Neural implicit surface learning has shown significant progress in multi-view\n3D reconstruction, where an object is represented by multilayer perceptrons\nthat provide continuous implicit surface representation and view-dependent\nradiance. However, current methods often fail to accurately reconstruct\nreflective surfaces, leading to severe ambiguity. To overcome this issue, we\npropose Ref-NeuS, which aims to reduce ambiguity by attenuating the importance\nof reflective surfaces. Specifically, we utilize an anomaly detector to\nestimate an explicit reflection score with the guidance of multi-view context\nto localize reflective surfaces. Afterward, we design a reflection-aware\nphotometric loss that adaptively reduces ambiguity by modeling rendered color\nas a Gaussian distribution, with the reflection score representing the\nvariance. We show that together with a reflection direction-dependent radiance,\nour model achieves high-quality surface reconstruction on reflective surfaces\nand outperforms the state-of-the-arts by a large margin. Besides, our model is\nalso comparable on general surfaces.\n","authors":["Wenhang Ge","Tao Hu","Haoyu Zhao","Shu Liu","Ying-Cong Chen"],"pdf_url":"https://arxiv.org/pdf/2303.10840v1.pdf","comment":"Project webpage: https://g3956.github.io/"},{"id":"http://arxiv.org/abs/2303.10837v1","updated":"2023-03-20T02:44:35Z","published":"2023-03-20T02:44:35Z","title":"FedML-HE: An Efficient Homomorphic-Encryption-Based Privacy-Preserving\n  Federated Learning System","summary":"  Federated Learning (FL) enables machine learning model training on\ndistributed edge devices by aggregating local model updates rather than local\ndata. However, privacy concerns arise as the FL server's access to local model\nupdates can potentially reveal sensitive personal information by performing\nattacks like gradient inversion recovery. To address these concerns,\nprivacy-preserving methods, such as Homomorphic Encryption (HE)-based\napproaches, have been proposed. Despite HE's post-quantum security advantages,\nits applications suffer from impractical overheads. In this paper, we present\nFedML-HE, the first practical system for efficient HE-based secure federated\naggregation that provides a user/device-friendly deployment platform. FL-HE\nutilizes a novel universal overhead optimization scheme, significantly reducing\nboth computation and communication overheads during deployment while providing\ncustomizable privacy guarantees. Our optimized system demonstrates considerable\noverhead reduction, particularly for large models (e.g., ~10x reduction for\nHE-federated training of ResNet-50 and ~40x reduction for BERT), demonstrating\nthe potential for scalable HE-based FL deployment.\n","authors":["Weizhao Jin","Yuhang Yao","Shanshan Han","Carlee Joe-Wong","Srivatsan Ravi","Salman Avestimehr","Chaoyang He"],"pdf_url":"https://arxiv.org/pdf/2303.10837v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10834v1","updated":"2023-03-20T02:40:16Z","published":"2023-03-20T02:40:16Z","title":"Object-Centric Slot Diffusion","summary":"  Despite remarkable recent advances, making object-centric learning work for\ncomplex natural scenes remains the main challenge. The recent success of\nadopting the transformer-based image generative model in object-centric\nlearning suggests that having a highly expressive image generator is crucial\nfor dealing with complex scenes. In this paper, inspired by this observation,\nwe aim to answer the following question: can we benefit from the other pillar\nof modern deep generative models, i.e., the diffusion models, for\nobject-centric learning and what are the pros and cons of such a model? To this\nend, we propose a new object-centric learning model, Latent Slot Diffusion\n(LSD). LSD can be seen from two perspectives. From the perspective of\nobject-centric learning, it replaces the conventional slot decoders with a\nlatent diffusion model conditioned on the object slots. Conversely, from the\nperspective of diffusion models, it is the first unsupervised compositional\nconditional diffusion model which, unlike traditional diffusion models, does\nnot require supervised annotation such as a text description to learn to\ncompose. In experiments on various object-centric tasks, including the FFHQ\ndataset for the first time in this line of research, we demonstrate that LSD\nsignificantly outperforms the state-of-the-art transformer-based decoder,\nparticularly when the scene is more complex. We also show a superior quality in\nunsupervised compositional generation.\n","authors":["Jindong Jiang","Fei Deng","Gautam Singh","Sungjin Ahn"],"pdf_url":"https://arxiv.org/pdf/2303.10834v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.06405v2","updated":"2023-03-20T02:34:18Z","published":"2023-02-03T20:56:01Z","title":"An Optical XNOR-Bitcount Based Accelerator for Efficient Inference of\n  Binary Neural Networks","summary":"  Binary Neural Networks (BNNs) are increasingly preferred over full-precision\nConvolutional Neural Networks(CNNs) to reduce the memory and computational\nrequirements of inference processing with minimal accuracy drop. BNNs convert\nCNN model parameters to 1-bit precision, allowing inference of BNNs to be\nprocessed with simple XNOR and bitcount operations. This makes BNNs amenable to\nhardware acceleration. Several photonic integrated circuits (PICs) based BNN\naccelerators have been proposed. Although these accelerators provide remarkably\nhigher throughput and energy efficiency than their electronic counterparts, the\nutilized XNOR and bitcount circuits in these accelerators need to be further\nenhanced to improve their area, energy efficiency, and throughput. This paper\naims to fulfill this need. For that, we invent a single-MRR-based optical XNOR\ngate (OXG). Moreover, we present a novel design of bitcount circuit which we\nrefer to as Photo-Charge Accumulator (PCA). We employ multiple OXGs in a\ncascaded manner using dense wavelength division multiplexing (DWDM) and connect\nthem to the PCA, to forge a novel Optical XNOR-Bitcount based Binary Neural\nNetwork Accelerator (OXBNN). Our evaluation for the inference of four modern\nBNNs indicates that OXBNN provides improvements of up to 62x and 7.6x in\nframes-per-second (FPS) and FPS/W (energy efficiency), respectively, on\ngeometric mean over two PIC-based BNN accelerators from prior work. We\ndeveloped a transaction-level, event-driven python-based simulator for\nevaluation of accelerators (https://github.com/uky-UCAT/B_ONN_SIM).\n","authors":["Sairam Sri Vatsavai","Venkata Sai Praneeth Karempudi","Ishan Thakkar"],"pdf_url":"https://arxiv.org/pdf/2302.06405v2.pdf","comment":"To Appear at IEEE ISQED 2023"},{"id":"http://arxiv.org/abs/2303.10828v1","updated":"2023-03-20T02:02:50Z","published":"2023-03-20T02:02:50Z","title":"Data Might be Enough: Bridge Real-World Traffic Signal Control Using\n  Offline Reinforcement Learning","summary":"  Applying reinforcement learning (RL) to traffic signal control (TSC) has\nbecome a promising solution. However, most RL-based methods focus solely on\noptimization within simulators and give little thought to deployment issues in\nthe real world. Online RL-based methods, which require interaction with the\nenvironment, are limited in their interactions with the real-world environment.\nAdditionally, acquiring an offline dataset for offline RL is challenging in the\nreal world. Moreover, most real-world intersections prefer a cyclical phase\nstructure. To address these challenges, we propose: (1) a cyclical offline\ndataset (COD), designed based on common real-world scenarios to facilitate easy\ncollection; (2) an offline RL model called DataLight, capable of learning\nsatisfactory control strategies from the COD; and (3) a method called Arbitrary\nTo Cyclical (ATC), which can transform most RL-based methods into cyclical\nsignal control. Extensive experiments using real-world datasets on simulators\ndemonstrate that: (1) DataLight outperforms most existing methods and achieves\ncomparable results with the best-performing method; (2) introducing ATC into\nsome recent RL-based methods achieves satisfactory performance; and (3) COD is\nreliable, with DataLight remaining robust even with a small amount of data.\nThese results suggest that the cyclical offline dataset might be enough for\noffline RL for TSC. Our proposed methods make significant contributions to the\nTSC field and successfully bridge the gap between simulation experiments and\nreal-world applications. Our code is released on Github.\n","authors":["Liang Zhang","Jianming Deng"],"pdf_url":"https://arxiv.org/pdf/2303.10828v1.pdf","comment":"9 pages, 5 figures"},{"id":"http://arxiv.org/abs/2303.09863v2","updated":"2023-03-20T01:30:30Z","published":"2023-03-17T10:01:32Z","title":"Deep Nonparametric Estimation of Intrinsic Data Structures by Chart\n  Autoencoders: Generalization Error and Robustness","summary":"  Autoencoders have demonstrated remarkable success in learning low-dimensional\nlatent features of high-dimensional data across various applications. Assuming\nthat data are sampled near a low-dimensional manifold, we employ chart\nautoencoders, which encode data into low-dimensional latent features on a\ncollection of charts, preserving the topology and geometry of the data\nmanifold. Our paper establishes statistical guarantees on the generalization\nerror of chart autoencoders, and we demonstrate their denoising capabilities by\nconsidering $n$ noisy training samples, along with their noise-free\ncounterparts, on a $d$-dimensional manifold. By training autoencoders, we show\nthat chart autoencoders can effectively denoise the input data with normal\nnoise. We prove that, under proper network architectures, chart autoencoders\nachieve a squared generalization error in the order of $\\displaystyle\nn^{-\\frac{2}{d+2}}\\log^4 n$, which depends on the intrinsic dimension of the\nmanifold and only weakly depends on the ambient dimension and noise level. We\nfurther extend our theory on data with noise containing both normal and\ntangential components, where chart autoencoders still exhibit a denoising\neffect for the normal component. As a special case, our theory also applies to\nclassical autoencoders, as long as the data manifold has a global\nparametrization. Our results provide a solid theoretical foundation for the\neffectiveness of autoencoders, which is further validated through several\nnumerical experiments.\n","authors":["Hao Liu","Alex Havrilla","Rongjie Lai","Wenjing Liao"],"pdf_url":"https://arxiv.org/pdf/2303.09863v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.10211v2","updated":"2023-03-20T00:37:10Z","published":"2022-10-18T23:31:15Z","title":"A Catch-22 of Reservoir Computing","summary":"  Reservoir Computing (RC) is a simple and efficient model-free framework for\nforecasting the behavior of nonlinear dynamical systems from data. Here, we\nshow that there exist commonly-studied systems for which leading RC frameworks\nstruggle to learn the dynamics unless key information about the underlying\nsystem is already known. We focus on the important problem of basin prediction\n-- determining which attractor a system will converge to from its initial\nconditions. First, we show that the predictions of standard RC models (echo\nstate networks) depend critically on warm-up time, requiring a warm-up\ntrajectory containing almost the entire transient in order to identify the\ncorrect attractor even after being trained with optimal hyperparameters.\nAccordingly, we turn to Next-Generation Reservoir Computing (NGRC), an\nattractive variant of RC that requires negligible warm-up time. By\nincorporating the exact nonlinearities in the original equations, we show that\nNGRC can accurately reconstruct intricate and high-dimensional basins of\nattraction, even with sparse training data (e.g., a single transient\ntrajectory). Yet, a tiny uncertainty on the exact nonlinearity can already\nbreak NGRC, rendering the prediction accuracy no better than chance. Our\nresults highlight the challenges faced by data-driven methods in learning the\ndynamics of multistable systems and suggest potential avenues to make these\napproaches more robust.\n","authors":["Yuanzhao Zhang","Sean P. Cornelius"],"pdf_url":"https://arxiv.org/pdf/2210.10211v2.pdf","comment":"added new results on standard RC; expanded intro and discussion"},{"id":"http://arxiv.org/abs/2303.10800v1","updated":"2023-03-20T00:24:05Z","published":"2023-03-20T00:24:05Z","title":"A Global Model Approach to Robust Few-Shot SAR Automatic Target\n  Recognition","summary":"  In real-world scenarios, it may not always be possible to collect hundreds of\nlabeled samples per class for training deep learning-based SAR Automatic Target\nRecognition (ATR) models. This work specifically tackles the few-shot SAR ATR\nproblem, where only a handful of labeled samples may be available to support\nthe task of interest. Our approach is composed of two stages. In the first, a\nglobal representation model is trained via self-supervised learning on a large\npool of diverse and unlabeled SAR data. In the second stage, the global model\nis used as a fixed feature extractor and a classifier is trained to partition\nthe feature space given the few-shot support samples, while simultaneously\nbeing calibrated to detect anomalous inputs. Unlike competing approaches which\nrequire a pristine labeled dataset for pretraining via meta-learning, our\napproach learns highly transferable features from unlabeled data that have\nlittle-to-no relation to the downstream task. We evaluate our method in\nstandard and extended MSTAR operating conditions and find it to achieve high\naccuracy and robust out-of-distribution detection in many different few-shot\nsettings. Our results are particularly significant because they show the merit\nof a global model approach to SAR ATR, which makes minimal assumptions, and\nprovides many axes for extendability.\n","authors":["Nathan Inkawhich"],"pdf_url":"https://arxiv.org/pdf/2303.10800v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.03660v3","updated":"2023-03-20T00:22:11Z","published":"2021-12-07T12:43:05Z","title":"A generalization gap estimation for overparameterized models via the\n  Langevin functional variance","summary":"  This paper discusses the estimation of the generalization gap, the difference\nbetween generalization performance and training performance, for\noverparameterized models including neural networks. We first show that a\nfunctional variance, a key concept in defining a widely-applicable information\ncriterion, characterizes the generalization gap even in overparameterized\nsettings where a conventional theory cannot be applied. As the computational\ncost of the functional variance is expensive for the overparameterized models,\nwe propose an efficient approximation of the function variance, the Langevin\napproximation of the functional variance (Langevin FV). This method leverages\nonly the $1$st-order gradient of the squared loss function, without referencing\nthe $2$nd-order gradient; this ensures that the computation is efficient and\nthe implementation is consistent with gradient-based optimization algorithms.\nWe demonstrate the Langevin FV numerically by estimating the generalization\ngaps of overparameterized linear regression and non-linear neural network\nmodels, containing more than a thousand of parameters therein.\n","authors":["Akifumi Okuno","Keisuke Yano"],"pdf_url":"https://arxiv.org/pdf/2112.03660v3.pdf","comment":"40 pages, no figure, accepted to Journal of Computational and\n  Graphical Statistics"},{"id":"http://arxiv.org/abs/2206.04119v2","updated":"2023-03-20T00:22:03Z","published":"2022-06-08T18:35:08Z","title":"Diffusion probabilistic modeling of protein backbones in 3D for the\n  motif-scaffolding problem","summary":"  Construction of a scaffold structure that supports a desired motif,\nconferring protein function, shows promise for the design of vaccines and\nenzymes. But a general solution to this motif-scaffolding problem remains open.\nCurrent machine-learning techniques for scaffold design are either limited to\nunrealistically small scaffolds (up to length 20) or struggle to produce\nmultiple diverse scaffolds. We propose to learn a distribution over diverse and\nlonger protein backbone structures via an E(3)-equivariant graph neural\nnetwork. We develop SMCDiff to efficiently sample scaffolds from this\ndistribution conditioned on a given motif; our algorithm is the first to\ntheoretically guarantee conditional samples from a diffusion model in the\nlarge-compute limit. We evaluate our designed backbones by how well they align\nwith AlphaFold2-predicted structures. We show that our method can (1) sample\nscaffolds up to 80 residues and (2) achieve structurally diverse scaffolds for\na fixed motif.\n","authors":["Brian L. Trippe","Jason Yim","Doug Tischer","David Baker","Tamara Broderick","Regina Barzilay","Tommi Jaakkola"],"pdf_url":"https://arxiv.org/pdf/2206.04119v2.pdf","comment":"Appearing in ICLR 2023. Code available:\n  github.com/blt2114/ProtDiff_SMCDiff"},{"id":"http://arxiv.org/abs/2110.15829v5","updated":"2023-03-20T23:39:56Z","published":"2021-10-29T14:46:32Z","title":"Holistic Deep Learning","summary":"  This paper presents a novel holistic deep learning framework that\nsimultaneously addresses the challenges of vulnerability to input\nperturbations, overparametrization, and performance instability from different\ntrain-validation splits. The proposed framework holistically improves accuracy,\nrobustness, sparsity, and stability over standard deep learning models, as\ndemonstrated by extensive experiments on both tabular and image data sets. The\nresults are further validated by ablation experiments and SHAP value analysis,\nwhich reveal the interactions and trade-offs between the different evaluation\nmetrics. To support practitioners applying our framework, we provide a\nprescriptive approach that offers recommendations for selecting an appropriate\ntraining loss function based on their specific objectives. All the code to\nreproduce the results can be found at https://github.com/kimvc7/HDL.\n","authors":["Dimitris Bertsimas","Kimberly Villalobos Carballo","Léonard Boussioux","Michael Lingzhi Li","Alex Paskov","Ivan Paskov"],"pdf_url":"https://arxiv.org/pdf/2110.15829v5.pdf","comment":"Under review at Machine Learning"},{"id":"http://arxiv.org/abs/2303.11494v1","updated":"2023-03-20T23:19:05Z","published":"2023-03-20T23:19:05Z","title":"FlexVDW: A machine learning approach to account for protein flexibility\n  in ligand docking","summary":"  Most widely used ligand docking methods assume a rigid protein structure.\nThis leads to problems when the structure of the target protein deforms upon\nligand binding. In particular, the ligand's true binding pose is often scored\nvery unfavorably due to apparent clashes between ligand and protein atoms,\nwhich lead to extremely high values of the calculated van der Waals energy\nterm. Traditionally, this problem has been addressed by explicitly searching\nfor receptor conformations to account for the flexibility of the receptor in\nligand binding. Here we present a deep learning model trained to take receptor\nflexibility into account implicitly when predicting van der Waals energy. We\nshow that incorporating this machine-learned energy term into a\nstate-of-the-art physics-based scoring function improves small molecule ligand\npose prediction results in cases with substantial protein deformation, without\ndegrading performance in cases with minimal protein deformation. This work\ndemonstrates the feasibility of learning effects of protein flexibility on\nligand binding without explicitly modeling changes in protein structure.\n","authors":["Patricia Suriana","Joseph M. Paggi","Ron O. Dror"],"pdf_url":"https://arxiv.org/pdf/2303.11494v1.pdf","comment":"Published at the MLDD workshop, International Conference on Learning\n  Representations (ICLR) 2023"},{"id":"http://arxiv.org/abs/2303.08955v2","updated":"2023-03-20T22:35:49Z","published":"2023-03-15T21:55:07Z","title":"Large-scale End-of-Life Prediction of Hard Disks in Distributed\n  Datacenters","summary":"  On a daily basis, data centers process huge volumes of data backed by the\nproliferation of inexpensive hard disks. Data stored in these disks serve a\nrange of critical functional needs from financial, and healthcare to aerospace.\nAs such, premature disk failure and consequent loss of data can be\ncatastrophic. To mitigate the risk of failures, cloud storage providers perform\ncondition-based monitoring and replace hard disks before they fail. By\nestimating the remaining useful life of hard disk drives, one can predict the\ntime-to-failure of a particular device and replace it at the right time,\nensuring maximum utilization whilst reducing operational costs. In this work,\nlarge-scale predictive analyses are performed using severely skewed health\nstatistics data by incorporating customized feature engineering and a suite of\nsequence learners. Past work suggests using LSTMs as an excellent approach to\npredicting remaining useful life. To this end, we present an encoder-decoder\nLSTM model where the context gained from understanding health statistics\nsequences aid in predicting an output sequence of the number of days remaining\nbefore a disk potentially fails. The models developed in this work are trained\nand tested across an exhaustive set of all of the 10 years of S.M.A.R.T. health\ndata in circulation from Backblaze and on a wide variety of disk instances. It\ncloses the knowledge gap on what full-scale training achieves on thousands of\ndevices and advances the state-of-the-art by providing tangible metrics for\nevaluation and generalization for practitioners looking to extend their\nworkflow to all years of health data in circulation across disk manufacturers.\nThe encoder-decoder LSTM posted an RMSE of 0.83 during training and 0.86 during\ntesting over the exhaustive 10 year data while being able to generalize\ncompetitively over other drives from the Seagate family.\n","authors":["Rohan Mohapatra","Austin Coursey","Saptarshi Sengupta"],"pdf_url":"https://arxiv.org/pdf/2303.08955v2.pdf","comment":"8 pages, 9 figures and 6 tables"},{"id":"http://arxiv.org/abs/2303.11483v1","updated":"2023-03-20T22:34:57Z","published":"2023-03-20T22:34:57Z","title":"Automatic Measures for Evaluating Generative Design Methods for\n  Architects","summary":"  The recent explosion of high-quality image-to-image methods has prompted\ninterest in applying image-to-image methods towards artistic and design tasks.\nOf interest for architects is to use these methods to generate design proposals\nfrom conceptual sketches, usually hand-drawn sketches that are quickly\ndeveloped and can embody a design intent. More specifically, instantiating a\nsketch into a visual that can be used to elicit client feedback is typically a\ntime consuming task, and being able to speed up this iteration time is\nimportant. While the body of work in generative methods has been impressive,\nthere has been a mismatch between the quality measures used to evaluate the\noutputs of these systems and the actual expectations of architects. In\nparticular, most recent image-based works place an emphasis on realism of\ngenerated images. While important, this is one of several criteria architects\nlook for. In this work, we describe the expectations architects have for design\nproposals from conceptual sketches, and identify corresponding automated\nmetrics from the literature. We then evaluate several image-to-image generative\nmethods that may address these criteria and examine their performance across\nthese metrics. From these results, we identify certain challenges with\nhand-drawn conceptual sketches and describe possible future avenues of\ninvestigation to address them.\n","authors":["Eric Yeh","Briland Hitaj","Vidyasagar Sadhu","Anirban Roy","Takuma Nakabayashi","Yoshito Tsuji"],"pdf_url":"https://arxiv.org/pdf/2303.11483v1.pdf","comment":"4 pages, 6 figures, 1 table"},{"id":"http://arxiv.org/abs/2211.07484v2","updated":"2023-03-20T22:13:29Z","published":"2022-11-14T16:08:44Z","title":"Contextual Bandits with Packing and Covering Constraints: A Modular\n  Lagrangian Approach via Regression","summary":"  We consider a variant of contextual bandits in which the algorithm consumes\nmultiple resources subject to linear constraints on total consumption. This\nproblem generalizes contextual bandits with knapsacks (CBwK), allowing for\npacking and covering constraints, as well as positive and negative resource\nconsumption. We present a new algorithm that is simple, computationally\nefficient, and admits vanishing regret. It is statistically optimal for CBwK\nwhen an algorithm must stop once some constraint is violated. Our algorithm\nbuilds on LagrangeBwK (Immorlica et al., FOCS 2019) , a Lagrangian-based\ntechnique for CBwK, and SquareCB (Foster and Rakhlin, ICML 2020), a\nregression-based technique for contextual bandits. Our analysis leverages the\ninherent modularity of both techniques.\n","authors":["Aleksandrs Slivkins","Karthik Abinav Sankararaman","Dylan Foster"],"pdf_url":"https://arxiv.org/pdf/2211.07484v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.08892v2","updated":"2023-03-20T22:04:06Z","published":"2022-07-18T19:06:18Z","title":"D3G: Learning Multi-robot Coordination from Demonstrations","summary":"  This paper develops a Distributed Differentiable Dynamic Game (D3G)\nframework, which enables learning multi-robot coordination from demonstrations.\nWe represent multi-robot coordination as a dynamic game, where the behavior of\na robot is dictated by its own dynamics and objective that also depends on\nothers' behavior. The coordination thus can be adapted by tuning the objective\nand dynamics of each robot. The proposed D3G enables each robot to\nautomatically tune its individual dynamics and objectives in a distributed\nmanner by minimizing the mismatch between its trajectory and demonstrations.\nThis learning framework features a new design, including a forward-pass, where\nall robots collaboratively seek Nash equilibrium of a game, and a\nbackward-pass, where gradients are propagated via the communication graph. We\ntest the D3G in simulation with two types of robots given different task\nconfigurations. The results validate the capability of D3G for learning\nmulti-robot coordination from demonstrations.\n","authors":["Xuan Wang","Yizhi Zhou","Wanxin Jin"],"pdf_url":"https://arxiv.org/pdf/2207.08892v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11473v1","updated":"2023-03-20T22:03:44Z","published":"2023-03-20T22:03:44Z","title":"Sandwiched Video Compression: Efficiently Extending the Reach of\n  Standard Codecs with Neural Wrappers","summary":"  We propose sandwiched video compression -- a video compression system that\nwraps neural networks around a standard video codec. The sandwich framework\nconsists of a neural pre- and post-processor with a standard video codec\nbetween them. The networks are trained jointly to optimize a rate-distortion\nloss function with the goal of significantly improving over the standard codec\nin various compression scenarios. End-to-end training in this setting requires\na differentiable proxy for the standard video codec, which incorporates\ntemporal processing with motion compensation, inter/intra mode decisions, and\nin-loop filtering. We propose differentiable approximations to key video codec\ncomponents and demonstrate that the neural codes of the sandwich lead to\nsignificantly better rate-distortion performance compared to compressing the\noriginal frames of the input video in two important scenarios. When\ntransporting high-resolution video via low-resolution HEVC, the sandwich system\nobtains 6.5 dB improvements over standard HEVC. More importantly, using the\nwell-known perceptual similarity metric, LPIPS, we observe $~30 \\%$\nimprovements in rate at the same quality over HEVC. Last but not least we show\nthat pre- and post-processors formed by very modestly-parameterized,\nlight-weight networks can closely approximate these results.\n","authors":["Berivan Isik","Onur G. Guleryuz","Danhang Tang","Jonathan Taylor","Philip A. Chou"],"pdf_url":"https://arxiv.org/pdf/2303.11473v1.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2303.11470v1","updated":"2023-03-20T21:54:30Z","published":"2023-03-20T21:54:30Z","title":"Did You Train on My Dataset? Towards Public Dataset Protection with\n  Clean-Label Backdoor Watermarking","summary":"  The huge supporting training data on the Internet has been a key factor in\nthe success of deep learning models. However, this abundance of\npublic-available data also raises concerns about the unauthorized exploitation\nof datasets for commercial purposes, which is forbidden by dataset licenses. In\nthis paper, we propose a backdoor-based watermarking approach that serves as a\ngeneral framework for safeguarding public-available data. By inserting a small\nnumber of watermarking samples into the dataset, our approach enables the\nlearning model to implicitly learn a secret function set by defenders. This\nhidden function can then be used as a watermark to track down third-party\nmodels that use the dataset illegally. Unfortunately, existing backdoor\ninsertion methods often entail adding arbitrary and mislabeled data to the\ntraining set, leading to a significant drop in performance and easy detection\nby anomaly detection algorithms. To overcome this challenge, we introduce a\nclean-label backdoor watermarking framework that uses imperceptible\nperturbations to replace mislabeled samples. As a result, the watermarking\nsamples remain consistent with the original labels, making them difficult to\ndetect. Our experiments on text, image, and audio datasets demonstrate that the\nproposed framework effectively safeguards datasets with minimal impact on\noriginal task performance. We also show that adding just 1% of watermarking\nsamples can inject a traceable watermarking function and that our watermarking\nsamples are stealthy and look benign upon visual inspection.\n","authors":["Ruixiang Tang","Qizhang Feng","Ninghao Liu","Fan Yang","Xia Hu"],"pdf_url":"https://arxiv.org/pdf/2303.11470v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11464v1","updated":"2023-03-20T21:45:30Z","published":"2023-03-20T21:45:30Z","title":"Seven open problems in applied combinatorics","summary":"  We present and discuss seven different open problems in applied\ncombinatorics. The application areas relevant to this compilation include\nquantum computing, algorithmic differentiation, topological data analysis,\niterative methods, hypergraph cut algorithms, and power systems.\n","authors":["Sinan G. Aksoy","Ryan Bennink","Yuzhou Chen","José Frías","Yulia R. Gel","Bill Kay","Uwe Naumann","Carlos Ortiz Marrero","Anthony V. Petyuk","Sandip Roy","Ignacio Segovia-Dominguez","Nate Veldt","Stephen J. Young"],"pdf_url":"https://arxiv.org/pdf/2303.11464v1.pdf","comment":"43 pages, 5 figures"},{"id":"http://arxiv.org/abs/2210.09306v2","updated":"2023-03-20T21:33:41Z","published":"2022-10-17T17:59:49Z","title":"Mitigating Covertly Unsafe Text within Natural Language Systems","summary":"  An increasingly prevalent problem for intelligent technologies is text\nsafety, as uncontrolled systems may generate recommendations to their users\nthat lead to injury or life-threatening consequences. However, the degree of\nexplicitness of a generated statement that can cause physical harm varies. In\nthis paper, we distinguish types of text that can lead to physical harm and\nestablish one particularly underexplored category: covertly unsafe text. Then,\nwe further break down this category with respect to the system's information\nand discuss solutions to mitigate the generation of text in each of these\nsubcategories. Ultimately, our work defines the problem of covertly unsafe\nlanguage that causes physical harm and argues that this subtle yet dangerous\nissue needs to be prioritized by stakeholders and regulators. We highlight\nmitigation strategies to inspire future researchers to tackle this challenging\nproblem and help improve safety within smart systems.\n","authors":["Alex Mei","Anisha Kabir","Sharon Levy","Melanie Subbiah","Emily Allaway","John Judge","Desmond Patton","Bruce Bimber","Kathleen McKeown","William Yang Wang"],"pdf_url":"https://arxiv.org/pdf/2210.09306v2.pdf","comment":"In Findings of the 2022 Conference on Empirical Methods in Natural\n  Language Processing"},{"id":"http://arxiv.org/abs/2303.11459v1","updated":"2023-03-20T21:31:51Z","published":"2023-03-20T21:31:51Z","title":"Fairness-Aware Graph Filter Design","summary":"  Graphs are mathematical tools that can be used to represent complex\nreal-world systems, such as financial markets and social networks. Hence,\nmachine learning (ML) over graphs has attracted significant attention recently.\nHowever, it has been demonstrated that ML over graphs amplifies the already\nexisting bias towards certain under-represented groups in various\ndecision-making problems due to the information aggregation over biased graph\nstructures. Faced with this challenge, in this paper, we design a fair graph\nfilter that can be employed in a versatile manner for graph-based learning\ntasks. The design of the proposed filter is based on a bias analysis and its\noptimality in mitigating bias compared to its fairness-agnostic counterpart is\nestablished. Experiments on real-world networks for node classification\ndemonstrate the efficacy of the proposed filter design in mitigating bias,\nwhile attaining similar utility and better stability compared to baseline\nalgorithms.\n","authors":["O. Deniz Kose","Yanning Shen","Gonzalo Mateos"],"pdf_url":"https://arxiv.org/pdf/2303.11459v1.pdf","comment":"6 pages, 1 figure, 2 tables"},{"id":"http://arxiv.org/abs/2209.12849v3","updated":"2023-03-20T21:25:23Z","published":"2022-09-26T16:58:00Z","title":"AirTrack: Onboard Deep Learning Framework for Long-Range Aircraft\n  Detection and Tracking","summary":"  Detect-and-Avoid (DAA) capabilities are critical for safe operations of\nunmanned aircraft systems (UAS). This paper introduces, AirTrack, a real-time\nvision-only detect and tracking framework that respects the size, weight, and\npower (SWaP) constraints of sUAS systems. Given the low Signal-to-Noise ratios\n(SNR) of far away aircraft, we propose using full resolution images in a deep\nlearning framework that aligns successive images to remove ego-motion. The\naligned images are then used downstream in cascaded primary and secondary\nclassifiers to improve detection and tracking performance on multiple metrics.\nWe show that AirTrack outperforms state-of-the art baselines on the Amazon\nAirborne Object Tracking (AOT) Dataset. Multiple real world flight tests with a\nCessna 182 interacting with general aviation traffic and additional\nnear-collision flight tests with a Bell helicopter flying towards a UAS in a\ncontrolled setting showcase that the proposed approach satisfies the newly\nintroduced ASTM F3442/F3442M standard for DAA. Empirical evaluations show that\nour system has a probability of track of more than 95% up to a range of 700m.\nVideo available at https://youtu.be/H3lL_Wjxjpw .\n","authors":["Sourish Ghosh","Jay Patrikar","Brady Moon","Milad Moghassem Hamidi","Sebastian Scherer"],"pdf_url":"https://arxiv.org/pdf/2209.12849v3.pdf","comment":"7 pages, 5 figures, ICRA 2023"},{"id":"http://arxiv.org/abs/2303.11455v1","updated":"2023-03-20T21:14:06Z","published":"2023-03-20T21:14:06Z","title":"Large Language Models and Simple, Stupid Bugs","summary":"  With the advent of powerful neural language models, AI-based systems to\nassist developers in coding tasks are becoming widely available; Copilot is one\nsuch system. Copilot uses Codex, a large language model (LLM), to complete code\nconditioned on a preceding \"prompt\". Codex, however, is trained on public\nGitHub repositories, viz., on code that may include bugs and vulnerabilities.\nPrevious studies [1], [2] show Codex reproduces vulnerabilities seen in\ntraining. In this study, we examine how prone Codex is to generate an\ninteresting bug category, single statement bugs, commonly referred to as\nsimple, stupid bugs or SStuBs in the MSR community. We find that Codex and\nsimilar LLMs do help avoid some SStuBs, but do produce known, verbatim SStuBs\nas much as 2x as likely than known, verbatim correct code. We explore the\nconsequences of the Codex generated SStuBs and propose avoidance strategies\nthat suggest the possibility of reducing the production of known, verbatim\nSStubs, and increase the possibility of producing known, verbatim fixes.\n","authors":["Kevin Jesse","Toufique Ahmed","Premkumar T. Devanbu","Emily Morgan"],"pdf_url":"https://arxiv.org/pdf/2303.11455v1.pdf","comment":"Accepted at International Conference on Mining Software Repositories\n  (MSR-2023)"},{"id":"http://arxiv.org/abs/2202.02892v3","updated":"2023-03-20T21:06:06Z","published":"2022-02-07T00:15:03Z","title":"Lossy Compression of Noisy Data for Private and Data-Efficient Learning","summary":"  Storage-efficient privacy-preserving learning is crucial due to increasing\namounts of sensitive user data required for modern learning tasks. We propose a\nframework for reducing the storage cost of user data while at the same time\nproviding privacy guarantees, without essential loss in the utility of the data\nfor learning. Our method comprises noise injection followed by lossy\ncompression. We show that, when appropriately matching the lossy compression to\nthe distribution of the added noise, the compressed examples converge, in\ndistribution, to that of the noise-free training data as the sample size of the\ntraining data (or the dimension of the training data) increases. In this sense,\nthe utility of the data for learning is essentially maintained, while reducing\nstorage and privacy leakage by quantifiable amounts. We present experimental\nresults on the CelebA dataset for gender classification and find that our\nsuggested pipeline delivers in practice on the promise of the theory: the\nindividuals in the images are unrecognizable (or less recognizable, depending\non the noise level), overall storage of the data is substantially reduced, with\nno essential loss (and in some cases a slight boost) to the classification\naccuracy. As an added bonus, our experiments suggest that our method yields a\nsubstantial boost to robustness in the face of adversarial test data.\n","authors":["Berivan Isik","Tsachy Weissman"],"pdf_url":"https://arxiv.org/pdf/2202.02892v3.pdf","comment":"Published at the IEEE Journal on Selected Areas in Information Theory\n  (JSAIT). Preliminary version was presented at the IEEE International\n  Symposium on Information Theory (ISIT), 2022, with a slightly different\n  title, \"Learning under Storage and Privacy Constraints.\""},{"id":"http://arxiv.org/abs/2303.11454v1","updated":"2023-03-20T21:05:47Z","published":"2023-03-20T21:05:47Z","title":"How (Implicit) Regularization of ReLU Neural Networks Characterizes the\n  Learned Function -- Part II: the Multi-D Case of Two Layers with Random First\n  Layer","summary":"  Randomized neural networks (randomized NNs), where only the terminal layer's\nweights are optimized constitute a powerful model class to reduce computational\ntime in training the neural network model. At the same time, these models\ngeneralize surprisingly well in various regression and classification tasks. In\nthis paper, we give an exact macroscopic characterization (i.e., a\ncharacterization in function space) of the generalization behavior of\nrandomized, shallow NNs with ReLU activation (RSNs). We show that RSNs\ncorrespond to a generalized additive model (GAM)-typed regression in which\ninfinitely many directions are considered: the infinite generalized additive\nmodel (IGAM). The IGAM is formalized as solution to an optimization problem in\nfunction space for a specific regularization functional and a fairly general\nloss. This work is an extension to multivariate NNs of prior work, where we\nshowed how wide RSNs with ReLU activation behave like spline regression under\ncertain conditions and if the input is one-dimensional.\n","authors":["Jakob Heiss","Josef Teichmann","Hanna Wutte"],"pdf_url":"https://arxiv.org/pdf/2303.11454v1.pdf","comment":"16 pages + appendix"},{"id":"http://arxiv.org/abs/2303.11453v1","updated":"2023-03-20T21:05:44Z","published":"2023-03-20T21:05:44Z","title":"Greedy Pruning with Group Lasso Provably Generalizes for Matrix Sensing\n  and Neural Networks with Quadratic Activations","summary":"  Pruning schemes have been widely used in practice to reduce the complexity of\ntrained models with a massive number of parameters. Several practical studies\nhave shown that pruning an overparameterized model and fine-tuning generalizes\nwell to new samples. Although the above pipeline, which we refer to as pruning\n+ fine-tuning, has been extremely successful in lowering the complexity of\ntrained models, there is very little known about the theory behind this\nsuccess. In this paper we address this issue by investigating the pruning +\nfine-tuning framework on the overparameterized matrix sensing problem, with the\nground truth denoted $U_\\star \\in \\mathbb{R}^{d \\times r}$ and the\noverparameterized model $U \\in \\mathbb{R}^{d \\times k}$ with $k \\gg r$. We\nstudy the approximate local minima of the empirical mean square error,\naugmented with a smooth version of a group Lasso regularizer, $\\sum_{i=1}^k \\|\nU e_i \\|_2$ and show that pruning the low $\\ell_2$-norm columns results in a\nsolution $U_{\\text{prune}}$ which has the minimum number of columns $r$, yet is\nclose to the ground truth in training loss. Initializing the subsequent\nfine-tuning phase from $U_{\\text{prune}}$, the resulting solution converges\nlinearly to a generalization error of $O(\\sqrt{rd/n})$ ignoring lower order\nterms, which is statistically optimal. While our analysis provides insights\ninto the role of regularization in pruning, we also show that running gradient\ndescent in the absence of regularization results in models which {are not\nsuitable for greedy pruning}, i.e., many columns could have their $\\ell_2$ norm\ncomparable to that of the maximum. Lastly, we extend our results for the\ntraining and pruning of two-layer neural networks with quadratic activation\nfunctions. Our results provide the first rigorous insights on why greedy\npruning + fine-tuning leads to smaller models which also generalize well.\n","authors":["Nived Rajaraman"," Devvrit","Aryan Mokhtari","Kannan Ramchandran"],"pdf_url":"https://arxiv.org/pdf/2303.11453v1.pdf","comment":"60 pages, 2 figures"},{"id":"http://arxiv.org/abs/2303.11449v1","updated":"2023-03-20T20:49:57Z","published":"2023-03-20T20:49:57Z","title":"Bias mitigation techniques in image classification: fair machine\n  learning in human heritage collections","summary":"  A major problem with using automated classification systems is that if they\nare not engineered correctly and with fairness considerations, they could be\ndetrimental to certain populations. Furthermore, while engineers have developed\ncutting-edge technologies for image classification, there is still a gap in the\napplication of these models in human heritage collections, where data sets\nusually consist of low-quality pictures of people with diverse ethnicity,\ngender, and age. In this work, we evaluate three bias mitigation techniques\nusing two state-of-the-art neural networks, Xception and EfficientNet, for\ngender classification. Moreover, we explore the use of transfer learning using\na fair data set to overcome the training data scarcity. We evaluated the\neffectiveness of the bias mitigation pipeline on a cultural heritage collection\nof photographs from the 19th and 20th centuries, and we used the FairFace data\nset for the transfer learning experiments. After the evaluation, we found that\ntransfer learning is a good technique that allows better performance when\nworking with a small data set. Moreover, the fairest classifier was found to be\naccomplished using transfer learning, threshold change, re-weighting and image\naugmentation as bias mitigation methods.\n","authors":["Dalia Ortiz Pablo","Sushruth Badri","Erik Norén","Christoph Nötzli"],"pdf_url":"https://arxiv.org/pdf/2303.11449v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11448v1","updated":"2023-03-20T20:49:08Z","published":"2023-03-20T20:49:08Z","title":"Geometrical aspects of lattice gauge equivariant convolutional neural\n  networks","summary":"  Lattice gauge equivariant convolutional neural networks (L-CNNs) are a\nframework for convolutional neural networks that can be applied to non-Abelian\nlattice gauge theories without violating gauge symmetry. We demonstrate how\nL-CNNs can be equipped with global group equivariance. This allows us to extend\nthe formulation to be equivariant not just under translations but under global\nlattice symmetries such as rotations and reflections. Additionally, we provide\na geometric formulation of L-CNNs and show how convolutions in L-CNNs arise as\na special case of gauge equivariant neural networks on SU($N$) principal\nbundles.\n","authors":["Jimmy Aronsson","David I. Müller","Daniel Schuh"],"pdf_url":"https://arxiv.org/pdf/2303.11448v1.pdf","comment":"22 pages"},{"id":"http://arxiv.org/abs/2211.09769v2","updated":"2023-03-20T20:47:14Z","published":"2022-11-17T18:47:50Z","title":"DeepSense 6G: A Large-Scale Real-World Multi-Modal Sensing and\n  Communication Dataset","summary":"  This article presents the DeepSense 6G dataset, which is a large-scale\ndataset based on real-world measurements of co-existing multi-modal sensing and\ncommunication data. The DeepSense 6G dataset is built to advance deep learning\nresearch in a wide range of applications in the intersection of multi-modal\nsensing, communication, and positioning. This article provides a detailed\noverview of the DeepSense dataset structure, adopted testbeds, data collection\nand processing methodology, deployment scenarios, and example applications,\nwith the objective of facilitating the adoption and reproducibility of\nmulti-modal sensing and communication datasets.\n","authors":["Ahmed Alkhateeb","Gouranga Charan","Tawfik Osman","Andrew Hredzak","João Morais","Umut Demirhan","Nikhil Srinivas"],"pdf_url":"https://arxiv.org/pdf/2211.09769v2.pdf","comment":"The dataset is available on the DeepSense 6G website\n  http://deepsense6g.net/"},{"id":"http://arxiv.org/abs/2302.14808v2","updated":"2023-03-20T20:39:45Z","published":"2023-02-28T17:58:54Z","title":"Opto-UNet: Optimized UNet for Segmentation of Varicose Veins in Optical\n  Coherence Tomography","summary":"  Human veins are important for carrying the blood from the body-parts to the\nheart. The improper functioning of the human veins may arise from several\nvenous diseases. Varicose vein is one such disease wherein back flow of blood\ncan occur, often resulting in increased venous pressure or restricted blood\nflow due to changes in the structure of vein. To examine the functional\ncharacteristics of the varicose vein, it is crucial to study the physical and\nbio mechanical properties of the vein. This work proposes a segmentation model\nOpto-UNet, for segmenting the venous wall structure. Optical Coherence\nTomography system is used to acquire images of varicose vein. As the extracted\nvein is not uniform in shape, hence adequate method of segmentation is required\nto segment the venous wall. Opto-UNet model is based on the U-Net architecture\nwherein a new block is integrated into the architecture, employing atrous and\nseparable convolution to extract spatially wide-range and separable features\nmaps for attaining advanced performance. Furthermore, the depth wise separable\nconvolution significantly reduces the complexity of the network by optimizing\nthe number of parameters. The model achieves accuracy of 0.9830, sensitivity of\n0.8425 and specificity of 0.9980 using 8.54 million number of parameters. These\nresults indicate that model is highly adequate in segmenting the varicose vein\nwall without deteriorating the segmentation quality along with reduced\ncomplexity\n","authors":["Maryam Viqar","Violeta Madjarova","Vipul Baghel","Elena Stoykova"],"pdf_url":"https://arxiv.org/pdf/2302.14808v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.07444v2","updated":"2023-03-20T20:36:48Z","published":"2023-02-15T03:27:55Z","title":"A Case Study on Designing Evaluations of ML Explanations with Simulated\n  User Studies","summary":"  When conducting user studies to ascertain the usefulness of model\nexplanations in aiding human decision-making, it is important to use real-world\nuse cases, data, and users. However, this process can be resource-intensive,\nallowing only a limited number of explanation methods to be evaluated.\nSimulated user evaluations (SimEvals), which use machine learning models as a\nproxy for human users, have been proposed as an intermediate step to select\npromising explanation methods. In this work, we conduct the first SimEvals on a\nreal-world use case to evaluate whether explanations can better support\nML-assisted decision-making in e-commerce fraud detection. We study whether\nSimEvals can corroborate findings from a user study conducted in this fraud\ndetection context. In particular, we find that SimEvals suggest that all\nconsidered explainers are equally performant, and none beat a baseline without\nexplanations -- this matches the conclusions of the original user study. Such\ncorrespondences between our results and the original user study provide initial\nevidence in favor of using SimEvals before running user studies. We also\nexplore the use of SimEvals as a cheap proxy to explore an alternative user\nstudy set-up. We hope that this work motivates further study of when and how\nSimEvals should be used to aid in the design of real-world evaluations.\n","authors":["Ada Martin","Valerie Chen","Sérgio Jesus","Pedro Saleiro"],"pdf_url":"https://arxiv.org/pdf/2302.07444v2.pdf","comment":"9 pages, 2 figures. Will appear in ICLR 2023's TrustML-(un)Limited\n  workshop"},{"id":"http://arxiv.org/abs/2303.11435v1","updated":"2023-03-20T20:28:17Z","published":"2023-03-20T20:28:17Z","title":"Inversion by Direct Iteration: An Alternative to Denoising Diffusion for\n  Image Restoration","summary":"  Inversion by Direct Iteration (InDI) is a new formulation for supervised\nimage restoration that avoids the so-called ``regression to the mean'' effect\nand produces more realistic and detailed images than existing regression-based\nmethods. It does this by gradually improving image quality in small steps,\nsimilar to generative denoising diffusion models.\n  Image restoration is an ill-posed problem where multiple high-quality images\nare plausible reconstructions of a given low-quality input. Therefore, the\noutcome of a single step regression model is typically an aggregate of all\npossible explanations, therefore lacking details and realism. % The main\nadvantage of InDI is that it does not try to predict the clean target image in\na single step but instead gradually improves the image in small steps,\nresulting in better perceptual quality.\n  While generative denoising diffusion models also work in small steps, our\nformulation is distinct in that it does not require knowledge of any analytic\nform of the degradation process. Instead, we directly learn an iterative\nrestoration process from low-quality and high-quality paired examples. InDI can\nbe applied to virtually any image degradation, given paired training data. In\nconditional denoising diffusion image restoration the denoising network\ngenerates the restored image by repeatedly denoising an initial image of pure\nnoise, conditioned on the degraded input. Contrary to conditional denoising\nformulations, InDI directly proceeds by iteratively restoring the input\nlow-quality image, producing high-quality results on a variety of image\nrestoration tasks, including motion and out-of-focus deblurring,\nsuper-resolution, compression artifact removal, and denoising.\n","authors":["Mauricio Delbracio","Peyman Milanfar"],"pdf_url":"https://arxiv.org/pdf/2303.11435v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11434v1","updated":"2023-03-20T20:27:11Z","published":"2023-03-20T20:27:11Z","title":"ResDTA: Predicting Drug-Target Binding Affinity Using Residual Skip\n  Connections","summary":"  The discovery of novel drug target (DT) interactions is an important step in\nthe drug development process. The majority of computer techniques for\npredicting DT interactions have focused on binary classification, with the goal\nof determining whether or not a DT pair interacts. Protein ligand interactions,\non the other hand, assume a continuous range of binding strength values, also\nknown as binding affinity, and forecasting this value remains a difficulty. As\nthe amount of affinity data in DT knowledge-bases grows, advanced learning\ntechniques such as deep learning architectures can be used to predict binding\naffinities. In this paper, we present a deep-learning-based methodology for\npredicting DT binding affinities using just sequencing information from both\ntargets and drugs. The results show that the proposed deep learning-based model\nthat uses the 1D representations of targets and drugs is an effective approach\nfor drug target binding affinity prediction and it does not require additional\nchemical domain knowledge to work with. The model in which high-level\nrepresentations of a drug and a target are constructed via CNNs that uses\nresidual skip connections and also with an additional stream to create a\nhigh-level combined representation of the drug-target pair achieved the best\nConcordance Index (CI) performance in one of the largest benchmark datasets,\noutperforming the recent state-of-the-art method AttentionDTA and many other\nmachine-learning and deep-learning based baseline methods for DT binding\naffinity prediction that uses the 1D representations of targets and drugs.\n","authors":["Partho Ghosh","Md. Aynal Haque"],"pdf_url":"https://arxiv.org/pdf/2303.11434v1.pdf","comment":"40 pages, 10 figures, 2 tables. arXiv admin note: substantial text\n  overlap with arXiv:1801.10193, arXiv:1902.04166 by other authors"},{"id":"http://arxiv.org/abs/2303.11428v1","updated":"2023-03-20T20:18:04Z","published":"2023-03-20T20:18:04Z","title":"Lamarr: LHCb ultra-fast simulation based on machine learning models\n  deployed within Gauss","summary":"  About 90% of the computing resources available to the LHCb experiment has\nbeen spent to produce simulated data samples for Run 2 of the Large Hadron\nCollider at CERN. The upgraded LHCb detector will be able to collect larger\ndata samples, requiring many more simulated events to analyze the data to be\ncollected in Run 3. Simulation is a key necessity of analysis to interpret\nsignal vs background and measure efficiencies. The needed simulation will far\nexceed the pledged resources, requiring an evolution in technologies and\ntechniques to produce these simulated data samples. In this contribution, we\ndiscuss Lamarr, a Gaudi-based framework to speed-up the simulation production\nparametrizing both the detector response and the reconstruction algorithms of\nthe LHCb experiment. Deep Generative Models powered by several algorithms and\nstrategies are employed to effectively parametrize the high-level response of\nthe single components of the LHCb detector, encoding within neural networks the\nexperimental errors and uncertainties introduced in the detection and\nreconstruction phases. Where possible, models are trained directly on real\ndata, statistically subtracting any background components through weights\napplication. Embedding Lamarr in the general LHCb Gauss Simulation framework\nallows to combine its execution with any of the available generators in a\nseamless way. The resulting software package enables a simulation process\ncompletely independent of the Detailed Simulation used to date.\n","authors":["Matteo Barbetti"],"pdf_url":"https://arxiv.org/pdf/2303.11428v1.pdf","comment":"Under review in Journal of Physics: Conference Series (ACAT 2022)"},{"id":"http://arxiv.org/abs/2209.14390v6","updated":"2023-03-20T20:05:33Z","published":"2022-09-28T19:28:54Z","title":"Neighborhood Gradient Clustering: An Efficient Decentralized Learning\n  Method for Non-IID Data Distributions","summary":"  Decentralized learning over distributed datasets can have significantly\ndifferent data distributions across the agents. The current state-of-the-art\ndecentralized algorithms mostly assume the data distributions to be Independent\nand Identically Distributed. This paper focuses on improving decentralized\nlearning over non-IID data. We propose \\textit{Neighborhood Gradient Clustering\n(NGC)}, a novel decentralized learning algorithm that modifies the local\ngradients of each agent using self- and cross-gradient information.\nCross-gradients for a pair of neighboring agents are the derivatives of the\nmodel parameters of an agent with respect to the dataset of the other agent. In\nparticular, the proposed method replaces the local gradients of the model with\nthe weighted mean of the self-gradients, model-variant cross-gradients\n(derivatives of the neighbors' parameters with respect to the local dataset),\nand data-variant cross-gradients (derivatives of the local model with respect\nto its neighbors' datasets). The data-variant cross-gradients are aggregated\nthrough an additional communication round without breaking the privacy\nconstraints. Further, we present \\textit{CompNGC}, a compressed version of\n\\textit{NGC} that reduces the communication overhead by $32 \\times$. We\ntheoretically analyze the convergence rate of the proposed algorithm and\ndemonstrate its efficiency over non-IID data sampled from {various vision and\nlanguage} datasets trained. Our experiments demonstrate that \\textit{NGC} and\n\\textit{CompNGC} outperform (by $0-6\\%$) the existing SoTA decentralized\nlearning algorithm over non-IID data with significantly less compute and memory\nrequirements. Further, our experiments show that the model-variant\ncross-gradient information available locally at each agent can improve the\nperformance over non-IID data by $1-35\\%$ without additional communication\ncost.\n","authors":["Sai Aparna Aketi","Sangamesh Kodge","Kaushik Roy"],"pdf_url":"https://arxiv.org/pdf/2209.14390v6.pdf","comment":"29 pages, 5 figures, 16 tables. arXiv admin note: text overlap with\n  arXiv:2103.02051 by other authors"}],"Multimedia":[{"id":"http://arxiv.org/abs/2207.12261v2","updated":"2023-03-20T13:34:48Z","published":"2022-07-06T13:56:48Z","title":"GraphCFC: A Directed Graph based Cross-modal Feature Complementation\n  Approach for Multimodal Conversational Emotion Recognition","summary":"  Emotion Recognition in Conversation (ERC) plays a significant part in\nHuman-Computer Interaction (HCI) systems since it can provide empathetic\nservices. Multimodal ERC can mitigate the drawbacks of uni-modal approaches.\nRecently, Graph Neural Networks (GNNs) have been widely used in a variety of\nfields due to their superior performance in relation modeling. In multimodal\nERC, GNNs are capable of extracting both long-distance contextual information\nand inter-modal interactive information. Unfortunately, since existing methods\nsuch as MMGCN directly fuse multiple modalities, redundant information may be\ngenerated and diverse information may be lost. In this work, we present a\ndirected Graph based Cross-modal Feature Complementation (GraphCFC) module that\ncan efficiently model contextual and interactive information. GraphCFC\nalleviates the problem of heterogeneity gap in multimodal fusion by utilizing\nmultiple subspace extractors and Pair-wise Cross-modal Complementary (PairCC)\nstrategy. We extract various types of edges from the constructed graph for\nencoding, thus enabling GNNs to extract crucial contextual and interactive\ninformation more accurately when performing message passing. Furthermore, we\ndesign a GNN structure called GAT-MLP, which can provide a new unified network\nframework for multimodal learning. The experimental results on two benchmark\ndatasets show that our GraphCFC outperforms the state-of-the-art (SOTA)\napproaches.\n","authors":["Jiang Li","Xiaoping Wang","Guoqing Lv","Zhigang Zeng"],"pdf_url":"https://arxiv.org/pdf/2207.12261v2.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2303.09858v2","updated":"2023-03-20T13:00:49Z","published":"2023-03-17T09:37:41Z","title":"MedLocker: A Transferable Adversarial Watermarking for Preventing\n  Unauthorized Analysis of Medical Image Dataset","summary":"  The collection of medical image datasets is a demanding and laborious process\nthat requires significant resources. Furthermore, these medical datasets may\ncontain personally identifiable information, necessitating measures to ensure\nthat unauthorized access is prevented. Failure to do so could violate the\nintellectual property rights of the dataset owner and potentially compromise\nthe privacy of patients. As a result, safeguarding medical datasets and\npreventing unauthorized usage by AI diagnostic models is a pressing challenge.\nTo address this challenge, we propose a novel visible adversarial watermarking\nmethod for medical image copyright protection, called MedLocker. Our approach\ninvolves continuously optimizing the position and transparency of a watermark\nlogo, which reduces the performance of the target model, leading to incorrect\npredictions. Importantly, we ensure that our method minimizes the impact on\nclinical visualization by constraining watermark positions using semantical\nmasks (WSM), which are bounding boxes of lesion regions based on semantic\nsegmentation. To ensure the transferability of the watermark across different\nmodels, we verify the cross-model transferability of the watermark generated on\na single model. Additionally, we generate a unique watermark parameter list\neach time, which can be used as a certification to verify the authorization. We\nevaluate the performance of MedLocker on various mainstream backbones and\nvalidate the feasibility of adversarial watermarking for copyright protection\non two widely-used diabetic retinopathy detection datasets. Our results\ndemonstrate that MedLocker can effectively protect the copyright of medical\ndatasets and prevent unauthorized users from analyzing medical images with AI\ndiagnostic models.\n","authors":["Bangzheng Pu","Xingxing Wei","Shiji Zhao","Huazhu Fu"],"pdf_url":"https://arxiv.org/pdf/2303.09858v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.08453v2","updated":"2023-03-20T10:52:26Z","published":"2023-02-16T17:56:08Z","title":"T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for\n  Text-to-Image Diffusion Models","summary":"  The incredible generative ability of large-scale text-to-image (T2I) models\nhas demonstrated strong power of learning complex structures and meaningful\nsemantics. However, relying solely on text prompts cannot fully take advantage\nof the knowledge learned by the model, especially when flexible and accurate\ncontrolling (e.g., color and structure) is needed. In this paper, we aim to\n``dig out\" the capabilities that T2I models have implicitly learned, and then\nexplicitly use them to control the generation more granularly. Specifically, we\npropose to learn simple and lightweight T2I-Adapters to align internal\nknowledge in T2I models with external control signals, while freezing the\noriginal large T2I models. In this way, we can train various adapters according\nto different conditions, achieving rich control and editing effects in the\ncolor and structure of the generation results. Further, the proposed\nT2I-Adapters have attractive properties of practical value, such as\ncomposability and generalization ability. Extensive experiments demonstrate\nthat our T2I-Adapter has promising generation quality and a wide range of\napplications.\n","authors":["Chong Mou","Xintao Wang","Liangbin Xie","Yanze Wu","Jian Zhang","Zhongang Qi","Ying Shan","Xiaohu Qie"],"pdf_url":"https://arxiv.org/pdf/2302.08453v2.pdf","comment":"Tech Report. GitHub: https://github.com/TencentARC/T2I-Adapter"},{"id":"http://arxiv.org/abs/2303.10961v1","updated":"2023-03-20T09:37:41Z","published":"2023-03-20T09:37:41Z","title":"LFACon: Introducing Anglewise Attention to No-Reference Quality\n  Assessment in Light Field Space","summary":"  Light field imaging can capture both the intensity information and the\ndirection information of light rays. It naturally enables a\nsix-degrees-of-freedom viewing experience and deep user engagement in virtual\nreality. Compared to 2D image assessment, light field image quality assessment\n(LFIQA) needs to consider not only the image quality in the spatial domain but\nalso the quality consistency in the angular domain. However, there is a lack of\nmetrics to effectively reflect the angular consistency and thus the angular\nquality of a light field image (LFI). Furthermore, the existing LFIQA metrics\nsuffer from high computational costs due to the excessive data volume of LFIs.\nIn this paper, we propose a novel concept of \"anglewise attention\" by\nintroducing a multihead self-attention mechanism to the angular domain of an\nLFI. This mechanism better reflects the LFI quality. In particular, we propose\nthree new attention kernels, including anglewise self-attention, anglewise grid\nattention, and anglewise central attention. These attention kernels can realize\nangular self-attention, extract multiangled features globally or selectively,\nand reduce the computational cost of feature extraction. By effectively\nincorporating the proposed kernels, we further propose our light field\nattentional convolutional neural network (LFACon) as an LFIQA metric. Our\nexperimental results show that the proposed LFACon metric significantly\noutperforms the state-of-the-art LFIQA metrics. For the majority of distortion\ntypes, LFACon attains the best performance with lower complexity and less\ncomputational time.\n","authors":["Qiang Qu","Xiaoming Chen","Yuk Ying Chung","Weidong Cai"],"pdf_url":"https://arxiv.org/pdf/2303.10961v1.pdf","comment":"Accepted for IEEE VR 2023 (TVCG Special Issues) (Early Access)"},{"id":"http://arxiv.org/abs/2207.06983v3","updated":"2023-03-20T07:01:49Z","published":"2022-07-14T15:06:37Z","title":"Multitrack Music Transformer","summary":"  Existing approaches for generating multitrack music with transformer models\nhave been limited in terms of the number of instruments, the length of the\nmusic segments and slow inference. This is partly due to the memory\nrequirements of the lengthy input sequences necessitated by existing\nrepresentations. In this work, we propose a new multitrack music representation\nthat allows a diverse set of instruments while keeping a short sequence length.\nOur proposed Multitrack Music Transformer (MMT) achieves comparable performance\nwith state-of-the-art systems, landing in between two recently proposed models\nin a subjective listening test, while achieving substantial speedups and memory\nreductions over both, making the method attractive for real time improvisation\nor near real time creative applications. Further, we propose a new measure for\nanalyzing musical self-attention and show that the trained model attends more\nto notes that form a consonant interval with the current note and to notes that\nare 4N beats away from the current step.\n","authors":["Hao-Wen Dong","Ke Chen","Shlomo Dubnov","Julian McAuley","Taylor Berg-Kirkpatrick"],"pdf_url":"https://arxiv.org/pdf/2207.06983v3.pdf","comment":"Accepted by ICASSP 2023. Audio samples available at\n  https://salu133445.github.io/mmt/ . Source code available at\n  https://github.com/salu133445/mmt"},{"id":"http://arxiv.org/abs/2303.08536v2","updated":"2023-03-20T07:01:45Z","published":"2023-03-15T11:29:36Z","title":"Watch or Listen: Robust Audio-Visual Speech Recognition with Visual\n  Corruption Modeling and Reliability Scoring","summary":"  This paper deals with Audio-Visual Speech Recognition (AVSR) under multimodal\ninput corruption situations where audio inputs and visual inputs are both\ncorrupted, which is not well addressed in previous research directions.\nPrevious studies have focused on how to complement the corrupted audio inputs\nwith the clean visual inputs with the assumption of the availability of clean\nvisual inputs. However, in real life, clean visual inputs are not always\naccessible and can even be corrupted by occluded lip regions or noises. Thus,\nwe firstly analyze that the previous AVSR models are not indeed robust to the\ncorruption of multimodal input streams, the audio and the visual inputs,\ncompared to uni-modal models. Then, we design multimodal input corruption\nmodeling to develop robust AVSR models. Lastly, we propose a novel AVSR\nframework, namely Audio-Visual Reliability Scoring module (AV-RelScore), that\nis robust to the corrupted multimodal inputs. The AV-RelScore can determine\nwhich input modal stream is reliable or not for the prediction and also can\nexploit the more reliable streams in prediction. The effectiveness of the\nproposed method is evaluated with comprehensive experiments on popular\nbenchmark databases, LRS2 and LRS3. We also show that the reliability scores\nobtained by AV-RelScore well reflect the degree of corruption and make the\nproposed model focus on the reliable multimodal representations.\n","authors":["Joanna Hong","Minsu Kim","Jeongsoo Choi","Yong Man Ro"],"pdf_url":"https://arxiv.org/pdf/2303.08536v2.pdf","comment":"Accepted at CVPR 2023. Implementation available:\n  https://github.com/joannahong/AV-RelScore"},{"id":"http://arxiv.org/abs/2209.05653v4","updated":"2023-03-20T00:53:24Z","published":"2022-09-13T00:01:23Z","title":"Semantic2Graph: Graph-based Multi-modal Feature Fusion for Action\n  Segmentation in Videos","summary":"  Video action segmentation and recognition tasks have been widely applied in\nmany fields. Most previous studies employ large-scale, high computational\nvisual models to understand videos comprehensively. However, few studies\ndirectly employ the graph model to reason about the video. The graph model\nprovides the benefits of fewer parameters, low computational cost, a large\nreceptive field, and flexible neighborhood message aggregation. In this paper,\nwe present a graph-based method named Semantic2Graph, to turn the video action\nsegmentation and recognition problem into node classification of graphs. To\npreserve fine-grained relations in videos, we construct the graph structure of\nvideos at the frame-level and design three types of edges: temporal, semantic,\nand self-loop. We combine visual, structural, and semantic features as node\nattributes. Semantic edges are used to model long-term spatio-temporal\nrelations, while the semantic features are the embedding of the label-text\nbased on the textual prompt. A Graph Neural Networks (GNNs) model is used to\nlearn multi-modal feature fusion. Experimental results show that Semantic2Graph\nachieves improvement on GTEA and 50Salads, compared to the state-of-the-art\nresults. Multiple ablation experiments further confirm the effectiveness of\nsemantic features in improving model performance, and semantic edges enable\nSemantic2Graph to capture long-term dependencies at a low cost.\n","authors":["Junbin Zhang","Pei-Hsuan Tsai","Meng-Hsun Tsai"],"pdf_url":"https://arxiv.org/pdf/2209.05653v4.pdf","comment":"13 pages, 3 figures, 9 tables. This paper was submitted to Springer"},{"id":"http://arxiv.org/abs/2303.11473v1","updated":"2023-03-20T22:03:44Z","published":"2023-03-20T22:03:44Z","title":"Sandwiched Video Compression: Efficiently Extending the Reach of\n  Standard Codecs with Neural Wrappers","summary":"  We propose sandwiched video compression -- a video compression system that\nwraps neural networks around a standard video codec. The sandwich framework\nconsists of a neural pre- and post-processor with a standard video codec\nbetween them. The networks are trained jointly to optimize a rate-distortion\nloss function with the goal of significantly improving over the standard codec\nin various compression scenarios. End-to-end training in this setting requires\na differentiable proxy for the standard video codec, which incorporates\ntemporal processing with motion compensation, inter/intra mode decisions, and\nin-loop filtering. We propose differentiable approximations to key video codec\ncomponents and demonstrate that the neural codes of the sandwich lead to\nsignificantly better rate-distortion performance compared to compressing the\noriginal frames of the input video in two important scenarios. When\ntransporting high-resolution video via low-resolution HEVC, the sandwich system\nobtains 6.5 dB improvements over standard HEVC. More importantly, using the\nwell-known perceptual similarity metric, LPIPS, we observe $~30 \\%$\nimprovements in rate at the same quality over HEVC. Last but not least we show\nthat pre- and post-processors formed by very modestly-parameterized,\nlight-weight networks can closely approximate these results.\n","authors":["Berivan Isik","Onur G. Guleryuz","Danhang Tang","Jonathan Taylor","Philip A. Chou"],"pdf_url":"https://arxiv.org/pdf/2303.11473v1.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2303.11470v1","updated":"2023-03-20T21:54:30Z","published":"2023-03-20T21:54:30Z","title":"Did You Train on My Dataset? Towards Public Dataset Protection with\n  Clean-Label Backdoor Watermarking","summary":"  The huge supporting training data on the Internet has been a key factor in\nthe success of deep learning models. However, this abundance of\npublic-available data also raises concerns about the unauthorized exploitation\nof datasets for commercial purposes, which is forbidden by dataset licenses. In\nthis paper, we propose a backdoor-based watermarking approach that serves as a\ngeneral framework for safeguarding public-available data. By inserting a small\nnumber of watermarking samples into the dataset, our approach enables the\nlearning model to implicitly learn a secret function set by defenders. This\nhidden function can then be used as a watermark to track down third-party\nmodels that use the dataset illegally. Unfortunately, existing backdoor\ninsertion methods often entail adding arbitrary and mislabeled data to the\ntraining set, leading to a significant drop in performance and easy detection\nby anomaly detection algorithms. To overcome this challenge, we introduce a\nclean-label backdoor watermarking framework that uses imperceptible\nperturbations to replace mislabeled samples. As a result, the watermarking\nsamples remain consistent with the original labels, making them difficult to\ndetect. Our experiments on text, image, and audio datasets demonstrate that the\nproposed framework effectively safeguards datasets with minimal impact on\noriginal task performance. We also show that adding just 1% of watermarking\nsamples can inject a traceable watermarking function and that our watermarking\nsamples are stealthy and look benign upon visual inspection.\n","authors":["Ruixiang Tang","Qizhang Feng","Ninghao Liu","Fan Yang","Xia Hu"],"pdf_url":"https://arxiv.org/pdf/2303.11470v1.pdf","comment":null}]},"2023-03-19T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2303.10794v1","updated":"2023-03-19T23:41:04Z","published":"2023-03-19T23:41:04Z","title":"PheME: A deep ensemble framework for improving phenotype prediction from\n  multi-modal data","summary":"  Detailed phenotype information is fundamental to accurate diagnosis and risk\nestimation of diseases. As a rich source of phenotype information, electronic\nhealth records (EHRs) promise to empower diagnostic variant interpretation.\nHowever, how to accurately and efficiently extract phenotypes from the\nheterogeneous EHR data remains a challenge. In this work, we present PheME, an\nEnsemble framework using Multi-modality data of structured EHRs and\nunstructured clinical notes for accurate Phenotype prediction. Firstly, we\nemploy multiple deep neural networks to learn reliable representations from the\nsparse structured EHR data and redundant clinical notes. A multi-modal model\nthen aligns multi-modal features onto the same latent space to predict\nphenotypes. Secondly, we leverage ensemble learning to combine outputs from\nsingle-modal models and multi-modal models to improve phenotype predictions. We\nchoose seven diseases to evaluate the phenotyping performance of the proposed\nframework. Experimental results show that using multi-modal data significantly\nimproves phenotype prediction in all diseases, the proposed ensemble learning\nframework can further boost the performance.\n","authors":["Shenghan Zhang","Haoxuan Li","Ruixiang Tang","Sirui Ding","Laila Rasmy","Degui Zhi","Na Zou","Xia Hu"],"pdf_url":"https://arxiv.org/pdf/2303.10794v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10782v1","updated":"2023-03-19T22:15:05Z","published":"2023-03-19T22:15:05Z","title":"On the Importance of Signer Overlap for Sign Language Detection","summary":"  Sign language detection, identifying if someone is signing or not, is\nbecoming crucially important for its applications in remote conferencing\nsoftware and for selecting useful sign data for training sign language\nrecognition or translation tasks. We argue that the current benchmark data sets\nfor sign language detection estimate overly positive results that do not\ngeneralize well due to signer overlap between train and test partitions. We\nquantify this with a detailed analysis of the effect of signer overlap on\ncurrent sign detection benchmark data sets. Comparing accuracy with and without\noverlap on the DGS corpus and Signing in the Wild, we observed a relative\ndecrease in accuracy of 4.17% and 6.27%, respectively. Furthermore, we propose\nnew data set partitions that are free of overlap and allow for more realistic\nperformance assessment. We hope this work will contribute to improving the\naccuracy and generalization of sign language detection systems.\n","authors":["Abhilash Pal","Stephan Huber","Cyrine Chaabani","Alessandro Manzotti","Oscar Koller"],"pdf_url":"https://arxiv.org/pdf/2303.10782v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.04088v2","updated":"2023-03-19T20:52:21Z","published":"2022-12-08T05:46:32Z","title":"LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large\n  Language Models","summary":"  This study focuses on using large language models (LLMs) as a planner for\nembodied agents that can follow natural language instructions to complete\ncomplex tasks in a visually-perceived environment. The high data cost and poor\nsample efficiency of existing methods hinders the development of versatile\nagents that are capable of many tasks and can learn new tasks quickly. In this\nwork, we propose a novel method, LLM-Planner, that harnesses the power of large\nlanguage models to do few-shot planning for embodied agents. We further propose\na simple but effective way to enhance LLMs with physical grounding to generate\nand update plans that are grounded in the current environment. Experiments on\nthe ALFRED dataset show that our method can achieve very competitive few-shot\nperformance: Despite using less than 0.5% of paired training data, LLM-Planner\nachieves competitive performance with recent baselines that are trained using\nthe full training data. Existing methods can barely complete any task\nsuccessfully under the same few-shot setting. Our work opens the door for\ndeveloping versatile and sample-efficient embodied agents that can quickly\nlearn many tasks. Website: https://dki-lab.github.io/LLM-Planner\n","authors":["Chan Hee Song","Jiaman Wu","Clayton Washington","Brian M. Sadler","Wei-Lun Chao","Yu Su"],"pdf_url":"https://arxiv.org/pdf/2212.04088v2.pdf","comment":"14 pages, 5 figures"},{"id":"http://arxiv.org/abs/2303.10699v1","updated":"2023-03-19T16:07:42Z","published":"2023-03-19T16:07:42Z","title":"FVQA 2.0: Introducing Adversarial Samples into Fact-based Visual\n  Question Answering","summary":"  The widely used Fact-based Visual Question Answering (FVQA) dataset contains\nvisually-grounded questions that require information retrieval using common\nsense knowledge graphs to answer. It has been observed that the original\ndataset is highly imbalanced and concentrated on a small portion of its\nassociated knowledge graph. We introduce FVQA 2.0 which contains adversarial\nvariants of test questions to address this imbalance. We show that systems\ntrained with the original FVQA train sets can be vulnerable to adversarial\nsamples and we demonstrate an augmentation scheme to reduce this vulnerability\nwithout human annotations.\n","authors":["Weizhe Lin","Zhilin Wang","Bill Byrne"],"pdf_url":"https://arxiv.org/pdf/2303.10699v1.pdf","comment":"Accepted to EACL 2023 Findings"},{"id":"http://arxiv.org/abs/2303.10659v1","updated":"2023-03-19T13:47:56Z","published":"2023-03-19T13:47:56Z","title":"COVID-19 event extraction from Twitter via extractive question answering\n  with continuous prompts","summary":"  As COVID-19 ravages the world, social media analytics could augment\ntraditional surveys in assessing how the pandemic evolves and capturing\nconsumer chatter that could help healthcare agencies in addressing it. This\ntypically involves mining disclosure events that mention testing positive for\nthe disease or discussions surrounding perceptions and beliefs in preventative\nor treatment options. The 2020 shared task on COVID-19 event extraction\n(conducted as part of the W-NUT workshop during the EMNLP conference)\nintroduced a new Twitter dataset for benchmarking event extraction from\nCOVID-19 tweets. In this paper, we cast the problem of event extraction as\nextractive question answering using recent advances in continuous prompting in\nlanguage models. On the shared task test dataset, our approach leads to over 5%\nabsolute micro-averaged F1-score improvement over prior best results, across\nall COVID-19 event slots. Our ablation study shows that continuous prompts have\na major impact on the eventual performance.\n","authors":["Yuhang Jiang","Ramakanth Kavuluru"],"pdf_url":"https://arxiv.org/pdf/2303.10659v1.pdf","comment":"Accepted to appear in MEDINFO 2023. Dataset:\n  https://github.com/viczong/extract_COVID19_events_from_Twitter; Code:\n  https://github.com/bionlproc/twitter-covid-QA-extraction"},{"id":"http://arxiv.org/abs/2210.07316v3","updated":"2023-03-19T13:37:01Z","published":"2022-10-13T19:42:08Z","title":"MTEB: Massive Text Embedding Benchmark","summary":"  Text embeddings are commonly evaluated on a small set of datasets from a\nsingle task not covering their possible applications to other tasks. It is\nunclear whether state-of-the-art embeddings on semantic textual similarity\n(STS) can be equally well applied to other tasks like clustering or reranking.\nThis makes progress in the field difficult to track, as various models are\nconstantly being proposed without proper evaluation. To solve this problem, we\nintroduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding\ntasks covering a total of 58 datasets and 112 languages. Through the\nbenchmarking of 33 models on MTEB, we establish the most comprehensive\nbenchmark of text embeddings to date. We find that no particular text embedding\nmethod dominates across all tasks. This suggests that the field has yet to\nconverge on a universal text embedding method and scale it up sufficiently to\nprovide state-of-the-art results on all embedding tasks. MTEB comes with\nopen-source code and a public leaderboard at\nhttps://github.com/embeddings-benchmark/mteb.\n","authors":["Niklas Muennighoff","Nouamane Tazi","Loïc Magne","Nils Reimers"],"pdf_url":"https://arxiv.org/pdf/2210.07316v3.pdf","comment":"24 pages, 14 tables, 6 figures"},{"id":"http://arxiv.org/abs/2302.06337v3","updated":"2023-03-19T13:01:47Z","published":"2023-02-13T13:14:23Z","title":"Learning from Noisy Crowd Labels with Logics","summary":"  This paper explores the integration of symbolic logic knowledge into deep\nneural networks for learning from noisy crowd labels. We introduce Logic-guided\nLearning from Noisy Crowd Labels (Logic-LNCL), an EM-alike iterative logic\nknowledge distillation framework that learns from both noisy labeled data and\nlogic rules of interest. Unlike traditional EM methods, our framework contains\na ``pseudo-E-step'' that distills from the logic rules a new type of learning\ntarget, which is then used in the ``pseudo-M-step'' for training the\nclassifier. Extensive evaluations on two real-world datasets for text sentiment\nclassification and named entity recognition demonstrate that the proposed\nframework improves the state-of-the-art and provides a new solution to learning\nfrom noisy crowd labels.\n","authors":["Zhijun Chen","Hailong Sun","Haoqian He","Pengpeng Chen"],"pdf_url":"https://arxiv.org/pdf/2302.06337v3.pdf","comment":"12 pages, 7 figures, accepted by ICDE-2023"},{"id":"http://arxiv.org/abs/2212.12061v2","updated":"2023-03-19T12:10:02Z","published":"2022-12-22T22:27:26Z","title":"MN-DS: A Multilabeled News Dataset for News Articles Hierarchical\n  Classification","summary":"  This article presents a dataset of 10,917 news articles with hierarchical\nnews categories collected between January 1st 2019, and December 31st 2019. We\nmanually labelled the articles based on a hierarchical taxonomy with 17\nfirst-level and 109 second-level categories. This dataset can be used to train\nmachine learning models for automatically classifying news articles by topic.\nThis dataset can be helpful for researchers working on news structuring,\nclassification, and predicting future events based on released news.\n","authors":["Alina Petukhova","Nuno Fachada"],"pdf_url":"https://arxiv.org/pdf/2212.12061v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.08745v3","updated":"2023-03-19T11:53:20Z","published":"2023-01-20T08:51:36Z","title":"Is ChatGPT A Good Translator? Yes With GPT-4 As The Engine","summary":"  This report provides a preliminary evaluation of ChatGPT for machine\ntranslation, including translation prompt, multilingual translation, and\ntranslation robustness. We adopt the prompts advised by ChatGPT to trigger its\ntranslation ability and find that the candidate prompts generally work well and\nshow minor performance differences. By evaluating on a number of benchmark test\nsets, we find that ChatGPT performs competitively with commercial translation\nproducts (e.g., Google Translate) on high-resource European languages but lags\nbehind significantly on low-resource or distant languages. For distant\nlanguages, we explore an interesting strategy named $\\mathbf{pivot~prompting}$\nthat asks ChatGPT to translate the source sentence into a high-resource pivot\nlanguage before into the target language, which improves the translation\nperformance significantly. As for the translation robustness, ChatGPT does not\nperform as well as the commercial systems on biomedical abstracts or Reddit\ncomments but exhibits good results on spoken language. With the launch of the\nGPT-4 engine, the translation performance of ChatGPT is significantly boosted,\nbecoming comparable to commercial translation products, even for distant\nlanguages. In other words,\n$\\mathbf{ChatGPT~has~already~become~a~good~translator!}$ Scripts and data:\nhttps://github.com/wxjiao/Is-ChatGPT-A-Good-Translator\n","authors":["Wenxiang Jiao","Wenxuan Wang","Jen-tse Huang","Xing Wang","Zhaopeng Tu"],"pdf_url":"https://arxiv.org/pdf/2301.08745v3.pdf","comment":"8 pages; added GPT-3 data statistics reference; added GPT-4 results"},{"id":"http://arxiv.org/abs/2209.15214v6","updated":"2023-03-19T11:28:38Z","published":"2022-09-30T04:03:26Z","title":"Construction and Applications of Billion-Scale Pre-Trained Multimodal\n  Business Knowledge Graph","summary":"  Business Knowledge Graphs (KGs) are important to many enterprises today,\nproviding factual knowledge and structured data that steer many products and\nmake them more intelligent. Despite their promising benefits, building business\nKG necessitates solving prohibitive issues of deficient structure and multiple\nmodalities. In this paper, we advance the understanding of the practical\nchallenges related to building KG in non-trivial real-world systems. We\nintroduce the process of building an open business knowledge graph (OpenBG)\nderived from a well-known enterprise, Alibaba Group. Specifically, we define a\ncore ontology to cover various abstract products and consumption demands, with\nfine-grained taxonomy and multimodal facts in deployed applications. OpenBG is\nan open business KG of unprecedented scale: 2.6 billion triples with more than\n88 million entities covering over 1 million core classes/concepts and 2,681\ntypes of relations. We release all the open resources (OpenBG benchmarks)\nderived from it for the community and report experimental results of KG-centric\ntasks. We also run up an online competition based on OpenBG benchmarks, and has\nattracted thousands of teams. We further pre-train OpenBG and apply it to many\nKG- enhanced downstream tasks in business scenarios, demonstrating the\neffectiveness of billion-scale multimodal knowledge for e-commerce. All the\nresources with codes have been released at\n\\url{https://github.com/OpenBGBenchmark/OpenBG}.\n","authors":["Shumin Deng","Chengming Wang","Zhoubo Li","Ningyu Zhang","Zelin Dai","Hehong Chen","Feiyu Xiong","Ming Yan","Qiang Chen","Mosha Chen","Jiaoyan Chen","Jeff Z. Pan","Bryan Hooi","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2209.15214v6.pdf","comment":"OpenBG. Accepted by ICDE 2023. The project is released at\n  https://github.com/OpenBGBenchmark/OpenBG . Website: https://kg.alibaba.com/\n  , Leaderboard: https://tianchi.aliyun.com/dataset/dataDetail?dataId=122271"},{"id":"http://arxiv.org/abs/2303.10612v1","updated":"2023-03-19T09:24:48Z","published":"2023-03-19T09:24:48Z","title":"Bangla Grammatical Error Detection Using T5 Transformer Model","summary":"  This paper presents a method for detecting grammatical errors in Bangla using\na Text-to-Text Transfer Transformer (T5) Language Model, using the small\nvariant of BanglaT5, fine-tuned on a corpus of 9385 sentences where errors were\nbracketed by the dedicated demarcation symbol. The T5 model was primarily\ndesigned for translation and is not specifically designed for this task, so\nextensive post-processing was necessary to adapt it to the task of error\ndetection. Our experiments show that the T5 model can achieve low Levenshtein\nDistance in detecting grammatical errors in Bangla, but post-processing is\nessential to achieve optimal performance. The final average Levenshtein\nDistance after post-processing the output of the fine-tuned model was 1.0394 on\na test set of 5000 sentences. This paper also presents a detailed analysis of\nthe errors detected by the model and discusses the challenges of adapting a\ntranslation model for grammar. Our approach can be extended to other languages,\ndemonstrating the potential of T5 models for detecting grammatical errors in a\nwide range of languages.\n","authors":["H. A. Z. Sameen Shahgir","Khondker Salman Sayeed"],"pdf_url":"https://arxiv.org/pdf/2303.10612v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10606v1","updated":"2023-03-19T08:57:39Z","published":"2023-03-19T08:57:39Z","title":"CTRAN: CNN-Transformer-based Network for Natural Language Understanding","summary":"  Intent-detection and slot-filling are the two main tasks in natural language\nunderstanding. In this study, we propose CTRAN, a novel encoder-decoder\nCNN-Transformer-based architecture for intent-detection and slot-filling. In\nthe encoder, we use BERT, followed by several convolutional layers, and\nrearrange the output using window feature sequence. We use stacked Transformer\nencoders after the window feature sequence. For the intent-detection decoder,\nwe utilize self-attention followed by a linear layer. In the slot-filling\ndecoder, we introduce the aligned Transformer decoder, which utilizes a zero\ndiagonal mask, aligning output tags with input tokens. We apply our network on\nATIS and SNIPS, and surpass the current state-of-the-art in slot-filling on\nboth datasets. Furthermore, we incorporate the language model as word\nembeddings, and show that this strategy yields a better result when compared to\nthe language model as an encoder.\n","authors":["Mehrdad Rafiepour","Javad Salimi Sartakhti"],"pdf_url":"https://arxiv.org/pdf/2303.10606v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10583v1","updated":"2023-03-19T06:09:52Z","published":"2023-03-19T06:09:52Z","title":"Toward Artificial Empathy for Human-Centered Design: A Framework","summary":"  In the early stages of the design process, designers explore opportunities by\ndiscovering unmet needs and developing innovative concepts as potential\nsolutions. From a human-centered design perspective, designers must develop\nempathy with people to truly understand their needs. However, developing\nempathy is a complex and subjective process that relies heavily on the\ndesigner's empathetic capability. Therefore, the development of empathetic\nunderstanding is intuitive, and the discovery of underlying needs is often\nserendipitous. This paper aims to provide insights from artificial intelligence\nresearch to indicate the future direction of AI-driven human-centered design,\ntaking into account the essential role of empathy. Specifically, we conduct an\ninterdisciplinary investigation of research areas such as data-driven user\nstudies, empathetic understanding development, and artificial empathy. Based on\nthis foundation, we discuss the role that artificial empathy can play in\nhuman-centered design and propose an artificial empathy framework for\nhuman-centered design. Building on the mechanisms behind empathy and insights\nfrom empathetic design research, the framework aims to break down the rather\ncomplex and subjective concept of empathy into components and modules that can\npotentially be modeled computationally. Furthermore, we discuss the expected\nbenefits of developing such systems and identify current research gaps to\nencourage future research efforts.\n","authors":["Qihao Zhu","Jianxi Luo"],"pdf_url":"https://arxiv.org/pdf/2303.10583v1.pdf","comment":"Submitted to IDETC2023"},{"id":"http://arxiv.org/abs/2303.10573v1","updated":"2023-03-19T05:22:12Z","published":"2023-03-19T05:22:12Z","title":"Extracting Incidents, Effects, and Requested Advice from MeToo Posts","summary":"  Survivors of sexual harassment frequently share their experiences on social\nmedia, revealing their feelings and emotions and seeking advice. We observed\nthat on Reddit, survivors regularly share long posts that describe a\ncombination of (i) a sexual harassment incident, (ii) its effect on the\nsurvivor, including their feelings and emotions, and (iii) the advice being\nsought. We term such posts MeToo posts, even though they may not be so tagged\nand may appear in diverse subreddits. A prospective helper (such as a counselor\nor even a casual reader) must understand a survivor's needs from such posts.\nBut long posts can be time-consuming to read and respond to.\n  Accordingly, we address the problem of extracting key information from a long\nMeToo post. We develop a natural language-based model to identify sentences\nfrom a post that describe any of the above three categories.\n  On ten-fold cross-validation of a dataset, our model achieves a macro F1\nscore of 0.82.\n  In addition, we contribute MeThree, a dataset comprising 8,947 labeled\nsentences extracted from Reddit posts. We apply the LIWC-22 toolkit on MeThree\nto understand how different language patterns in sentences of the three\ncategories can reveal differences in emotional tone, authenticity, and other\naspects.\n","authors":["Vaibhav Garg","Jiaqing Yuan","Rujie Xi","Munindar P. Singh"],"pdf_url":"https://arxiv.org/pdf/2303.10573v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10560v1","updated":"2023-03-19T04:05:10Z","published":"2023-03-19T04:05:10Z","title":"How People Respond to the COVID-19 Pandemic on Twitter: A Comparative\n  Analysis of Emotional Expressions from US and India","summary":"  The COVID-19 pandemic has claimed millions of lives worldwide and elicited\nheightened emotions. This study examines the expression of various emotions\npertaining to COVID-19 in the United States and India as manifested in over 54\nmillion tweets, covering the fifteen-month period from February 2020 through\nApril 2021, a period which includes the beginnings of the huge and disastrous\nincrease in COVID-19 cases that started to ravage India in March 2021.\nEmploying pre-trained emotion analysis and topic modeling algorithms, four\ndistinct types of emotions (fear, anger, happiness, and sadness) and their\ntime- and location-associated variations were examined. Results revealed\nsignificant country differences and temporal changes in the relative\nproportions of fear, anger, and happiness, with fear declining and anger and\nhappiness fluctuating in 2020 until new situations over the first four months\nof 2021 reversed the trends. Detected differences are discussed briefly in\nterms of the latent topics revealed and through the lens of appraisal theories\nof emotions, and the implications of the findings are discussed.\n","authors":["Brandon Siyuan Loh","Raj Kumar Gupta","Ajay Vishwanath","Andrew Ortony","Yinping Yang"],"pdf_url":"https://arxiv.org/pdf/2303.10560v1.pdf","comment":"13 pages, 3 figures, 1 table, 2 appendices"},{"id":"http://arxiv.org/abs/2210.08817v2","updated":"2023-03-19T03:39:16Z","published":"2022-10-17T08:06:56Z","title":"PACIFIC: Towards Proactive Conversational Question Answering over\n  Tabular and Textual Data in Finance","summary":"  To facilitate conversational question answering (CQA) over hybrid contexts in\nfinance, we present a new dataset, named PACIFIC. Compared with existing CQA\ndatasets, PACIFIC exhibits three key features: (i) proactivity, (ii) numerical\nreasoning, and (iii) hybrid context of tables and text. A new task is defined\naccordingly to study Proactive Conversational Question Answering (PCQA), which\ncombines clarification question generation and CQA. In addition, we propose a\nnovel method, namely UniPCQA, to adapt a hybrid format of input and output\ncontent in PCQA into the Seq2Seq problem, including the reformulation of the\nnumerical reasoning process as code generation. UniPCQA performs multi-task\nlearning over all sub-tasks in PCQA and incorporates a simple ensemble strategy\nto alleviate the error propagation issue in the multi-task learning by\ncross-validating top-$k$ sampled Seq2Seq outputs. We benchmark the PACIFIC\ndataset with extensive baselines and provide comprehensive evaluations on each\nsub-task of PCQA.\n","authors":["Yang Deng","Wenqiang Lei","Wenxuan Zhang","Wai Lam","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2210.08817v2.pdf","comment":"Accepted by EMNLP 2022 (main conference)"},{"id":"http://arxiv.org/abs/2303.10527v1","updated":"2023-03-19T01:40:31Z","published":"2023-03-19T01:40:31Z","title":"Two Kinds of Recall","summary":"  It is an established assumption that pattern-based models are good at\nprecision, while learning based models are better at recall. But is that really\nthe case? I argue that there are two kinds of recall: d-recall, reflecting\ndiversity, and e-recall, reflecting exhaustiveness. I demonstrate through\nexperiments that while neural methods are indeed significantly better at\nd-recall, it is sometimes the case that pattern-based methods are still\nsubstantially better at e-recall. Ideal methods should aim for both kinds, and\nthis ideal should in turn be reflected in our evaluations.\n","authors":["Yoav Goldberg"],"pdf_url":"https://arxiv.org/pdf/2303.10527v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2303.01994v2","updated":"2023-03-19T20:22:39Z","published":"2023-03-03T15:06:03Z","title":"Discovery and Recognition of Formula Concepts using Machine Learning","summary":"  Citation-based Information Retrieval (IR) methods for scientific documents\nhave proven effective for IR applications, such as Plagiarism Detection or\nLiterature Recommender Systems in academic disciplines that use many\nreferences. In science, technology, engineering, and mathematics, researchers\noften employ mathematical concepts through formula notation to refer to prior\nknowledge. Our long-term goal is to generalize citation-based IR methods and\napply this generalized method to both classical references and mathematical\nconcepts. In this paper, we suggest how mathematical formulas could be cited\nand define a Formula Concept Retrieval task with two subtasks: Formula Concept\nDiscovery (FCD) and Formula Concept Recognition (FCR). While FCD aims at the\ndefinition and exploration of a 'Formula Concept' that names bundled equivalent\nrepresentations of a formula, FCR is designed to match a given formula to a\nprior assigned unique mathematical concept identifier. We present machine\nlearning-based approaches to address the FCD and FCR tasks. We then evaluate\nthese approaches on a standardized test collection (NTCIR arXiv dataset). Our\nFCD approach yields a precision of 68% for retrieving equivalent\nrepresentations of frequent formulas and a recall of 72% for extracting the\nformula name from the surrounding text. FCD and FCR enable the citation of\nformulas within mathematical documents and facilitate semantic search and\nquestion answering as well as document similarity assessments for plagiarism\ndetection or recommender systems.\n","authors":["Philipp Scharpf","Moritz Schubotz","Howard S. Cohl","Corinna Breitinger","Bela Gipp"],"pdf_url":"https://arxiv.org/pdf/2303.01994v2.pdf","comment":"Accepted by Scientometrics (Springer) journal"},{"id":"http://arxiv.org/abs/2210.07316v3","updated":"2023-03-19T13:37:01Z","published":"2022-10-13T19:42:08Z","title":"MTEB: Massive Text Embedding Benchmark","summary":"  Text embeddings are commonly evaluated on a small set of datasets from a\nsingle task not covering their possible applications to other tasks. It is\nunclear whether state-of-the-art embeddings on semantic textual similarity\n(STS) can be equally well applied to other tasks like clustering or reranking.\nThis makes progress in the field difficult to track, as various models are\nconstantly being proposed without proper evaluation. To solve this problem, we\nintroduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding\ntasks covering a total of 58 datasets and 112 languages. Through the\nbenchmarking of 33 models on MTEB, we establish the most comprehensive\nbenchmark of text embeddings to date. We find that no particular text embedding\nmethod dominates across all tasks. This suggests that the field has yet to\nconverge on a universal text embedding method and scale it up sufficiently to\nprovide state-of-the-art results on all embedding tasks. MTEB comes with\nopen-source code and a public leaderboard at\nhttps://github.com/embeddings-benchmark/mteb.\n","authors":["Niklas Muennighoff","Nouamane Tazi","Loïc Magne","Nils Reimers"],"pdf_url":"https://arxiv.org/pdf/2210.07316v3.pdf","comment":"24 pages, 14 tables, 6 figures"},{"id":"http://arxiv.org/abs/2209.15214v6","updated":"2023-03-19T11:28:38Z","published":"2022-09-30T04:03:26Z","title":"Construction and Applications of Billion-Scale Pre-Trained Multimodal\n  Business Knowledge Graph","summary":"  Business Knowledge Graphs (KGs) are important to many enterprises today,\nproviding factual knowledge and structured data that steer many products and\nmake them more intelligent. Despite their promising benefits, building business\nKG necessitates solving prohibitive issues of deficient structure and multiple\nmodalities. In this paper, we advance the understanding of the practical\nchallenges related to building KG in non-trivial real-world systems. We\nintroduce the process of building an open business knowledge graph (OpenBG)\nderived from a well-known enterprise, Alibaba Group. Specifically, we define a\ncore ontology to cover various abstract products and consumption demands, with\nfine-grained taxonomy and multimodal facts in deployed applications. OpenBG is\nan open business KG of unprecedented scale: 2.6 billion triples with more than\n88 million entities covering over 1 million core classes/concepts and 2,681\ntypes of relations. We release all the open resources (OpenBG benchmarks)\nderived from it for the community and report experimental results of KG-centric\ntasks. We also run up an online competition based on OpenBG benchmarks, and has\nattracted thousands of teams. We further pre-train OpenBG and apply it to many\nKG- enhanced downstream tasks in business scenarios, demonstrating the\neffectiveness of billion-scale multimodal knowledge for e-commerce. All the\nresources with codes have been released at\n\\url{https://github.com/OpenBGBenchmark/OpenBG}.\n","authors":["Shumin Deng","Chengming Wang","Zhoubo Li","Ningyu Zhang","Zelin Dai","Hehong Chen","Feiyu Xiong","Ming Yan","Qiang Chen","Mosha Chen","Jiaoyan Chen","Jeff Z. Pan","Bryan Hooi","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2209.15214v6.pdf","comment":"OpenBG. Accepted by ICDE 2023. The project is released at\n  https://github.com/OpenBGBenchmark/OpenBG . Website: https://kg.alibaba.com/\n  , Leaderboard: https://tianchi.aliyun.com/dataset/dataDetail?dataId=122271"},{"id":"http://arxiv.org/abs/2205.11857v2","updated":"2023-03-19T10:19:31Z","published":"2022-05-24T07:33:28Z","title":"Comprehensive Privacy Analysis on Federated Recommender System against\n  Attribute Inference Attacks","summary":"  In recent years, recommender systems are crucially important for the delivery\nof personalized services that satisfy users' preferences. With personalized\nrecommendation services, users can enjoy a variety of recommendations such as\nmovies, books, ads, restaurants, and more. Despite the great benefits,\npersonalized recommendations typically require the collection of personal data\nfor user modelling and analysis, which can make users susceptible to attribute\ninference attacks. Specifically, the vulnerability of existing centralized\nrecommenders under attribute inference attacks leaves malicious attackers a\nbackdoor to infer users' private attributes, as the systems remember\ninformation of their training data (i.e., interaction data and side\ninformation). An emerging practice is to implement recommender systems in the\nfederated setting, which enables all user devices to collaboratively learn a\nshared global recommender while keeping all the training data on device.\nHowever, the privacy issues in federated recommender systems have been rarely\nexplored. In this paper, we first design a novel attribute inference attacker\nto perform a comprehensive privacy analysis of the state-of-the-art federated\nrecommender models. The experimental results show that the vulnerability of\neach model component against attribute inference attack is varied, highlighting\nthe need for new defense approaches. Therefore, we propose a novel adaptive\nprivacy-preserving approach to protect users' sensitive data in the presence of\nattribute inference attacks and meanwhile maximize the recommendation accuracy.\nExtensive experimental results on two real-world datasets validate the superior\nperformance of our model on both recommendation effectiveness and resistance to\ninference attacks.\n","authors":["Shijie Zhang","Wei Yuan","Hongzhi Yin"],"pdf_url":"https://arxiv.org/pdf/2205.11857v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10539v1","updated":"2023-03-19T02:33:19Z","published":"2023-03-19T02:33:19Z","title":"Textless Speech-to-Music Retrieval Using Emotion Similarity","summary":"  We introduce a framework that recommends music based on the emotions of\nspeech. In content creation and daily life, speech contains information about\nhuman emotions, which can be enhanced by music. Our framework focuses on a\ncross-domain retrieval system to bridge the gap between speech and music via\nemotion labels. We explore different speech representations and report their\nimpact on different speech types, including acting voice and wake-up words. We\nalso propose an emotion similarity regularization term in cross-domain\nretrieval tasks. By incorporating the regularization term into training,\nsimilar speech-and-music pairs in the emotion space are closer in the joint\nembedding space. Our comprehensive experimental results show that the proposed\nmodel is effective in textless speech-to-music retrieval.\n","authors":["SeungHeon Doh","Minz Won","Keunwoo Choi","Juhan Nam"],"pdf_url":"https://arxiv.org/pdf/2303.10539v1.pdf","comment":"To Appear IEEE ICASSP 2023"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2303.10794v1","updated":"2023-03-19T23:41:04Z","published":"2023-03-19T23:41:04Z","title":"PheME: A deep ensemble framework for improving phenotype prediction from\n  multi-modal data","summary":"  Detailed phenotype information is fundamental to accurate diagnosis and risk\nestimation of diseases. As a rich source of phenotype information, electronic\nhealth records (EHRs) promise to empower diagnostic variant interpretation.\nHowever, how to accurately and efficiently extract phenotypes from the\nheterogeneous EHR data remains a challenge. In this work, we present PheME, an\nEnsemble framework using Multi-modality data of structured EHRs and\nunstructured clinical notes for accurate Phenotype prediction. Firstly, we\nemploy multiple deep neural networks to learn reliable representations from the\nsparse structured EHR data and redundant clinical notes. A multi-modal model\nthen aligns multi-modal features onto the same latent space to predict\nphenotypes. Secondly, we leverage ensemble learning to combine outputs from\nsingle-modal models and multi-modal models to improve phenotype predictions. We\nchoose seven diseases to evaluate the phenotyping performance of the proposed\nframework. Experimental results show that using multi-modal data significantly\nimproves phenotype prediction in all diseases, the proposed ensemble learning\nframework can further boost the performance.\n","authors":["Shenghan Zhang","Haoxuan Li","Ruixiang Tang","Sirui Ding","Laila Rasmy","Degui Zhi","Na Zou","Xia Hu"],"pdf_url":"https://arxiv.org/pdf/2303.10794v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.02218v2","updated":"2023-03-19T23:14:12Z","published":"2022-11-04T02:05:58Z","title":"Fully Bayesian inference for latent variable Gaussian process models","summary":"  Real engineering and scientific applications often involve one or more\nqualitative inputs. Standard Gaussian processes (GPs), however, cannot directly\naccommodate qualitative inputs. The recently introduced latent variable\nGaussian process (LVGP) overcomes this issue by first mapping each qualitative\nfactor to underlying latent variables (LVs), and then uses any standard GP\ncovariance function over these LVs. The LVs are estimated similarly to the\nother GP hyperparameters through maximum likelihood estimation, and then\nplugged into the prediction expressions. However, this plug-in approach will\nnot account for uncertainty in estimation of the LVs, which can be significant\nespecially with limited training data. In this work, we develop a fully\nBayesian approach for the LVGP model and for visualizing the effects of the\nqualitative inputs via their LVs. We also develop approximations for scaling up\nLVGPs and fully Bayesian inference for the LVGP hyperparameters. We conduct\nnumerical studies comparing plug-in inference against fully Bayesian inference\nover a few engineering models and material design applications. In contrast to\nprevious studies on standard GP modeling that have largely concluded that a\nfully Bayesian treatment offers limited improvements, our results show that for\nLVGP modeling it offers significant improvements in prediction accuracy and\nuncertainty quantification over the plug-in approach.\n","authors":["Suraj Yerramilli","Akshay Iyer","Wei Chen","Daniel W. Apley"],"pdf_url":"https://arxiv.org/pdf/2211.02218v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10789v1","updated":"2023-03-19T23:00:41Z","published":"2023-03-19T23:00:41Z","title":"A hybrid CNN-RNN approach for survival analysis in a Lung Cancer\n  Screening study","summary":"  In this study, we present a hybrid CNN-RNN approach to investigate long-term\nsurvival of subjects in a lung cancer screening study. Subjects who died of\ncardiovascular and respiratory causes were identified whereby the CNN model was\nused to capture imaging features in the CT scans and the RNN model was used to\ninvestigate time series and thus global information. The models were trained on\nsubjects who underwent cardiovascular and respiratory deaths and a control\ncohort matched to participant age, gender, and smoking history. The combined\nmodel can achieve an AUC of 0.76 which outperforms humans at cardiovascular\nmortality prediction. The corresponding F1 and Matthews Correlation Coefficient\nare 0.63 and 0.42 respectively. The generalisability of the model is further\nvalidated on an 'external' cohort. The same models were applied to survival\nanalysis with the Cox Proportional Hazard model. It was demonstrated that\nincorporating the follow-up history can lead to improvement in survival\nprediction. The Cox neural network can achieve an IPCW C-index of 0.75 on the\ninternal dataset and 0.69 on an external dataset. Delineating imaging features\nassociated with long-term survival can help focus preventative interventions\nappropriately, particularly for under-recognised pathologies thereby\npotentially reducing patient morbidity.\n","authors":["Yaozhi Lu","Shahab Aslani","An Zhao","Ahmed Shahin","David Barber","Mark Emberton","Daniel C. Alexander","Joseph Jacob"],"pdf_url":"https://arxiv.org/pdf/2303.10789v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.06015v2","updated":"2023-03-19T22:36:28Z","published":"2023-02-12T22:12:35Z","title":"A Theoretical Understanding of Shallow Vision Transformers: Learning,\n  Generalization, and Sample Complexity","summary":"  Vision Transformers (ViTs) with self-attention modules have recently achieved\ngreat empirical success in many vision tasks. Due to non-convex interactions\nacross layers, however, theoretical learning and generalization analysis is\nmostly elusive. Based on a data model characterizing both label-relevant and\nlabel-irrelevant tokens, this paper provides the first theoretical analysis of\ntraining a shallow ViT, i.e., one self-attention layer followed by a two-layer\nperceptron, for a classification task. We characterize the sample complexity to\nachieve a zero generalization error. Our sample complexity bound is positively\ncorrelated with the inverse of the fraction of label-relevant tokens, the token\nnoise level, and the initial model error. We also prove that a training process\nusing stochastic gradient descent (SGD) leads to a sparse attention map, which\nis a formal verification of the general intuition about the success of\nattention. Moreover, this paper indicates that a proper token sparsification\ncan improve the test performance by removing label-irrelevant and/or noisy\ntokens, including spurious correlations. Empirical experiments on synthetic\ndata and CIFAR-10 dataset justify our theoretical results and generalize to\ndeeper ViTs.\n","authors":["Hongkang Li","Meng Wang","Sijia Liu","Pin-yu Chen"],"pdf_url":"https://arxiv.org/pdf/2302.06015v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.02393v2","updated":"2023-03-19T22:30:58Z","published":"2023-03-04T11:53:33Z","title":"Seq-HyGAN: Sequence Classification via Hypergraph Attention Network","summary":"  Sequence classification has a wide range of real-world applications in\ndifferent domains, such as genome classification in health and anomaly\ndetection in business. However, the lack of explicit features in sequence data\nmakes it difficult for machine learning models. While Neural Network (NN)\nmodels address this with learning features automatically, they are limited to\ncapturing adjacent structural connections and ignore global, higher-order\ninformation between the sequences. To address these challenges in the sequence\nclassification problems, we propose a novel Hypergraph Attention Network model,\nnamely Seq-HyGAN. To capture the complex structural similarity between sequence\ndata, we first create a hypergraph where the sequences are depicted as\nhyperedges and subsequences extracted from sequences are depicted as nodes.\nAdditionally, we introduce an attention-based Hypergraph Neural Network model\nthat utilizes a two-level attention mechanism. This model generates a sequence\nrepresentation as a hyperedge while simultaneously learning the crucial\nsubsequences for each sequence. We conduct extensive experiments on four data\nsets to assess and compare our model with several state-of-the-art methods.\nExperimental results demonstrate that our proposed Seq-HyGAN model can\neffectively classify sequence data and significantly outperform the baselines.\nWe also conduct case studies to investigate the contribution of each module in\nSeq-HyGAN.\n","authors":["Khaled Mohammed Saifuddin","Corey May","Farhan Tanvir","Muhammad Ifte Khairul Islam","Esra Akbas"],"pdf_url":"https://arxiv.org/pdf/2303.02393v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10780v1","updated":"2023-03-19T22:07:27Z","published":"2023-03-19T22:07:27Z","title":"A Comprehensive Review of Spiking Neural Networks: Interpretation,\n  Optimization, Efficiency, and Best Practices","summary":"  Biological neural networks continue to inspire breakthroughs in neural\nnetwork performance. And yet, one key area of neural computation that has been\nunder-appreciated and under-investigated is biologically plausible,\nenergy-efficient spiking neural networks, whose potential is especially\nattractive for low-power, mobile, or otherwise hardware-constrained settings.\nWe present a literature review of recent developments in the interpretation,\noptimization, efficiency, and accuracy of spiking neural networks. Key\ncontributions include identification, discussion, and comparison of\ncutting-edge methods in spiking neural network optimization, energy-efficiency,\nand evaluation, starting from first principles so as to be accessible to new\npractitioners.\n","authors":["Kai Malcom","Josue Casco-Rodriguez"],"pdf_url":"https://arxiv.org/pdf/2303.10780v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10778v1","updated":"2023-03-19T21:58:37Z","published":"2023-03-19T21:58:37Z","title":"Deep Declarative Dynamic Time Warping for End-to-End Learning of\n  Alignment Paths","summary":"  This paper addresses learning end-to-end models for time series data that\ninclude a temporal alignment step via dynamic time warping (DTW). Existing\napproaches to differentiable DTW either differentiate through a fixed warping\npath or apply a differentiable relaxation to the min operator found in the\nrecursive steps used to solve the DTW problem. We instead propose a DTW layer\nbased around bi-level optimisation and deep declarative networks, which we name\nDecDTW. By formulating DTW as a continuous, inequality constrained optimisation\nproblem, we can compute gradients for the solution of the optimal alignment\n(with respect to the underlying time series) using implicit differentiation. An\ninteresting byproduct of this formulation is that DecDTW outputs the optimal\nwarping path between two time series as opposed to a soft approximation,\nrecoverable from Soft-DTW. We show that this property is particularly useful\nfor applications where downstream loss functions are defined on the optimal\nalignment path itself. This naturally occurs, for instance, when learning to\nimprove the accuracy of predicted alignments against ground truth alignments.\nWe evaluate DecDTW on two such applications, namely the audio-to-score\nalignment task in music information retrieval and the visual place recognition\ntask in robotics, demonstrating state-of-the-art results in both.\n","authors":["Ming Xu","Sourav Garg","Michael Milford","Stephen Gould"],"pdf_url":"https://arxiv.org/pdf/2303.10778v1.pdf","comment":"ICLR 2023 (Poster)"},{"id":"http://arxiv.org/abs/2303.10774v1","updated":"2023-03-19T21:54:13Z","published":"2023-03-19T21:54:13Z","title":"Cross-GAN Auditing: Unsupervised Identification of Attribute Level\n  Similarities and Differences between Pretrained Generative Models","summary":"  Generative Adversarial Networks (GANs) are notoriously difficult to train\nespecially for complex distributions and with limited data. This has driven the\nneed for tools to audit trained networks in human intelligible format, for\nexample, to identify biases or ensure fairness. Existing GAN audit tools are\nrestricted to coarse-grained, model-data comparisons based on summary\nstatistics such as FID or recall. In this paper, we propose an alternative\napproach that compares a newly developed GAN against a prior baseline. To this\nend, we introduce Cross-GAN Auditing (xGA) that, given an established\n\"reference\" GAN and a newly proposed \"client\" GAN, jointly identifies\nintelligible attributes that are either common across both GANs, novel to the\nclient GAN, or missing from the client GAN. This provides both users and model\ndevelopers an intuitive assessment of similarity and differences between GANs.\nWe introduce novel metrics to evaluate attribute-based GAN auditing approaches\nand use these metrics to demonstrate quantitatively that xGA outperforms\nbaseline approaches. We also include qualitative results that illustrate the\ncommon, novel and missing attributes identified by xGA from GANs trained on a\nvariety of image datasets.\n","authors":["Matthew L. Olson","Shusen Liu","Rushil Anirudh","Jayaraman J. Thiagarajan","Peer-Timo Bremer","Weng-Keen Wong"],"pdf_url":"https://arxiv.org/pdf/2303.10774v1.pdf","comment":"CVPR 2023. Source code is available at\n  https://github.com/mattolson93/cross_gan_auditing"},{"id":"http://arxiv.org/abs/2204.05781v3","updated":"2023-03-19T21:32:59Z","published":"2022-04-06T07:45:05Z","title":"Forecasting Cryptocurrency Returns from Sentiment Signals: An Analysis\n  of BERT Classifiers and Weak Supervision","summary":"  Anticipating price developments in financial markets is a topic of continued\ninterest in forecasting. Funneled by advancements in deep learning and natural\nlanguage processing (NLP) together with the availability of vast amounts of\ntextual data in form of news articles, social media postings, etc., an\nincreasing number of studies incorporate text-based predictors in forecasting\nmodels. We contribute to this literature by introducing weak learning, a\nrecently proposed NLP approach to address the problem that text data is\nunlabeled. Without a dependent variable, it is not possible to finetune\npretrained NLP models on a custom corpus. We confirm that finetuning using weak\nlabels enhances the predictive value of text-based features and raises forecast\naccuracy in the context of predicting cryptocurrency returns. More\nfundamentally, the modeling paradigm we present, weak labeling domain-specific\ntext and finetuning pretrained NLP models, is universally applicable in\n(financial) forecasting and unlocks new ways to leverage text data.\n","authors":["Duygu Ider","Stefan Lessmann"],"pdf_url":"https://arxiv.org/pdf/2204.05781v3.pdf","comment":"29 pages"},{"id":"http://arxiv.org/abs/2212.04088v2","updated":"2023-03-19T20:52:21Z","published":"2022-12-08T05:46:32Z","title":"LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large\n  Language Models","summary":"  This study focuses on using large language models (LLMs) as a planner for\nembodied agents that can follow natural language instructions to complete\ncomplex tasks in a visually-perceived environment. The high data cost and poor\nsample efficiency of existing methods hinders the development of versatile\nagents that are capable of many tasks and can learn new tasks quickly. In this\nwork, we propose a novel method, LLM-Planner, that harnesses the power of large\nlanguage models to do few-shot planning for embodied agents. We further propose\na simple but effective way to enhance LLMs with physical grounding to generate\nand update plans that are grounded in the current environment. Experiments on\nthe ALFRED dataset show that our method can achieve very competitive few-shot\nperformance: Despite using less than 0.5% of paired training data, LLM-Planner\nachieves competitive performance with recent baselines that are trained using\nthe full training data. Existing methods can barely complete any task\nsuccessfully under the same few-shot setting. Our work opens the door for\ndeveloping versatile and sample-efficient embodied agents that can quickly\nlearn many tasks. Website: https://dki-lab.github.io/LLM-Planner\n","authors":["Chan Hee Song","Jiaman Wu","Clayton Washington","Brian M. Sadler","Wei-Lun Chao","Yu Su"],"pdf_url":"https://arxiv.org/pdf/2212.04088v2.pdf","comment":"14 pages, 5 figures"},{"id":"http://arxiv.org/abs/2207.11386v2","updated":"2023-03-19T20:39:59Z","published":"2022-07-23T01:17:23Z","title":"Learning an Adaptive Forwarding Strategy for Mobile Wireless Networks:\n  Resource Usage vs. Latency","summary":"  Designing effective routing strategies for mobile wireless networks is\nchallenging due to the need to seamlessly adapt routing behavior to spatially\ndiverse and temporally changing network conditions. In this work, we use deep\nreinforcement learning (DeepRL) to learn a scalable and generalizable\nsingle-copy routing strategy for such networks. We make the following\ncontributions: i) we design a reward function that enables the DeepRL agent to\nexplicitly trade-off competing network goals, such as minimizing delay vs. the\nnumber of transmissions per packet; ii) we propose a novel set of relational\nneighborhood, path, and context features to characterize mobile wireless\nnetworks and model device mobility independently of a specific network\ntopology; and iii) we use a flexible training approach that allows us to\ncombine data from all packets and devices into a single offline centralized\ntraining set to train a single DeepRL agent. To evaluate generalizeability and\nscalability, we train our DeepRL agent on one mobile network scenario and then\ntest it on other mobile scenarios, varying the number of devices and\ntransmission ranges. Our results show our learned single-copy routing strategy\noutperforms all other strategies in terms of delay except for the optimal\nstrategy, even on scenarios on which the DeepRL agent was not trained.\n","authors":["Victoria Manfredi","Alicia P. Wolfe","Xiaolan Zhang","Bing Wang"],"pdf_url":"https://arxiv.org/pdf/2207.11386v2.pdf","comment":"39 pages, 9 figures. Added new features as input to DRL model, added\n  new mobility model to evaluation, updated related work, and updated\n  simulations"},{"id":"http://arxiv.org/abs/2303.10761v1","updated":"2023-03-19T20:27:51Z","published":"2023-03-19T20:27:51Z","title":"Calibration of Neural Networks","summary":"  Neural networks solving real-world problems are often required not only to\nmake accurate predictions but also to provide a confidence level in the\nforecast. The calibration of a model indicates how close the estimated\nconfidence is to the true probability. This paper presents a survey of\nconfidence calibration problems in the context of neural networks and provides\nan empirical comparison of calibration methods. We analyze problem statement,\ncalibration definitions, and different approaches to evaluation: visualizations\nand scalar measures that estimate whether the model is well-calibrated. We\nreview modern calibration techniques: based on post-processing or requiring\nchanges in training. Empirical experiments cover various datasets and models,\ncomparing calibration methods according to different criteria.\n","authors":["Ruslan Vasilev","Alexander D'yakonov"],"pdf_url":"https://arxiv.org/pdf/2303.10761v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10758v1","updated":"2023-03-19T20:24:33Z","published":"2023-03-19T20:24:33Z","title":"Lower Generalization Bounds for GD and SGD in Smooth Stochastic Convex\n  Optimization","summary":"  Recent progress was made in characterizing the generalization error of\ngradient methods for general convex loss by the learning theory community. In\nthis work, we focus on how training longer might affect generalization in\nsmooth stochastic convex optimization (SCO) problems. We first provide tight\nlower bounds for general non-realizable SCO problems. Furthermore, existing\nupper bound results suggest that sample complexity can be improved by assuming\nthe loss is realizable, i.e. an optimal solution simultaneously minimizes all\nthe data points. However, this improvement is compromised when training time is\nlong and lower bounds are lacking. Our paper examines this observation by\nproviding excess risk lower bounds for gradient descent (GD) and stochastic\ngradient descent (SGD) in two realizable settings: 1) realizable with $T =\nO(n)$, and (2) realizable with $T = \\Omega(n)$, where $T$ denotes the number of\ntraining iterations and $n$ is the size of the training dataset. These bounds\nare novel and informative in characterizing the relationship between $T$ and\n$n$. In the first small training horizon case, our lower bounds almost tightly\nmatch and provide the first optimal certificates for the corresponding upper\nbounds. However, for the realizable case with $T = \\Omega(n)$, a gap exists\nbetween the lower and upper bounds. We provide a conjecture to address this\nproblem, that the gap can be closed by improving upper bounds, which is\nsupported by our analyses in one-dimensional and linear regression scenarios.\n","authors":["Peiyuan Zhang","Jiaye Teng","Jingzhao Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.10758v1.pdf","comment":"30 pages"},{"id":"http://arxiv.org/abs/2303.01994v2","updated":"2023-03-19T20:22:39Z","published":"2023-03-03T15:06:03Z","title":"Discovery and Recognition of Formula Concepts using Machine Learning","summary":"  Citation-based Information Retrieval (IR) methods for scientific documents\nhave proven effective for IR applications, such as Plagiarism Detection or\nLiterature Recommender Systems in academic disciplines that use many\nreferences. In science, technology, engineering, and mathematics, researchers\noften employ mathematical concepts through formula notation to refer to prior\nknowledge. Our long-term goal is to generalize citation-based IR methods and\napply this generalized method to both classical references and mathematical\nconcepts. In this paper, we suggest how mathematical formulas could be cited\nand define a Formula Concept Retrieval task with two subtasks: Formula Concept\nDiscovery (FCD) and Formula Concept Recognition (FCR). While FCD aims at the\ndefinition and exploration of a 'Formula Concept' that names bundled equivalent\nrepresentations of a formula, FCR is designed to match a given formula to a\nprior assigned unique mathematical concept identifier. We present machine\nlearning-based approaches to address the FCD and FCR tasks. We then evaluate\nthese approaches on a standardized test collection (NTCIR arXiv dataset). Our\nFCD approach yields a precision of 68% for retrieving equivalent\nrepresentations of frequent formulas and a recall of 72% for extracting the\nformula name from the surrounding text. FCD and FCR enable the citation of\nformulas within mathematical documents and facilitate semantic search and\nquestion answering as well as document similarity assessments for plagiarism\ndetection or recommender systems.\n","authors":["Philipp Scharpf","Moritz Schubotz","Howard S. Cohl","Corinna Breitinger","Bela Gipp"],"pdf_url":"https://arxiv.org/pdf/2303.01994v2.pdf","comment":"Accepted by Scientometrics (Springer) journal"},{"id":"http://arxiv.org/abs/2303.10757v1","updated":"2023-03-19T20:21:29Z","published":"2023-03-19T20:21:29Z","title":"Multiscale Audio Spectrogram Transformer for Efficient Audio\n  Classification","summary":"  Audio event has a hierarchical architecture in both time and frequency and\ncan be grouped together to construct more abstract semantic audio classes. In\nthis work, we develop a multiscale audio spectrogram Transformer (MAST) that\nemploys hierarchical representation learning for efficient audio\nclassification. Specifically, MAST employs one-dimensional (and\ntwo-dimensional) pooling operators along the time (and frequency domains) in\ndifferent stages, and progressively reduces the number of tokens and increases\nthe feature dimensions. MAST significantly outperforms AST~\\cite{gong2021ast}\nby 22.2\\%, 4.4\\% and 4.7\\% on Kinetics-Sounds, Epic-Kitchens-100 and VGGSound\nin terms of the top-1 accuracy without external training data. On the\ndownloaded AudioSet dataset, which has over 20\\% missing audios, MAST also\nachieves slightly better accuracy than AST. In addition, MAST is 5x more\nefficient in terms of multiply-accumulates (MACs) with 42\\% reduction in the\nnumber of parameters compared to AST. Through clustering metrics and\nvisualizations, we demonstrate that the proposed MAST can learn semantically\nmore separable feature representations from audio signals.\n","authors":["Wentao Zhu","Mohamed Omar"],"pdf_url":"https://arxiv.org/pdf/2303.10757v1.pdf","comment":"ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.10741v1","updated":"2023-03-19T19:09:41Z","published":"2023-03-19T19:09:41Z","title":"Computer Vision Estimation of Emotion Reaction Intensity in the Wild","summary":"  Emotions play an essential role in human communication. Developing computer\nvision models for automatic recognition of emotion expression can aid in a\nvariety of domains, including robotics, digital behavioral healthcare, and\nmedia analytics. There are three types of emotional representations which are\ntraditionally modeled in affective computing research: Action Units, Valence\nArousal (VA), and Categorical Emotions. As part of an effort to move beyond\nthese representations towards more fine-grained labels, we describe our\nsubmission to the newly introduced Emotional Reaction Intensity (ERI)\nEstimation challenge in the 5th competition for Affective Behavior Analysis\nin-the-Wild (ABAW). We developed four deep neural networks trained in the\nvisual domain and a multimodal model trained with both visual and audio\nfeatures to predict emotion reaction intensity. Our best performing model on\nthe Hume-Reaction dataset achieved an average Pearson correlation coefficient\nof 0.4080 on the test set using a pre-trained ResNet50 model. This work\nprovides a first step towards the development of production-grade models which\npredict emotion reaction intensities rather than discrete emotion categories.\n","authors":["Yang Qian","Ali Kargarandehkordi","Onur Cezmi Mutlu","Saimourya Surabhi","Mohammadmahdi Honarmand","Dennis Paul Wall","Peter Washington"],"pdf_url":"https://arxiv.org/pdf/2303.10741v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10738v1","updated":"2023-03-19T18:55:22Z","published":"2023-03-19T18:55:22Z","title":"MIA-3DCNN: COVID-19 Detection Based on a 3D CNN","summary":"  Early and accurate diagnosis of COVID-19 is essential to control the rapid\nspread of the pandemic and mitigate sequelae in the population. Current\ndiagnostic methods, such as RT-PCR, are effective but require time to provide\nresults and can quickly overwhelm clinics, requiring individual laboratory\nanalysis. Automatic detection methods have the potential to significantly\nreduce diagnostic time. To this end, learning-based methods using lung imaging\nhave been explored. Although they require specialized hardware, automatic\nevaluation methods can be performed simultaneously, making diagnosis faster.\nConvolutional neural networks have been widely used to detect pneumonia caused\nby COVID-19 in lung images. This work describes an architecture based on 3D\nconvolutional neural networks for detecting COVID-19 in computed tomography\nimages. Despite the challenging scenario present in the dataset, the results\nobtained with our architecture demonstrated to be quite promising.\n","authors":["Igor Kenzo Ishikawa Oshiro Nakashima","Giovanna Vendramini","Helio Pedrini"],"pdf_url":"https://arxiv.org/pdf/2303.10738v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10732v1","updated":"2023-03-19T18:37:18Z","published":"2023-03-19T18:37:18Z","title":"AutoEn: An AutoML method based on ensembles of predefined Machine\n  Learning pipelines for supervised Traffic Forecasting","summary":"  Intelligent Transportation Systems are producing tons of hardly manageable\ntraffic data, which motivates the use of Machine Learning (ML) for data-driven\napplications, such as Traffic Forecasting (TF). TF is gaining relevance due to\nits ability to mitigate traffic congestion by forecasting future traffic\nstates. However, TF poses one big challenge to the ML paradigm, known as the\nModel Selection Problem (MSP): deciding the most suitable combination of data\npreprocessing techniques and ML method for traffic data collected under\ndifferent transportation circumstances. In this context, Automated Machine\nLearning (AutoML), the automation of the ML workflow from data preprocessing to\nmodel validation, arises as a promising strategy to deal with the MSP in\nproblem domains wherein expert ML knowledge is not always an available or\naffordable asset, such as TF. Various AutoML frameworks have been used to\napproach the MSP in TF. Most are based on online optimisation processes to\nsearch for the best-performing pipeline on a given dataset. This online\noptimisation could be complemented with meta-learning to warm-start the search\nphase and/or the construction of ensembles using pipelines derived from the\noptimisation process. However, given the complexity of the search space and the\nhigh computational cost of tuning-evaluating pipelines generated, online\noptimisation is only beneficial when there is a long time to obtain the final\nmodel. Thus, we introduce AutoEn, which is a simple and efficient method for\nautomatically generating multi-classifier ensembles from a predefined set of ML\npipelines. We compare AutoEn against Auto-WEKA and Auto-sklearn, two AutoML\nmethods commonly used in TF. Experimental results demonstrate that AutoEn can\nlead to better or more competitive results in the general-purpose domain and in\nTF.\n","authors":["Juan S. Angarita-Zapata","Antonio D. Masegosa","Isaac Triguero"],"pdf_url":"https://arxiv.org/pdf/2303.10732v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10730v1","updated":"2023-03-19T18:29:54Z","published":"2023-03-19T18:29:54Z","title":"Pretrained Vision Models for Predicting High-Risk Breast Cancer Stage","summary":"  Cancer is increasingly a global health issue. Seconding cardiovascular\ndiseases, cancers are the second biggest cause of death in the world with\nmillions of people succumbing to the disease every year. According to the World\nHealth Organization (WHO) report, by the end of 2020, more than 7.8 million\nwomen have been diagnosed with breast cancer, making it the world's most\nprevalent cancer. In this paper, using the Nightingale Open Science dataset of\ndigital pathology (breast biopsy) images, we leverage the capabilities of\npre-trained computer vision models for the breast cancer stage prediction task.\nWhile individual models achieve decent performances, we find out that the\npredictions of an ensemble model are more efficient, and offer a winning\nsolution\\footnote{https://www.nightingalescience.org/updates/hbc1-results}. We\nalso provide analyses of the results and explore pathways for better\ninterpretability and generalization. Our code is open-source at\n\\url{https://github.com/bonaventuredossou/nightingale_winning_solution}\n","authors":["Bonaventure F. P. Dossou","Yenoukoume S. K. Gbenou","Miglanche Ghomsi Nono"],"pdf_url":"https://arxiv.org/pdf/2303.10730v1.pdf","comment":"Accepted at Machine Learning for Global Health Workshop, ICLR 2023"},{"id":"http://arxiv.org/abs/2205.15150v3","updated":"2023-03-19T18:20:39Z","published":"2022-05-30T14:47:27Z","title":"Principal Component Analysis based frameworks for efficient missing data\n  imputation algorithms","summary":"  Missing data is a commonly occurring problem in practice. Many imputation\nmethods have been developed to fill in the missing entries. However, not all of\nthem can scale to high-dimensional data, especially the multiple imputation\ntechniques. Meanwhile, the data nowadays tends toward high-dimensional.\nTherefore, in this work, we propose Principal Component Analysis Imputation\n(PCAI), a simple but versatile framework based on Principal Component Analysis\n(PCA) to speed up the imputation process and alleviate memory issues of many\navailable imputation techniques, without sacrificing the imputation quality in\nterm of MSE. In addition, the frameworks can be used even when some or all of\nthe missing features are categorical, or when the number of missing features is\nlarge. Next, we introduce PCA Imputation - Classification (PIC), an application\nof PCAI for classification problems with some adjustments. We validate our\napproach by experiments on various scenarios, which shows that PCAI and PIC can\nwork with various imputation algorithms, including the state-of-the-art ones\nand improve the imputation speed significantly, while achieving competitive\nmean square error/classification accuracy compared to direct imputation (i.e.,\nimpute directly on the missing data).\n","authors":["Thu Nguyen","Hoang Thien Ly","Michael Alexander Riegler","Pål Halvorsen","Hugo L. Hammer"],"pdf_url":"https://arxiv.org/pdf/2205.15150v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10728v1","updated":"2023-03-19T18:10:15Z","published":"2023-03-19T18:10:15Z","title":"Training Deep Boltzmann Networks with Sparse Ising Machines","summary":"  The slowing down of Moore's law has driven the development of unconventional\ncomputing paradigms, such as specialized Ising machines tailored to solve\ncombinatorial optimization problems. In this paper, we show a new application\ndomain for probabilistic bit (p-bit) based Ising machines by training deep\ngenerative AI models with them. Using sparse, asynchronous, and massively\nparallel Ising machines we train deep Boltzmann networks in a hybrid\nprobabilistic-classical computing setup. We use the full MNIST dataset without\nany downsampling or reduction in hardware-aware network topologies implemented\nin moderately sized Field Programmable Gate Arrays (FPGA). Our machine, which\nuses only 4,264 nodes (p-bits) and about 30,000 parameters, achieves the same\nclassification accuracy (90%) as an optimized software-based restricted\nBoltzmann Machine (RBM) with approximately 3.25 million parameters.\nAdditionally, the sparse deep Boltzmann network can generate new handwritten\ndigits, a task the 3.25 million parameter RBM fails at despite achieving the\nsame accuracy. Our hybrid computer takes a measured 50 to 64 billion\nprobabilistic flips per second, which is at least an order of magnitude faster\nthan superficially similar Graphics and Tensor Processing Unit (GPU/TPU) based\nimplementations. The massively parallel architecture can comfortably perform\nthe contrastive divergence algorithm (CD-n) with up to n = 10 million sweeps\nper update, beyond the capabilities of existing software implementations. These\nresults demonstrate the potential of using Ising machines for traditionally\nhard-to-train deep generative Boltzmann networks, with further possible\nimprovement in nanodevice-based realizations.\n","authors":["Shaila Niazi","Navid Anjum Aadit","Masoud Mohseni","Shuvro Chowdhury","Yao Qin","Kerem Y. Camsari"],"pdf_url":"https://arxiv.org/pdf/2303.10728v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10727v1","updated":"2023-03-19T18:08:18Z","published":"2023-03-19T18:08:18Z","title":"ERSAM: Neural Architecture Search For Energy-Efficient and Real-Time\n  Social Ambiance Measurement","summary":"  Social ambiance describes the context in which social interactions happen,\nand can be measured using speech audio by counting the number of concurrent\nspeakers. This measurement has enabled various mental health tracking and\nhuman-centric IoT applications. While on-device Socal Ambiance Measure (SAM) is\nhighly desirable to ensure user privacy and thus facilitate wide adoption of\nthe aforementioned applications, the required computational complexity of\nstate-of-the-art deep neural networks (DNNs) powered SAM solutions stands at\nodds with the often constrained resources on mobile devices. Furthermore, only\nlimited labeled data is available or practical when it comes to SAM under\nclinical settings due to various privacy constraints and the required human\neffort, further challenging the achievable accuracy of on-device SAM solutions.\nTo this end, we propose a dedicated neural architecture search framework for\nEnergy-efficient and Real-time SAM (ERSAM). Specifically, our ERSAM framework\ncan automatically search for DNNs that push forward the achievable accuracy vs.\nhardware efficiency frontier of mobile SAM solutions. For example,\nERSAM-delivered DNNs only consume 40 mW x 12 h energy and 0.05 seconds\nprocessing latency for a 5 seconds audio segment on a Pixel 3 phone, while only\nachieving an error rate of 14.3% on a social ambiance dataset generated by\nLibriSpeech. We can expect that our ERSAM framework can pave the way for\nubiquitous on-device SAM solutions which are in growing demand.\n","authors":["Chaojian Li","Wenwan Chen","Jiayi Yuan"," Yingyan"," Lin","Ashutosh Sabharwal"],"pdf_url":"https://arxiv.org/pdf/2303.10727v1.pdf","comment":"Accepted by ICASSP'23"},{"id":"http://arxiv.org/abs/2303.10725v1","updated":"2023-03-19T17:46:40Z","published":"2023-03-19T17:46:40Z","title":"SIESTA: Efficient Online Continual Learning with Sleep","summary":"  In supervised continual learning, a deep neural network (DNN) is updated with\nan ever-growing data stream. Unlike the offline setting where data is shuffled,\nwe cannot make any distributional assumptions about the data stream. Ideally,\nonly one pass through the dataset is needed for computational efficiency.\nHowever, existing methods are inadequate and make many assumptions that cannot\nbe made for real-world applications, while simultaneously failing to improve\ncomputational efficiency. In this paper, we do not propose a novel method.\nInstead, we present SIESTA, an incremental improvement to the continual\nlearning algorithm REMIND. Unlike REMIND, SIESTA uses a wake/sleep framework\nfor training, which is well aligned to the needs of on-device learning. SIESTA\nis far more computationally efficient than existing methods, enabling continual\nlearning on ImageNet-1K in under 3 hours on a single GPU; moreover, in the\naugmentation-free setting it matches the performance of the offline learner, a\nmilestone critical to driving adoption of continual learning in real-world\napplications.\n","authors":["Md Yousuf Harun","Jhair Gallardo","Tyler L. Hayes","Ronald Kemker","Christopher Kanan"],"pdf_url":"https://arxiv.org/pdf/2303.10725v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2201.13387v2","updated":"2023-03-19T17:46:07Z","published":"2022-01-31T17:52:01Z","title":"L-SVRG and L-Katyusha with Adaptive Sampling","summary":"  Stochastic gradient-based optimization methods, such as L-SVRG and its\naccelerated variant L-Katyusha (Kovalev et al., 2020), are widely used to train\nmachine learning models.The theoretical and empirical performance of L-SVRG and\nL-Katyusha can be improved by sampling observations from a non-uniform\ndistribution (Qian et al., 2021). However,designing a desired sampling\ndistribution requires prior knowledge of smoothness constants, which can be\ncomputationally intractable to obtain in practice when the dimension of the\nmodel parameter is high. To address this issue, we propose an adaptive sampling\nstrategy for L-SVRG and L-Katyusha that can learn the sampling distribution\nwith little computational overhead, while allowing it to change with iterates,\nand at the same time does not require any prior knowledge of the problem\nparameters. We prove convergence guarantees for L-SVRG and L-Katyusha for\nconvex objectives when the sampling distribution changes with iterates. Our\nresults show that even without prior information, the proposed adaptive\nsampling strategy matches, and in some cases even surpasses, the performance of\nthe sampling scheme in Qian et al. (2021). Extensive simulations support our\ntheory and the practical utility of the proposed sampling scheme on real data.\n","authors":["Boxin Zhao","Boxiang Lyu","Mladen Kolar"],"pdf_url":"https://arxiv.org/pdf/2201.13387v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10722v1","updated":"2023-03-19T17:34:58Z","published":"2023-03-19T17:34:58Z","title":"Q-RBSA: High-Resolution 3D EBSD Map Generation Using An Efficient\n  Quaternion Transformer Network","summary":"  Gathering 3D material microstructural information is time-consuming,\nexpensive, and energy-intensive. Acquisition of 3D data has been accelerated by\ndevelopments in serial sectioning instrument capabilities; however, for\ncrystallographic information, the electron backscatter diffraction (EBSD)\nimaging modality remains rate limiting. We propose a physics-based efficient\ndeep learning framework to reduce the time and cost of collecting 3D EBSD maps.\nOur framework uses a quaternion residual block self-attention network (QRBSA)\nto generate high-resolution 3D EBSD maps from sparsely sectioned EBSD maps. In\nQRBSA, quaternion-valued convolution effectively learns local relations in\norientation space, while self-attention in the quaternion domain captures\nlong-range correlations. We apply our framework to 3D data collected from\ncommercially relevant titanium alloys, showing both qualitatively and\nquantitatively that our method can predict missing samples (EBSD information\nbetween sparsely sectioned mapping points) as compared to high-resolution\nground truth 3D EBSD maps.\n","authors":["Devendra K. Jangid","Neal R. Brodnik","McLean P. Echlin","Tresa M. Pollock","Samantha H. Daly","B. S. Manjunath"],"pdf_url":"https://arxiv.org/pdf/2303.10722v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10720v1","updated":"2023-03-19T17:30:44Z","published":"2023-03-19T17:30:44Z","title":"Trainable Projected Gradient Method for Robust Fine-tuning","summary":"  Recent studies on transfer learning have shown that selectively fine-tuning a\nsubset of layers or customizing different learning rates for each layer can\ngreatly improve robustness to out-of-distribution (OOD) data and retain\ngeneralization capability in the pre-trained models. However, most of these\nmethods employ manually crafted heuristics or expensive hyper-parameter\nsearches, which prevent them from scaling up to large datasets and neural\nnetworks. To solve this problem, we propose Trainable Projected Gradient Method\n(TPGM) to automatically learn the constraint imposed for each layer for a\nfine-grained fine-tuning regularization. This is motivated by formulating\nfine-tuning as a bi-level constrained optimization problem. Specifically, TPGM\nmaintains a set of projection radii, i.e., distance constraints between the\nfine-tuned model and the pre-trained model, for each layer, and enforces them\nthrough weight projections. To learn the constraints, we propose a bi-level\noptimization to automatically learn the best set of projection radii in an\nend-to-end manner. Theoretically, we show that the bi-level optimization\nformulation is the key to learning different constraints for each layer.\nEmpirically, with little hyper-parameter search cost, TPGM outperforms existing\nfine-tuning methods in OOD performance while matching the best in-distribution\n(ID) performance. For example, when fine-tuned on DomainNet-Real and ImageNet,\ncompared to vanilla fine-tuning, TPGM shows $22\\%$ and $10\\%$ relative OOD\nimprovement respectively on their sketch counterparts. Code is available at\n\\url{https://github.com/PotatoTian/TPGM}.\n","authors":["Junjiao Tian","Xiaoliang Dai","Chih-Yao Ma","Zecheng He","Yen-Cheng Liu","Zsolt Kira"],"pdf_url":"https://arxiv.org/pdf/2303.10720v1.pdf","comment":"Accepted to CVPR2023"},{"id":"http://arxiv.org/abs/2303.03995v2","updated":"2023-03-19T17:17:11Z","published":"2023-03-07T15:51:03Z","title":"Group conditional validity via multi-group learning","summary":"  We consider the problem of distribution-free conformal prediction and the\ncriterion of group conditional validity. This criterion is motivated by many\npractical scenarios including hidden stratification and group fairness.\nExisting methods achieve such guarantees under either restrictive grouping\nstructure or distributional assumptions, or they are overly-conservative under\nheteroskedastic noise. We propose a simple reduction to the problem of\nachieving validity guarantees for individual populations by leveraging\nalgorithms for a problem called multi-group learning. This allows us to port\ntheoretical guarantees from multi-group learning to obtain obtain sample\ncomplexity guarantees for conformal prediction. We also provide a new algorithm\nfor multi-group learning for groups with hierarchical structure. Using this\nalgorithm in our reduction leads to improved sample complexity guarantees with\na simpler predictor structure.\n","authors":["Samuel Deng","Navid Ardeshir","Daniel Hsu"],"pdf_url":"https://arxiv.org/pdf/2303.03995v2.pdf","comment":"Valid prediction intervals constructed by proposed method do not\n  appear to be any shorter than those constructed by baseline methods"},{"id":"http://arxiv.org/abs/2209.03416v4","updated":"2023-03-19T16:34:47Z","published":"2022-09-07T18:34:48Z","title":"Bispectral Neural Networks","summary":"  We present a neural network architecture, Bispectral Neural Networks (BNNs)\nfor learning representations that are invariant to the actions of compact\ncommutative groups on the space over which a signal is defined. The model\nincorporates the ansatz of the bispectrum, an analytically defined group\ninvariant that is complete -- that is, it preserves all signal structure while\nremoving only the variation due to group actions. Here, we demonstrate that\nBNNs are able to simultaneously learn groups, their irreducible\nrepresentations, and corresponding equivariant and complete-invariant maps\npurely from the symmetries implicit in data. Further, we demonstrate that the\ncompleteness property endows these networks with strong invariance-based\nadversarial robustness. This work establishes Bispectral Neural Networks as a\npowerful computational primitive for robust invariant representation learning\n","authors":["Sophia Sanborn","Christian Shewmake","Bruno Olshausen","Christopher Hillar"],"pdf_url":"https://arxiv.org/pdf/2209.03416v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.14305v2","updated":"2023-03-19T16:25:10Z","published":"2022-11-25T18:59:10Z","title":"SpaText: Spatio-Textual Representation for Controllable Image Generation","summary":"  Recent text-to-image diffusion models are able to generate convincing results\nof unprecedented quality. However, it is nearly impossible to control the\nshapes of different regions/objects or their layout in a fine-grained fashion.\nPrevious attempts to provide such controls were hindered by their reliance on a\nfixed set of labels. To this end, we present SpaText - a new method for\ntext-to-image generation using open-vocabulary scene control. In addition to a\nglobal text prompt that describes the entire scene, the user provides a\nsegmentation map where each region of interest is annotated by a free-form\nnatural language description. Due to lack of large-scale datasets that have a\ndetailed textual description for each region in the image, we choose to\nleverage the current large-scale text-to-image datasets and base our approach\non a novel CLIP-based spatio-textual representation, and show its effectiveness\non two state-of-the-art diffusion models: pixel-based and latent-based. In\naddition, we show how to extend the classifier-free guidance method in\ndiffusion models to the multi-conditional case and present an alternative\naccelerated inference algorithm. Finally, we offer several automatic evaluation\nmetrics and use them, in addition to FID scores and a user study, to evaluate\nour method and show that it achieves state-of-the-art results on image\ngeneration with free-form textual scene control.\n","authors":["Omri Avrahami","Thomas Hayes","Oran Gafni","Sonal Gupta","Yaniv Taigman","Devi Parikh","Dani Lischinski","Ohad Fried","Xi Yin"],"pdf_url":"https://arxiv.org/pdf/2211.14305v2.pdf","comment":"CVPR 2023. Project page available at:\n  https://omriavrahami.com/spatext"},{"id":"http://arxiv.org/abs/2303.10702v1","updated":"2023-03-19T16:17:19Z","published":"2023-03-19T16:17:19Z","title":"Evaluation of Convolution Primitives for Embedded Neural Networks on\n  32-bit Microcontrollers","summary":"  Deploying neural networks on constrained hardware platforms such as 32-bit\nmicrocontrollers is a challenging task because of the large memory, computing\nand energy requirements of their inference process. To tackle these issues,\nseveral convolution primitives have been proposed to make the standard\nconvolution more computationally efficient. However, few of these primitives\nare really implemented for 32-bit microcontrollers. In this work, we collect\ndifferent state-of-the-art convolutional primitives and propose an\nimplementation for ARM Cortex-M processor family with an open source deployment\nplatform (NNoM). Then, we carry out experimental characterization tests on\nthese implementations. Our benchmark reveals a linear relationship between\ntheoretical MACs and energy consumption. Thus showing the advantages of using\ncomputationally efficient primitives like shift convolution. We discuss about\nthe significant reduction in latency and energy consumption due to the use of\nSIMD instructions and highlight the importance of data reuse in those\nperformance gains. For reproducibility purpose and further experiments, codes\nand experiments are publicly available.\n","authors":["Baptiste Nguyen","Pierre-Alain Moellic","Sylvain Blayac"],"pdf_url":"https://arxiv.org/pdf/2303.10702v1.pdf","comment":"ISDA 2022"},{"id":"http://arxiv.org/abs/2303.05518v2","updated":"2023-03-19T16:01:30Z","published":"2023-03-09T16:05:10Z","title":"Computably Continuous Reinforcement-Learning Objectives are\n  PAC-learnable","summary":"  In reinforcement learning, the classic objectives of maximizing discounted\nand finite-horizon cumulative rewards are PAC-learnable: There are algorithms\nthat learn a near-optimal policy with high probability using a finite amount of\nsamples and computation. In recent years, researchers have introduced\nobjectives and corresponding reinforcement-learning algorithms beyond the\nclassic cumulative rewards, such as objectives specified as linear temporal\nlogic formulas. However, questions about the PAC-learnability of these new\nobjectives have remained open.\n  This work demonstrates the PAC-learnability of general reinforcement-learning\nobjectives through sufficient conditions for PAC-learnability in two analysis\nsettings. In particular, for the analysis that considers only sample\ncomplexity, we prove that if an objective given as an oracle is uniformly\ncontinuous, then it is PAC-learnable. Further, for the analysis that considers\ncomputational complexity, we prove that if an objective is computable, then it\nis PAC-learnable. In other words, if a procedure computes successive\napproximations of the objective's value, then the objective is PAC-learnable.\n  We give three applications of our condition on objectives from the literature\nwith previously unknown PAC-learnability and prove that these objectives are\nPAC-learnable. Overall, our result helps verify existing objectives'\nPAC-learnability. Also, as some studied objectives that are not uniformly\ncontinuous have been shown to be not PAC-learnable, our results could guide the\ndesign of new PAC-learnable objectives.\n","authors":["Cambridge Yang","Michael Littman","Michael Carbin"],"pdf_url":"https://arxiv.org/pdf/2303.05518v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10695v1","updated":"2023-03-19T15:58:04Z","published":"2023-03-19T15:58:04Z","title":"On the Convergence of Decentralized Federated Learning Under Imperfect\n  Information Sharing","summary":"  Decentralized learning and optimization is a central problem in control that\nencompasses several existing and emerging applications, such as federated\nlearning. While there exists a vast literature on this topic and most methods\ncentered around the celebrated average-consensus paradigm, less attention has\nbeen devoted to scenarios where the communication between the agents may be\nimperfect. To this end, this paper presents three different algorithms of\nDecentralized Federated Learning (DFL) in the presence of imperfect information\nsharing modeled as noisy communication channels. The first algorithm, Federated\nNoisy Decentralized Learning (FedNDL1), comes from the literature, where the\nnoise is added to their parameters to simulate the scenario of the presence of\nnoisy communication channels. This algorithm shares parameters to form a\nconsensus with the clients based on a communication graph topology through a\nnoisy communication channel. The proposed second algorithm (FedNDL2) is similar\nto the first algorithm but with added noise to the parameters, and it performs\nthe gossip averaging before the gradient optimization. The proposed third\nalgorithm\n  (FedNDL3), on the other hand, shares the gradients through noisy\ncommunication channels instead of the parameters. Theoretical and experimental\nresults demonstrate that under imperfect information sharing, the third scheme\nthat mixes gradients is more robust in the presence of a noisy channel compared\nwith the algorithms from the literature that mix the parameters.\n","authors":["Vishnu Pandi Chellapandi","Antesh Upadhyay","Abolfazl Hashemi","Stanislaw H /. Zak"],"pdf_url":"https://arxiv.org/pdf/2303.10695v1.pdf","comment":"24 pages, 2 figures"},{"id":"http://arxiv.org/abs/2303.10694v1","updated":"2023-03-19T15:56:50Z","published":"2023-03-19T15:56:50Z","title":"Improving Uncertainty Quantification of Deep Classifiers via\n  Neighborhood Conformal Prediction: Novel Algorithm and Theoretical Analysis","summary":"  Safe deployment of deep neural networks in high-stake real-world applications\nrequires theoretically sound uncertainty quantification. Conformal prediction\n(CP) is a principled framework for uncertainty quantification of deep models in\nthe form of prediction set for classification tasks with a user-specified\ncoverage (i.e., true class label is contained with high probability). This\npaper proposes a novel algorithm referred to as Neighborhood Conformal\nPrediction (NCP) to improve the efficiency of uncertainty quantification from\nCP for deep classifiers (i.e., reduce prediction set size). The key idea behind\nNCP is to use the learned representation of the neural network to identify k\nnearest-neighbors calibration examples for a given testing input and assign\nthem importance weights proportional to their distance to create adaptive\nprediction sets. We theoretically show that if the learned data representation\nof the neural network satisfies some mild conditions, NCP will produce smaller\nprediction sets than traditional CP algorithms. Our comprehensive experiments\non CIFAR-10, CIFAR-100, and ImageNet datasets using diverse deep neural\nnetworks strongly demonstrate that NCP leads to significant reduction in\nprediction set size over prior CP methods.\n","authors":["Subhankar Ghosh","Taha Belkhouja","Yan Yan","Janardhan Rao Doppa"],"pdf_url":"https://arxiv.org/pdf/2303.10694v1.pdf","comment":"To appear in the Proceedings of the AAAI Conference on Artificial\n  Intelligence, 2023"},{"id":"http://arxiv.org/abs/2303.10681v1","updated":"2023-03-19T15:00:47Z","published":"2023-03-19T15:00:47Z","title":"Generative Adversarial Classification Network with Application to\n  Network Traffic Classification","summary":"  Large datasets in machine learning often contain missing data, which\nnecessitates the imputation of missing data values. In this work, we are\nmotivated by network traffic classification, where traditional data imputation\nmethods do not perform well. We recognize that no existing method directly\naccounts for classification accuracy during data imputation. Therefore, we\npropose a joint data imputation and data classification method, termed\ngenerative adversarial classification network (GACN), whose architecture\ncontains a generator network, a discriminator network, and a classification\nnetwork, which are iteratively optimized toward the ultimate objective of\nclassification accuracy. For the scenario where some data samples are\nunlabeled, we further propose an extension termed semi-supervised GACN\n(SSGACN), which is able to use the partially labeled data to improve\nclassification accuracy. We conduct experiments with real-world network traffic\ndata traces, which demonstrate that GACN and SS-GACN can more accurately impute\ndata features that are more important for classification, and they outperform\nexisting methods in terms of classification accuracy.\n","authors":["Rozhina Ghanavi","Ben Liang","Ali Tizghadam"],"pdf_url":"https://arxiv.org/pdf/2303.10681v1.pdf","comment":"6 pages, 4 figures, 2 tables"},{"id":"http://arxiv.org/abs/2303.10677v1","updated":"2023-03-19T14:44:37Z","published":"2023-03-19T14:44:37Z","title":"A Survey of Federated Learning for Connected and Automated Vehicles","summary":"  Connected and Automated Vehicles (CAVs) are one of the emerging technologies\nin the automotive domain that has the potential to alleviate the issues of\naccidents, traffic congestion, and pollutant emissions, leading to a safe,\nefficient, and sustainable transportation system. Machine learning-based\nmethods are widely used in CAVs for crucial tasks like perception, motion\nplanning, and motion control, where machine learning models in CAVs are solely\ntrained using the local vehicle data, and the performance is not certain when\nexposed to new environments or unseen conditions. Federated learning (FL) is an\neffective solution for CAVs that enables a collaborative model development with\nmultiple vehicles in a distributed learning framework. FL enables CAVs to learn\nfrom a wide range of driving environments and improve their overall performance\nwhile ensuring the privacy and security of local vehicle data. In this paper,\nwe review the progress accomplished by researchers in applying FL to CAVs. A\nbroader view of the various data modalities and algorithms that have been\nimplemented on CAVs is provided. Specific applications of FL are reviewed in\ndetail, and an analysis of the challenges and future scope of research are\npresented.\n","authors":["Vishnu Pandi Chellapandi","Liangqi Yuan","Stanislaw H /. Zak","Ziran Wang"],"pdf_url":"https://arxiv.org/pdf/2303.10677v1.pdf","comment":"8 pages, 1 figure"},{"id":"http://arxiv.org/abs/2303.02384v2","updated":"2023-03-19T14:39:50Z","published":"2023-03-04T11:30:16Z","title":"Hierarchical Training of Deep Neural Networks Using Early Exiting","summary":"  Deep neural networks provide state-of-the-art accuracy for vision tasks but\nthey require significant resources for training. Thus, they are trained on\ncloud servers far from the edge devices that acquire the data. This issue\nincreases communication cost, runtime and privacy concerns. In this study, a\nnovel hierarchical training method for deep neural networks is proposed that\nuses early exits in a divided architecture between edge and cloud workers to\nreduce the communication cost, training runtime and privacy concerns. The\nmethod proposes a brand-new use case for early exits to separate the backward\npass of neural networks between the edge and the cloud during the training\nphase. We address the issues of most available methods that due to the\nsequential nature of the training phase, cannot train the levels of hierarchy\nsimultaneously or they do it with the cost of compromising privacy. In\ncontrast, our method can use both edge and cloud workers simultaneously, does\nnot share the raw input data with the cloud and does not require communication\nduring the backward pass. Several simulations and on-device experiments for\ndifferent neural network architectures demonstrate the effectiveness of this\nmethod. It is shown that the proposed method reduces the training runtime by\n29% and 61% in CIFAR-10 classification experiment for VGG-16 and ResNet-18 when\nthe communication with the cloud is done at a low bit rate channel. This gain\nin the runtime is achieved whilst the accuracy drop is negligible. This method\nis advantageous for online learning of high-accuracy deep neural networks on\nlow-resource devices such as mobile phones or robots as a part of an edge-cloud\nsystem, making them more flexible in facing new tasks and classes of data.\n","authors":["Yamin Sepehri","Pedram Pad","Ahmet Caner Yüzügüler","Pascal Frossard","L. Andrea Dunbar"],"pdf_url":"https://arxiv.org/pdf/2303.02384v2.pdf","comment":"12 pages, 9 figures, 1 Table"},{"id":"http://arxiv.org/abs/2303.10674v1","updated":"2023-03-19T14:33:42Z","published":"2023-03-19T14:33:42Z","title":"URM4DMU: an user represention model for darknet markets users","summary":"  Darknet markets provide a large platform for trading illicit goods and\nservices due to their anonymity. Learning an invariant representation of each\nuser based on their posts on different markets makes it easy to aggregate user\ninformation across different platforms, which helps identify anonymous users.\nTraditional user representation methods mainly rely on modeling the text\ninformation of posts and cannot capture the temporal content and the forum\ninteraction of posts. While recent works mainly use CNN to model the text\ninformation of posts, failing to effectively model posts whose length changes\nfrequently in an episode. To address the above problems, we propose a model\nnamed URM4DMU(User Representation Model for Darknet Markets Users) which mainly\nimproves the post representation by augmenting convolutional operators and\nself-attention with an adaptive gate mechanism. It performs much better when\ncombined with the temporal content and the forum interaction of posts. We\ndemonstrate the effectiveness of URM4DMU on four darknet markets. The average\nimprovements on MRR value and Recall@10 are 22.5% and 25.5% over the\nstate-of-the-art method respectively.\n","authors":["Hongmeng Liu","Jiapeng Zhao","Yixuan Huo","Yuyan Wang","Chun Liao","Liyan Shen","Shiyao Cui","Jinqiao Shi"],"pdf_url":"https://arxiv.org/pdf/2303.10674v1.pdf","comment":"9pages"},{"id":"http://arxiv.org/abs/2303.10665v1","updated":"2023-03-19T14:12:57Z","published":"2023-03-19T14:12:57Z","title":"Multi-Agent Reinforcement Learning via Mean Field Control: Common Noise,\n  Major Agents and Approximation Properties","summary":"  Recently, mean field control (MFC) has provided a tractable and theoretically\nfounded approach to otherwise difficult cooperative multi-agent control.\nHowever, the strict assumption of many independent, homogeneous agents may be\ntoo stringent in practice. In this work, we propose a novel discrete-time\ngeneralization of Markov decision processes and MFC to both many minor agents\nand potentially complex major agents -- major-minor mean field control (M3FC).\nIn contrast to deterministic MFC, M3FC allows for stochastic minor agent\ndistributions with strong correlation between minor agents through the major\nagent state, which can model arbitrary problem details not bound to any agent.\nTheoretically, we give rigorous approximation properties with novel proofs for\nboth M3FC and existing MFC models in the finite multi-agent problem, together\nwith a dynamic programming principle for solving such problems. In the\ninfinite-horizon discounted case, existence of an optimal stationary policy\nfollows. Algorithmically, we propose the major-minor mean field proximal policy\noptimization algorithm (M3FPPO) as a novel multi-agent reinforcement learning\nalgorithm and demonstrate its success in illustrative M3FC-type problems.\n","authors":["Kai Cui","Christian Fabian","Heinz Koeppl"],"pdf_url":"https://arxiv.org/pdf/2303.10665v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07768v2","updated":"2023-03-19T13:48:05Z","published":"2023-03-14T10:18:31Z","title":"DBSCAN of Multi-Slice Clustering for Third-Order Tensors","summary":"  Several methods for triclustering three-dimensional data require the cluster\nsize or the number of clusters in each dimension to be specified. To address\nthis issue, the Multi-Slice Clustering (MSC) for 3-order tensor finds signal\nslices that lie in a low dimensional subspace for a rank-one tensor dataset in\norder to find a cluster based on the threshold similarity. We propose an\nextension algorithm called MSC-DBSCAN to extract the different clusters of\nslices that lie in the different subspaces from the data if the dataset is a\nsum of r rank-one tensor (r > 1). Our algorithm uses the same input as the MSC\nalgorithm and can find the same solution for rank-one tensor data as MSC.\n","authors":["Dina Faneva Andriantsiory","Joseph Ben Geloun","Mustapha Lebbah"],"pdf_url":"https://arxiv.org/pdf/2303.07768v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.01104v3","updated":"2023-03-19T13:44:49Z","published":"2023-01-03T13:58:39Z","title":"KoopmanLab: machine learning for solving complex physics equations","summary":"  Numerous physics theories are rooted in partial differential equations\n(PDEs). However, the increasingly intricate physics equations, especially those\nthat lack analytic solutions or closed forms, have impeded the further\ndevelopment of physics. Computationally solving PDEs by classic numerical\napproaches suffers from the trade-off between accuracy and efficiency and is\nnot applicable to the empirical data generated by unknown latent PDEs. To\novercome this challenge, we present KoopmanLab, an efficient module of the\nKoopman neural operator family, for learning PDEs without analytic solutions or\nclosed forms. Our module consists of multiple variants of the Koopman neural\noperator (KNO), a kind of mesh-independent neural-network-based PDE solvers\ndeveloped following dynamic system theory. The compact variants of KNO can\naccurately solve PDEs with small model sizes while the large variants of KNO\nare more competitive in predicting highly complicated dynamic systems govern by\nunknown, high-dimensional, and non-linear PDEs. All variants are validated by\nmesh-independent and long-term prediction experiments implemented on\nrepresentative PDEs (e.g., the Navier-Stokes equation and the Bateman-Burgers\nequation in fluid mechanics) and ERA5 (i.e., one of the largest high-resolution\nglobal-scale climate data sets in earth physics). These demonstrations suggest\nthe potential of KoopmanLab to be a fundamental tool in diverse physics studies\nrelated to equations or dynamic systems.\n","authors":["Wei Xiong","Muyuan Ma","Xiaomeng Huang","Ziyang Zhang","Pei Sun","Yang Tian"],"pdf_url":"https://arxiv.org/pdf/2301.01104v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.07316v3","updated":"2023-03-19T13:37:01Z","published":"2022-10-13T19:42:08Z","title":"MTEB: Massive Text Embedding Benchmark","summary":"  Text embeddings are commonly evaluated on a small set of datasets from a\nsingle task not covering their possible applications to other tasks. It is\nunclear whether state-of-the-art embeddings on semantic textual similarity\n(STS) can be equally well applied to other tasks like clustering or reranking.\nThis makes progress in the field difficult to track, as various models are\nconstantly being proposed without proper evaluation. To solve this problem, we\nintroduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding\ntasks covering a total of 58 datasets and 112 languages. Through the\nbenchmarking of 33 models on MTEB, we establish the most comprehensive\nbenchmark of text embeddings to date. We find that no particular text embedding\nmethod dominates across all tasks. This suggests that the field has yet to\nconverge on a universal text embedding method and scale it up sufficiently to\nprovide state-of-the-art results on all embedding tasks. MTEB comes with\nopen-source code and a public leaderboard at\nhttps://github.com/embeddings-benchmark/mteb.\n","authors":["Niklas Muennighoff","Nouamane Tazi","Loïc Magne","Nils Reimers"],"pdf_url":"https://arxiv.org/pdf/2210.07316v3.pdf","comment":"24 pages, 14 tables, 6 figures"},{"id":"http://arxiv.org/abs/2303.10653v1","updated":"2023-03-19T13:30:33Z","published":"2023-03-19T13:30:33Z","title":"Randomized Adversarial Training via Taylor Expansion","summary":"  In recent years, there has been an explosion of research into developing more\nrobust deep neural networks against adversarial examples. Adversarial training\nappears as one of the most successful methods. To deal with both the robustness\nagainst adversarial examples and the accuracy over clean examples, many works\ndevelop enhanced adversarial training methods to achieve various trade-offs\nbetween them. Leveraging over the studies that smoothed update on weights\nduring training may help find flat minima and improve generalization, we\nsuggest reconciling the robustness-accuracy trade-off from another perspective,\ni.e., by adding random noise into deterministic weights. The randomized weights\nenable our design of a novel adversarial training method via Taylor expansion\nof a small Gaussian noise, and we show that the new adversarial training method\ncan flatten loss landscape and find flat minima. With PGD, CW, and Auto\nAttacks, an extensive set of experiments demonstrate that our method enhances\nthe state-of-the-art adversarial training methods, boosting both robustness and\nclean accuracy. The code is available at\nhttps://github.com/Alexkael/Randomized-Adversarial-Training.\n","authors":["Gaojie Jin","Xinping Yi","Dengyu Wu","Ronghui Mu","Xiaowei Huang"],"pdf_url":"https://arxiv.org/pdf/2303.10653v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.10650v1","updated":"2023-03-19T13:03:51Z","published":"2023-03-19T13:03:51Z","title":"Logic of Differentiable Logics: Towards a Uniform Semantics of DL","summary":"  Differentiable logics (DL) have recently been proposed as a method of\ntraining neural networks to satisfy logical specifications. A DL consists of a\nsyntax in which specifications are stated and an interpretation function that\ntranslates expressions in the syntax into loss functions. These loss functions\ncan then be used during training with standard gradient descent algorithms. The\nvariety of existing DLs and the differing levels of formality with which they\nare treated makes a systematic comparative study of their properties and\nimplementations difficult. This paper remedies this problem by suggesting a\nmeta-language for defining DLs that we call the Logic of Differentiable Logics,\nor LDL. Syntactically, it generalises the syntax of existing DLs to FOL, and\nfor the first time introduces the formalism for reasoning about vectors and\nlearners. Semantically, it introduces a general interpretation function that\ncan be instantiated to define loss functions arising from different existing\nDLs. We use LDL to establish several theoretical properties of existing DLs,\nand to conduct their empirical study in neural network verification.\n","authors":["Natalia Ślusarz","Ekaterina Komendantskaya","Matthew L. Daggitt","Robert Stewart","Kathrin Stark"],"pdf_url":"https://arxiv.org/pdf/2303.10650v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2302.06337v3","updated":"2023-03-19T13:01:47Z","published":"2023-02-13T13:14:23Z","title":"Learning from Noisy Crowd Labels with Logics","summary":"  This paper explores the integration of symbolic logic knowledge into deep\nneural networks for learning from noisy crowd labels. We introduce Logic-guided\nLearning from Noisy Crowd Labels (Logic-LNCL), an EM-alike iterative logic\nknowledge distillation framework that learns from both noisy labeled data and\nlogic rules of interest. Unlike traditional EM methods, our framework contains\na ``pseudo-E-step'' that distills from the logic rules a new type of learning\ntarget, which is then used in the ``pseudo-M-step'' for training the\nclassifier. Extensive evaluations on two real-world datasets for text sentiment\nclassification and named entity recognition demonstrate that the proposed\nframework improves the state-of-the-art and provides a new solution to learning\nfrom noisy crowd labels.\n","authors":["Zhijun Chen","Hailong Sun","Haoqian He","Pengpeng Chen"],"pdf_url":"https://arxiv.org/pdf/2302.06337v3.pdf","comment":"12 pages, 7 figures, accepted by ICDE-2023"},{"id":"http://arxiv.org/abs/2206.04959v3","updated":"2023-03-19T12:15:44Z","published":"2022-06-10T09:15:48Z","title":"Merak: An Efficient Distributed DNN Training Framework with Automated 3D\n  Parallelism for Giant Foundation Models","summary":"  Foundation models are becoming the dominant deep learning technologies.\nPretraining a foundation model is always time-consumed due to the large scale\nof both the model parameter and training dataset. Besides being\ncomputing-intensive, the training process is extremely memory-intensive and\ncommunication-intensive. These features make it necessary to apply 3D\nparallelism, which integrates data parallelism, pipeline model parallelism and\ntensor model parallelism, to achieve high training efficiency.\n  To achieve this goal, some custom software frameworks such as Megatron-LM and\nDeepSpeed are developed. However, current 3D parallelism frameworks still meet\ntwo issues: i) they are not transparent to model developers, which need to\nmanually modify the model to parallelize training. ii) their utilization of\ncomputation, GPU memory and network bandwidth are not sufficient. We propose\nMerak, an automated 3D parallelism deep learning training framework with high\nresource utilization. Merak automatically deploys with an automatic model\npartitioner, which uses a graph sharding algorithm on a proxy representation of\nthe model. Merak also presents the non-intrusive API for scaling out foundation\nmodel training with minimal code modification. In addition, we design a\nhigh-performance 3D parallel runtime engine in Merak. It uses several\ntechniques to exploit available training resources, including shifted critical\npath pipeline schedule that brings a higher computation utilization,\nstage-aware recomputation that makes use of idle worker memory, and\nsub-pipelined tensor model parallelism that overlaps communication and\ncomputation. Experiments on 64 GPUs show Merak can speedup the training\nperformance over the state-of-the-art 3D parallelism frameworks of models with\n1.5, 2.5, 8.3, and 20 billion parameters by up to 1.42X, 1.39X, 1.43X, and\n1.61X, respectively.\n","authors":["Zhiquan Lai","Shengwei Li","Xudong Tang","Keshi Ge","Weijie Liu","Yabo Duan","Linbo Qiao","Dongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2206.04959v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.12061v2","updated":"2023-03-19T12:10:02Z","published":"2022-12-22T22:27:26Z","title":"MN-DS: A Multilabeled News Dataset for News Articles Hierarchical\n  Classification","summary":"  This article presents a dataset of 10,917 news articles with hierarchical\nnews categories collected between January 1st 2019, and December 31st 2019. We\nmanually labelled the articles based on a hierarchical taxonomy with 17\nfirst-level and 109 second-level categories. This dataset can be used to train\nmachine learning models for automatically classifying news articles by topic.\nThis dataset can be helpful for researchers working on news structuring,\nclassification, and predicting future events based on released news.\n","authors":["Alina Petukhova","Nuno Fachada"],"pdf_url":"https://arxiv.org/pdf/2212.12061v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.15214v6","updated":"2023-03-19T11:28:38Z","published":"2022-09-30T04:03:26Z","title":"Construction and Applications of Billion-Scale Pre-Trained Multimodal\n  Business Knowledge Graph","summary":"  Business Knowledge Graphs (KGs) are important to many enterprises today,\nproviding factual knowledge and structured data that steer many products and\nmake them more intelligent. Despite their promising benefits, building business\nKG necessitates solving prohibitive issues of deficient structure and multiple\nmodalities. In this paper, we advance the understanding of the practical\nchallenges related to building KG in non-trivial real-world systems. We\nintroduce the process of building an open business knowledge graph (OpenBG)\nderived from a well-known enterprise, Alibaba Group. Specifically, we define a\ncore ontology to cover various abstract products and consumption demands, with\nfine-grained taxonomy and multimodal facts in deployed applications. OpenBG is\nan open business KG of unprecedented scale: 2.6 billion triples with more than\n88 million entities covering over 1 million core classes/concepts and 2,681\ntypes of relations. We release all the open resources (OpenBG benchmarks)\nderived from it for the community and report experimental results of KG-centric\ntasks. We also run up an online competition based on OpenBG benchmarks, and has\nattracted thousands of teams. We further pre-train OpenBG and apply it to many\nKG- enhanced downstream tasks in business scenarios, demonstrating the\neffectiveness of billion-scale multimodal knowledge for e-commerce. All the\nresources with codes have been released at\n\\url{https://github.com/OpenBGBenchmark/OpenBG}.\n","authors":["Shumin Deng","Chengming Wang","Zhoubo Li","Ningyu Zhang","Zelin Dai","Hehong Chen","Feiyu Xiong","Ming Yan","Qiang Chen","Mosha Chen","Jiaoyan Chen","Jeff Z. Pan","Bryan Hooi","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2209.15214v6.pdf","comment":"OpenBG. Accepted by ICDE 2023. The project is released at\n  https://github.com/OpenBGBenchmark/OpenBG . Website: https://kg.alibaba.com/\n  , Leaderboard: https://tianchi.aliyun.com/dataset/dataDetail?dataId=122271"},{"id":"http://arxiv.org/abs/2303.10630v1","updated":"2023-03-19T11:20:43Z","published":"2023-03-19T11:20:43Z","title":"Experimenting with Normalization Layers in Federated Learning on non-IID\n  scenarios","summary":"  Training Deep Learning (DL) models require large, high-quality datasets,\noften assembled with data from different institutions. Federated Learning (FL)\nhas been emerging as a method for privacy-preserving pooling of datasets\nemploying collaborative training from different institutions by iteratively\nglobally aggregating locally trained models. One critical performance challenge\nof FL is operating on datasets not independently and identically distributed\n(non-IID) among the federation participants. Even though this fragility cannot\nbe eliminated, it can be debunked by a suitable optimization of two\nhyper-parameters: layer normalization methods and collaboration frequency\nselection. In this work, we benchmark five different normalization layers for\ntraining Neural Networks (NNs), two families of non-IID data skew, and two\ndatasets. Results show that Batch Normalization, widely employed for\ncentralized DL, is not the best choice for FL, whereas Group and Layer\nNormalization consistently outperform Batch Normalization. Similarly, frequent\nmodel aggregation decreases convergence speed and mode quality.\n","authors":["Bruno Casella","Roberto Esposito","Antonio Sciarappa","Carlo Cavazzoni","Marco Aldinucci"],"pdf_url":"https://arxiv.org/pdf/2303.10630v1.pdf","comment":"10 pages, Submitted to IEEE Transactions on Neural Networks and\n  Learning Systems"},{"id":"http://arxiv.org/abs/2303.10624v1","updated":"2023-03-19T10:38:29Z","published":"2023-03-19T10:38:29Z","title":"PFSL: Personalized & Fair Split Learning with Data & Label Privacy for\n  thin clients","summary":"  The traditional framework of federated learning (FL) requires each client to\nre-train their models in every iteration, making it infeasible for\nresource-constrained mobile devices to train deep-learning (DL) models. Split\nlearning (SL) provides an alternative by using a centralized server to offload\nthe computation of activations and gradients for a subset of the model but\nsuffers from problems of slow convergence and lower accuracy. In this paper, we\nimplement PFSL, a new framework of distributed split learning where a large\nnumber of thin clients perform transfer learning in parallel, starting with a\npre-trained DL model without sharing their data or labels with a central\nserver. We implement a lightweight step of personalization of client models to\nprovide high performance for their respective data distributions. Furthermore,\nwe evaluate performance fairness amongst clients under a work fairness\nconstraint for various scenarios of non-i.i.d. data distributions and unequal\nsample sizes. Our accuracy far exceeds that of current SL algorithms and is\nvery close to that of centralized learning on several real-life benchmarks. It\nhas a very low computation cost compared to FL variants and promises to deliver\nthe full benefits of DL to extremely thin, resource-constrained clients.\n","authors":["Manas Wadhwa","Gagan Raj Gupta","Ashutosh Sahu","Rahul Saini","Vidhi Mittal"],"pdf_url":"https://arxiv.org/pdf/2303.10624v1.pdf","comment":"To be published in : THE 23RD IEEE/ACM INTERNATIONAL SYMPOSIUM ON\n  Cluster, Cloud and Internet Computing. Granted: Open Research Objects (ORO)\n  and Research Objects Reviewed (ROR) badges. See\n  https://www.niso.org/publications/rp-31-2021-badging for definitions of the\n  badges. Code available at: https://github.com/mnswdhw/PFSL"}],"Multimedia":[{"id":"http://arxiv.org/abs/2303.10794v1","updated":"2023-03-19T23:41:04Z","published":"2023-03-19T23:41:04Z","title":"PheME: A deep ensemble framework for improving phenotype prediction from\n  multi-modal data","summary":"  Detailed phenotype information is fundamental to accurate diagnosis and risk\nestimation of diseases. As a rich source of phenotype information, electronic\nhealth records (EHRs) promise to empower diagnostic variant interpretation.\nHowever, how to accurately and efficiently extract phenotypes from the\nheterogeneous EHR data remains a challenge. In this work, we present PheME, an\nEnsemble framework using Multi-modality data of structured EHRs and\nunstructured clinical notes for accurate Phenotype prediction. Firstly, we\nemploy multiple deep neural networks to learn reliable representations from the\nsparse structured EHR data and redundant clinical notes. A multi-modal model\nthen aligns multi-modal features onto the same latent space to predict\nphenotypes. Secondly, we leverage ensemble learning to combine outputs from\nsingle-modal models and multi-modal models to improve phenotype predictions. We\nchoose seven diseases to evaluate the phenotyping performance of the proposed\nframework. Experimental results show that using multi-modal data significantly\nimproves phenotype prediction in all diseases, the proposed ensemble learning\nframework can further boost the performance.\n","authors":["Shenghan Zhang","Haoxuan Li","Ruixiang Tang","Sirui Ding","Laila Rasmy","Degui Zhi","Na Zou","Xia Hu"],"pdf_url":"https://arxiv.org/pdf/2303.10794v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.09371v3","updated":"2023-03-19T12:31:20Z","published":"2022-11-17T06:55:49Z","title":"CapEnrich: Enriching Caption Semantics for Web Images via Cross-modal\n  Pre-trained Knowledge","summary":"  Automatically generating textual descriptions for massive unlabeled images on\nthe web can greatly benefit realistic web applications, e.g. multimodal\nretrieval and recommendation. However, existing models suffer from the problem\nof generating ``over-generic'' descriptions, such as their tendency to generate\nrepetitive sentences with common concepts for different images. These generic\ndescriptions fail to provide sufficient textual semantics for ever-changing web\nimages. Inspired by the recent success of Vision-Language Pre-training (VLP)\nmodels that learn diverse image-text concept alignment during pretraining, we\nexplore leveraging their cross-modal pre-trained knowledge to automatically\nenrich the textual semantics of image descriptions. With no need for additional\nhuman annotations, we propose a plug-and-play framework, i.e CapEnrich, to\ncomplement the generic image descriptions with more semantic details.\nSpecifically, we first propose an automatic data-building strategy to get\ndesired training sentences, based on which we then adopt prompting strategies,\ni.e. learnable and template prompts, to incentivize VLP models to generate more\ntextual details. For learnable templates, we fix the whole VLP model and only\ntune the prompt vectors, which leads to two advantages: 1) the pre-training\nknowledge of VLP models can be reserved as much as possible to describe diverse\nvisual concepts; 2) only lightweight trainable parameters are required, so it\nis friendly to low data resources. Extensive experiments show that our method\nsignificantly improves the descriptiveness and diversity of generated sentences\nfor web images. The code is available at https://github.com/yaolinli/CapEnrich.\n","authors":["Linli Yao","Weijing Chen","Qin Jin"],"pdf_url":"https://arxiv.org/pdf/2211.09371v3.pdf","comment":"Accepted to WWW2023"},{"id":"http://arxiv.org/abs/2303.10539v1","updated":"2023-03-19T02:33:19Z","published":"2023-03-19T02:33:19Z","title":"Textless Speech-to-Music Retrieval Using Emotion Similarity","summary":"  We introduce a framework that recommends music based on the emotions of\nspeech. In content creation and daily life, speech contains information about\nhuman emotions, which can be enhanced by music. Our framework focuses on a\ncross-domain retrieval system to bridge the gap between speech and music via\nemotion labels. We explore different speech representations and report their\nimpact on different speech types, including acting voice and wake-up words. We\nalso propose an emotion similarity regularization term in cross-domain\nretrieval tasks. By incorporating the regularization term into training,\nsimilar speech-and-music pairs in the emotion space are closer in the joint\nembedding space. Our comprehensive experimental results show that the proposed\nmodel is effective in textless speech-to-music retrieval.\n","authors":["SeungHeon Doh","Minz Won","Keunwoo Choi","Juhan Nam"],"pdf_url":"https://arxiv.org/pdf/2303.10539v1.pdf","comment":"To Appear IEEE ICASSP 2023"}]},"2023-03-18T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2303.10512v1","updated":"2023-03-18T22:36:25Z","published":"2023-03-18T22:36:25Z","title":"Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning","summary":"  Fine-tuning large pre-trained language models on downstream tasks has become\nan important paradigm in NLP. However, common practice fine-tunes all of the\nparameters in a pre-trained model, which becomes prohibitive when a large\nnumber of downstream tasks are present. Therefore, many fine-tuning methods are\nproposed to learn incremental updates of pre-trained weights in a parameter\nefficient way, e.g., low-rank increments. These methods often evenly distribute\nthe budget of incremental updates across all pre-trained weight matrices, and\noverlook the varying importance of different weight parameters. As a\nconsequence, the fine-tuning performance is suboptimal. To bridge this gap, we\npropose AdaLoRA, which adaptively allocates the parameter budget among weight\nmatrices according to their importance score. In particular, AdaLoRA\nparameterizes the incremental updates in the form of singular value\ndecomposition. Such a novel approach allows us to effectively prune the\nsingular values of unimportant updates, which is essentially to reduce their\nparameter budget but circumvent intensive exact SVD computations. We conduct\nextensive experiments with several pre-trained models on natural language\nprocessing, question answering, and natural language generation to validate the\neffectiveness of AdaLoRA. Results demonstrate that AdaLoRA manifests notable\nimprovement over baselines, especially in the low budget settings. Our code is\npublicly available at https://github.com/QingruZhang/AdaLoRA .\n","authors":["Qingru Zhang","Minshuo Chen","Alexander Bukharin","Pengcheng He","Yu Cheng","Weizhu Chen","Tuo Zhao"],"pdf_url":"https://arxiv.org/pdf/2303.10512v1.pdf","comment":"The 11th International Conference on Learning Representations (ICLR\n  2023)"},{"id":"http://arxiv.org/abs/2303.10510v1","updated":"2023-03-18T22:19:09Z","published":"2023-03-18T22:19:09Z","title":"A Deep Learning System for Domain-specific speech Recognition","summary":"  As human-machine voice interfaces provide easy access to increasingly\nintelligent machines, many state-of-the-art automatic speech recognition (ASR)\nsystems are proposed. However, commercial ASR systems usually have poor\nperformance on domain-specific speech especially under low-resource settings.\nThe author works with pre-trained DeepSpeech2 and Wav2Vec2 acoustic models to\ndevelop benefit-specific ASR systems. The domain-specific data are collected\nusing proposed semi-supervised learning annotation with little human\nintervention. The best performance comes from a fine-tuned Wav2Vec2-Large-LV60\nacoustic model with an external KenLM, which surpasses the Google and AWS ASR\nsystems on benefit-specific speech. The viability of using error prone ASR\ntranscriptions as part of spoken language understanding (SLU) is also\ninvestigated. Results of a benefit-specific natural language understanding\n(NLU) task show that the domain-specific fine-tuned ASR system can outperform\nthe commercial ASR systems even when its transcriptions have higher word error\nrate (WER), and the results between fine-tuned ASR and human transcriptions are\nsimilar.\n","authors":["Yanan Jia"],"pdf_url":"https://arxiv.org/pdf/2303.10510v1.pdf","comment":"4th International Conference on Natural Language Processing and\n  Computational Linguistics (NLPCL 2023)"},{"id":"http://arxiv.org/abs/2303.10475v1","updated":"2023-03-18T19:17:47Z","published":"2023-03-18T19:17:47Z","title":"Is Prompt All You Need? No. A Comprehensive and Broader View of\n  Instruction Learning","summary":"  Task semantics can be expressed by a set of input-to-output examples or a\npiece of textual instruction. Conventional machine learning approaches for\nnatural language processing (NLP) mainly rely on the availability of\nlarge-scale sets of task-specific examples. Two issues arise: first, collecting\ntask-specific labeled examples does not apply to scenarios where tasks may be\ntoo complicated or costly to annotate, or the system is required to handle a\nnew task immediately; second, this is not user-friendly since end-users are\nprobably more willing to provide task description rather than a set of examples\nbefore using the system. Therefore, the community is paying increasing interest\nin a new supervision-seeking paradigm for NLP: learning from task instructions.\nDespite its impressive progress, there are some common issues that the\ncommunity struggles with. This survey paper tries to summarize the current\nresearch on instruction learning, particularly, by answering the following\nquestions: (i) what is task instruction, and what instruction types exist? (ii)\nhow to model instructions? (iii) what factors influence and explain the\ninstructions' performance? (iv) what challenges remain in instruction learning?\nTo our knowledge, this is the first comprehensive survey about textual\ninstructions.\n","authors":["Renze Lou","Kai Zhang","Wenpeng Yin"],"pdf_url":"https://arxiv.org/pdf/2303.10475v1.pdf","comment":"Work is still in progress. The paper list is available at\n  https://github.com/RenzeLou/awesome-instruction-learning"},{"id":"http://arxiv.org/abs/2202.09791v4","updated":"2023-03-18T18:43:05Z","published":"2022-02-20T11:14:04Z","title":"Contextual Semantic Embeddings for Ontology Subsumption Prediction","summary":"  Automating ontology construction and curation is an important but challenging\ntask in knowledge engineering and artificial intelligence. Prediction by\nmachine learning techniques such as contextual semantic embedding is a\npromising direction, but the relevant research is still preliminary especially\nfor expressive ontologies in Web Ontology Language (OWL). In this paper, we\npresent a new subsumption prediction method named BERTSubs for classes of OWL\nontology. It exploits the pre-trained language model BERT to compute contextual\nembeddings of a class, where customized templates are proposed to incorporate\nthe class context (e.g., neighbouring classes) and the logical existential\nrestriction. BERTSubs is able to predict multiple kinds of subsumers including\nnamed classes from the same ontology or another ontology, and existential\nrestrictions from the same ontology. Extensive evaluation on five real-world\nontologies for three different subsumption tasks has shown the effectiveness of\nthe templates and that BERTSubs can dramatically outperform the baselines that\nuse (literal-aware) knowledge graph embeddings, non-contextual word embeddings\nand the state-of-the-art OWL ontology embeddings.\n","authors":["Jiaoyan Chen","Yuan He","Yuxia Geng","Ernesto Jimenez-Ruiz","Hang Dong","Ian Horrocks"],"pdf_url":"https://arxiv.org/pdf/2202.09791v4.pdf","comment":"Accepted by World Wide Web Journal"},{"id":"http://arxiv.org/abs/2303.10464v1","updated":"2023-03-18T17:56:01Z","published":"2023-03-18T17:56:01Z","title":"SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language\n  Models","summary":"  The pre-training and fine-tuning paradigm has contributed to a number of\nbreakthroughs in Natural Language Processing (NLP). Instead of directly\ntraining on a downstream task, language models are first pre-trained on large\ndatasets with cross-domain knowledge (e.g., Pile, MassiveText, etc.) and then\nfine-tuned on task-specific data (e.g., natural language generation, text\nsummarization, etc.). Scaling the model and dataset size has helped improve the\nperformance of LLMs, but unfortunately, this also leads to highly prohibitive\ncomputational costs. Pre-training LLMs often require orders of magnitude more\nFLOPs than fine-tuning and the model capacity often remains the same between\nthe two phases. To achieve training efficiency w.r.t training FLOPs, we propose\nto decouple the model capacity between the two phases and introduce Sparse\nPre-training and Dense Fine-tuning (SPDF). In this work, we show the benefits\nof using unstructured weight sparsity to train only a subset of weights during\npre-training (Sparse Pre-training) and then recover the representational\ncapacity by allowing the zeroed weights to learn (Dense Fine-tuning). We\ndemonstrate that we can induce up to 75% sparsity into a 1.3B parameter GPT-3\nXL model resulting in a 2.5x reduction in pre-training FLOPs, without a\nsignificant loss in accuracy on the downstream tasks relative to the dense\nbaseline. By rigorously evaluating multiple downstream tasks, we also establish\na relationship between sparsity, task complexity, and dataset size. Our work\npresents a promising direction to train large GPT models at a fraction of the\ntraining FLOPs using weight sparsity while retaining the benefits of\npre-trained textual representations for downstream tasks.\n","authors":["Vithursan Thangarasa","Abhay Gupta","William Marshall","Tianda Li","Kevin Leong","Dennis DeCoste","Sean Lie","Shreyas Saxena"],"pdf_url":"https://arxiv.org/pdf/2303.10464v1.pdf","comment":"Presented at the ICLR 2023 Workshop on Sparsity in Neural Networks"},{"id":"http://arxiv.org/abs/2303.10443v1","updated":"2023-03-18T15:55:49Z","published":"2023-03-18T15:55:49Z","title":"GazeReader: Detecting Unknown Word Using Webcam for English as a Second\n  Language (ESL) Learners","summary":"  Automatic unknown word detection techniques can enable new applications for\nassisting English as a Second Language (ESL) learners, thus improving their\nreading experiences. However, most modern unknown word detection methods\nrequire dedicated eye-tracking devices with high precision that are not easily\naccessible to end-users. In this work, we propose GazeReader, an unknown word\ndetection method only using a webcam. GazeReader tracks the learner's gaze and\nthen applies a transformer-based machine learning model that encodes the text\ninformation to locate the unknown word. We applied knowledge enhancement\nincluding term frequency, part of speech, and named entity recognition to\nimprove the performance. The user study indicates that the accuracy and\nF1-score of our method were 98.09% and 75.73%, respectively. Lastly, we\nexplored the design scope for ESL reading and discussed the findings.\n","authors":["Jiexin Ding","Bowen Zhao","Yuqi Huang","Yuntao Wang","Yuanchun Shi"],"pdf_url":"https://arxiv.org/pdf/2303.10443v1.pdf","comment":"This paper has been accepted by ACM CHI 2023"},{"id":"http://arxiv.org/abs/2303.10439v1","updated":"2023-03-18T15:39:23Z","published":"2023-03-18T15:39:23Z","title":"Stop Words for Processing Software Engineering Documents: Do they\n  Matter?","summary":"  Stop words, which are considered non-predictive, are often eliminated in\nnatural language processing tasks. However, the definition of uninformative\nvocabulary is vague, so most algorithms use general knowledge-based stop lists\nto remove stop words. There is an ongoing debate among academics about the\nusefulness of stop word elimination, especially in domain-specific settings. In\nthis work, we investigate the usefulness of stop word removal in a software\nengineering context. To do this, we replicate and experiment with three\nsoftware engineering research tools from related work. Additionally, we\nconstruct a corpus of software engineering domain-related text from 10,000\nStack Overflow questions and identify 200 domain-specific stop words using\ntraditional information-theoretic methods. Our results show that the use of\ndomain-specific stop words significantly improved the performance of research\ntools compared to the use of a general stop list and that 17 out of 19\nevaluation measures showed better performance.\n","authors":["Yaohou Fan","Chetan Arora","Christoph Treude"],"pdf_url":"https://arxiv.org/pdf/2303.10439v1.pdf","comment":"Accepted for publication at the 2nd Intl. Workshop on NL-based\n  Software Engineering (NLBSE 2023)"},{"id":"http://arxiv.org/abs/2303.10430v1","updated":"2023-03-18T14:54:57Z","published":"2023-03-18T14:54:57Z","title":"NoisyHate: Benchmarking Content Moderation Machine Learning Models with\n  Human-Written Perturbations Online","summary":"  Online texts with toxic content are a threat in social media that might cause\ncyber harassment. Although many platforms applied measures, such as machine\nlearning-based hate-speech detection systems, to diminish their effect, those\ntoxic content publishers can still evade the system by modifying the spelling\nof toxic words. Those modified words are also known as human-written text\nperturbations. Many research works developed certain techniques to generate\nadversarial samples to help the machine learning models obtain the ability to\nrecognize those perturbations. However, there is still a gap between those\nmachine-generated perturbations and human-written perturbations. In this paper,\nwe introduce a benchmark test set containing human-written perturbations online\nfor toxic speech detection models. We also recruited a group of workers to\nevaluate the quality of this test set and dropped low-quality samples.\nMeanwhile, to check if our perturbation can be normalized to its clean version,\nwe applied spell corrector algorithms on this dataset. Finally, we test this\ndata on state-of-the-art language models, such as BERT and RoBERTa, and black\nbox APIs, such as perspective API, to demonstrate the adversarial attack with\nreal human-written perturbations is still effective.\n","authors":["Yiran Ye","Thai Le","Dongwon Lee"],"pdf_url":"https://arxiv.org/pdf/2303.10430v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10420v1","updated":"2023-03-18T14:02:04Z","published":"2023-03-18T14:02:04Z","title":"A Comprehensive Capability Analysis of GPT-3 and GPT-3.5 Series Models","summary":"  GPT series models, such as GPT-3, CodeX, InstructGPT, ChatGPT, and so on,\nhave gained considerable attention due to their exceptional natural language\nprocessing capabilities. However, despite the abundance of research on the\ndifference in capabilities between GPT series models and fine-tuned models,\nthere has been limited attention given to the evolution of GPT series models'\ncapabilities over time. To conduct a comprehensive analysis of the capabilities\nof GPT series models, we select six representative models, comprising two GPT-3\nseries models (i.e., davinci and text-davinci-001) and four GPT-3.5 series\nmodels (i.e., code-davinci-002, text-davinci-002, text-davinci-003, and\ngpt-3.5-turbo). We evaluate their performance on nine natural language\nunderstanding (NLU) tasks using 21 datasets. In particular, we compare the\nperformance and robustness of different models for each task under zero-shot\nand few-shot scenarios. Our extensive experiments reveal that the overall\nability of GPT series models on NLU tasks does not increase gradually as the\nmodels evolve, especially with the introduction of the RLHF training strategy.\nWhile this strategy enhances the models' ability to generate human-like\nresponses, it also compromises their ability to solve some tasks. Furthermore,\nour findings indicate that there is still room for improvement in areas such as\nmodel robustness.\n","authors":["Junjie Ye","Xuanting Chen","Nuo Xu","Can Zu","Zekai Shao","Shichun Liu","Yuhan Cui","Zeyang Zhou","Chao Gong","Yang Shen","Jie Zhou","Siming Chen","Tao Gui","Qi Zhang","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2303.10420v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10395v1","updated":"2023-03-18T11:15:33Z","published":"2023-03-18T11:15:33Z","title":"A Graph-Guided Reasoning Approach for Open-ended Commonsense Question\n  Answering","summary":"  Recently, end-to-end trained models for multiple-choice commonsense question\nanswering (QA) have delivered promising results. However, such\nquestion-answering systems cannot be directly applied in real-world scenarios\nwhere answer candidates are not provided. Hence, a new benchmark challenge set\nfor open-ended commonsense reasoning (OpenCSR) has been recently released,\nwhich contains natural science questions without any predefined choices. On the\nOpenCSR challenge set, many questions require implicit multi-hop reasoning and\nhave a large decision space, reflecting the difficult nature of this task.\nExisting work on OpenCSR sorely focuses on improving the retrieval process,\nwhich extracts relevant factual sentences from a textual knowledge base,\nleaving the important and non-trivial reasoning task outside the scope. In this\nwork, we extend the scope to include a reasoner that constructs a\nquestion-dependent open knowledge graph based on retrieved supporting facts and\nemploys a sequential subgraph reasoning process to predict the answer. The\nsubgraph can be seen as a concise and compact graphical explanation of the\nprediction. Experiments on two OpenCSR datasets show that the proposed model\nachieves great performance on benchmark OpenCSR datasets.\n","authors":["Zhen Han","Yue Feng","Mingming Sun"],"pdf_url":"https://arxiv.org/pdf/2303.10395v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10384v1","updated":"2023-03-18T10:36:33Z","published":"2023-03-18T10:36:33Z","title":"Powerful and Extensible WFST Framework for RNN-Transducer Losses","summary":"  This paper presents a framework based on Weighted Finite-State Transducers\n(WFST) to simplify the development of modifications for RNN-Transducer (RNN-T)\nloss. Existing implementations of RNN-T use CUDA-related code, which is hard to\nextend and debug. WFSTs are easy to construct and extend, and allow debugging\nthrough visualization. We introduce two WFST-powered RNN-T implementations: (1)\n\"Compose-Transducer\", based on a composition of the WFST graphs from acoustic\nand textual schema -- computationally competitive and easy to modify; (2)\n\"Grid-Transducer\", which constructs the lattice directly for further\ncomputations -- most compact, and computationally efficient. We illustrate the\nease of extensibility through introduction of a new W-Transducer loss -- the\nadaptation of the Connectionist Temporal Classification with Wild Cards.\nW-Transducer (W-RNNT) consistently outperforms the standard RNN-T in a\nweakly-supervised data setup with missing parts of transcriptions at the\nbeginning and end of utterances. All RNN-T losses are implemented with the k2\nframework and are available in the NeMo toolkit.\n","authors":["Aleksandr Laptev","Vladimir Bataev","Igor Gitman","Boris Ginsburg"],"pdf_url":"https://arxiv.org/pdf/2303.10384v1.pdf","comment":"To appear in Proc. ICASSP 2023, June 04-10, 2023, Rhodes island,\n  Greece. 5 pages, 5 figures, 3 tables"},{"id":"http://arxiv.org/abs/2303.10368v1","updated":"2023-03-18T08:57:09Z","published":"2023-03-18T08:57:09Z","title":"An Empirical Study of Pre-trained Language Models in Simple Knowledge\n  Graph Question Answering","summary":"  Large-scale pre-trained language models (PLMs) such as BERT have recently\nachieved great success and become a milestone in natural language processing\n(NLP). It is now the consensus of the NLP community to adopt PLMs as the\nbackbone for downstream tasks. In recent works on knowledge graph question\nanswering (KGQA), BERT or its variants have become necessary in their KGQA\nmodels. However, there is still a lack of comprehensive research and comparison\nof the performance of different PLMs in KGQA. To this end, we summarize two\nbasic KGQA frameworks based on PLMs without additional neural network modules\nto compare the performance of nine PLMs in terms of accuracy and efficiency. In\naddition, we present three benchmarks for larger-scale KGs based on the popular\nSimpleQuestions benchmark to investigate the scalability of PLMs. We carefully\nanalyze the results of all PLMs-based KGQA basic frameworks on these benchmarks\nand two other popular datasets, WebQuestionSP and FreebaseQA, and find that\nknowledge distillation techniques and knowledge enhancement methods in PLMs are\npromising for KGQA. Furthermore, we test ChatGPT, which has drawn a great deal\nof attention in the NLP community, demonstrating its impressive capabilities\nand limitations in zero-shot KGQA. We have released the code and benchmarks to\npromote the use of PLMs on KGQA.\n","authors":["Nan Hu","Yike Wu","Guilin Qi","Dehai Min","Jiaoyan Chen","Jeff Z. Pan","Zafar Ali"],"pdf_url":"https://arxiv.org/pdf/2303.10368v1.pdf","comment":"Accepted by World Wide Web Journal"},{"id":"http://arxiv.org/abs/2302.02078v2","updated":"2023-03-18T08:29:52Z","published":"2023-02-04T03:30:07Z","title":"FGSI: Distant Supervision for Relation Extraction method based on\n  Fine-Grained Semantic Information","summary":"  The main purpose of relation extraction is to extract the semantic\nrelationships between tagged pairs of entities in a sentence, which plays an\nimportant role in the semantic understanding of sentences and the construction\nof knowledge graphs. In this paper, we propose that the key semantic\ninformation within a sentence plays a key role in the relationship extraction\nof entities. We propose the hypothesis that the key semantic information inside\nthe sentence plays a key role in entity relationship extraction. And based on\nthis hypothesis, we split the sentence into three segments according to the\nlocation of the entity from the inside of the sentence, and find the\nfine-grained semantic features inside the sentence through the intra-sentence\nattention mechanism to reduce the interference of irrelevant noise information.\nThe proposed relational extraction model can make full use of the available\npositive semantic information. The experimental results show that the proposed\nrelation extraction model improves the accuracy-recall curves and P@N values\ncompared with existing methods, which proves the effectiveness of this model.\n","authors":["Chenghong Sun","Weidong Ji","Guohui Zhou","Hui Guo","Zengxiang Yin","Yuqi Yue"],"pdf_url":"https://arxiv.org/pdf/2302.02078v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.00465v2","updated":"2023-03-18T06:54:57Z","published":"2023-03-01T12:46:00Z","title":"Uzbek text's correspondence with the educational potential of pupils: a\n  case study of the School corpus","summary":"  One of the major challenges of an educational system is choosing appropriate\ncontent considering pupils' age and intellectual potential. In this article the\nexperiment of primary school grades (from 1st to 4th grades) is considered for\nautomatically determining the correspondence of an educational materials\nrecommended for pupils by using the School corpus where it includes the dataset\nof 25 school textbooks confirmed by the Ministry of preschool and school\neducation of the Republic of Uzbekistan. In this case, TF-IDF scores of the\ntexts are determined, they are converted into a vector representation, and the\ngiven educational materials are compared with the corresponding class of the\nSchool corpus using the cosine similarity algorithm. Based on the results of\nthe calculation, it is determined whether the given educational material is\nappropriate or not appropriate for the pupils' educational potential.\n","authors":["Khabibulla Madatov","Sanatbek Matlatipov","Mersaid Aripov"],"pdf_url":"https://arxiv.org/pdf/2303.00465v2.pdf","comment":"Preprint of the paper accepted to The 10th Language & Technology\n  Conference: Human Language Technologies as a Challenge for Computer Science\n  and Linguistics. April 21-23, 2023, Poznan, Poland"},{"id":"http://arxiv.org/abs/2303.10330v1","updated":"2023-03-18T04:31:07Z","published":"2023-03-18T04:31:07Z","title":"Exploring Partial Knowledge Base Inference in Biomedical Entity Linking","summary":"  Biomedical entity linking (EL) consists of named entity recognition (NER) and\nnamed entity disambiguation (NED). EL models are trained on corpora labeled by\na predefined KB. However, it is a common scenario that only entities within a\nsubset of the KB are precious to stakeholders. We name this scenario partial\nknowledge base inference: training an EL model with one KB and inferring on the\npart of it without further training. In this work, we give a detailed\ndefinition and evaluation procedures for this practically valuable but\nsignificantly understudied scenario and evaluate methods from three\nrepresentative EL paradigms. We construct partial KB inference benchmarks and\nwitness a catastrophic degradation in EL performance due to dramatically\nprecision drop. Our findings reveal these EL paradigms can not correctly handle\nunlinkable mentions (NIL), so they are not robust to partial KB inference. We\nalso propose two simple-and-effective redemption methods to combat the NIL\nissue with little computational overhead.\n","authors":["Hongyi Yuan","Keming Lu","Zheng Yuan"],"pdf_url":"https://arxiv.org/pdf/2303.10330v1.pdf","comment":"13 pages, 3 figures. The first two authors are contributed equally"},{"id":"http://arxiv.org/abs/2303.10328v1","updated":"2023-03-18T04:28:01Z","published":"2023-03-18T04:28:01Z","title":"Revisiting Automatic Question Summarization Evaluation in the Biomedical\n  Domain","summary":"  Automatic evaluation metrics have been facilitating the rapid development of\nautomatic summarization methods by providing instant and fair assessments of\nthe quality of summaries. Most metrics have been developed for the general\ndomain, especially news and meeting notes, or other language-generation tasks.\nHowever, these metrics are applied to evaluate summarization systems in\ndifferent domains, such as biomedical question summarization. To better\nunderstand whether commonly used evaluation metrics are capable of evaluating\nautomatic summarization in the biomedical domain, we conduct human evaluations\nof summarization quality from four different aspects of a biomedical question\nsummarization task. Based on human judgments, we identify different noteworthy\nfeatures for current automatic metrics and summarization systems as well. We\nalso release a dataset of our human annotations to aid the research of\nsummarization evaluation metrics in the biomedical domain.\n","authors":["Hongyi Yuan","Yaoyun Zhang","Fei Huang","Songfang Huang"],"pdf_url":"https://arxiv.org/pdf/2303.10328v1.pdf","comment":"8 pages, 1 figure"},{"id":"http://arxiv.org/abs/2112.03073v3","updated":"2023-03-18T03:55:06Z","published":"2021-11-26T07:58:11Z","title":"Active Learning for Event Extraction with Memory-based Loss Prediction\n  Model","summary":"  Event extraction (EE) plays an important role in many industrial application\nscenarios, and high-quality EE methods require a large amount of manual\nannotation data to train supervised learning models. However, the cost of\nobtaining annotation data is very high, especially for annotation of domain\nevents, which requires the participation of experts from corresponding domain.\nSo we introduce active learning (AL) technology to reduce the cost of event\nannotation. But the existing AL methods have two main problems, which make them\nnot well used for event extraction. Firstly, the existing pool-based selection\nstrategies have limitations in terms of computational cost and sample validity.\nSecondly, the existing evaluation of sample importance lacks the use of local\nsample information. In this paper, we present a novel deep AL method for EE. We\npropose a batch-based selection strategy and a Memory-Based Loss Prediction\nmodel (MBLP) to select unlabeled samples efficiently. During the selection\nprocess, we use an internal-external sample loss ranking method to evaluate the\nsample importance by using local information. Finally, we propose a delayed\ntraining strategy to train the MBLP model. Extensive experiments are performed\non three domain datasets, and our method outperforms other state-of-the-art\nmethods.\n","authors":["Shirong Shen","Zhen Li","Guilin Qi"],"pdf_url":"https://arxiv.org/pdf/2112.03073v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10311v1","updated":"2023-03-18T02:46:49Z","published":"2023-03-18T02:46:49Z","title":"On the rise of fear speech in online social media","summary":"  Recently, social media platforms are heavily moderated to prevent the spread\nof online hate speech, which is usually fertile in toxic words and is directed\ntoward an individual or a community. Owing to such heavy moderation, newer and\nmore subtle techniques are being deployed. One of the most striking among these\nis fear speech. Fear speech, as the name suggests, attempts to incite fear\nabout a target community. Although subtle, it might be highly effective, often\npushing communities toward a physical conflict. Therefore, understanding their\nprevalence in social media is of paramount importance. This article presents a\nlarge-scale study to understand the prevalence of 400K fear speech and over\n700K hate speech posts collected from Gab.com. Remarkably, users posting a\nlarge number of fear speech accrue more followers and occupy more central\npositions in social networks than users posting a large number of hate speech.\nThey can also reach out to benign users more effectively than hate speech users\nthrough replies, reposts, and mentions. This connects to the fact that, unlike\nhate speech, fear speech has almost zero toxic content, making it look\nplausible. Moreover, while fear speech topics mostly portray a community as a\nperpetrator using a (fake) chain of argumentation, hate speech topics hurl\ndirect multitarget insults, thus pointing to why general users could be more\ngullible to fear speech. Our findings transcend even to other platforms\n(Twitter and Facebook) and thus necessitate using sophisticated moderation\npolicies and mass awareness to combat fear speech.\n","authors":["Punyajoy Saha","Kiran Garimella","Narla Komal Kalyan","Saurabh Kumar Pandey","Pauras Mangesh Meher","Binny Mathew","Animesh Mukherjee"],"pdf_url":"https://arxiv.org/pdf/2303.10311v1.pdf","comment":"16 pages, 9 tables, 15 figures, accepted in Proceedings of the\n  National Academy of Sciences of the United States of America"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2303.10497v1","updated":"2023-03-18T20:50:33Z","published":"2023-03-18T20:50:33Z","title":"Examining the Potential for Conversational Exploratory Search using a\n  Smart Speaker Digital Assistant","summary":"  Online Digital Assistants, such as Amazon Alexa, Google Assistant, Apple Siri\nare very popular and provide a range or services to their users, a key function\nis their ability to satisfy user information needs from the sources available\nto them. Users may often regard these applications as providing search services\nsimilar to Google type search engines. However, while it is clear that they are\nin general able to answer factoid questions effectively, it is much less\nobvious how well they support less specific or exploratory type search tasks.\nWe describe an investigation examining the behaviour of the standard Amazon\nAlexa for exploratory search tasks. The results of our study show that it not\neffective in addressing these types of information needs. We propose extensions\nto Alexa designed to overcome these shortcomings. Our Custom Alexa application\nextends Alexa's conversational functionality for exploratory search. A user\nstudy shows that our extended Alexa application both enables users to more\nsuccessfully complete exploratory search tasks and is well accepted by our test\nusers.\n","authors":["Abhishek Kaushik","Gareth J. F. Jones"],"pdf_url":"https://arxiv.org/pdf/2303.10497v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.07145v2","updated":"2023-03-18T08:29:20Z","published":"2023-02-14T15:53:12Z","title":"Practical Cross-System Shilling Attacks with Limited Access to Data","summary":"  In shilling attacks, an adversarial party injects a few fake user profiles\ninto a Recommender System (RS) so that the target item can be promoted or\ndemoted. Although much effort has been devoted to developing shilling attack\nmethods, we find that existing approaches are still far from practical. In this\npaper, we analyze the properties a practical shilling attack method should have\nand propose a new concept of Cross-system Attack. With the idea of Cross-system\nAttack, we design a Practical Cross-system Shilling Attack (PC-Attack)\nframework that requires little information about the victim RS model and the\ntarget RS data for conducting attacks. PC-Attack is trained to capture graph\ntopology knowledge from public RS data in a self-supervised manner. Then, it is\nfine-tuned on a small portion of target data that is easy to access to\nconstruct fake profiles. Extensive experiments have demonstrated the\nsuperiority of PC-Attack over state-of-the-art baselines. Our implementation of\nPC-Attack is available at https://github.com/KDEGroup/PC-Attack.\n","authors":["Meifang Zeng","Ke Li","Bingchuan Jiang","Liujuan Cao","Hui Li"],"pdf_url":"https://arxiv.org/pdf/2302.07145v2.pdf","comment":"AAAI 2023"},{"id":"http://arxiv.org/abs/2303.08537v2","updated":"2023-03-18T02:30:32Z","published":"2023-03-15T11:30:16Z","title":"Graph-less Collaborative Filtering","summary":"  Graph neural networks (GNNs) have shown the power in representation learning\nover graph-structured user-item interaction data for collaborative filtering\n(CF) task. However, with their inherently recursive message propagation among\nneighboring nodes, existing GNN-based CF models may generate indistinguishable\nand inaccurate user (item) representations due to the over-smoothing and noise\neffect with low-pass Laplacian smoothing operators. In addition, the recursive\ninformation propagation with the stacked aggregators in the entire graph\nstructures may result in poor scalability in practical applications. Motivated\nby these limitations, we propose a simple and effective collaborative filtering\nmodel (SimRec) that marries the power of knowledge distillation and contrastive\nlearning. In SimRec, adaptive transferring knowledge is enabled between the\nteacher GNN model and a lightweight student network, to not only preserve the\nglobal collaborative signals, but also address the over-smoothing issue with\nrepresentation recalibration. Empirical results on public datasets show that\nSimRec archives better efficiency while maintaining superior recommendation\nperformance compared with various strong baselines. Our implementations are\npublicly available at: https://github.com/HKUDS/SimRec.\n","authors":["Lianghao Xia","Chao Huang","Jiao Shi","Yong Xu"],"pdf_url":"https://arxiv.org/pdf/2303.08537v2.pdf","comment":"Accepted by ACM WWW 2023"},{"id":"http://arxiv.org/abs/2302.04545v2","updated":"2023-03-18T02:10:10Z","published":"2023-02-09T10:20:23Z","title":"Lorentz Equivariant Model for Knowledge-Enhanced Hyperbolic\n  Collaborative Filtering","summary":"  Introducing prior auxiliary information from the knowledge graph (KG) to\nassist the user-item graph can improve the comprehensive performance of the\nrecommender system. Many recent studies show that the ensemble properties of\nhyperbolic spaces fit the scale-free and hierarchical characteristics exhibited\nin the above two types of graphs well. However, existing hyperbolic methods\nignore the consideration of equivariance, thus they cannot generalize symmetric\nfeatures under given transformations, which seriously limits the capability of\nthe model. Moreover, they cannot balance preserving the heterogeneity and\nmining the high-order entity information to users across two graphs. To fill\nthese gaps, we propose a rigorously Lorentz group equivariant\nknowledge-enhanced collaborative filtering model (LECF). Innovatively, we\njointly update the attribute embeddings (containing the high-order entity\nsignals from the KG) and hyperbolic embeddings (the distance between hyperbolic\nembeddings reveals the recommendation tendency) by the LECF layer with Lorentz\nEquivariant Transformation. Moreover, we propose Hyperbolic Sparse Attention\nMechanism to sample the most informative neighbor nodes. Lorentz equivariance\nis strictly maintained throughout the entire model, and enforcing equivariance\nis proven necessary experimentally. Extensive experiments on three real-world\nbenchmarks demonstrate that LECF remarkably outperforms state-of-the-art\nmethods.\n","authors":["Bosong Huang","Weihao Yu","Ruzhong Xie","Jing Xiao","Jin Huang"],"pdf_url":"https://arxiv.org/pdf/2302.04545v2.pdf","comment":"11 pages, 6 figures"}],"Multimedia":[{"id":"http://arxiv.org/abs/2303.10446v1","updated":"2023-03-18T16:09:10Z","published":"2023-03-18T16:09:10Z","title":"A Content Adaptive Learnable Time-Frequency Representation For Audio\n  Signal Processing","summary":"  We propose a learnable content adaptive front end for audio signal\nprocessing. Before the modern advent of deep learning, we used fixed\nrepresentation non-learnable front-ends like spectrogram or mel-spectrogram\nwith/without neural architectures. With convolutional architectures supporting\nvarious applications such as ASR and acoustic scene understanding, a shift to a\nlearnable front ends occurred in which both the type of basis functions and the\nweight were learned from scratch and optimized for the particular task of\ninterest. With the shift to transformer-based architectures with no\nconvolutional blocks present, a linear layer projects small waveform patches\nonto a small latent dimension before feeding them to a transformer\narchitecture. In this work, we propose a way of computing a content-adaptive\nlearnable time-frequency representation. We pass each audio signal through a\nbank of convolutional filters, each giving a fixed-dimensional vector. It is\nakin to learning a bank of finite impulse-response filterbanks and passing the\ninput signal through the optimum filter bank depending on the content of the\ninput signal. A content-adaptive learnable time-frequency representation may be\nmore broadly applicable, beyond the experiments in this paper.\n","authors":["Prateek Verma","Chris Chafe"],"pdf_url":"https://arxiv.org/pdf/2303.10446v1.pdf","comment":"5 pages, 4 figures. 2023 IEEE International Conference on Acoustics,\n  Speech, and Signal Processing, Rhodes, Greece"},{"id":"http://arxiv.org/abs/2303.10372v1","updated":"2023-03-18T09:36:59Z","published":"2023-03-18T09:36:59Z","title":"Just Noticeable Visual Redundancy Forecasting: A Deep Multimodal-driven\n  Approach","summary":"  Just noticeable difference (JND) refers to the maximum visual change that\nhuman eyes cannot perceive, and it has a wide range of applications in\nmultimedia systems. However, most existing JND approaches only focus on a\nsingle modality, and rarely consider the complementary effects of multimodal\ninformation. In this article, we investigate the JND modeling from an\nend-to-end homologous multimodal perspective, namely hmJND-Net. Specifically,\nwe explore three important visually sensitive modalities, including saliency,\ndepth, and segmentation. To better utilize homologous multimodal information,\nwe establish an effective fusion method via summation enhancement and\nsubtractive offset, and align homologous multimodal features based on a\nself-attention driven encoder-decoder paradigm. Extensive experimental results\non eight different benchmark datasets validate the superiority of our hmJND-Net\nover eight representative methods.\n","authors":["Wuyuan Xie","Shukang Wang","Sukun Tian","Lirong Huang","Ye Liu","Miaohui Wang"],"pdf_url":"https://arxiv.org/pdf/2303.10372v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10369v1","updated":"2023-03-18T09:04:55Z","published":"2023-03-18T09:04:55Z","title":"Blind Multimodal Quality Assessment: A Brief Survey and A Case Study of\n  Low-light Images","summary":"  Blind image quality assessment (BIQA) aims at automatically and accurately\nforecasting objective scores for visual signals, which has been widely used to\nmonitor product and service quality in low-light applications, covering\nsmartphone photography, video surveillance, autonomous driving, etc. Recent\ndevelopments in this field are dominated by unimodal solutions inconsistent\nwith human subjective rating patterns, where human visual perception is\nsimultaneously reflected by multiple sensory information (e.g., sight and\nhearing). In this article, we present a unique blind multimodal quality\nassessment (BMQA) of low-light images from subjective evaluation to objective\nscore. To investigate the multimodal mechanism, we first establish a multimodal\nlow-light image quality (MLIQ) database with authentic low-light distortions,\ncontaining image and audio modality pairs. Further, we specially design the key\nmodules of BMQA, considering multimodal quality representation, latent feature\nalignment and fusion, and hybrid self-supervised and supervised learning.\nExtensive experiments show that our BMQA yields state-of-the-art accuracy on\nthe proposed MLIQ benchmark database. In particular, we also build an\nindependent single-image modality Dark-4K database, which is used to verify its\napplicability and generalization performance in mainstream unimodal\napplications. Qualitative and quantitative results on Dark-4K show that BMQA\nachieves superior performance to existing BIQA approaches as long as a\npre-trained quality semantic description model is provided. The proposed\nframework and two databases as well as the collected BIQA methods and\nevaluation metrics are made publicly available.\n","authors":["Miaohui Wang","Zhuowei Xu","Mai Xu","Weisi Lin"],"pdf_url":"https://arxiv.org/pdf/2303.10369v1.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2303.10335v1","updated":"2023-03-18T04:50:07Z","published":"2023-03-18T04:50:07Z","title":"Multimodal Continuous Emotion Recognition: A Technical Report for ABAW5","summary":"  We used two multimodal models for continuous valence-arousal recognition\nusing visual, audio, and linguistic information. The first model is the same as\nwe used in ABAW2 and ABAW3, which employs the leader-follower attention. The\nsecond model has the same architecture for spatial and temporal encoding. As\nfor the fusion block, it employs a compact and straightforward channel\nattention, borrowed from the End2You toolkit. Unlike our previous attempts that\nuse Vggish feature directly as the audio feature, this time we feed the\npre-trained VGG model using logmel-spectrogram and finetune it during the\ntraining. To make full use of the data and alleviate over-fitting,\ncross-validation is carried out. The fold with the highest concordance\ncorrelation coefficient is selected for submission. The code is to be available\nat https://github.com/sucv/ABAW5.\n","authors":["Su Zhang","Ziyuan Zhao","Cuntai Guan"],"pdf_url":"https://arxiv.org/pdf/2303.10335v1.pdf","comment":"4 pages. arXiv admin note: substantial text overlap with\n  arXiv:2203.13031"},{"id":"http://arxiv.org/abs/2303.10325v1","updated":"2023-03-18T04:01:53Z","published":"2023-03-18T04:01:53Z","title":"Smartbanner: Intelligent banner design framework that strikes a balance\n  between creative freedom and design rules","summary":"  Companies use banners extensively to promote their products, and the\nintelligent automatic synthesis of banners is a challenging event. Under the\npremise of inputting only a small amount of information such as product, text\nand size, it can synthesize styles with high freedom and richness, but at the\nsame time, it must satisfy the design specifications of advertisers for\nadvertising and scenes. We propose an intelligent banner design framework that\nstrikes a balance between creative freedom and design rules, called\nsmartbanner. Smartbanner consists of planner, actuator, adjuster and generator.\nThe banner is synthesized through the combined framework, which fully liberates\nthe designer and reduces the threshold and cost of design. It increases the\nclick-through rate by 30%, improves the human efficiency of designers by 500%\nunder the condition of ensuring the quality of creation, and synthesizes\nhundreds of millions of pictures in batches throughout the year.\n","authors":["Guandong Li","Xian Yang"],"pdf_url":"https://arxiv.org/pdf/2303.10325v1.pdf","comment":null}]},"2023-03-21T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2303.12060v1","updated":"2023-03-21T17:51:23Z","published":"2023-03-21T17:51:23Z","title":"VideoXum: Cross-modal Visual and Textural Summarization of Videos","summary":"  Video summarization aims to distill the most important information from a\nsource video to produce either an abridged clip or a textual narrative.\nTraditionally, different methods have been proposed depending on whether the\noutput is a video or text, thus ignoring the correlation between the two\nsemantically related tasks of visual summarization and textual summarization.\nWe propose a new joint video and text summarization task. The goal is to\ngenerate both a shortened video clip along with the corresponding textual\nsummary from a long video, collectively referred to as a cross-modal summary.\nThe generated shortened video clip and text narratives should be semantically\nwell aligned. To this end, we first build a large-scale human-annotated dataset\n-- VideoXum (X refers to different modalities). The dataset is reannotated\nbased on ActivityNet. After we filter out the videos that do not meet the\nlength requirements, 14,001 long videos remain in our new dataset. Each video\nin our reannotated dataset has human-annotated video summaries and the\ncorresponding narrative summaries. We then design a novel end-to-end model --\nVTSUM-BILP to address the challenges of our proposed task. Moreover, we propose\na new metric called VT-CLIPScore to help evaluate the semantic consistency of\ncross-modality summary. The proposed model achieves promising performance on\nthis new task and establishes a benchmark for future research.\n","authors":["Jingyang Lin","Hang Hua","Ming Chen","Yikang Li","Jenhao Hsiao","Chiuman Ho","Jiebo Luo"],"pdf_url":"https://arxiv.org/pdf/2303.12060v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12057v1","updated":"2023-03-21T17:48:00Z","published":"2023-03-21T17:48:00Z","title":"Large Language Models Can Be Used to Estimate the Ideologies of\n  Politicians in a Zero-Shot Learning Setting","summary":"  The mass aggregation of knowledge embedded in large language models (LLMs)\nholds the promise of new solutions to problems of observability and measurement\nin the social sciences. We examine the utility of one such model for a\nparticularly difficult measurement task: measuring the latent ideology of\nlawmakers, which allows us to better understand functions that are core to\ndemocracy, such as how politics shape policy and how political actors represent\ntheir constituents. We scale the senators of the 116th United States Congress\nalong the liberal-conservative spectrum by prompting ChatGPT to select the more\nliberal (or conservative) senator in pairwise comparisons. We show that the LLM\nproduced stable answers across repeated iterations, did not hallucinate, and\nwas not simply regurgitating information from a single source. This new scale\nstrongly correlates with pre-existing liberal-conservative scales such as\nNOMINATE, but also differs in several important ways, such as correctly placing\nsenators who vote against their party for far-left or far-right ideological\nreasons on the extreme ends. The scale also highly correlates with ideological\nmeasures based on campaign giving and political activists' perceptions of these\nsenators. In addition to the potential for better-automated data collection and\ninformation retrieval, our results suggest LLMs are likely to open new avenues\nfor measuring latent constructs like ideology that rely on aggregating large\nquantities of data from public sources.\n","authors":["Patrick Y. Wu","Joshua A. Tucker","Jonathan Nagler","Solomon Messing"],"pdf_url":"https://arxiv.org/pdf/2303.12057v1.pdf","comment":"18 pages, 4 figures"},{"id":"http://arxiv.org/abs/2303.12029v1","updated":"2023-03-21T17:14:04Z","published":"2023-03-21T17:14:04Z","title":"Wearing Masks Implies Refuting Trump?: Towards Target-specific User\n  Stance Prediction across Events in COVID-19 and US Election 2020","summary":"  People who share similar opinions towards controversial topics could form an\necho chamber and may share similar political views toward other topics as well.\nThe existence of such connections, which we call connected behavior, gives\nresearchers a unique opportunity to predict how one would behave for a future\nevent given their past behaviors. In this work, we propose a framework to\nconduct connected behavior analysis. Neural stance detection models are trained\non Twitter data collected on three seemingly independent topics, i.e., wearing\na mask, racial equality, and Trump, to detect people's stance, which we\nconsider as their online behavior in each topic-related event. Our results\nreveal a strong connection between the stances toward the three topical events\nand demonstrate the power of past behaviors in predicting one's future\nbehavior.\n","authors":["Hong Zhang","Haewoon Kwak","Wei Gao","Jisun An"],"pdf_url":"https://arxiv.org/pdf/2303.12029v1.pdf","comment":"10 pages, 2 pages, WebSci 2023, April 30-May 1, 2023, Evanston, TX,\n  USA"},{"id":"http://arxiv.org/abs/2303.12024v1","updated":"2023-03-21T17:04:44Z","published":"2023-03-21T17:04:44Z","title":"cTBL: Augmenting Large Language Models for Conversational Tables","summary":"  An open challenge in multimodal conversational AI requires augmenting large\nlanguage models with information from textual and non-textual sources for\nmulti-turn dialogue. To address this problem, this paper introduces\nConversational Tables (cTBL), a three-step encoder-decoder approach to retrieve\ntabular information and generate dialogue responses grounded on the retrieved\ninformation. cTBL uses Transformer encoder embeddings for Dense Table Retrieval\nand obtains up to 5% relative improvement in Top-1 and Top-3 accuracy over\nsparse retrieval on the HyrbiDialogue dataset. Additionally, cTBL performs\ntabular knowledge retrieval using both encoder and decoder models, resulting in\nup to 46% relative improvement in ROUGE scores and better human evaluation for\nresponse generation on HyrbiDialogue.\n","authors":["Anirudh S Sundar","Larry Heck"],"pdf_url":"https://arxiv.org/pdf/2303.12024v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12023v1","updated":"2023-03-21T16:56:05Z","published":"2023-03-21T16:56:05Z","title":"Logical Reasoning over Natural Language as Knowledge Representation: A\n  Survey","summary":"  Logical reasoning is central to human cognition and intelligence. Past\nresearch of logical reasoning within AI uses formal language as knowledge\nrepresentation~(and symbolic reasoners). However, reasoning with formal\nlanguage has proved challenging~(e.g., brittleness and knowledge-acquisition\nbottleneck). This paper provides a comprehensive overview on a new paradigm of\nlogical reasoning, which uses natural language as knowledge representation~(and\npretrained language models as reasoners), including philosophical definition\nand categorization of logical reasoning, advantages of the new paradigm,\nbenchmarks and methods, challenges of the new paradigm, desirable tasks &\nmethods in the future, and relation to related NLP fields. This new paradigm is\npromising since it not only alleviates many challenges of formal representation\nbut also has advantages over end-to-end neural methods.\n","authors":["Zonglin Yang","Xinya Du","Rui Mao","Jinjie Ni","Erik Cambria"],"pdf_url":"https://arxiv.org/pdf/2303.12023v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.01593v2","updated":"2023-03-21T14:22:00Z","published":"2023-03-02T21:35:15Z","title":"QAID: Question Answering Inspired Few-shot Intent Detection","summary":"  Intent detection with semantically similar fine-grained intents is a\nchallenging task. To address it, we reformulate intent detection as a\nquestion-answering retrieval task by treating utterances and intent names as\nquestions and answers. To that end, we utilize a question-answering retrieval\narchitecture and adopt a two stages training schema with batch contrastive\nloss. In the pre-training stage, we improve query representations through\nself-supervised training. Then, in the fine-tuning stage, we increase\ncontextualized token-level similarity scores between queries and answers from\nthe same intent. Our results on three few-shot intent detection benchmarks\nachieve state-of-the-art performance.\n","authors":["Asaf Yehudai","Matan Vetzler","Yosi Mass","Koren Lazar","Doron Cohen","Boaz Carmeli"],"pdf_url":"https://arxiv.org/pdf/2303.01593v2.pdf","comment":"ICLR paper"},{"id":"http://arxiv.org/abs/2204.04916v3","updated":"2023-03-21T12:58:01Z","published":"2022-04-11T07:33:26Z","title":"A Token-level Contrastive Framework for Sign Language Translation","summary":"  Sign Language Translation (SLT) is a promising technology to bridge the\ncommunication gap between the deaf and the hearing people. Recently,\nresearchers have adopted Neural Machine Translation (NMT) methods, which\nusually require large-scale corpus for training, to achieve SLT. However, the\npublicly available SLT corpus is very limited, which causes the collapse of the\ntoken representations and the inaccuracy of the generated tokens. To alleviate\nthis issue, we propose ConSLT, a novel token-level \\textbf{Con}trastive\nlearning framework for \\textbf{S}ign \\textbf{L}anguage \\textbf{T}ranslation ,\nwhich learns effective token representations by incorporating token-level\ncontrastive learning into the SLT decoding process. Concretely, ConSLT treats\neach token and its counterpart generated by different dropout masks as positive\npairs during decoding, and then randomly samples $K$ tokens in the vocabulary\nthat are not in the current sentence to construct negative examples. We conduct\ncomprehensive experiments on two benchmarks (PHOENIX14T and CSL-Daily) for both\nend-to-end and cascaded settings. The experimental results demonstrate that\nConSLT can achieve better translation quality than the strong baselines.\n","authors":["Biao Fu","Peigen Ye","Liang Zhang","Pei Yu","Cong Hu","Yidong Chen","Xiaodong Shi"],"pdf_url":"https://arxiv.org/pdf/2204.04916v3.pdf","comment":"Accepted to ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.11812v1","updated":"2023-03-21T12:55:54Z","published":"2023-03-21T12:55:54Z","title":"Chinese Intermediate English Learners outdid ChatGPT in deep cohesion:\n  Evidence from English narrative writing","summary":"  ChatGPT is a publicly available chatbot that can quickly generate texts on\ngiven topics, but it is unknown whether the chatbot is really superior to human\nwriters in all aspects of writing and whether its writing quality can be\nprominently improved on the basis of updating commands. Consequently, this\nstudy compared the writing performance on a narrative topic by ChatGPT and\nChinese intermediate English (CIE) learners so as to reveal the chatbot's\nadvantage and disadvantage in writing. The data were analyzed in terms of five\ndiscourse components using Coh-Metrix (a special instrument for analyzing\nlanguage discourses), and the results revealed that ChatGPT performed better\nthan human writers in narrativity, word concreteness, and referential cohesion,\nbut worse in syntactic simplicity and deep cohesion in its initial version.\nAfter more revision commands were updated, while the resulting version was\nfacilitated in syntactic simplicity, yet it is still lagged far behind CIE\nlearners' writing in deep cohesion. In addition, the correlation analysis of\nthe discourse components suggests that narrativity was correlated with\nreferential cohesion in both ChatGPT and human writers, but the correlations\nvaried within each group.\n","authors":["Tongquan Zhou","Siyi Cao","Siruo Zhou","Yao Zhang","Aijing He"],"pdf_url":"https://arxiv.org/pdf/2303.11812v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.09699v2","updated":"2023-03-21T12:10:18Z","published":"2022-11-15T19:07:53Z","title":"PromptCap: Prompt-Guided Image Captioning for VQA with GPT-3","summary":"  Knowledge-based visual question answering (VQA) involves questions that\nrequire world knowledge beyond the image to yield the correct answer. Large\nlanguage models (LMs) like GPT-3 are particularly helpful for this task because\nof their strong knowledge retrieval and reasoning capabilities. To enable LM to\nunderstand images, prior work uses a captioning model to convert images into\ntext. However, when summarizing an image in a single caption sentence, which\nvisual entities to describe are often underspecified. Generic image captions\noften miss visual details essential for the LM to answer visual questions\ncorrectly. To address this challenge, we propose PromptCap (Prompt-guided image\nCaptioning), a captioning model designed to serve as a better connector between\nimages and black-box LMs. Different from generic captions, PromptCap takes a\nnatural-language prompt to control the visual entities to describe in the\ngenerated caption. The prompt contains a question that the caption should aid\nin answering. To avoid extra annotation, PromptCap is trained by examples\nsynthesized with GPT-3 and existing datasets. We demonstrate PromptCap's\neffectiveness on an existing pipeline in which GPT-3 is prompted with image\ncaptions to carry out VQA. PromptCap outperforms generic captions by a large\nmargin and achieves state-of-the-art accuracy on knowledge-based VQA tasks\n(60.4% on OK-VQA and 59.6% on A-OKVQA). Zero-shot results on WebQA show that\nPromptCap generalizes well to unseen domains.\n","authors":["Yushi Hu","Hang Hua","Zhengyuan Yang","Weijia Shi","Noah A. Smith","Jiebo Luo"],"pdf_url":"https://arxiv.org/pdf/2211.09699v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11750v1","updated":"2023-03-21T11:17:37Z","published":"2023-03-21T11:17:37Z","title":"LEAPT: Learning Adaptive Prefix-to-prefix Translation For Simultaneous\n  Machine Translation","summary":"  Simultaneous machine translation, which aims at a real-time translation, is\nuseful in many live scenarios but very challenging due to the trade-off between\naccuracy and latency. To achieve the balance for both, the model needs to wait\nfor appropriate streaming text (READ policy) and then generates its translation\n(WRITE policy). However, WRITE policies of previous work either are specific to\nthe method itself due to the end-to-end training or suffer from the input\nmismatch between training and decoding for the non-end-to-end training.\nTherefore, it is essential to learn a generic and better WRITE policy for\nsimultaneous machine translation. Inspired by strategies utilized by human\ninterpreters and \"wait\" policies, we propose a novel adaptive prefix-to-prefix\ntraining policy called LEAPT, which allows our machine translation model to\nlearn how to translate source sentence prefixes and make use of the future\ncontext. Experiments show that our proposed methods greatly outperform\ncompetitive baselines and achieve promising results.\n","authors":["Lei Lin","Shuangtao Li","Xiaodong Shi"],"pdf_url":"https://arxiv.org/pdf/2303.11750v1.pdf","comment":"Accepted by ICASSP 2023"},{"id":"http://arxiv.org/abs/2302.14115v2","updated":"2023-03-21T11:01:09Z","published":"2023-02-27T19:53:49Z","title":"Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense\n  Video Captioning","summary":"  In this work, we introduce Vid2Seq, a multi-modal single-stage dense event\ncaptioning model pretrained on narrated videos which are readily-available at\nscale. The Vid2Seq architecture augments a language model with special time\ntokens, allowing it to seamlessly predict event boundaries and textual\ndescriptions in the same output sequence. Such a unified model requires\nlarge-scale training data, which is not available in current annotated\ndatasets. We show that it is possible to leverage unlabeled narrated videos for\ndense video captioning, by reformulating sentence boundaries of transcribed\nspeech as pseudo event boundaries, and using the transcribed speech sentences\nas pseudo event captions. The resulting Vid2Seq model pretrained on the\nYT-Temporal-1B dataset improves the state of the art on a variety of dense\nvideo captioning benchmarks including YouCook2, ViTT and ActivityNet Captions.\nVid2Seq also generalizes well to the tasks of video paragraph captioning and\nvideo clip captioning, and to few-shot settings. Our code is publicly available\nat https://antoyang.github.io/vid2seq.html.\n","authors":["Antoine Yang","Arsha Nagrani","Paul Hongsuck Seo","Antoine Miech","Jordi Pont-Tuset","Ivan Laptev","Josef Sivic","Cordelia Schmid"],"pdf_url":"https://arxiv.org/pdf/2302.14115v2.pdf","comment":"CVPR 2023 Camera-Ready; Project Webpage:\n  https://antoyang.github.io/vid2seq.html ; 18 pages; 6 figures"},{"id":"http://arxiv.org/abs/2303.11708v1","updated":"2023-03-21T10:01:49Z","published":"2023-03-21T10:01:49Z","title":"The Open-domain Paradox for Chatbots: Common Ground as the Basis for\n  Human-like Dialogue","summary":"  There is a surge in interest in the development of open-domain chatbots,\ndriven by the recent advancements of large language models. The \"openness\" of\nthe dialogue is expected to be maximized by providing minimal information to\nthe users about the common ground they can expect, including the presumed joint\nactivity. However, evidence suggests that the effect is the opposite. Asking\nusers to \"just chat about anything\" results in a very narrow form of dialogue,\nwhich we refer to as the \"open-domain paradox\". In this paper, we explain this\nparadox through the theory of common ground as the basis for human-like\ncommunication. Furthermore, we question the assumptions behind open-domain\nchatbots and identify paths forward for enabling common ground in\nhuman-computer dialogue.\n","authors":["Gabriel Skantze","A. Seza Doğruöz"],"pdf_url":"https://arxiv.org/pdf/2303.11708v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11141v2","updated":"2023-03-21T09:03:14Z","published":"2023-03-20T14:19:58Z","title":"DocRED-FE: A Document-Level Fine-Grained Entity And Relation Extraction\n  Dataset","summary":"  Joint entity and relation extraction (JERE) is one of the most important\ntasks in information extraction. However, most existing works focus on\nsentence-level coarse-grained JERE, which have limitations in real-world\nscenarios. In this paper, we construct a large-scale document-level\nfine-grained JERE dataset DocRED-FE, which improves DocRED with Fine-Grained\nEntity Type. Specifically, we redesign a hierarchical entity type schema\nincluding 11 coarse-grained types and 119 fine-grained types, and then\nre-annotate DocRED manually according to this schema. Through comprehensive\nexperiments we find that: (1) DocRED-FE is challenging to existing JERE models;\n(2) Our fine-grained entity types promote relation classification. We make\nDocRED-FE with instruction and the code for our baselines publicly available at\nhttps://github.com/PKU-TANGENT/DOCRED-FE.\n","authors":["Hongbo Wang","Weimin Xiong","Yifan Song","Dawei Zhu","Yu Xia","Sujian Li"],"pdf_url":"https://arxiv.org/pdf/2303.11141v2.pdf","comment":"Accepted by IEEE ICASSP 2023. The first two authors contribute\n  equally"},{"id":"http://arxiv.org/abs/2206.06924v5","updated":"2023-03-21T08:47:13Z","published":"2022-06-14T15:43:44Z","title":"The Maximum Linear Arrangement Problem for trees under projectivity and\n  planarity","summary":"  A linear arrangement is a mapping $\\pi$ from the $n$ vertices of a graph $G$\nto $n$ distinct consecutive integers. Linear arrangements can be represented by\ndrawing the vertices along a horizontal line and drawing the edges as\nsemicircles above said line. In this setting, the length of an edge is defined\nas the absolute value of the difference between the positions of its two\nvertices in the arrangement, and the cost of an arrangement as the sum of all\nedge lengths. Here we study two variants of the Maximum Linear Arrangement\nproblem (MaxLA), which consists of finding an arrangement that maximizes the\ncost. In the planar variant for free trees, vertices have to be arranged in\nsuch a way that there are no edge crossings. In the projective variant for\nrooted trees, arrangements have to be planar and the root of the tree cannot be\ncovered by any edge. In this paper we present algorithms that are linear in\ntime and space to solve planar and projective MaxLA for trees. We also prove\nseveral properties of maximum projective and planar arrangements, and show that\ncaterpillar trees maximize planar MaxLA over all trees of a fixed size thereby\ngeneralizing a previous extremal result on trees.\n","authors":["Lluís Alemany-Puig","Juan Luis Esteban","Ramon Ferrer-i-Cancho"],"pdf_url":"https://arxiv.org/pdf/2206.06924v5.pdf","comment":"The fourth version is incorrect. We are sure the right files were\n  uploaded but for whatever reason, it looks like we uploaded the wrong files.\n  The abstract in the fourth version is correct though"},{"id":"http://arxiv.org/abs/2303.11660v1","updated":"2023-03-21T08:08:04Z","published":"2023-03-21T08:08:04Z","title":"Simple Yet Effective Synthetic Dataset Construction for Unsupervised\n  Opinion Summarization","summary":"  Opinion summarization provides an important solution for summarizing opinions\nexpressed among a large number of reviews. However, generating aspect-specific\nand general summaries is challenging due to the lack of annotated data. In this\nwork, we propose two simple yet effective unsupervised approaches to generate\nboth aspect-specific and general opinion summaries by training on synthetic\ndatasets constructed with aspect-related review contents. Our first approach,\nSeed Words Based Leave-One-Out (SW-LOO), identifies aspect-related portions of\nreviews simply by exact-matching aspect seed words and outperforms existing\nmethods by 3.4 ROUGE-L points on SPACE and 0.5 ROUGE-1 point on OPOSUM+ for\naspect-specific opinion summarization. Our second approach, Natural Language\nInference Based Leave-One-Out (NLI-LOO) identifies aspect-related sentences\nutilizing an NLI model in a more general setting without using seed words and\noutperforms existing approaches by 1.2 ROUGE-L points on SPACE for\naspect-specific opinion summarization and remains competitive on other metrics.\n","authors":["Ming Shen","Jie Ma","Shuai Wang","Yogarshi Vyas","Kalpit Dixit","Miguel Ballesteros","Yassine Benajiba"],"pdf_url":"https://arxiv.org/pdf/2303.11660v1.pdf","comment":"EACL 2023 Findings"},{"id":"http://arxiv.org/abs/2303.11648v1","updated":"2023-03-21T07:46:57Z","published":"2023-03-21T07:46:57Z","title":"Improving Content Retrievability in Search with Controllable Query\n  Generation","summary":"  An important goal of online platforms is to enable content discovery, i.e.\nallow users to find a catalog entity they were not familiar with. A\npre-requisite to discover an entity, e.g. a book, with a search engine is that\nthe entity is retrievable, i.e. there are queries for which the system will\nsurface such entity in the top results. However, machine-learned search engines\nhave a high retrievability bias, where the majority of the queries return the\nsame entities. This happens partly due to the predominance of narrow intent\nqueries, where users create queries using the title of an already known entity,\ne.g. in book search 'harry potter'. The amount of broad queries where users\nwant to discover new entities, e.g. in music search 'chill lyrical electronica\nwith an atmospheric feeling to it', and have a higher tolerance to what they\nmight find, is small in comparison. We focus here on two factors that have a\nnegative impact on the retrievability of the entities (I) the training data\nused for dense retrieval models and (II) the distribution of narrow and broad\nintent queries issued in the system. We propose CtrlQGen, a method that\ngenerates queries for a chosen underlying intent-narrow or broad. We can use\nCtrlQGen to improve factor (I) by generating training data for dense retrieval\nmodels comprised of diverse synthetic queries. CtrlQGen can also be used to\ndeal with factor (II) by suggesting queries with broader intents to users. Our\nresults on datasets from the domains of music, podcasts, and books reveal that\nwe can significantly decrease the retrievability bias of a dense retrieval\nmodel when using CtrlQGen. First, by using the generated queries as training\ndata for dense models we make 9% of the entities retrievable (go from zero to\nnon-zero retrievability). Second, by suggesting broader queries to users, we\ncan make 12% of the entities retrievable in the best case.\n","authors":["Gustavo Penha","Enrico Palumbo","Maryam Aziz","Alice Wang","Hugues Bouchard"],"pdf_url":"https://arxiv.org/pdf/2303.11648v1.pdf","comment":"Accepted for publication in the International World Wide Web\n  Conference 2023"},{"id":"http://arxiv.org/abs/2203.03235v2","updated":"2023-03-21T07:43:30Z","published":"2022-03-07T09:47:53Z","title":"Pre-trained Token-replaced Detection Model as Few-shot Learner","summary":"  Pre-trained masked language models have demonstrated remarkable ability as\nfew-shot learners. In this paper, as an alternative, we propose a novel\napproach to few-shot learning with pre-trained token-replaced detection models\nlike ELECTRA. In this approach, we reformulate a classification or a regression\ntask as a token-replaced detection problem. Specifically, we first define a\ntemplate and label description words for each task and put them into the input\nto form a natural language prompt. Then, we employ the pre-trained\ntoken-replaced detection model to predict which label description word is the\nmost original (i.e., least replaced) among all label description words in the\nprompt. A systematic evaluation on 16 datasets demonstrates that our approach\noutperforms few-shot learners with pre-trained masked language models in both\none-sentence and two-sentence learning tasks.\n","authors":["Zicheng Li","Shoushan Li","Guodong Zhou"],"pdf_url":"https://arxiv.org/pdf/2203.03235v2.pdf","comment":"Accepted to COLING 2022. The code is publicly available at\n  https://github.com/cjfarmer/TRD_FSL"},{"id":"http://arxiv.org/abs/2005.13316v2","updated":"2023-03-21T07:39:27Z","published":"2020-05-27T12:21:36Z","title":"Tracking, exploring and analyzing recent developments in German-language\n  online press in the face of the coronavirus crisis: cOWIDplus Analysis and\n  cOWIDplus Viewer","summary":"  The coronavirus pandemic may be the largest crisis the world has had to face\nsince World War II. It does not come as a surprise that it is also having an\nimpact on language as our primary communication tool. We present three\ninter-connected resources that are designed to capture and illustrate these\neffects on a subset of the German language: An RSS corpus of German-language\nnewsfeeds (with freely available untruncated unigram frequency lists), a static\nbut continuously updated HTML page tracking the diversity of the used\nvocabulary and a web application that enables other researchers and the broader\npublic to explore these effects without any or with little knowledge of corpus\nrepresentation/exploration or statistical analyses.\n","authors":["Sascha Wolfer","Alexander Koplenig","Frank Michaelis","Carolin Müller-Spitzer"],"pdf_url":"https://arxiv.org/pdf/2005.13316v2.pdf","comment":"13 pages, 6 figures, 1 table, 3852 words"},{"id":"http://arxiv.org/abs/2303.11621v1","updated":"2023-03-21T06:41:50Z","published":"2023-03-21T06:41:50Z","title":"Heterogeneous-Branch Collaborative Learning for Dialogue Generation","summary":"  With the development of deep learning, advanced dialogue generation methods\nusually require a greater amount of computational resources. One promising\napproach to obtaining a high-performance and lightweight model is knowledge\ndistillation, which relies heavily on the pre-trained powerful teacher.\nCollaborative learning, also known as online knowledge distillation, is an\neffective way to conduct one-stage group distillation in the absence of a\nwell-trained large teacher model. However, previous work has a severe branch\nhomogeneity problem due to the same training objective and the independent\nidentical training sets. To alleviate this problem, we consider the dialogue\nattributes in the training of network branches. Each branch learns the\nattribute-related features based on the selected subset. Furthermore, we\npropose a dual group-based knowledge distillation method, consisting of\npositive distillation and negative distillation, to further diversify the\nfeatures of different branches in a steadily and interpretable way. The\nproposed approach significantly improves branch heterogeneity and outperforms\nstate-of-the-art collaborative learning methods on two widely used open-domain\ndialogue datasets.\n","authors":["Yiwei Li","Shaoxiong Feng","Bin Sun","Kan Li"],"pdf_url":"https://arxiv.org/pdf/2303.11621v1.pdf","comment":"Accepted by AAAI 2023"},{"id":"http://arxiv.org/abs/2211.06552v3","updated":"2023-03-21T06:38:48Z","published":"2022-11-12T02:36:32Z","title":"Collecting Interactive Multi-modal Datasets for Grounded Language\n  Understanding","summary":"  Human intelligence can remarkably adapt quickly to new tasks and\nenvironments. Starting from a very young age, humans acquire new skills and\nlearn how to solve new tasks either by imitating the behavior of others or by\nfollowing provided natural language instructions. To facilitate research which\ncan enable similar capabilities in machines, we made the following\ncontributions (1) formalized the collaborative embodied agent using natural\nlanguage task; (2) developed a tool for extensive and scalable data collection;\nand (3) collected the first dataset for interactive grounded language\nunderstanding.\n","authors":["Shrestha Mohanty","Negar Arabzadeh","Milagro Teruel","Yuxuan Sun","Artem Zholus","Alexey Skrynnik","Mikhail Burtsev","Kavya Srinet","Aleksandr Panov","Arthur Szlam","Marc-Alexandre Côté","Julia Kiseleva"],"pdf_url":"https://arxiv.org/pdf/2211.06552v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11607v1","updated":"2023-03-21T06:00:39Z","published":"2023-03-21T06:00:39Z","title":"Transformers in Speech Processing: A Survey","summary":"  The remarkable success of transformers in the field of natural language\nprocessing has sparked the interest of the speech-processing community, leading\nto an exploration of their potential for modeling long-range dependencies\nwithin speech sequences. Recently, transformers have gained prominence across\nvarious speech-related domains, including automatic speech recognition, speech\nsynthesis, speech translation, speech para-linguistics, speech enhancement,\nspoken dialogue systems, and numerous multimodal applications. In this paper,\nwe present a comprehensive survey that aims to bridge research studies from\ndiverse subfields within speech technology. By consolidating findings from\nacross the speech technology landscape, we provide a valuable resource for\nresearchers interested in harnessing the power of transformers to advance the\nfield. We identify the challenges encountered by transformers in speech\nprocessing while also offering insights into potential solutions to address\nthese issues.\n","authors":["Siddique Latif","Aun Zaidi","Heriberto Cuayahuitl","Fahad Shamshad","Moazzam Shoukat","Junaid Qadir"],"pdf_url":"https://arxiv.org/pdf/2303.11607v1.pdf","comment":"under-review"},{"id":"http://arxiv.org/abs/2303.05382v2","updated":"2023-03-21T05:47:11Z","published":"2023-03-06T16:36:17Z","title":"ChatGPT Is on the Horizon: Could a Large Language Model Be All We Need\n  for Intelligent Transportation?","summary":"  ChatGPT, developed by OpenAI, is one of the milestone large language models\n(LLMs) with 6 billion parameters. ChatGPT has demonstrated the impressive\nlanguage understanding capability of LLM, particularly in generating\nconversational response. As LLMs start to gain more attention in various\nresearch or engineering domains, it is time to envision how LLM may\nrevolutionize the way we approach intelligent transportation systems. This\npaper explores the future applications of LLM in addressing key transportation\nproblems. By leveraging LLM with cross-modal encoder, an intelligent system can\nalso process traffic data from different modalities and execute transportation\noperations through an LLM. We present and validate these potential\ntransportation applications equipped by LLM. To further demonstrate this\npotential, we also provide a concrete smartphone-based crash report\nauto-generation and analysis framework as a use case. Despite the potential\nbenefits, challenges related to data privacy, data quality, and model bias must\nbe considered. Overall, the use of LLM in intelligent transport systems holds\npromise for more efficient, intelligent, and sustainable transportation systems\nthat further improve daily life around the world.\n","authors":["Ou Zheng","Mohamed Abdel-Aty","Dongdong Wang","Zijin Wang","Shengxuan Ding"],"pdf_url":"https://arxiv.org/pdf/2303.05382v2.pdf","comment":"Submitted to Nature - Machine Intelligence (13 Pages, 8 Figures)"},{"id":"http://arxiv.org/abs/2111.09543v3","updated":"2023-03-21T05:17:08Z","published":"2021-11-18T06:48:00Z","title":"DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with\n  Gradient-Disentangled Embedding Sharing","summary":"  This paper presents a new pre-trained language model, DeBERTaV3, which\nimproves the original DeBERTa model by replacing mask language modeling (MLM)\nwith replaced token detection (RTD), a more sample-efficient pre-training task.\nOur analysis shows that vanilla embedding sharing in ELECTRA hurts training\nefficiency and model performance. This is because the training losses of the\ndiscriminator and the generator pull token embeddings in different directions,\ncreating the \"tug-of-war\" dynamics. We thus propose a new gradient-disentangled\nembedding sharing method that avoids the tug-of-war dynamics, improving both\ntraining efficiency and the quality of the pre-trained model. We have\npre-trained DeBERTaV3 using the same settings as DeBERTa to demonstrate its\nexceptional performance on a wide range of downstream natural language\nunderstanding (NLU) tasks. Taking the GLUE benchmark with eight tasks as an\nexample, the DeBERTaV3 Large model achieves a 91.37% average score, which is\n1.37% over DeBERTa and 1.91% over ELECTRA, setting a new state-of-the-art\n(SOTA) among the models with a similar structure. Furthermore, we have\npre-trained a multi-lingual model mDeBERTa and observed a larger improvement\nover strong baselines compared to English models. For example, the mDeBERTa\nBase achieves a 79.8% zero-shot cross-lingual accuracy on XNLI and a 3.6%\nimprovement over XLM-R Base, creating a new SOTA on this benchmark. We have\nmade our pre-trained models and inference code publicly available at\nhttps://github.com/microsoft/DeBERTa.\n","authors":["Pengcheng He","Jianfeng Gao","Weizhu Chen"],"pdf_url":"https://arxiv.org/pdf/2111.09543v3.pdf","comment":"16 pages, 10 tables, 2 Figures. The DeBERTaV3 model significantly\n  improves performance of the downstream NLU tasks over models with a similar\n  structure, e.g. DeBERTaV3 large achieves 91.37% average GLUE score which is\n  1.37% over DeBERTa large. XSmall has only 22M backbone parameters, but\n  significantly outperforms RoBERTa/XLNet-base. Paper is published as a\n  conference paper at ICLR 2023"},{"id":"http://arxiv.org/abs/2211.09778v3","updated":"2023-03-21T04:54:55Z","published":"2022-11-17T18:52:19Z","title":"I Can't Believe There's No Images! Learning Visual Tasks Using only\n  Language Data","summary":"  Many high-level skills that are required for computer vision tasks, such as\nparsing questions, comparing and contrasting semantics, and writing\ndescriptions, are also required in other domains such as natural language\nprocessing. In this paper, we ask whether it is possible to learn those skills\nfrom textual data and then transfer them to vision tasks without ever training\non visual training data. Key to our approach is exploiting the joint embedding\nspace of contrastively trained vision and language encoders. In practice, there\ncan be systematic differences between embedding spaces for different modalities\nin contrastive models, and we analyze how these differences affect our approach\nand study strategies to mitigate this concern. We produce models using only\ntext training data on four representative tasks: image captioning, visual\nentailment, visual question answering and visual news, and evaluate them on\nstandard benchmarks using images. We find these models generally perform close\nto models trained on images, while surpassing prior work for captioning and\nvisual entailment in this text only setting by over 9 points, and outperforming\nall prior work on visual news by over 30 points. We also showcase a variety of\nstylistic image captioning models that are trained using no image data and no\nhuman-curated language data, but instead using readily-available text data from\nbooks, the web, or language models.\n","authors":["Sophia Gu","Christopher Clark","Aniruddha Kembhavi"],"pdf_url":"https://arxiv.org/pdf/2211.09778v3.pdf","comment":"website (https://prior.allenai.org/projects/close), code\n  (https://github.com/allenai/close)"},{"id":"http://arxiv.org/abs/2303.11593v1","updated":"2023-03-21T04:47:45Z","published":"2023-03-21T04:47:45Z","title":"Difficulty in learning chirality for Transformer fed with SMILES","summary":"  Recent years have seen development of descriptor generation based on\nrepresentation learning of extremely diverse molecules, especially those that\napply natural language processing (NLP) models to SMILES, a literal\nrepresentation of molecular structure. However, little research has been done\non how these models understand chemical structure. To address this, we\ninvestigated the relationship between the learning progress of SMILES and\nchemical structure using a representative NLP model, the Transformer. The\nresults suggest that while the Transformer learns partial structures of\nmolecules quickly, it requires extended training to understand overall\nstructures. Consistently, the accuracy of molecular property predictions using\ndescriptors generated from models at different learning steps was similar from\nthe beginning to the end of training. Furthermore, we found that the\nTransformer requires particularly long training to learn chirality and\nsometimes stagnates with low translation accuracy due to misunderstanding of\nenantiomers. These findings are expected to deepen understanding of NLP models\nin chemistry.\n","authors":["Yasuhiro Yoshikai","Tadahaya Mizuno","Shumpei Nemoto","Hiroyuki Kusuhara"],"pdf_url":"https://arxiv.org/pdf/2303.11593v1.pdf","comment":"20 pages, 6 figures"},{"id":"http://arxiv.org/abs/2303.08006v2","updated":"2023-03-21T03:35:10Z","published":"2023-03-09T00:09:58Z","title":"Data-Efficient Learning of Natural Language to Linear Temporal Logic\n  Translators for Robot Task Specification","summary":"  To make robots accessible to a broad audience, it is critical to endow them\nwith the ability to take universal modes of communication, like commands given\nin natural language, and extract a concrete desired task specification, defined\nusing a formal language like linear temporal logic (LTL). In this paper, we\npresent a learning-based approach for translating from natural language\ncommands to LTL specifications with very limited human-labeled training data.\nThis is in stark contrast to existing natural-language to LTL translators,\nwhich require large human-labeled datasets, often in the form of labeled pairs\nof LTL formulas and natural language commands, to train the translator. To\nreduce reliance on human data, our approach generates a large synthetic\ntraining dataset through algorithmic generation of LTL formulas, conversion to\nstructured English, and then exploiting the paraphrasing capabilities of modern\nlarge language models (LLMs) to synthesize a diverse corpus of natural language\ncommands corresponding to the LTL formulas. We use this generated data to\nfinetune an LLM and apply a constrained decoding procedure at inference time to\nensure the returned LTL formula is syntactically correct. We evaluate our\napproach on three existing LTL/natural language datasets and show that we can\ntranslate natural language commands at 75\\% accuracy with far less human data\n($\\le$12 annotations). Moreover, when training on large human-annotated\ndatasets, our method achieves higher test accuracy (95\\% on average) than prior\nwork. Finally, we show the translated formulas can be used to plan\nlong-horizon, multi-stage tasks on a 12D quadrotor.\n","authors":["Jiayi Pan","Glen Chou","Dmitry Berenson"],"pdf_url":"https://arxiv.org/pdf/2303.08006v2.pdf","comment":"Accepted at ICRA 2023"},{"id":"http://arxiv.org/abs/2205.12676v2","updated":"2023-03-21T03:34:36Z","published":"2022-05-25T11:38:04Z","title":"Evaluating Inclusivity, Equity, and Accessibility of NLP Technology: A\n  Case Study for Indian Languages","summary":"  In order for NLP technology to be widely applicable, fair, and useful, it\nneeds to serve a diverse set of speakers across the world's languages, be\nequitable, i.e., not unduly biased towards any particular language, and be\ninclusive of all users, particularly in low-resource settings where compute\nconstraints are common. In this paper, we propose an evaluation paradigm that\nassesses NLP technologies across all three dimensions. While diversity and\ninclusion have received attention in recent literature, equity is currently\nunexplored. We propose to address this gap using the Gini coefficient, a\nwell-established metric used for estimating societal wealth inequality. Using\nour paradigm, we highlight the distressed state of current technologies for\nIndian (IN) languages (a linguistically large and diverse set, with a varied\nspeaker population), across all three dimensions. To improve upon these\nmetrics, we demonstrate the importance of region-specific choices in model\nbuilding and dataset creation, and more importantly, propose a novel,\ngeneralisable approach to optimal resource allocation during fine-tuning.\nFinally, we discuss steps to mitigate these biases and encourage the community\nto employ multi-faceted evaluation when building linguistically diverse and\nequitable technologies.\n","authors":["Simran Khanuja","Sebastian Ruder","Partha Talukdar"],"pdf_url":"https://arxiv.org/pdf/2205.12676v2.pdf","comment":"Accepted to EACL Findings, 2023"},{"id":"http://arxiv.org/abs/2301.04347v2","updated":"2023-03-21T03:04:09Z","published":"2023-01-11T07:52:59Z","title":"Counteracts: Testing Stereotypical Representation in Pre-trained\n  Language Models","summary":"  Language models have demonstrated strong performance on various natural\nlanguage understanding tasks. Similar to humans, language models could also\nhave their own bias that is learned from the training data. As more and more\ndownstream tasks integrate language models as part of the pipeline, it is\nnecessary to understand the internal stereotypical representation and the\nmethods to mitigate the negative effects. In this paper, we proposed a simple\nmethod to test the internal stereotypical representation in pre-trained\nlanguage models using counterexamples. We mainly focused on gender bias, but\nthe method can be extended to other types of bias. We evaluated models on 9\ndifferent cloze-style prompts consisting of knowledge and base prompts. Our\nresults indicate that pre-trained language models show a certain amount of\nrobustness when using unrelated knowledge, and prefer shallow linguistic cues,\nsuch as word position and syntactic structure, to alter the internal\nstereotypical representation. Such findings shed light on how to manipulate\nlanguage models in a neutral approach for both finetuning and evaluation.\n","authors":["Damin Zhang"],"pdf_url":"https://arxiv.org/pdf/2301.04347v2.pdf","comment":"FACCT; to be submitted to"},{"id":"http://arxiv.org/abs/2303.10475v2","updated":"2023-03-21T01:27:16Z","published":"2023-03-18T19:17:47Z","title":"Is Prompt All You Need? No. A Comprehensive and Broader View of\n  Instruction Learning","summary":"  Task semantics can be expressed by a set of input-to-output examples or a\npiece of textual instruction. Conventional machine learning approaches for\nnatural language processing (NLP) mainly rely on the availability of\nlarge-scale sets of task-specific examples. Two issues arise: first, collecting\ntask-specific labeled examples does not apply to scenarios where tasks may be\ntoo complicated or costly to annotate, or the system is required to handle a\nnew task immediately; second, this is not user-friendly since end-users are\nprobably more willing to provide task description rather than a set of examples\nbefore using the system. Therefore, the community is paying increasing interest\nin a new supervision-seeking paradigm for NLP: learning from task instructions.\nDespite its impressive progress, there are some common issues that the\ncommunity struggles with. This survey paper tries to summarize the current\nresearch on instruction learning, particularly, by answering the following\nquestions: (i) what is task instruction, and what instruction types exist? (ii)\nhow to model instructions? (iii) what factors influence and explain the\ninstructions' performance? (iv) what challenges remain in instruction learning?\nTo our knowledge, this is the first comprehensive survey about textual\ninstructions.\n","authors":["Renze Lou","Kai Zhang","Wenpeng Yin"],"pdf_url":"https://arxiv.org/pdf/2303.10475v2.pdf","comment":"Work is still in progress. The paper list is available at\n  https://github.com/RenzeLou/awesome-instruction-learning"},{"id":"http://arxiv.org/abs/2303.11525v1","updated":"2023-03-21T01:06:37Z","published":"2023-03-21T01:06:37Z","title":"SIFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency","summary":"  Recent works have explored the use of weight sparsity to improve the training\nefficiency (test accuracy w.r.t training FLOPs) of deep neural networks (DNNs).\nThese works aim to reduce training FLOPs but training with sparse weights often\nleads to accuracy loss or requires longer train schedules, making the resulting\ntraining efficiency less clear. In contrast, we focus on using sparsity to\nincrease accuracy while using the same FLOPS as the dense model and show\ntraining efficiency gains through higher accuracy. In this work, we introduce\nSIFT, a family of Sparse Iso-FLOP Transformations which are used as drop-in\nreplacements for dense layers to improve their representational capacity and\nFLOP efficiency. Each transformation is parameterized by a single parameter\n(sparsity level) and provides a larger search space to find optimal sparse\nmasks. Without changing any training hyperparameters, replacing dense layers\nwith SIFT leads to significant improvements across computer vision (CV) and\nnatural language processing (NLP) tasks, including ResNet-18 on ImageNet\n(+3.5%) and GPT-3 Small on WikiText-103 (-0.4 PPL), both matching larger dense\nmodel variants with 2x or more FLOPs. To the best of our knowledge, this is the\nfirst work to demonstrate the use of sparsity for improving accuracy of dense\nmodels via a simple-to-use set of sparse transformations. Code is available at:\nhttps://github.com/CerebrasResearch/SIFT.\n","authors":["Shreyas Saxena","Vithursan Thangarasa","Abhay Gupta","Sean Lie"],"pdf_url":"https://arxiv.org/pdf/2303.11525v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12220v1","updated":"2023-03-21T22:43:41Z","published":"2023-03-21T22:43:41Z","title":"A Unified Taxonomy of Deep Syntactic Relations","summary":"  This paper analyzes multiple deep-syntactic frameworks with the goal of\ncreating a proposal for a set of universal semantic role labels. The proposal\nexamines various theoretic linguistic perspectives and focuses on Meaning-Text\nTheory and Functional Generative Description frameworks.\n  For the purpose of this research, data from four languages is used -- Spanish\nand Catalan (Taule et al., 2011), Czech (Hajic et al., 2017), and English\n(Hajic et al., 2012). This proposal is oriented towards Universal Dependencies\n(de Marneffe et al., 2021) with a further intention of applying the universal\nsemantic role labels to the UD data.\n","authors":["Kira Droganova","Daniel Zeman"],"pdf_url":"https://arxiv.org/pdf/2303.12220v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12208v1","updated":"2023-03-21T21:49:39Z","published":"2023-03-21T21:49:39Z","title":"MAGVLT: Masked Generative Vision-and-Language Transformer","summary":"  While generative modeling on multimodal image-text data has been actively\ndeveloped with large-scale paired datasets, there have been limited attempts to\ngenerate both image and text data by a single model rather than a generation of\none fixed modality conditioned on the other modality. In this paper, we explore\na unified generative vision-and-language (VL) model that can produce both\nimages and text sequences. Especially, we propose a generative VL transformer\nbased on the non-autoregressive mask prediction, named MAGVLT, and compare it\nwith an autoregressive generative VL transformer (ARGVLT). In comparison to\nARGVLT, the proposed MAGVLT enables bidirectional context encoding, fast\ndecoding by parallel token predictions in an iterative refinement, and extended\nediting capabilities such as image and text infilling. For rigorous training of\nour MAGVLT with image-text pairs from scratch, we combine the image-to-text,\ntext-to-image, and joint image-and-text mask prediction tasks. Moreover, we\ndevise two additional tasks based on the step-unrolled mask prediction and the\nselective prediction on the mixture of two image-text pairs. Experimental\nresults on various downstream generation tasks of VL benchmarks show that our\nMAGVLT outperforms ARGVLT by a large margin even with significant inference\nspeedup. Particularly, MAGVLT achieves competitive results on both zero-shot\nimage-to-text and text-to-image generation tasks from MS-COCO by one\nmoderate-sized model (fewer than 500M parameters) even without the use of\nmonomodal data and networks.\n","authors":["Sungwoong Kim","Daejin Jo","Donghoon Lee","Jongmin Kim"],"pdf_url":"https://arxiv.org/pdf/2303.12208v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.12135v1","updated":"2023-03-21T18:48:11Z","published":"2023-03-21T18:48:11Z","title":"Understand Legal Documents with Contextualized Large Language Models","summary":"  The growth of pending legal cases in populous countries, such as India, has\nbecome a major issue. Developing effective techniques to process and understand\nlegal documents is extremely useful in resolving this problem. In this paper,\nwe present our systems for SemEval-2023 Task 6: understanding legal texts (Modi\net al., 2023). Specifically, we first develop the Legal-BERT-HSLN model that\nconsiders the comprehensive context information in both intra- and\ninter-sentence levels to predict rhetorical roles (subtask A) and then train a\nLegal-LUKE model, which is legal-contextualized and entity-aware, to recognize\nlegal entities (subtask B). Our evaluations demonstrate that our designed\nmodels are more accurate than baselines, e.g., with an up to 15.0% better F1\nscore in subtask B. We achieved notable performance in the task leaderboard,\ne.g., 0.834 micro F1 score, and ranked No.5 out of 27 teams in subtask A.\n","authors":["Xin Jin","Yuchen Wang"],"pdf_url":"https://arxiv.org/pdf/2303.12135v1.pdf","comment":"SemEval 2023"},{"id":"http://arxiv.org/abs/2303.12132v1","updated":"2023-03-21T18:45:09Z","published":"2023-03-21T18:45:09Z","title":"Fundamentals of Generative Large Language Models and Perspectives in\n  Cyber-Defense","summary":"  Generative Language Models gained significant attention in late 2022 / early\n2023, notably with the introduction of models refined to act consistently with\nusers' expectations of interactions with AI (conversational models). Arguably\nthe focal point of public attention has been such a refinement of the GPT3\nmodel -- the ChatGPT and its subsequent integration with auxiliary\ncapabilities, including search as part of Microsoft Bing. Despite extensive\nprior research invested in their development, their performance and\napplicability to a range of daily tasks remained unclear and niche. However,\ntheir wider utilization without a requirement for technical expertise, made in\nlarge part possible through conversational fine-tuning, revealed the extent of\ntheir true capabilities in a real-world environment. This has garnered both\npublic excitement for their potential applications and concerns about their\ncapabilities and potential malicious uses. This review aims to provide a brief\noverview of the history, state of the art, and implications of Generative\nLanguage Models in terms of their principles, abilities, limitations, and\nfuture prospects -- especially in the context of cyber-defense, with a focus on\nthe Swiss operational environment.\n","authors":["Andrei Kucharavy","Zachary Schillaci","Loïc Maréchal","Maxime Würsch","Ljiljana Dolamic","Remi Sabonnadiere","Dimitri Percia David","Alain Mermoud","Vincent Lenders"],"pdf_url":"https://arxiv.org/pdf/2303.12132v1.pdf","comment":"41 pages (without references), 13 figures; public report of\n  Cyber-Defence Campus"},{"id":"http://arxiv.org/abs/2303.12112v1","updated":"2023-03-21T18:03:14Z","published":"2023-03-21T18:03:14Z","title":"Positive-Augmented Constrastive Learning for Image and Video Captioning\n  Evaluation","summary":"  The CLIP model has been recently proven to be very effective for a variety of\ncross-modal tasks, including the evaluation of captions generated from\nvision-and-language architectures. In this paper, we propose a new recipe for a\ncontrastive-based evaluation metric for image captioning, namely\nPositive-Augmented Contrastive learning Score (PAC-S), that in a novel way\nunifies the learning of a contrastive visual-semantic space with the addition\nof generated images and text on curated data. Experiments spanning several\ndatasets demonstrate that our new metric achieves the highest correlation with\nhuman judgments on both images and videos, outperforming existing\nreference-based metrics like CIDEr and SPICE and reference-free metrics like\nCLIP-Score. Finally, we test the system-level correlation of the proposed\nmetric when considering popular image captioning approaches, and assess the\nimpact of employing different cross-modal features. Our source code and trained\nmodels are publicly available at: https://github.com/aimagelab/pacscore.\n","authors":["Sara Sarto","Manuele Barraco","Marcella Cornia","Lorenzo Baraldi","Rita Cucchiara"],"pdf_url":"https://arxiv.org/pdf/2303.12112v1.pdf","comment":"CVPR 2023 (highlight paper)"},{"id":"http://arxiv.org/abs/2303.12513v1","updated":"2023-03-21T17:30:40Z","published":"2023-03-21T17:30:40Z","title":"Is BERT Blind? Exploring the Effect of Vision-and-Language Pretraining\n  on Visual Language Understanding","summary":"  Most humans use visual imagination to understand and reason about language,\nbut models such as BERT reason about language using knowledge acquired during\ntext-only pretraining. In this work, we investigate whether vision-and-language\npretraining can improve performance on text-only tasks that involve implicit\nvisual reasoning, focusing primarily on zero-shot probing methods. We propose a\nsuite of visual language understanding (VLU) tasks for probing the visual\nreasoning abilities of text encoder models, as well as various non-visual\nnatural language understanding (NLU) tasks for comparison. We also contribute a\nnovel zero-shot knowledge probing method, Stroop probing, for applying models\nsuch as CLIP to text-only tasks without needing a prediction head such as the\nmasked language modelling head of models like BERT. We show that SOTA\nmultimodally trained text encoders outperform unimodally trained text encoders\non the VLU tasks while being underperformed by them on the NLU tasks, lending\nnew context to previously mixed results regarding the NLU capabilities of\nmultimodal models. We conclude that exposure to images during pretraining\naffords inherent visual reasoning knowledge that is reflected in language-only\ntasks that require implicit visual reasoning. Our findings bear importance in\nthe broader context of multimodal learning, providing principled guidelines for\nthe choice of text encoders used in such contexts.\n","authors":["Morris Alper","Michael Fiman","Hadar Averbuch-Elor"],"pdf_url":"https://arxiv.org/pdf/2303.12513v1.pdf","comment":"To be presented in CVPR 2023. Project webpage:\n  https://isbertblind.github.io/"},{"id":"http://arxiv.org/abs/2207.03038v2","updated":"2023-03-21T11:32:18Z","published":"2022-07-07T01:47:19Z","title":"Dual-Stream Transformer for Generic Event Boundary Captioning","summary":"  This paper describes our champion solution for the CVPR2022 Generic Event\nBoundary Captioning (GEBC) competition. GEBC requires the captioning model to\nhave a comprehension of instantaneous status changes around the given video\nboundary, which makes it much more challenging than conventional video\ncaptioning task. In this paper, a Dual-Stream Transformer with improvements on\nboth video content encoding and captions generation is proposed: (1) We utilize\nthree pre-trained models to extract the video features from different\ngranularities. Moreover, we exploit the types of boundary as hints to help the\nmodel generate captions. (2) We particularly design an model, termed as\nDual-Stream Transformer, to learn discriminative representations for boundary\ncaptioning. (3) Towards generating content-relevant and human-like captions, we\nimprove the description quality by designing a word-level ensemble strategy.\nThe promising results on the GEBC test split demonstrate the efficacy of our\nproposed model.\n","authors":["Xin Gu","Hanhua Ye","Guang Chen","Yufei Wang","Libo Zhang","Longyin Wen"],"pdf_url":"https://arxiv.org/pdf/2207.03038v2.pdf","comment":"Accepted to CVPR 2023 LOVEU Workshop"},{"id":"http://arxiv.org/abs/2303.13367v1","updated":"2023-03-21T14:35:07Z","published":"2023-03-21T14:35:07Z","title":"ChatGPT and a New Academic Reality: AI-Written Research Papers and the\n  Ethics of the Large Language Models in Scholarly Publishing","summary":"  This paper discusses OpenAIs ChatGPT, a generative pre-trained transformer,\nwhich uses natural language processing to fulfill text-based user requests\n(i.e., a chatbot). The history and principles behind ChatGPT and similar models\nare discussed. This technology is then discussed in relation to its potential\nimpact on academia and scholarly research and publishing. ChatGPT is seen as a\npotential model for the automated preparation of essays and other types of\nscholarly manuscripts. Potential ethical issues that could arise with the\nemergence of large language models like GPT-3, the underlying technology behind\nChatGPT, and its usage by academics and researchers, are discussed and situated\nwithin the context of broader advancements in artificial intelligence, machine\nlearning, and natural language processing for research and scholarly\npublishing.\n","authors":["Brady Lund","Ting Wang","Nishith Reddy Mannuru","Bing Nie","Somipam Shimray","Ziang Wang"],"pdf_url":"https://arxiv.org/pdf/2303.13367v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13373v1","updated":"2023-03-21T07:25:36Z","published":"2023-03-21T07:25:36Z","title":"Fine-tuning ClimateBert transformer with ClimaText for the disclosure\n  analysis of climate-related financial risks","summary":"  In recent years there has been a growing demand from financial agents,\nespecially from particular and institutional investors, for companies to report\non climate-related financial risks. A vast amount of information, in text\nformat, can be expected to be disclosed in the short term by firms in order to\nidentify these types of risks in their financial and non financial reports,\nparticularly in response to the growing regulation that is being passed on the\nmatter. To this end, this paper applies state-of-the-art NLP techniques to\nachieve the detection of climate change in text corpora. We use transfer\nlearning to fine-tune two transformer models, BERT and ClimateBert -a recently\npublished DistillRoBERTa-based model that has been specifically tailored for\nclimate text classification-. These two algorithms are based on the transformer\narchitecture which enables learning the contextual relationships between words\nin a text. We carry out the fine-tuning process of both models on the novel\nClima-Text database, consisting of data collected from Wikipedia, 10K Files\nReports and web-based claims. Our text classification model obtained from the\nClimateBert fine-tuning process on ClimaText, outperforms the models created\nwith BERT and the current state-of-the-art transformer in this particular\nproblem. Our study is the first one to implement on the ClimaText database the\nrecently published ClimateBert algorithm. Based on our results, it can be said\nthat ClimateBert fine-tuned on ClimaText is an outstanding tool within the NLP\npre-trained transformer models that may and should be used by investors,\ninstitutional agents and companies themselves to monitor the disclosure of\nclimate risk in financial reports. In addition, our transfer learning\nmethodology is cheap in computational terms, thus allowing any organization to\nperform it.\n","authors":["Eduardo C. Garrido-Merchán","Cristina González-Barthe","María Coronado Vaca"],"pdf_url":"https://arxiv.org/pdf/2303.13373v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2303.12079v1","updated":"2023-03-21T17:59:57Z","published":"2023-03-21T17:59:57Z","title":"OmniTracker: Unifying Object Tracking by Tracking-with-Detection","summary":"  Object tracking (OT) aims to estimate the positions of target objects in a\nvideo sequence. Depending on whether the initial states of target objects are\nspecified by provided annotations in the first frame or the categories, OT\ncould be classified as instance tracking (e.g., SOT and VOS) and category\ntracking (e.g., MOT, MOTS, and VIS) tasks. Combing the advantages of the best\npractices developed in both communities, we propose a novel\ntracking-with-detection paradigm, where tracking supplements appearance priors\nfor detection and detection provides tracking with candidate bounding boxes for\nassociation. Equipped with such a design, a unified tracking model,\nOmniTracker, is further presented to resolve all the tracking tasks with a\nfully shared network architecture, model weights, and inference pipeline.\nExtensive experiments on 7 tracking datasets, including LaSOT, TrackingNet,\nDAVIS16-17, MOT17, MOTS20, and YTVIS19, demonstrate that OmniTracker achieves\non-par or even better results than both task-specific and unified tracking\nmodels.\n","authors":["Junke Wang","Dongdong Chen","Zuxuan Wu","Chong Luo","Xiyang Dai","Lu Yuan","Yu-Gang Jiang"],"pdf_url":"https://arxiv.org/pdf/2303.12079v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12080v1","updated":"2023-03-21T17:59:57Z","published":"2023-03-21T17:59:57Z","title":"Natural Language-Assisted Sign Language Recognition","summary":"  Sign languages are visual languages which convey information by signers'\nhandshape, facial expression, body movement, and so forth. Due to the inherent\nrestriction of combinations of these visual ingredients, there exist a\nsignificant number of visually indistinguishable signs (VISigns) in sign\nlanguages, which limits the recognition capacity of vision neural networks. To\nmitigate the problem, we propose the Natural Language-Assisted Sign Language\nRecognition (NLA-SLR) framework, which exploits semantic information contained\nin glosses (sign labels). First, for VISigns with similar semantic meanings, we\npropose language-aware label smoothing by generating soft labels for each\ntraining sign whose smoothing weights are computed from the normalized semantic\nsimilarities among the glosses to ease training. Second, for VISigns with\ndistinct semantic meanings, we present an inter-modality mixup technique which\nblends vision and gloss features to further maximize the separability of\ndifferent signs under the supervision of blended labels. Besides, we also\nintroduce a novel backbone, video-keypoint network, which not only models both\nRGB videos and human body keypoints but also derives knowledge from sign videos\nof different temporal receptive fields. Empirically, our method achieves\nstate-of-the-art performance on three widely-adopted benchmarks: MSASL, WLASL,\nand NMFs-CSL. Codes are available at https://github.com/FangyunWei/SLRT.\n","authors":["Ronglai Zuo","Fangyun Wei","Brian Mak"],"pdf_url":"https://arxiv.org/pdf/2303.12080v1.pdf","comment":"Accepted by CVPR 2023. Codes are available at\n  https://github.com/FangyunWei/SLRT"},{"id":"http://arxiv.org/abs/2303.12078v1","updated":"2023-03-21T17:59:56Z","published":"2023-03-21T17:59:56Z","title":"Two-shot Video Object Segmentation","summary":"  Previous works on video object segmentation (VOS) are trained on densely\nannotated videos. Nevertheless, acquiring annotations in pixel level is\nexpensive and time-consuming. In this work, we demonstrate the feasibility of\ntraining a satisfactory VOS model on sparsely annotated videos-we merely\nrequire two labeled frames per training video while the performance is\nsustained. We term this novel training paradigm as two-shot video object\nsegmentation, or two-shot VOS for short. The underlying idea is to generate\npseudo labels for unlabeled frames during training and to optimize the model on\nthe combination of labeled and pseudo-labeled data. Our approach is extremely\nsimple and can be applied to a majority of existing frameworks. We first\npre-train a VOS model on sparsely annotated videos in a semi-supervised manner,\nwith the first frame always being a labeled one. Then, we adopt the pre-trained\nVOS model to generate pseudo labels for all unlabeled frames, which are\nsubsequently stored in a pseudo-label bank. Finally, we retrain a VOS model on\nboth labeled and pseudo-labeled data without any restrictions on the first\nframe. For the first time, we present a general way to train VOS models on\ntwo-shot VOS datasets. By using 7.3% and 2.9% labeled data of YouTube-VOS and\nDAVIS benchmarks, our approach achieves comparable results in contrast to the\ncounterparts trained on fully labeled set. Code and models are available at\nhttps://github.com/yk-pku/Two-shot-Video-Object-Segmentation.\n","authors":["Kun Yan","Xiao Li","Fangyun Wei","Jinglu Wang","Chenbin Zhang","Ping Wang","Yan Lu"],"pdf_url":"https://arxiv.org/pdf/2303.12078v1.pdf","comment":"Accepted by CVPR 2023. Code and models are available at\n  https://github.com/yk-pku/Two-shot-Video-Object-Segmentation"},{"id":"http://arxiv.org/abs/2303.12077v1","updated":"2023-03-21T17:59:22Z","published":"2023-03-21T17:59:22Z","title":"VAD: Vectorized Scene Representation for Efficient Autonomous Driving","summary":"  Autonomous driving requires a comprehensive understanding of the surrounding\nenvironment for reliable trajectory planning. Previous works rely on dense\nrasterized scene representation (e.g., agent occupancy and semantic map) to\nperform planning, which is computationally intensive and misses the\ninstance-level structure information. In this paper, we propose VAD, an\nend-to-end vectorized paradigm for autonomous driving, which models the driving\nscene as fully vectorized representation. The proposed vectorized paradigm has\ntwo significant advantages. On one hand, VAD exploits the vectorized agent\nmotion and map elements as explicit instance-level planning constraints which\neffectively improves planning safety. On the other hand, VAD runs much faster\nthan previous end-to-end planning methods by getting rid of\ncomputation-intensive rasterized representation and hand-designed\npost-processing steps. VAD achieves state-of-the-art end-to-end planning\nperformance on the nuScenes dataset, outperforming the previous best method by\na large margin (reducing the average collision rate by 48.4%). Besides, VAD\ngreatly improves the inference speed (up to 9.3x), which is critical for the\nreal-world deployment of an autonomous driving system. Code and models will be\nreleased for facilitating future research.\n","authors":["Bo Jiang","Shaoyu Chen","Qing Xu","Bencheng Liao","Jiajie Chen","Helong Zhou","Qian Zhang","Wenyu Liu","Chang Huang","Xinggang Wang"],"pdf_url":"https://arxiv.org/pdf/2303.12077v1.pdf","comment":"Code&Demos: https://github.com/hustvl/VAD"},{"id":"http://arxiv.org/abs/2303.12076v1","updated":"2023-03-21T17:59:20Z","published":"2023-03-21T17:59:20Z","title":"Dexterity from Touch: Self-Supervised Pre-Training of Tactile\n  Representations with Robotic Play","summary":"  Teaching dexterity to multi-fingered robots has been a longstanding challenge\nin robotics. Most prominent work in this area focuses on learning controllers\nor policies that either operate on visual observations or state estimates\nderived from vision. However, such methods perform poorly on fine-grained\nmanipulation tasks that require reasoning about contact forces or about objects\noccluded by the hand itself. In this work, we present T-Dex, a new approach for\ntactile-based dexterity, that operates in two phases. In the first phase, we\ncollect 2.5 hours of play data, which is used to train self-supervised tactile\nencoders. This is necessary to bring high-dimensional tactile readings to a\nlower-dimensional embedding. In the second phase, given a handful of\ndemonstrations for a dexterous task, we learn non-parametric policies that\ncombine the tactile observations with visual ones. Across five challenging\ndexterous tasks, we show that our tactile-based dexterity models outperform\npurely vision and torque-based models by an average of 1.7X. Finally, we\nprovide a detailed analysis on factors critical to T-Dex including the\nimportance of play data, architectures, and representation learning.\n","authors":["Irmak Guzey","Ben Evans","Soumith Chintala","Lerrel Pinto"],"pdf_url":"https://arxiv.org/pdf/2303.12076v1.pdf","comment":"Video and code can be accessed here:\n  https://tactile-dexterity.github.io/"},{"id":"http://arxiv.org/abs/2303.12074v1","updated":"2023-03-21T17:59:02Z","published":"2023-03-21T17:59:02Z","title":"CC3D: Layout-Conditioned Generation of Compositional 3D Scenes","summary":"  In this work, we introduce CC3D, a conditional generative model that\nsynthesizes complex 3D scenes conditioned on 2D semantic scene layouts, trained\nusing single-view images. Different from most existing 3D GANs that limit their\napplicability to aligned single objects, we focus on generating complex scenes\nwith multiple objects, by modeling the compositional nature of 3D scenes. By\ndevising a 2D layout-based approach for 3D synthesis and implementing a new 3D\nfield representation with a stronger geometric inductive bias, we have created\na 3D GAN that is both efficient and of high quality, while allowing for a more\ncontrollable generation process. Our evaluations on synthetic 3D-FRONT and\nreal-world KITTI-360 datasets demonstrate that our model generates scenes of\nimproved visual and geometric quality in comparison to previous works.\n","authors":["Sherwin Bahmani","Jeong Joon Park","Despoina Paschalidou","Xingguang Yan","Gordon Wetzstein","Leonidas Guibas","Andrea Tagliasacchi"],"pdf_url":"https://arxiv.org/pdf/2303.12074v1.pdf","comment":"Webpage: https://sherwinbahmani.github.io/cc3d/"},{"id":"http://arxiv.org/abs/2303.12073v1","updated":"2023-03-21T17:58:49Z","published":"2023-03-21T17:58:49Z","title":"3D Mitochondria Instance Segmentation with Spatio-Temporal Transformers","summary":"  Accurate 3D mitochondria instance segmentation in electron microscopy (EM) is\na challenging problem and serves as a prerequisite to empirically analyze their\ndistributions and morphology. Most existing approaches employ 3D convolutions\nto obtain representative features. However, these convolution-based approaches\nstruggle to effectively capture long-range dependencies in the volume\nmitochondria data, due to their limited local receptive field. To address this,\nwe propose a hybrid encoder-decoder framework based on a split spatio-temporal\nattention module that efficiently computes spatial and temporal self-attentions\nin parallel, which are later fused through a deformable convolution. Further,\nwe introduce a semantic foreground-background adversarial loss during training\nthat aids in delineating the region of mitochondria instances from the\nbackground clutter. Our extensive experiments on three benchmarks, Lucchi,\nMitoEM-R and MitoEM-H, reveal the benefits of the proposed contributions\nachieving state-of-the-art results on all three datasets. Our code and models\nare available at https://github.com/OmkarThawakar/STT-UNET.\n","authors":["Omkar Thawakar","Rao Muhammad Anwer","Jorma Laaksonen","Orly Reiner","Mubarak Shah","Fahad Shahbaz Khan"],"pdf_url":"https://arxiv.org/pdf/2303.12073v1.pdf","comment":"8 pages, 3 figures, 5 Tables, 2 page references"},{"id":"http://arxiv.org/abs/2303.12071v1","updated":"2023-03-21T17:58:28Z","published":"2023-03-21T17:58:28Z","title":"ProphNet: Efficient Agent-Centric Motion Forecasting with\n  Anchor-Informed Proposals","summary":"  Motion forecasting is a key module in an autonomous driving system. Due to\nthe heterogeneous nature of multi-sourced input, multimodality in agent\nbehavior, and low latency required by onboard deployment, this task is\nnotoriously challenging. To cope with these difficulties, this paper proposes a\nnovel agent-centric model with anchor-informed proposals for efficient\nmultimodal motion prediction. We design a modality-agnostic strategy to\nconcisely encode the complex input in a unified manner. We generate diverse\nproposals, fused with anchors bearing goal-oriented scene context, to induce\nmultimodal prediction that covers a wide range of future trajectories. Our\nnetwork architecture is highly uniform and succinct, leading to an efficient\nmodel amenable for real-world driving deployment. Experiments reveal that our\nagent-centric network compares favorably with the state-of-the-art methods in\nprediction accuracy, while achieving scene-centric level inference latency.\n","authors":["Xishun Wang","Tong Su","Fang Da","Xiaodong Yang"],"pdf_url":"https://arxiv.org/pdf/2303.12071v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.12068v1","updated":"2023-03-21T17:57:33Z","published":"2023-03-21T17:57:33Z","title":"Machine Learning for Brain Disorders: Transformers and Visual\n  Transformers","summary":"  Transformers were initially introduced for natural language processing (NLP)\ntasks, but fast they were adopted by most deep learning fields, including\ncomputer vision. They measure the relationships between pairs of input tokens\n(words in the case of text strings, parts of images for visual Transformers),\ntermed attention. The cost is exponential with the number of tokens. For image\nclassification, the most common Transformer Architecture uses only the\nTransformer Encoder in order to transform the various input tokens. However,\nthere are also numerous other applications in which the decoder part of the\ntraditional Transformer Architecture is also used. Here, we first introduce the\nAttention mechanism (Section 1), and then the Basic Transformer Block including\nthe Vision Transformer (Section 2). Next, we discuss some improvements of\nvisual Transformers to account for small datasets or less computation(Section\n3). Finally, we introduce Visual Transformers applied to tasks other than image\nclassification, such as detection, segmentation, generation and training\nwithout labels (Section 4) and other domains, such as video or multimodality\nusing text or audio data (Section 5).\n","authors":["Robin Courant","Maika Edberg","Nicolas Dufour","Vicky Kalogeiton"],"pdf_url":"https://arxiv.org/pdf/2303.12068v1.pdf","comment":"To appear in O. Colliot (Ed.), Machine Learning for Brain Disorders,\n  Springer"},{"id":"http://arxiv.org/abs/2303.12059v1","updated":"2023-03-21T17:51:23Z","published":"2023-03-21T17:51:23Z","title":"Motion Matters: Neural Motion Transfer for Better Camera Physiological\n  Sensing","summary":"  Machine learning models for camera-based physiological measurement can have\nweak generalization due to a lack of representative training data. Body motion\nis one of the most significant sources of noise when attempting to recover the\nsubtle cardiac pulse from a video. We explore motion transfer as a form of data\naugmentation to introduce motion variation while preserving physiological\nchanges. We adapt a neural video synthesis approach to augment videos for the\ntask of remote photoplethysmography (PPG) and study the effects of motion\naugmentation with respect to 1) the magnitude and 2) the type of motion. After\ntraining on motion-augmented versions of publicly available datasets, the\npresented inter-dataset results on five benchmark datasets show improvements of\nup to 75% over existing state-of-the-art results. Our findings illustrate the\nutility of motion transfer as a data augmentation technique for improving the\ngeneralization of models for camera-based physiological sensing. We release our\ncode and pre-trained models for using motion transfer as a data augmentation\ntechnique on our project page: https://motion-matters.github.io/\n","authors":["Akshay Paruchuri","Xin Liu","Yulu Pan","Shwetak Patel","Daniel McDuff","Soumyadip Sengupta"],"pdf_url":"https://arxiv.org/pdf/2303.12059v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12060v1","updated":"2023-03-21T17:51:23Z","published":"2023-03-21T17:51:23Z","title":"VideoXum: Cross-modal Visual and Textural Summarization of Videos","summary":"  Video summarization aims to distill the most important information from a\nsource video to produce either an abridged clip or a textual narrative.\nTraditionally, different methods have been proposed depending on whether the\noutput is a video or text, thus ignoring the correlation between the two\nsemantically related tasks of visual summarization and textual summarization.\nWe propose a new joint video and text summarization task. The goal is to\ngenerate both a shortened video clip along with the corresponding textual\nsummary from a long video, collectively referred to as a cross-modal summary.\nThe generated shortened video clip and text narratives should be semantically\nwell aligned. To this end, we first build a large-scale human-annotated dataset\n-- VideoXum (X refers to different modalities). The dataset is reannotated\nbased on ActivityNet. After we filter out the videos that do not meet the\nlength requirements, 14,001 long videos remain in our new dataset. Each video\nin our reannotated dataset has human-annotated video summaries and the\ncorresponding narrative summaries. We then design a novel end-to-end model --\nVTSUM-BILP to address the challenges of our proposed task. Moreover, we propose\na new metric called VT-CLIPScore to help evaluate the semantic consistency of\ncross-modality summary. The proposed model achieves promising performance on\nthis new task and establishes a benchmark for future research.\n","authors":["Jingyang Lin","Hang Hua","Ming Chen","Yikang Li","Jenhao Hsiao","Chiuman Ho","Jiebo Luo"],"pdf_url":"https://arxiv.org/pdf/2303.12060v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12054v1","updated":"2023-03-21T17:45:38Z","published":"2023-03-21T17:45:38Z","title":"Influencer Backdoor Attack on Semantic Segmentation","summary":"  When a small number of poisoned samples are injected into the training\ndataset of a deep neural network, the network can be induced to exhibit\nmalicious behavior during inferences, which poses potential threats to\nreal-world applications. While they have been intensively studied in\nclassification, backdoor attacks on semantic segmentation have been largely\noverlooked. Unlike classification, semantic segmentation aims to classify every\npixel within a given image. In this work, we explore backdoor attacks on\nsegmentation models to misclassify all pixels of a victim class by injecting a\nspecific trigger on non-victim pixels during inferences, which is dubbed\nInfluencer Backdoor Attack (IBA). IBA is expected to maintain the\nclassification accuracy of non-victim pixels and misleads classifications of\nall victim pixels in every single inference. Specifically, we consider two\ntypes of IBA scenarios, i.e., 1) Free-position IBA: the trigger can be\npositioned freely except for pixels of the victim class, and 2) Long-distance\nIBA: the trigger can only be positioned somewhere far from victim pixels, given\nthe possible practical constraint. Based on the context aggregation ability of\nsegmentation models, we propose techniques to improve IBA for the scenarios.\nConcretely, for free-position IBA, we propose a simple, yet effective Nearest\nNeighbor trigger injection strategy for poisoned sample creation. For\nlong-distance IBA, we propose a novel Pixel Random Labeling strategy. Our\nextensive experiments reveal that current segmentation models do suffer from\nbackdoor attacks, and verify that our proposed techniques can further increase\nattack performance.\n","authors":["Haoheng Lan","Jindong Gu","Philip Torr","Hengshuang Zhao"],"pdf_url":"https://arxiv.org/pdf/2303.12054v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12051v1","updated":"2023-03-21T17:43:26Z","published":"2023-03-21T17:43:26Z","title":"A Novel and Optimal Spectral Method for Permutation Synchronization","summary":"  Permutation synchronization is an important problem in computer science that\nconstitutes the key step of many computer vision tasks. The goal is to recover\n$n$ latent permutations from their noisy and incomplete pairwise measurements.\nIn recent years, spectral methods have gained increasing popularity thanks to\ntheir simplicity and computational efficiency. Spectral methods utilize the\nleading eigenspace $U$ of the data matrix and its block submatrices\n$U_1,U_2,\\ldots, U_n$ to recover the permutations. In this paper, we propose a\nnovel and statistically optimal spectral algorithm. Unlike the existing methods\nwhich use $\\{U_jU_1^\\top\\}_{j\\geq 2}$, ours constructs an anchor matrix $M$ by\naggregating useful information from all the block submatrices and estimates the\nlatent permutations through $\\{U_jM^\\top\\}_{j\\geq 1}$. This modification\novercomes a crucial limitation of the existing methods caused by the repetitive\nuse of $U_1$ and leads to an improved numerical performance. To establish the\noptimality of the proposed method, we carry out a fine-grained spectral\nanalysis and obtain a sharp exponential error bound that matches the minimax\nrate.\n","authors":["Duc Nguyen","Anderson Ye Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.12051v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12050v1","updated":"2023-03-21T17:41:36Z","published":"2023-03-21T17:41:36Z","title":"CurveCloudNet: Processing Point Clouds with 1D Structure","summary":"  Modern depth sensors such as LiDAR operate by sweeping laser-beams across the\nscene, resulting in a point cloud with notable 1D curve-like structures. In\nthis work, we introduce a new point cloud processing scheme and backbone,\ncalled CurveCloudNet, which takes advantage of the curve-like structure\ninherent to these sensors. While existing backbones discard the rich 1D\ntraversal patterns and rely on Euclidean operations, CurveCloudNet\nparameterizes the point cloud as a collection of polylines (dubbed a \"curve\ncloud\"), establishing a local surface-aware ordering on the points. Our method\napplies curve-specific operations to process the curve cloud, including a\nsymmetric 1D convolution, a ball grouping for merging points along curves, and\nan efficient 1D farthest point sampling algorithm on curves. By combining these\ncurve operations with existing point-based operations, CurveCloudNet is an\nefficient, scalable, and accurate backbone with low GPU memory requirements.\nEvaluations on the ShapeNet, Kortx, Audi Driving, and nuScenes datasets\ndemonstrate that CurveCloudNet outperforms both point-based and sparse-voxel\nbackbones in various segmentation settings, notably scaling better to large\nscenes than point-based alternatives while exhibiting better single object\nperformance than sparse-voxel alternatives.\n","authors":["Colton Stearns","Jiateng Liu","Davis Rempe","Despoina Paschalidou","Jeong Joon Park","Sebastien Mascha","Leonidas J. Guibas"],"pdf_url":"https://arxiv.org/pdf/2303.12050v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12048v1","updated":"2023-03-21T17:36:36Z","published":"2023-03-21T17:36:36Z","title":"Vox-E: Text-guided Voxel Editing of 3D Objects","summary":"  Large scale text-guided diffusion models have garnered significant attention\ndue to their ability to synthesize diverse images that convey complex visual\nconcepts. This generative power has more recently been leveraged to perform\ntext-to-3D synthesis. In this work, we present a technique that harnesses the\npower of latent diffusion models for editing existing 3D objects. Our method\ntakes oriented 2D images of a 3D object as input and learns a grid-based\nvolumetric representation of it. To guide the volumetric representation to\nconform to a target text prompt, we follow unconditional text-to-3D methods and\noptimize a Score Distillation Sampling (SDS) loss. However, we observe that\ncombining this diffusion-guided loss with an image-based regularization loss\nthat encourages the representation not to deviate too strongly from the input\nobject is challenging, as it requires achieving two conflicting goals while\nviewing only structure-and-appearance coupled 2D projections. Thus, we\nintroduce a novel volumetric regularization loss that operates directly in 3D\nspace, utilizing the explicit nature of our 3D representation to enforce\ncorrelation between the global structure of the original and edited object.\nFurthermore, we present a technique that optimizes cross-attention volumetric\ngrids to refine the spatial extent of the edits. Extensive experiments and\ncomparisons demonstrate the effectiveness of our approach in creating a myriad\nof edits which cannot be achieved by prior works.\n","authors":["Etai Sella","Gal Fiebelman","Peter Hedman","Hadar Averbuch-Elor"],"pdf_url":"https://arxiv.org/pdf/2303.12048v1.pdf","comment":"Project webpage: https://tau-vailab.github.io/Vox-E/"},{"id":"http://arxiv.org/abs/2303.12031v1","updated":"2023-03-21T17:16:01Z","published":"2023-03-21T17:16:01Z","title":"Semantic Latent Space Regression of Diffusion Autoencoders for Vertebral\n  Fracture Grading","summary":"  Vertebral fractures are a consequence of osteoporosis, with significant\nhealth implications for affected patients. Unfortunately, grading their\nseverity using CT exams is hard and subjective, motivating automated grading\nmethods. However, current approaches are hindered by imbalance and scarcity of\ndata and a lack of interpretability. To address these challenges, this paper\nproposes a novel approach that leverages unlabelled data to train a generative\nDiffusion Autoencoder (DAE) model as an unsupervised feature extractor. We\nmodel fracture grading as a continuous regression, which is more reflective of\nthe smooth progression of fractures. Specifically, we use a binary, supervised\nfracture classifier to construct a hyperplane in the DAE's latent space. We\nthen regress the severity of the fracture as a function of the distance to this\nhyperplane, calibrating the results to the Genant scale. Importantly, the\ngenerative nature of our method allows us to visualize different grades of a\ngiven vertebra, providing interpretability and insight into the features that\ncontribute to automated grading.\n","authors":["Matthias Keicher","Matan Atad","David Schinz","Alexandra S. Gersing","Sarah C. Foreman","Sophia S. Goller","Juergen Weissinger","Jon Rischewski","Anna-Sophia Dietrich","Benedikt Wiestler","Jan S. Kirschke","Nassir Navab"],"pdf_url":"https://arxiv.org/pdf/2303.12031v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2212.04825v2","updated":"2023-03-21T17:13:58Z","published":"2022-12-09T18:59:57Z","title":"A Whac-A-Mole Dilemma: Shortcuts Come in Multiples Where Mitigating One\n  Amplifies Others","summary":"  Machine learning models have been found to learn shortcuts -- unintended\ndecision rules that are unable to generalize -- undermining models'\nreliability. Previous works address this problem under the tenuous assumption\nthat only a single shortcut exists in the training data. Real-world images are\nrife with multiple visual cues from background to texture. Key to advancing the\nreliability of vision systems is understanding whether existing methods can\novercome multiple shortcuts or struggle in a Whac-A-Mole game, i.e., where\nmitigating one shortcut amplifies reliance on others. To address this\nshortcoming, we propose two benchmarks: 1) UrbanCars, a dataset with precisely\ncontrolled spurious cues, and 2) ImageNet-W, an evaluation set based on\nImageNet for watermark, a shortcut we discovered affects nearly every modern\nvision model. Along with texture and background, ImageNet-W allows us to study\nmultiple shortcuts emerging from training on natural images. We find computer\nvision models, including large foundation models -- regardless of training set,\narchitecture, and supervision -- struggle when multiple shortcuts are present.\nEven methods explicitly designed to combat shortcuts struggle in a Whac-A-Mole\ndilemma. To tackle this challenge, we propose Last Layer Ensemble, a\nsimple-yet-effective method to mitigate multiple shortcuts without Whac-A-Mole\nbehavior. Our results surface multi-shortcut mitigation as an overlooked\nchallenge critical to advancing the reliability of vision systems. The datasets\nand code are released: https://github.com/facebookresearch/Whac-A-Mole.\n","authors":["Zhiheng Li","Ivan Evtimov","Albert Gordo","Caner Hazirbas","Tal Hassner","Cristian Canton Ferrer","Chenliang Xu","Mark Ibrahim"],"pdf_url":"https://arxiv.org/pdf/2212.04825v2.pdf","comment":"CVPR 2023. Code is available at\n  https://github.com/facebookresearch/Whac-A-Mole"},{"id":"http://arxiv.org/abs/2303.12027v1","updated":"2023-03-21T17:09:03Z","published":"2023-03-21T17:09:03Z","title":"Joint Visual Grounding and Tracking with Natural Language Specification","summary":"  Tracking by natural language specification aims to locate the referred target\nin a sequence based on the natural language description. Existing algorithms\nsolve this issue in two steps, visual grounding and tracking, and accordingly\ndeploy the separated grounding model and tracking model to implement these two\nsteps, respectively. Such a separated framework overlooks the link between\nvisual grounding and tracking, which is that the natural language descriptions\nprovide global semantic cues for localizing the target for both two steps.\nBesides, the separated framework can hardly be trained end-to-end. To handle\nthese issues, we propose a joint visual grounding and tracking framework, which\nreformulates grounding and tracking as a unified task: localizing the referred\ntarget based on the given visual-language references. Specifically, we propose\na multi-source relation modeling module to effectively build the relation\nbetween the visual-language references and the test image. In addition, we\ndesign a temporal modeling module to provide a temporal clue with the guidance\nof the global semantic information for our model, which effectively improves\nthe adaptability to the appearance variations of the target. Extensive\nexperimental results on TNL2K, LaSOT, OTB99, and RefCOCOg demonstrate that our\nmethod performs favorably against state-of-the-art algorithms for both tracking\nand grounding. Code is available at https://github.com/lizhou-cs/JointNLT.\n","authors":["Li Zhou","Zikun Zhou","Kaige Mao","Zhenyu He"],"pdf_url":"https://arxiv.org/pdf/2303.12027v1.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2210.11456v3","updated":"2023-03-21T16:57:57Z","published":"2022-10-20T17:54:03Z","title":"MixMask: Revisiting Masking Strategy for Siamese ConvNets","summary":"  Recent advances in self-supervised learning have integrated Masked Image\nModeling (MIM) and Siamese Networks into a unified framework that leverages the\nbenefits of both techniques. However, several issues remain unaddressed when\napplying conventional erase-based masking with Siamese ConvNets. These include\n(I) the inability to drop uninformative masked regions in ConvNets as they\nprocess data continuously, resulting in low training efficiency compared to ViT\nmodels; and (II) the mismatch between erase-based masking and the\ncontrastive-based objective in Siamese ConvNets, which differs from the MIM\napproach. In this paper, we propose a filling-based masking strategy called\nMixMask to prevent information incompleteness caused by the randomly erased\nregions in an image in the vanilla masking method. Furthermore, we introduce a\nflexible loss function design that considers the semantic distance change\nbetween two different mixed views to adapt the integrated architecture and\nprevent mismatches between the transformed input and objective in Masked\nSiamese ConvNets (MSCN). We conducted extensive experiments on various\ndatasets, including CIFAR-100, Tiny-ImageNet, and ImageNet-1K. The results\ndemonstrate that our proposed framework achieves superior accuracy on linear\nprobing, semi-supervised, and supervised finetuning, outperforming the\nstate-of-the-art MSCN by a significant margin. Additionally, we demonstrate the\nsuperiority of our approach in object detection and segmentation tasks. Our\nsource code is available at https://github.com/LightnessOfBeing/MixMask.\n","authors":["Kirill Vishniakov","Eric Xing","Zhiqiang Shen"],"pdf_url":"https://arxiv.org/pdf/2210.11456v3.pdf","comment":"Technical report. Code is available at\n  https://github.com/LightnessOfBeing/MixMask"},{"id":"http://arxiv.org/abs/2303.12017v1","updated":"2023-03-21T16:54:01Z","published":"2023-03-21T16:54:01Z","title":"Learning Optical Flow and Scene Flow with Bidirectional Camera-LiDAR\n  Fusion","summary":"  In this paper, we study the problem of jointly estimating the optical flow\nand scene flow from synchronized 2D and 3D data. Previous methods either employ\na complex pipeline that splits the joint task into independent stages, or fuse\n2D and 3D information in an ``early-fusion'' or ``late-fusion'' manner. Such\none-size-fits-all approaches suffer from a dilemma of failing to fully utilize\nthe characteristic of each modality or to maximize the inter-modality\ncomplementarity. To address the problem, we propose a novel end-to-end\nframework, which consists of 2D and 3D branches with multiple bidirectional\nfusion connections between them in specific layers. Different from previous\nwork, we apply a point-based 3D branch to extract the LiDAR features, as it\npreserves the geometric structure of point clouds. To fuse dense image features\nand sparse point features, we propose a learnable operator named bidirectional\ncamera-LiDAR fusion module (Bi-CLFM). We instantiate two types of the\nbidirectional fusion pipeline, one based on the pyramidal coarse-to-fine\narchitecture (dubbed CamLiPWC), and the other one based on the recurrent\nall-pairs field transforms (dubbed CamLiRAFT). On FlyingThings3D, both CamLiPWC\nand CamLiRAFT surpass all existing methods and achieve up to a 47.9\\% reduction\nin 3D end-point-error from the best published result. Our best-performing\nmodel, CamLiRAFT, achieves an error of 4.26\\% on the KITTI Scene Flow\nbenchmark, ranking 1st among all submissions with much fewer parameters.\nBesides, our methods have strong generalization performance and the ability to\nhandle non-rigid motion. Code is available at\nhttps://github.com/MCG-NJU/CamLiFlow.\n","authors":["Haisong Liu","Tao Lu","Yihui Xu","Jia Liu","Limin Wang"],"pdf_url":"https://arxiv.org/pdf/2303.12017v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2111.10502"},{"id":"http://arxiv.org/abs/2303.12016v1","updated":"2023-03-21T16:52:08Z","published":"2023-03-21T16:52:08Z","title":"Automatic evaluation of herding behavior in towed fishing gear using\n  end-to-end training of CNN and attention-based networks","summary":"  This paper considers the automatic classification of herding behavior in the\ncluttered low-visibility environment that typically surrounds towed fishing\ngear. The paper compares three convolutional and attention-based deep action\nrecognition network architectures trained end-to-end on a small set of video\nsequences captured by a remotely controlled camera and classified by an expert\nin fishing technology. The sequences depict a scene in front of a fishing trawl\nwhere the conventional herding mechanism has been replaced by directed laser\nlight. The goal is to detect the presence of a fish in the sequence and\nclassify whether or not the fish reacts to the lasers. A two-stream CNN model,\na CNN-transformer hybrid, and a pure transformer model were trained end-to-end\nto achieve 63%, 54%, and 60% 10-fold classification accuracy on the three-class\ntask when compared to the human expert. Inspection of the activation maps\nlearned by the three networks raises questions about the attributes of the\nsequences the models may be learning, specifically whether changes in viewpoint\nintroduced by human camera operators that affect the position of laser lines in\nthe video frames may interfere with the classification. This underlines the\nimportance of careful experimental design when capturing scientific data for\nautomatic end-to-end evaluation and the usefulness of inspecting the trained\nmodels.\n","authors":["Orri Steinn Guðfinnsson","Týr Vilhjálmsson","Martin Eineborg","Torfi Thorhallsson"],"pdf_url":"https://arxiv.org/pdf/2303.12016v1.pdf","comment":"15 pages, 10 figures. To appear in Proceedings of the 5th Workshop on\n  Computer Vision for Analysis of Underwater Imagery (CVAUI) 2022, published as\n  part of the CVPR 2022 Proceedings"},{"id":"http://arxiv.org/abs/2303.12012v1","updated":"2023-03-21T16:49:41Z","published":"2023-03-21T16:49:41Z","title":"NeAT: Learning Neural Implicit Surfaces with Arbitrary Topologies from\n  Multi-view Images","summary":"  Recent progress in neural implicit functions has set new state-of-the-art in\nreconstructing high-fidelity 3D shapes from a collection of images. However,\nthese approaches are limited to closed surfaces as they require the surface to\nbe represented by a signed distance field. In this paper, we propose NeAT, a\nnew neural rendering framework that can learn implicit surfaces with arbitrary\ntopologies from multi-view images. In particular, NeAT represents the 3D\nsurface as a level set of a signed distance function (SDF) with a validity\nbranch for estimating the surface existence probability at the query positions.\nWe also develop a novel neural volume rendering method, which uses SDF and\nvalidity to calculate the volume opacity and avoids rendering points with low\nvalidity. NeAT supports easy field-to-mesh conversion using the classic\nMarching Cubes algorithm. Extensive experiments on DTU, MGN, and Deep Fashion\n3D datasets indicate that our approach is able to faithfully reconstruct both\nwatertight and non-watertight surfaces. In particular, NeAT significantly\noutperforms the state-of-the-art methods in the task of open surface\nreconstruction both quantitatively and qualitatively.\n","authors":["Xiaoxu Meng","Weikai Chen","Bo Yang"],"pdf_url":"https://arxiv.org/pdf/2303.12012v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10766v2","updated":"2023-03-21T16:39:10Z","published":"2023-03-19T20:52:44Z","title":"Multi-modal reward for visual relationships-based image captioning","summary":"  Deep neural networks have achieved promising results in automatic image\ncaptioning due to their effective representation learning and context-based\ncontent generation capabilities. As a prominent type of deep features used in\nmany of the recent image captioning methods, the well-known bottomup features\nprovide a detailed representation of different objects of the image in\ncomparison with the feature maps directly extracted from the raw image.\nHowever, the lack of high-level semantic information about the relationships\nbetween these objects is an important drawback of bottom-up features, despite\ntheir expensive and resource-demanding extraction procedure. To take advantage\nof visual relationships in caption generation, this paper proposes a deep\nneural network architecture for image captioning based on fusing the visual\nrelationships information extracted from an image's scene graph with the\nspatial feature maps of the image. A multi-modal reward function is then\nintroduced for deep reinforcement learning of the proposed network using a\ncombination of language and vision similarities in a common embedding space.\nThe results of extensive experimentation on the MSCOCO dataset show the\neffectiveness of using visual relationships in the proposed captioning method.\nMoreover, the results clearly indicate that the proposed multi-modal reward in\ndeep reinforcement learning leads to better model optimization, outperforming\nseveral state-of-the-art image captioning algorithms, while using light and\neasy to extract image features. A detailed experimental study of the components\nconstituting the proposed method is also presented.\n","authors":["Ali Abedi","Hossein Karshenas","Peyman Adibi"],"pdf_url":"https://arxiv.org/pdf/2303.10766v2.pdf","comment":"18 pages, 10 figures"},{"id":"http://arxiv.org/abs/2111.07632v2","updated":"2023-03-21T16:36:35Z","published":"2021-11-15T09:35:54Z","title":"CoReS: Compatible Representations via Stationarity","summary":"  In this paper, we propose a novel method to learn internal feature\nrepresentation models that are \\textit{compatible} with previously learned\nones. Compatible features enable for direct comparison of old and new learned\nfeatures, allowing them to be used interchangeably over time. This eliminates\nthe need for visual search systems to extract new features for all previously\nseen images in the gallery-set when sequentially upgrading the representation\nmodel. Extracting new features is typically quite expensive or infeasible in\nthe case of very large gallery-sets and/or real time systems (i.e.,\nface-recognition systems, social networks, life-long learning systems, robotics\nand surveillance systems). Our approach, called Compatible Representations via\nStationarity (CoReS), achieves compatibility by encouraging stationarity to the\nlearned representation model without relying on previously learned models.\nStationarity allows features' statistical properties not to change under time\nshift so that the current learned features are inter-operable with the old\nones. We evaluate single and sequential multi-model upgrading in growing\nlarge-scale training datasets and we show that our method improves the\nstate-of-the-art in achieving compatible features by a large margin. In\nparticular, upgrading ten times with training data taken from CASIA-WebFace and\nevaluating in Labeled Face in the Wild (LFW), we obtain a 49\\% increase in\nmeasuring the average number of times compatibility is achieved, which is a\n544\\% relative improvement over previous state-of-the-art.\n","authors":["Niccolo Biondi","Federico Pernici","Matteo Bruni","Alberto Del Bimbo"],"pdf_url":"https://arxiv.org/pdf/2111.07632v2.pdf","comment":"in IEEE Transactions on Pattern Analysis and Machine Intelligence"},{"id":"http://arxiv.org/abs/2302.06891v3","updated":"2023-03-21T16:33:56Z","published":"2023-02-14T08:27:42Z","title":"UKnow: A Unified Knowledge Protocol for Common-Sense Reasoning and\n  Vision-Language Pre-training","summary":"  This work presents a unified knowledge protocol, called UKnow, which\nfacilitates knowledge-based studies from the perspective of data. Particularly\nfocusing on visual and linguistic modalities, we categorize data knowledge into\nfive unit types, namely, in-image, in-text, cross-image, cross-text, and\nimage-text, and set up an efficient pipeline to help construct the multimodal\nknowledge graph from any data collection. Thanks to the logical information\nnaturally contained in knowledge graph, organizing datasets under UKnow format\nopens up more possibilities of data usage compared to the commonly used\nimage-text pairs. Following UKnow protocol, we collect, from public\ninternational news, a large-scale multimodal knowledge graph dataset that\nconsists of 1,388,568 nodes (with 571,791 vision-related ones) and 3,673,817\ntriplets. The dataset is also annotated with rich event tags, including 11\ncoarse labels and 9,185 fine labels. Experiments on four benchmarks demonstrate\nthe potential of UKnow in supporting common-sense reasoning and boosting\nvision-language pre-training with a single dataset, benefiting from its unified\nform of knowledge organization. Code, dataset, and models will be made publicly\navailable.\n","authors":["Biao Gong","Xiaoying Xie","Yutong Feng","Yiliang Lv","Yujun Shen","Deli Zhao"],"pdf_url":"https://arxiv.org/pdf/2302.06891v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12001v1","updated":"2023-03-21T16:33:40Z","published":"2023-03-21T16:33:40Z","title":"Visual Representation Learning from Unlabeled Video using Contrastive\n  Masked Autoencoders","summary":"  Masked Autoencoders (MAEs) learn self-supervised representations by randomly\nmasking input image patches and a reconstruction loss. Alternatively,\ncontrastive learning self-supervised methods encourage two versions of the same\ninput to have a similar representation, while pulling apart the representations\nfor different inputs. We propose ViC-MAE, a general method that combines both\nMAE and contrastive learning by pooling the local feature representations\nlearned under the MAE reconstruction objective and leveraging this global\nrepresentation under a contrastive objective across video frames. We show that\nvisual representations learned under ViC-MAE generalize well to both video\nclassification and image classification tasks. Using a backbone ViT-B/16\nnetwork pre-trained on the Moments in Time (MiT) dataset, we obtain\nstate-of-the-art transfer learning from video to images on Imagenet-1k by\nimproving 1.58% in absolute top-1 accuracy from a recent previous work.\nMoreover, our method maintains a competitive transfer-learning performance of\n81.50% top-1 accuracy on the Kinetics-400 video classification benchmark. In\naddition, we show that despite its simplicity, ViC-MAE yields improved results\ncompared to combining MAE pre-training with previously proposed contrastive\nobjectives such as VicReg and SiamSiam.\n","authors":["Jefferson Hernandez","Ruben Villegas","Vicente Ordonez"],"pdf_url":"https://arxiv.org/pdf/2303.12001v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11997v1","updated":"2023-03-21T16:31:53Z","published":"2023-03-21T16:31:53Z","title":"E-MLB: Multilevel Benchmark for Event-Based Camera Denoising","summary":"  Event cameras, such as dynamic vision sensors (DVS), are biologically\ninspired vision sensors that have advanced over conventional cameras in high\ndynamic range, low latency and low power consumption, showing great application\npotential in many fields. Event cameras are more sensitive to junction leakage\ncurrent and photocurrent as they output differential signals, losing the\nsmoothing function of the integral imaging process in the RGB camera. The\nlogarithmic conversion further amplifies noise, especially in low-contrast\nconditions. Recently, researchers proposed a series of datasets and evaluation\nmetrics but limitations remain: 1) the existing datasets are small in scale and\ninsufficient in noise diversity, which cannot reflect the authentic working\nenvironments of event cameras; and 2) the existing denoising evaluation metrics\nare mostly referenced evaluation metrics, relying on APS information or manual\nannotation. To address the above issues, we construct a large-scale event\ndenoising dataset (multilevel benchmark for event denoising, E-MLB) for the\nfirst time, which consists of 100 scenes, each with four noise levels, that is\n12 times larger than the largest existing denoising dataset. We also propose\nthe first nonreference event denoising metric, the event structural ratio\n(ESR), which measures the structural intensity of given events. ESR is inspired\nby the contrast metric, but is independent of the number of events and\nprojection direction. Based on the proposed benchmark and ESR, we evaluate the\nmost representative denoising algorithms, including classic and SOTA, and\nprovide denoising baselines under various scenes and noise levels. The\ncorresponding results and codes are available at\nhttps://github.com/KugaMaxx/cuke-emlb.\n","authors":["Saizhe Ding","Jinze Chen","Yang Wang","Yu Kang","Weiguo Song","Jie Cheng","Yang Cao"],"pdf_url":"https://arxiv.org/pdf/2303.11997v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11989v1","updated":"2023-03-21T16:21:02Z","published":"2023-03-21T16:21:02Z","title":"Text2Room: Extracting Textured 3D Meshes from 2D Text-to-Image Models","summary":"  We present Text2Room, a method for generating room-scale textured 3D meshes\nfrom a given text prompt as input. To this end, we leverage pre-trained 2D\ntext-to-image models to synthesize a sequence of images from different poses.\nIn order to lift these outputs into a consistent 3D scene representation, we\ncombine monocular depth estimation with a text-conditioned inpainting model.\nThe core idea of our approach is a tailored viewpoint selection such that the\ncontent of each image can be fused into a seamless, textured 3D mesh. More\nspecifically, we propose a continuous alignment strategy that iteratively fuses\nscene frames with the existing geometry to create a seamless mesh. Unlike\nexisting works that focus on generating single objects or zoom-out trajectories\nfrom text, our method generates complete 3D scenes with multiple objects and\nexplicit 3D geometry. We evaluate our approach using qualitative and\nquantitative metrics, demonstrating it as the first method to generate\nroom-scale 3D geometry with compelling textures from only text as input.\n","authors":["Lukas Höllein","Ang Cao","Andrew Owens","Justin Johnson","Matthias Nießner"],"pdf_url":"https://arxiv.org/pdf/2303.11989v1.pdf","comment":"video: https://youtu.be/fjRnFL91EZc project page:\n  https://lukashoel.github.io/text-to-room/ code:\n  https://github.com/lukasHoel/text2room"},{"id":"http://arxiv.org/abs/2212.07398v4","updated":"2023-03-21T16:16:41Z","published":"2022-12-14T18:31:47Z","title":"Policy Adaptation from Foundation Model Feedback","summary":"  Recent progress on vision-language foundation models have brought significant\nadvancement to building general-purpose robots. By using the pre-trained models\nto encode the scene and instructions as inputs for decision making, the\ninstruction-conditioned policy can generalize across different objects and\ntasks. While this is encouraging, the policy still fails in most cases given an\nunseen task or environment. In this work, we propose Policy Adaptation from\nFoundation model Feedback (PAFF). When deploying the trained policy to a new\ntask or a new environment, we first let the policy play with randomly generated\ninstructions to record the demonstrations. While the execution could be wrong,\nwe can use the pre-trained foundation models to provide feedback to relabel the\ndemonstrations. This automatically provides new pairs of\ndemonstration-instruction data for policy fine-tuning. We evaluate our method\non a broad range of experiments with the focus on generalization on unseen\nobjects, unseen tasks, unseen environments, and sim-to-real transfer. We show\nPAFF improves baselines by a large margin in all cases. Our project page is\navailable at https://geyuying.github.io/PAFF/\n","authors":["Yuying Ge","Annabella Macaluso","Li Erran Li","Ping Luo","Xiaolong Wang"],"pdf_url":"https://arxiv.org/pdf/2212.07398v4.pdf","comment":"Accepted by CVPR 2023; Project page: https://geyuying.github.io/PAFF/"},{"id":"http://arxiv.org/abs/2303.09554v3","updated":"2023-03-21T16:09:25Z","published":"2023-03-16T17:59:22Z","title":"PartNeRF: Generating Part-Aware Editable 3D Shapes without 3D\n  Supervision","summary":"  Impressive progress in generative models and implicit representations gave\nrise to methods that can generate 3D shapes of high quality. However, being\nable to locally control and edit shapes is another essential property that can\nunlock several content creation applications. Local control can be achieved\nwith part-aware models, but existing methods require 3D supervision and cannot\nproduce textures. In this work, we devise PartNeRF, a novel part-aware\ngenerative model for editable 3D shape synthesis that does not require any\nexplicit 3D supervision. Our model generates objects as a set of locally\ndefined NeRFs, augmented with an affine transformation. This enables several\nediting operations such as applying transformations on parts, mixing parts from\ndifferent objects etc. To ensure distinct, manipulable parts we enforce a hard\nassignment of rays to parts that makes sure that the color of each ray is only\ndetermined by a single NeRF. As a result, altering one part does not affect the\nappearance of the others. Evaluations on various ShapeNet categories\ndemonstrate the ability of our model to generate editable 3D objects of\nimproved fidelity, compared to previous part-based generative approaches that\nrequire 3D supervision or models relying on NeRFs.\n","authors":["Konstantinos Tertikas","Despoina Paschalidou","Boxiao Pan","Jeong Joon Park","Mikaela Angelina Uy","Ioannis Emiris","Yannis Avrithis","Leonidas Guibas"],"pdf_url":"https://arxiv.org/pdf/2303.09554v3.pdf","comment":"To appear in CVPR 2023, Project Page:\n  https://ktertikas.github.io/part_nerf"},{"id":"http://arxiv.org/abs/2203.15793v4","updated":"2023-03-21T16:06:20Z","published":"2022-03-29T17:50:43Z","title":"Instance Relation Graph Guided Source-Free Domain Adaptive Object\n  Detection","summary":"  Unsupervised Domain Adaptation (UDA) is an effective approach to tackle the\nissue of domain shift. Specifically, UDA methods try to align the source and\ntarget representations to improve the generalization on the target domain.\nFurther, UDA methods work under the assumption that the source data is\naccessible during the adaptation process. However, in real-world scenarios, the\nlabelled source data is often restricted due to privacy regulations, data\ntransmission constraints, or proprietary data concerns. The Source-Free Domain\nAdaptation (SFDA) setting aims to alleviate these concerns by adapting a\nsource-trained model for the target domain without requiring access to the\nsource data. In this paper, we explore the SFDA setting for the task of\nadaptive object detection. To this end, we propose a novel training strategy\nfor adapting a source-trained object detector to the target domain without\nsource data. More precisely, we design a novel contrastive loss to enhance the\ntarget representations by exploiting the objects relations for a given target\ndomain input. These object instance relations are modelled using an Instance\nRelation Graph (IRG) network, which are then used to guide the contrastive\nrepresentation learning. In addition, we utilize a student-teacher based\nknowledge distillation strategy to avoid overfitting to the noisy pseudo-labels\ngenerated by the source-trained model. Extensive experiments on multiple object\ndetection benchmark datasets show that the proposed approach is able to\nefficiently adapt source-trained object detectors to the target domain,\noutperforming previous state-of-the-art domain adaptive detection methods. Code\nand models are provided in\n\\href{https://viudomain.github.io/irg-sfda-web/}{https://viudomain.github.io/irg-sfda-web/}.\n","authors":["Vibashan VS","Poojan Oza","Vishal M. Patel"],"pdf_url":"https://arxiv.org/pdf/2203.15793v4.pdf","comment":"Accepted to CVPR 2023. Project site:\n  \\href{https://viudomain.github.io/irg-sfda-web/}{https://viudomain.github.io/irg-sfda-web/}"},{"id":"http://arxiv.org/abs/2303.11971v1","updated":"2023-03-21T16:04:09Z","published":"2023-03-21T16:04:09Z","title":"Defect Detection Approaches Based on Simulated Reference Image","summary":"  This work is addressing the problem of defect anomaly detection based on a\nclean reference image. Specifically, we focus on SEM semiconductor defects in\naddition to several natural image anomalies. There are well-known methods to\ncreate a simulation of an artificial reference image by its defect specimen. In\nthis work, we introduce several applications for this capability, that the\nsimulated reference is beneficial for improving their results. Among these\ndefect detection methods are classic computer vision applied on\ndifference-image, supervised deep-learning (DL) based on human labels, and\nunsupervised DL which is trained on feature-level patterns of normal reference\nimages. We show in this study how to incorporate correctly the simulated\nreference image for these defect and anomaly detection applications. As our\nexperiment demonstrates, simulated reference achieves higher performance than\nthe real reference of an image of a defect and anomaly. This advantage of\nsimulated reference occurs mainly due to the less noise and geometric\nvariations together with better alignment and registration to the original\ndefect background.\n","authors":["Nati Ofir","Yotam Ben Shoshan","Ran Badanes","Boris Sherman"],"pdf_url":"https://arxiv.org/pdf/2303.11971v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.11035v3","updated":"2023-03-21T16:03:50Z","published":"2022-10-20T06:08:03Z","title":"PointTAD: Multi-Label Temporal Action Detection with Learnable Query\n  Points","summary":"  Traditional temporal action detection (TAD) usually handles untrimmed videos\nwith small number of action instances from a single label (e.g., ActivityNet,\nTHUMOS). However, this setting might be unrealistic as different classes of\nactions often co-occur in practice. In this paper, we focus on the task of\nmulti-label temporal action detection that aims to localize all action\ninstances from a multi-label untrimmed video. Multi-label TAD is more\nchallenging as it requires for fine-grained class discrimination within a\nsingle video and precise localization of the co-occurring instances. To\nmitigate this issue, we extend the sparse query-based detection paradigm from\nthe traditional TAD and propose the multi-label TAD framework of PointTAD.\nSpecifically, our PointTAD introduces a small set of learnable query points to\nrepresent the important frames of each action instance. This point-based\nrepresentation provides a flexible mechanism to localize the discriminative\nframes at boundaries and as well the important frames inside the action.\nMoreover, we perform the action decoding process with the Multi-level\nInteractive Module to capture both point-level and instance-level action\nsemantics. Finally, our PointTAD employs an end-to-end trainable framework\nsimply based on RGB input for easy deployment. We evaluate our proposed method\non two popular benchmarks and introduce the new metric of detection-mAP for\nmulti-label TAD. Our model outperforms all previous methods by a large margin\nunder the detection-mAP metric, and also achieves promising results under the\nsegmentation-mAP metric. Code is available at\nhttps://github.com/MCG-NJU/PointTAD.\n","authors":["Jing Tan","Xiaotong Zhao","Xintian Shi","Bin Kang","Limin Wang"],"pdf_url":"https://arxiv.org/pdf/2210.11035v3.pdf","comment":"NeurIPS 2022 camera ready version"},{"id":"http://arxiv.org/abs/2303.11969v1","updated":"2023-03-21T16:01:55Z","published":"2023-03-21T16:01:55Z","title":"Explain To Me: Salience-Based Explainability for Synthetic Face\n  Detection Models","summary":"  The performance of convolutional neural networks has continued to improve\nover the last decade. At the same time, as model complexity grows, it becomes\nincreasingly more difficult to explain model decisions. Such explanations may\nbe of critical importance for reliable operation of human-machine pairing\nsetups, or for model selection when the \"best\" model among many\nequally-accurate models must be established. Saliency maps represent one\npopular way of explaining model decisions by highlighting image regions models\ndeem important when making a prediction. However, examining salience maps at\nscale is not practical. In this paper, we propose five novel methods of\nleveraging model salience to explain a model behavior at scale. These methods\nask: (a) what is the average entropy for a model's salience maps, (b) how does\nmodel salience change when fed out-of-set samples, (c) how closely does model\nsalience follow geometrical transformations, (d) what is the stability of model\nsalience across independent training runs, and (e) how does model salience\nreact to salience-guided image degradations. To assess the proposed measures on\na concrete and topical problem, we conducted a series of experiments for the\ntask of synthetic face detection with two types of models: those trained\ntraditionally with cross-entropy loss, and those guided by human salience when\ntraining to increase model generalizability. These two types of models are\ncharacterized by different, interpretable properties of their salience maps,\nwhich allows for the evaluation of the correctness of the proposed measures. We\noffer source codes for each measure along with this paper.\n","authors":["Colton Crum","Patrick Tinsley","Aidan Boyd","Jacob Piland","Christopher Sweet","Timothy Kelley","Kevin Bowyer","Adam Czajka"],"pdf_url":"https://arxiv.org/pdf/2303.11969v1.pdf","comment":"13 pages, 10 figures"},{"id":"http://arxiv.org/abs/2303.11963v1","updated":"2023-03-21T15:50:08Z","published":"2023-03-21T15:50:08Z","title":"NEMTO: Neural Environment Matting for Novel View and Relighting\n  Synthesis of Transparent Objects","summary":"  We propose NEMTO, the first end-to-end neural rendering pipeline to model 3D\ntransparent objects with complex geometry and unknown indices of refraction.\nCommonly used appearance modeling such as the Disney BSDF model cannot\naccurately address this challenging problem due to the complex light paths\nbending through refractions and the strong dependency of surface appearance on\nillumination. With 2D images of the transparent object as input, our method is\ncapable of high-quality novel view and relighting synthesis. We leverage\nimplicit Signed Distance Functions (SDF) to model the object geometry and\npropose a refraction-aware ray bending network to model the effects of light\nrefraction within the object. Our ray bending network is more tolerant to\ngeometric inaccuracies than traditional physically-based methods for rendering\ntransparent objects. We provide extensive evaluations on both synthetic and\nreal-world datasets to demonstrate our high-quality synthesis and the\napplicability of our method.\n","authors":["Dongqing Wang","Tong Zhang","Sabine Süsstrunk"],"pdf_url":"https://arxiv.org/pdf/2303.11963v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11950v1","updated":"2023-03-21T15:41:57Z","published":"2023-03-21T15:41:57Z","title":"Learning A Sparse Transformer Network for Effective Image Deraining","summary":"  Transformers-based methods have achieved significant performance in image\nderaining as they can model the non-local information which is vital for\nhigh-quality image reconstruction. In this paper, we find that most existing\nTransformers usually use all similarities of the tokens from the query-key\npairs for the feature aggregation. However, if the tokens from the query are\ndifferent from those of the key, the self-attention values estimated from these\ntokens also involve in feature aggregation, which accordingly interferes with\nthe clear image restoration. To overcome this problem, we propose an effective\nDeRaining network, Sparse Transformer (DRSformer) that can adaptively keep the\nmost useful self-attention values for feature aggregation so that the\naggregated features better facilitate high-quality image reconstruction.\nSpecifically, we develop a learnable top-k selection operator to adaptively\nretain the most crucial attention scores from the keys for each query for\nbetter feature aggregation. Simultaneously, as the naive feed-forward network\nin Transformers does not model the multi-scale information that is important\nfor latent clear image restoration, we develop an effective mixed-scale\nfeed-forward network to generate better features for image deraining. To learn\nan enriched set of hybrid features, which combines local context from CNN\noperators, we equip our model with mixture of experts feature compensator to\npresent a cooperation refinement deraining scheme. Extensive experimental\nresults on the commonly used benchmarks demonstrate that the proposed method\nachieves favorable performance against state-of-the-art approaches. The source\ncode and trained models are available at\nhttps://github.com/cschenxiang/DRSformer.\n","authors":["Xiang Chen","Hao Li","Mingqiang Li","Jinshan Pan"],"pdf_url":"https://arxiv.org/pdf/2303.11950v1.pdf","comment":"Accepted as a highlight paper in CVPR 2023"},{"id":"http://arxiv.org/abs/2303.11938v1","updated":"2023-03-21T15:38:26Z","published":"2023-03-21T15:38:26Z","title":"3D-CLFusion: Fast Text-to-3D Rendering with Contrastive Latent Diffusion","summary":"  We tackle the task of text-to-3D creation with pre-trained latent-based NeRFs\n(NeRFs that generate 3D objects given input latent code). Recent works such as\nDreamFusion and Magic3D have shown great success in generating 3D content using\nNeRFs and text prompts, but the current approach of optimizing a NeRF for every\ntext prompt is 1) extremely time-consuming and 2) often leads to low-resolution\noutputs. To address these challenges, we propose a novel method named\n3D-CLFusion which leverages the pre-trained latent-based NeRFs and performs\nfast 3D content creation in less than a minute. In particular, we introduce a\nlatent diffusion prior network for learning the w latent from the input CLIP\ntext/image embeddings. This pipeline allows us to produce the w latent without\nfurther optimization during inference and the pre-trained NeRF is able to\nperform multi-view high-resolution 3D synthesis based on the latent. We note\nthat the novelty of our model lies in that we introduce contrastive learning\nduring training the diffusion prior which enables the generation of the valid\nview-invariant latent code. We demonstrate through experiments the\neffectiveness of our proposed view-invariant diffusion process for fast\ntext-to-3D creation, e.g., 100 times faster than DreamFusion. We note that our\nmodel is able to serve as the role of a plug-and-play tool for text-to-3D with\npre-trained NeRFs.\n","authors":["Yu-Jhe Li","Kris Kitani"],"pdf_url":"https://arxiv.org/pdf/2303.11938v1.pdf","comment":"15 pages. Non-CMU authors are currently hidden due to an internal\n  legal review in progress of their company"},{"id":"http://arxiv.org/abs/2303.11932v1","updated":"2023-03-21T15:34:50Z","published":"2023-03-21T15:34:50Z","title":"Using Explanations to Guide Models","summary":"  Deep neural networks are highly performant, but might base their decision on\nspurious or background features that co-occur with certain classes, which can\nhurt generalization. To mitigate this issue, the usage of 'model guidance' has\ngained popularity recently: for this, models are guided to be \"right for the\nright reasons\" by regularizing the models' explanations to highlight the right\nfeatures. Experimental validation of these approaches has thus far however been\nlimited to relatively simple and / or synthetic datasets. To gain a better\nunderstanding of which model-guiding approaches actually transfer to more\nchallenging real-world datasets, in this work we conduct an in-depth evaluation\nacross various loss functions, attribution methods, models, and 'guidance\ndepths' on the PASCAL VOC 2007 and MS COCO 2014 datasets, and show that model\nguidance can sometimes even improve model performance. In this context, we\nfurther propose a novel energy loss, show its effectiveness in directing the\nmodel to focus on object features. We also show that these gains can be\nachieved even with a small fraction (e.g. 1%) of bounding box annotations,\nhighlighting the cost effectiveness of this approach. Lastly, we show that this\napproach can also improve generalization under distribution shifts. Code will\nbe made available.\n","authors":["Sukrut Rao","Moritz Böhle","Amin Parchami-Araghi","Bernt Schiele"],"pdf_url":"https://arxiv.org/pdf/2303.11932v1.pdf","comment":"38 pages, 35 figures, 4 tables"},{"id":"http://arxiv.org/abs/2106.09614v3","updated":"2023-03-21T15:25:02Z","published":"2021-06-17T15:52:19Z","title":"Robust Model-based Face Reconstruction through Weakly-Supervised Outlier\n  Segmentation","summary":"  In this work, we aim to enhance model-based face reconstruction by avoiding\nfitting the model to outliers, i.e. regions that cannot be well-expressed by\nthe model such as occluders or make-up. The core challenge for localizing\noutliers is that they are highly variable and difficult to annotate. To\novercome this challenging problem, we introduce a joint Face-autoencoder and\noutlier segmentation approach (FOCUS).In particular, we exploit the fact that\nthe outliers cannot be fitted well by the face model and hence can be localized\nwell given a high-quality model fitting. The main challenge is that the model\nfitting and the outlier segmentation are mutually dependent on each other, and\nneed to be inferred jointly. We resolve this chicken-and-egg problem with an\nEM-type training strategy, where a face autoencoder is trained jointly with an\noutlier segmentation network. This leads to a synergistic effect, in which the\nsegmentation network prevents the face encoder from fitting to the outliers,\nenhancing the reconstruction quality. The improved 3D face reconstruction, in\nturn, enables the segmentation network to better predict the outliers. To\nresolve the ambiguity between outliers and regions that are difficult to fit,\nsuch as eyebrows, we build a statistical prior from synthetic data that\nmeasures the systematic bias in model fitting. Experiments on the NoW testset\ndemonstrate that FOCUS achieves SOTA 3D face reconstruction performance among\nall baselines that are trained without 3D annotation. Moreover, our results on\nCelebA-HQ and the AR database show that the segmentation network can localize\noccluders accurately despite being trained without any segmentation annotation.\n","authors":["Chunlu Li","Andreas Morel-Forster","Thomas Vetter","Bernhard Egger","Adam Kortylewski"],"pdf_url":"https://arxiv.org/pdf/2106.09614v3.pdf","comment":"20 pages, CVPR2023"},{"id":"http://arxiv.org/abs/2211.14308v2","updated":"2023-03-21T15:22:30Z","published":"2022-11-25T18:59:46Z","title":"WALDO: Future Video Synthesis using Object Layer Decomposition and\n  Parametric Flow Prediction","summary":"  This paper presents WALDO (WArping Layer-Decomposed Objects), a novel\napproach to the prediction of future video frames from past ones. Individual\nimages are decomposed into multiple layers combining object masks and a small\nset of control points. The layer structure is shared across all frames in each\nvideo to build dense inter-frame connections. Complex scene motions are modeled\nby combining parametric geometric transformations associated with individual\nlayers, and video synthesis is broken down into discovering the layers\nassociated with past frames, predicting the corresponding transformations for\nupcoming ones and warping the associated object regions accordingly, and\nfilling in the remaining image parts. Extensive experiments on multiple\nbenchmarks including urban videos (Cityscapes and KITTI) and videos featuring\nnonrigid motions (UCF-Sports and H3.6M), show that our method consistently\noutperforms the state of the art by a significant margin in every case. Code,\npretrained models, and video samples synthesized by our approach can be found\nin the project webpage https://16lemoing.github.io/waldo.\n","authors":["Guillaume Le Moing","Jean Ponce","Cordelia Schmid"],"pdf_url":"https://arxiv.org/pdf/2211.14308v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11926v1","updated":"2023-03-21T15:19:20Z","published":"2023-03-21T15:19:20Z","title":"Exploring Object-Centric Temporal Modeling for Efficient Multi-View 3D\n  Object Detection","summary":"  In this paper, we propose a long-sequence modeling framework, named\nStreamPETR, for multi-view 3D object detection. Built upon the sparse query\ndesign in the PETR series, we systematically develop an object-centric temporal\nmechanism. The model is performed in an online manner and the long-term\nhistorical information is propagated through object queries frame by frame.\nBesides, we introduce a motion-aware layer normalization to model the movement\nof the objects. StreamPETR achieves significant performance improvements only\nwith negligible computation cost, compared to the single-frame baseline. On the\nstandard nuScenes benchmark, it reaches a new state-of-the-art performance\n(63.6% NDS). The lightweight version realizes 45.0% mAP and 31.7 FPS,\noutperforming the state-of-the-art method (SOLOFusion) by 2.3% mAP and 1.8x\nfaster FPS. Code will be available at\nhttps://github.com/exiawsh/StreamPETR.git.\n","authors":["Shihao Wang","Yingfei Liu","Tiancai Wang","Ying Li","Xiangyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.11926v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11923v1","updated":"2023-03-21T15:15:21Z","published":"2023-03-21T15:15:21Z","title":"Performance-aware Approximation of Global Channel Pruning for Multitask\n  CNNs","summary":"  Global channel pruning (GCP) aims to remove a subset of channels (filters)\nacross different layers from a deep model without hurting the performance.\nPrevious works focus on either single task model pruning or simply adapting it\nto multitask scenario, and still face the following problems when handling\nmultitask pruning: 1) Due to the task mismatch, a well-pruned backbone for\nclassification task focuses on preserving filters that can extract\ncategory-sensitive information, causing filters that may be useful for other\ntasks to be pruned during the backbone pruning stage; 2) For multitask\npredictions, different filters within or between layers are more closely\nrelated and interacted than that for single task prediction, making multitask\npruning more difficult. Therefore, aiming at multitask model compression, we\npropose a Performance-Aware Global Channel Pruning (PAGCP) framework. We first\ntheoretically present the objective for achieving superior GCP, by considering\nthe joint saliency of filters from intra- and inter-layers. Then a sequentially\ngreedy pruning strategy is proposed to optimize the objective, where a\nperformance-aware oracle criterion is developed to evaluate sensitivity of\nfilters to each task and preserve the globally most task-related filters.\nExperiments on several multitask datasets show that the proposed PAGCP can\nreduce the FLOPs and parameters by over 60% with minor performance drop, and\nachieves 1.2x$\\sim$3.3x acceleration on both cloud and mobile platforms.\n","authors":["Hancheng Ye","Bo Zhang","Tao Chen","Jiayuan Fan","Bin Wang"],"pdf_url":"https://arxiv.org/pdf/2303.11923v1.pdf","comment":"Accepted for publication in T-PAMI, our code is available at\n  http://www.github.com/HankYe/PAGCP.git"},{"id":"http://arxiv.org/abs/2303.11921v1","updated":"2023-03-21T15:12:20Z","published":"2023-03-21T15:12:20Z","title":"Context De-confounded Emotion Recognition","summary":"  Context-Aware Emotion Recognition (CAER) is a crucial and challenging task\nthat aims to perceive the emotional states of the target person with contextual\ninformation. Recent approaches invariably focus on designing sophisticated\narchitectures or mechanisms to extract seemingly meaningful representations\nfrom subjects and contexts. However, a long-overlooked issue is that a context\nbias in existing datasets leads to a significantly unbalanced distribution of\nemotional states among different context scenarios. Concretely, the harmful\nbias is a confounder that misleads existing models to learn spurious\ncorrelations based on conventional likelihood estimation, significantly\nlimiting the models' performance. To tackle the issue, this paper provides a\ncausality-based perspective to disentangle the models from the impact of such\nbias, and formulate the causalities among variables in the CAER task via a\ntailored causal graph. Then, we propose a Contextual Causal Intervention Module\n(CCIM) based on the backdoor adjustment to de-confound the confounder and\nexploit the true causal effect for model training. CCIM is plug-in and\nmodel-agnostic, which improves diverse state-of-the-art approaches by\nconsiderable margins. Extensive experiments on three benchmark datasets\ndemonstrate the effectiveness of our CCIM and the significance of causal\ninsight.\n","authors":["Dingkang Yang","Zhaoyu Chen","Yuzheng Wang","Shunli Wang","Mingcheng Li","Siao Liu","Xiao Zhao","Shuai Huang","Zhiyan Dong","Peng Zhai","Lihua Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.11921v1.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2107.02600v2","updated":"2023-03-21T15:11:44Z","published":"2021-07-06T13:20:14Z","title":"Stateless actor-critic for instance segmentation with high-level priors","summary":"  Instance segmentation is an important computer vision problem which remains\nchallenging despite impressive recent advances due to deep learning-based\nmethods. Given sufficient training data, fully supervised methods can yield\nexcellent performance, but annotation of ground-truth data remains a major\nbottleneck, especially for biomedical applications where it has to be performed\nby domain experts. The amount of labels required can be drastically reduced by\nusing rules derived from prior knowledge to guide the segmentation. However,\nthese rules are in general not differentiable and thus cannot be used with\nexisting methods. Here, we relax this requirement by using stateless actor\ncritic reinforcement learning, which enables non-differentiable rewards. We\nformulate the instance segmentation problem as graph partitioning and the actor\ncritic predicts the edge weights driven by the rewards, which are based on the\nconformity of segmented instances to high-level priors on object shape,\nposition or size. The experiments on toy and real datasets demonstrate that we\ncan achieve excellent performance without any direct supervision based only on\na rich set of priors.\n","authors":["Paul Hilt","Maedeh Zarvandi","Edgar Kaziakhmedov","Sourabh Bhide","Maria Leptin","Constantin Pape","Anna Kreshuk"],"pdf_url":"https://arxiv.org/pdf/2107.02600v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11917v1","updated":"2023-03-21T15:08:35Z","published":"2023-03-21T15:08:35Z","title":"Efficient Decision-based Black-box Patch Attacks on Video Recognition","summary":"  Although Deep Neural Networks (DNNs) have demonstrated excellent performance,\nthey are vulnerable to adversarial patches that introduce perceptible and\nlocalized perturbations to the input. Generating adversarial patches on images\nhas received much attention, while adversarial patches on videos have not been\nwell investigated. Further, decision-based attacks, where attackers only access\nthe predicted hard labels by querying threat models, have not been well\nexplored on video models either, even if they are practical in real-world video\nrecognition scenes. The absence of such studies leads to a huge gap in the\nrobustness assessment for video models. To bridge this gap, this work first\nexplores decision-based patch attacks on video models. We analyze that the huge\nparameter space brought by videos and the minimal information returned by\ndecision-based models both greatly increase the attack difficulty and query\nburden. To achieve a query-efficient attack, we propose a spatial-temporal\ndifferential evolution (STDE) framework. First, STDE introduces target videos\nas patch textures and only adds patches on keyframes that are adaptively\nselected by temporal difference. Second, STDE takes minimizing the patch area\nas the optimization objective and adopts spatialtemporal mutation and crossover\nto search for the global optimum without falling into the local optimum.\nExperiments show STDE has demonstrated state-of-the-art performance in terms of\nthreat, efficiency and imperceptibility. Hence, STDE has the potential to be a\npowerful tool for evaluating the robustness of video recognition models.\n","authors":["Kaixun Jiang","Zhaoyu Chen","Tony Huang","Jiafeng Wang","Dingkang Yang","Bo Li","Yan Wang","Wenqiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.11917v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11916v1","updated":"2023-03-21T15:06:35Z","published":"2023-03-21T15:06:35Z","title":"CompoDiff: Versatile Composed Image Retrieval With Latent Diffusion","summary":"  This paper proposes a novel diffusion-based model, CompoDiff, for solving\nComposed Image Retrieval (CIR) with latent diffusion and presents a newly\ncreated dataset of 18 million reference images, conditions, and corresponding\ntarget image triplets to train the model. CompoDiff not only achieves a new\nzero-shot state-of-the-art on a CIR benchmark such as FashionIQ but also\nenables a more versatile CIR by accepting various conditions, such as negative\ntext and image mask conditions, which are unavailable with existing CIR\nmethods. In addition, the CompoDiff features are on the intact CLIP embedding\nspace so that they can be directly used for all existing models exploiting the\nCLIP space. The code and dataset used for the training, and the pre-trained\nweights are available at https://github.com/navervision/CompoDiff\n","authors":["Geonmo Gu","Sanghyuk Chun","Wonjae Kim","HeeJae Jun","Yoohoon Kang","Sangdoo Yun"],"pdf_url":"https://arxiv.org/pdf/2303.11916v1.pdf","comment":"First two authors contributed equally; 23 pages, 4.8MB"},{"id":"http://arxiv.org/abs/2303.11910v1","updated":"2023-03-21T15:01:02Z","published":"2023-03-21T15:01:02Z","title":"360BEV: Panoramic Semantic Mapping for Indoor Bird's-Eye View","summary":"  Seeing only a tiny part of the whole is not knowing the full circumstance.\nBird's-eye-view (BEV) perception, a process of obtaining allocentric maps from\negocentric views, is restricted when using a narrow Field of View (FoV) alone.\nIn this work, mapping from 360{\\deg} panoramas to BEV semantics, the 360BEV\ntask, is established for the first time to achieve holistic representations of\nindoor scenes in a top-down view. Instead of relying on narrow-FoV image\nsequences, a panoramic image with depth information is sufficient to generate a\nholistic BEV semantic map. To benchmark 360BEV, we present two indoor datasets,\n360BEV-Matterport and 360BEV-Stanford, both of which include egocentric\npanoramic images and semantic segmentation labels, as well as allocentric\nsemantic maps. Besides delving deep into different mapping paradigms, we\npropose a dedicated solution for panoramic semantic mapping, namely 360Mapper.\nThrough extensive experiments, our methods achieve 44.32% and 45.78% in mIoU on\nboth datasets respectively, surpassing previous counterparts with gains of\n+7.60% and +9.70% in mIoU. Code and datasets will be available at:\n\\url{https://jamycheung.github.io/360BEV.html}.\n","authors":["Zhifeng Teng","Jiaming Zhang","Kailun Yang","Kunyu Peng","Hao Shi","Simon Reiß","Ke Cao","Rainer Stiefelhagen"],"pdf_url":"https://arxiv.org/pdf/2303.11910v1.pdf","comment":"Code and datasets will be available at:\n  \\url{https://jamycheung.github.io/360BEV.html}"},{"id":"http://arxiv.org/abs/2303.11909v1","updated":"2023-03-21T15:00:17Z","published":"2023-03-21T15:00:17Z","title":"The Multiscale Surface Vision Transformer","summary":"  Surface meshes are a favoured domain for representing structural and\nfunctional information on the human cortex, but their complex topology and\ngeometry pose significant challenges for deep learning analysis. While\nTransformers have excelled as domain-agnostic architectures for\nsequence-to-sequence learning, notably for structures where the translation of\nthe convolution operation is non-trivial, the quadratic cost of the\nself-attention operation remains an obstacle for many dense prediction tasks.\nInspired by some of the latest advances in hierarchical modelling with vision\ntransformers, we introduce the Multiscale Surface Vision Transformer (MS-SiT)\nas a backbone architecture for surface deep learning. The self-attention\nmechanism is applied within local-mesh-windows to allow for high-resolution\nsampling of the underlying data, while a shifted-window strategy improves the\nsharing of information between windows. Neighbouring patches are successively\nmerged, allowing the MS-SiT to learn hierarchical representations suitable for\nany prediction task. Results demonstrate that the MS-SiT outperforms existing\nsurface deep learning methods for neonatal phenotyping prediction tasks using\nthe Developing Human Connectome Project (dHCP) dataset. Furthermore, building\nthe MS-SiT backbone into a U-shaped architecture for surface segmentation\ndemonstrates competitive results on cortical parcellation using the UK Biobank\n(UKB) and manually-annotated MindBoggle datasets. Code and trained models are\npublicly available at\nhttps://github.com/metrics-lab/surface-vision-transformers .\n","authors":["Simon Dahan","Abdulah Fawaz","Mohamed A. Suliman","Mariana da Silva","Logan Z. J. Williams","Daniel Rueckert","Emma C. Robinson"],"pdf_url":"https://arxiv.org/pdf/2303.11909v1.pdf","comment":"14 pages, 4 figures"},{"id":"http://arxiv.org/abs/2303.11906v1","updated":"2023-03-21T14:52:52Z","published":"2023-03-21T14:52:52Z","title":"Solving Oscillation Problem in Post-Training Quantization Through a\n  Theoretical Perspective","summary":"  Post-training quantization (PTQ) is widely regarded as one of the most\nefficient compression methods practically, benefitting from its data privacy\nand low computation costs. We argue that an overlooked problem of oscillation\nis in the PTQ methods. In this paper, we take the initiative to explore and\npresent a theoretical proof to explain why such a problem is essential in PTQ.\nAnd then, we try to solve this problem by introducing a principled and\ngeneralized framework theoretically. In particular, we first formulate the\noscillation in PTQ and prove the problem is caused by the difference in module\ncapacity. To this end, we define the module capacity (ModCap) under\ndata-dependent and data-free scenarios, where the differentials between\nadjacent modules are used to measure the degree of oscillation. The problem is\nthen solved by selecting top-k differentials, in which the corresponding\nmodules are jointly optimized and quantized. Extensive experiments demonstrate\nthat our method successfully reduces the performance drop and is generalized to\ndifferent neural networks and PTQ methods. For example, with 2/4 bit ResNet-50\nquantization, our method surpasses the previous state-of-the-art method by\n1.9%. It becomes more significant on small model quantization, e.g. surpasses\nBRECQ method by 6.61% on MobileNetV2*0.5.\n","authors":["Yuexiao Ma","Huixia Li","Xiawu Zheng","Xuefeng Xiao","Rui Wang","Shilei Wen","Xin Pan","Fei Chao","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2303.11906v1.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2303.11898v1","updated":"2023-03-21T14:41:25Z","published":"2023-03-21T14:41:25Z","title":"Real-time volumetric rendering of dynamic humans","summary":"  We present a method for fast 3D reconstruction and real-time rendering of\ndynamic humans from monocular videos with accompanying parametric body fits.\nOur method can reconstruct a dynamic human in less than 3h using a single GPU,\ncompared to recent state-of-the-art alternatives that take up to 72h. These\nspeedups are obtained by using a lightweight deformation model solely based on\nlinear blend skinning, and an efficient factorized volumetric representation\nfor modeling the shape and color of the person in canonical pose. Moreover, we\npropose a novel local ray marching rendering which, by exploiting standard GPU\nhardware and without any baking or conversion of the radiance field, allows\nvisualizing the neural human on a mobile VR device at 40 frames per second with\nminimal loss of visual quality. Our experimental evaluation shows superior or\ncompetitive results with state-of-the art methods while obtaining large\ntraining speedup, using a simple model, and achieving real-time rendering.\n","authors":["Ignacio Rocco","Iurii Makarov","Filippos Kokkinos","David Novotny","Benjamin Graham","Natalia Neverova","Andrea Vedaldi"],"pdf_url":"https://arxiv.org/pdf/2303.11898v1.pdf","comment":"Project page: https://real-time-humans.github.io/"},{"id":"http://arxiv.org/abs/2303.11897v1","updated":"2023-03-21T14:41:02Z","published":"2023-03-21T14:41:02Z","title":"TIFA: Accurate and Interpretable Text-to-Image Faithfulness Evaluation\n  with Question Answering","summary":"  Despite thousands of researchers, engineers, and artists actively working on\nimproving text-to-image generation models, systems often fail to produce images\nthat accurately align with the text inputs. We introduce TIFA (Text-to-Image\nFaithfulness evaluation with question Answering), an automatic evaluation\nmetric that measures the faithfulness of a generated image to its text input\nvia visual question answering (VQA). Specifically, given a text input, we\nautomatically generate several question-answer pairs using a language model. We\ncalculate image faithfulness by checking whether existing VQA models can answer\nthese questions using the generated image. TIFA is a reference-free metric that\nallows for fine-grained and interpretable evaluations of generated images. TIFA\nalso has better correlations with human judgments than existing metrics. Based\non this approach, we introduce TIFA v1.0, a benchmark consisting of 4K diverse\ntext inputs and 25K questions across 12 categories (object, counting, etc.). We\npresent a comprehensive evaluation of existing text-to-image models using TIFA\nv1.0 and highlight the limitations and challenges of current models. For\ninstance, we find that current text-to-image models, despite doing well on\ncolor and material, still struggle in counting, spatial relations, and\ncomposing multiple objects. We hope our benchmark will help carefully measure\nthe research progress in text-to-image synthesis and provide valuable insights\nfor further research.\n","authors":["Yushi Hu","Benlin Liu","Jungo Kasai","Yizhong Wang","Mari Ostendorf","Ranjay Krishna","Noah A. Smith"],"pdf_url":"https://arxiv.org/pdf/2303.11897v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.00701v3","updated":"2023-03-21T14:39:49Z","published":"2022-05-02T07:45:51Z","title":"DeepGraviLens: a Multi-Modal Architecture for Classifying Gravitational\n  Lensing Data","summary":"  Gravitational lensing is the relativistic effect generated by massive bodies,\nwhich bend the space-time surrounding them. It is a deeply investigated topic\nin astrophysics and allows validating theoretical relativistic results and\nstudying faint astrophysical objects that would not be visible otherwise. In\nrecent years Machine Learning methods have been applied to support the analysis\nof the gravitational lensing phenomena by detecting lensing effects in data\nsets consisting of images associated with brightness variation time series.\nHowever, the state-of-art approaches either consider only images and neglect\ntime-series data or achieve relatively low accuracy on the most difficult data\nsets. This paper introduces DeepGraviLens, a novel multi-modal network that\nclassifies spatio-temporal data belonging to one non-lensed system type and\nthree lensed system types. It surpasses the current state of the art accuracy\nresults by $\\approx$ 19% to $\\approx$ 43%, depending on the considered data\nset. Such an improvement will enable the acceleration of the analysis of lensed\nobjects in upcoming astrophysical surveys, which will exploit the petabytes of\ndata collected, e.g., from the Vera C. Rubin Observatory.\n","authors":["Nicolò Oreste Pinciroli Vago","Piero Fraternali"],"pdf_url":"https://arxiv.org/pdf/2205.00701v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11884v1","updated":"2023-03-21T14:24:58Z","published":"2023-03-21T14:24:58Z","title":"Better Understanding Differences in Attribution Methods via Systematic\n  Evaluations","summary":"  Deep neural networks are very successful on many vision tasks, but hard to\ninterpret due to their black box nature. To overcome this, various post-hoc\nattribution methods have been proposed to identify image regions most\ninfluential to the models' decisions. Evaluating such methods is challenging\nsince no ground truth attributions exist. We thus propose three novel\nevaluation schemes to more reliably measure the faithfulness of those methods,\nto make comparisons between them more fair, and to make visual inspection more\nsystematic. To address faithfulness, we propose a novel evaluation setting\n(DiFull) in which we carefully control which parts of the input can influence\nthe output in order to distinguish possible from impossible attributions. To\naddress fairness, we note that different methods are applied at different\nlayers, which skews any comparison, and so evaluate all methods on the same\nlayers (ML-Att) and discuss how this impacts their performance on quantitative\nmetrics. For more systematic visualizations, we propose a scheme (AggAtt) to\nqualitatively evaluate the methods on complete datasets. We use these\nevaluation schemes to study strengths and shortcomings of some widely used\nattribution methods over a wide range of models. Finally, we propose a\npost-processing smoothing step that significantly improves the performance of\nsome attribution methods, and discuss its applicability.\n","authors":["Sukrut Rao","Moritz Böhle","Bernt Schiele"],"pdf_url":"https://arxiv.org/pdf/2303.11884v1.pdf","comment":"35 pages, 37 figures, 2 tables, extended version of arXiv:2205.10435"},{"id":"http://arxiv.org/abs/2303.11881v1","updated":"2023-03-21T14:24:26Z","published":"2023-03-21T14:24:26Z","title":"Protective Self-Adaptive Pruning to Better Compress DNNs","summary":"  Adaptive network pruning approach has recently drawn significant attention\ndue to its excellent capability to identify the importance and redundancy of\nlayers and filters and customize a suitable pruning solution. However, it\nremains unsatisfactory since current adaptive pruning methods rely mostly on an\nadditional monitor to score layer and filter importance, and thus faces high\ncomplexity and weak interpretability. To tackle these issues, we have deeply\nresearched the weight reconstruction process in iterative prune-train process\nand propose a Protective Self-Adaptive Pruning (PSAP) method. First of all,\nPSAP can utilize its own information, weight sparsity ratio, to adaptively\nadjust pruning ratio of layers before each pruning step. Moreover, we propose a\nprotective reconstruction mechanism to prevent important filters from being\npruned through supervising gradients and to avoid unrecoverable information\nloss as well. Our PSAP is handy and explicit because it merely depends on\nweights and gradients of model itself, instead of requiring an additional\nmonitor as in early works. Experiments on ImageNet and CIFAR-10 also\ndemonstrate its superiority to current works in both accuracy and compression\nratio, especially for compressing with a high ratio or pruning from scratch.\n","authors":["Liang Li","Pengfei Zhao"],"pdf_url":"https://arxiv.org/pdf/2303.11881v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11880v1","updated":"2023-03-21T14:24:06Z","published":"2023-03-21T14:24:06Z","title":"Focused and Collaborative Feedback Integration for Interactive Image\n  Segmentation","summary":"  Interactive image segmentation aims at obtaining a segmentation mask for an\nimage using simple user annotations. During each round of interaction, the\nsegmentation result from the previous round serves as feedback to guide the\nuser's annotation and provides dense prior information for the segmentation\nmodel, effectively acting as a bridge between interactions. Existing methods\noverlook the importance of feedback or simply concatenate it with the original\ninput, leading to underutilization of feedback and an increase in the number of\nrequired annotations. To address this, we propose an approach called Focused\nand Collaborative Feedback Integration (FCFI) to fully exploit the feedback for\nclick-based interactive image segmentation. FCFI first focuses on a local area\naround the new click and corrects the feedback based on the similarities of\nhigh-level features. It then alternately and collaboratively updates the\nfeedback and deep features to integrate the feedback into the features. The\nefficacy and efficiency of FCFI were validated on four benchmarks, namely\nGrabCut, Berkeley, SBD, and DAVIS. Experimental results show that FCFI achieved\nnew state-of-the-art performance with less computational overhead than previous\nmethods. The source code is available at\nhttps://github.com/veizgyauzgyauz/FCFI.\n","authors":["Qiaoqiao Wei","Hui Zhang","Jun-Hai Yong"],"pdf_url":"https://arxiv.org/pdf/2303.11880v1.pdf","comment":"Accepted for publication at CVPR 2023"},{"id":"http://arxiv.org/abs/2303.11866v1","updated":"2023-03-21T14:12:08Z","published":"2023-03-21T14:12:08Z","title":"Contrastive Alignment of Vision to Language Through Parameter-Efficient\n  Transfer Learning","summary":"  Contrastive vision-language models (e.g. CLIP) are typically created by\nupdating all the parameters of a vision model and language model through\ncontrastive training. Can such models be created by a small number of parameter\nupdates to an already-trained language model and vision model? The literature\ndescribes techniques that can create vision-language models by updating a small\nnumber of parameters in a language model, but these require already aligned\nvisual representations and are non-contrastive, hence unusable for\nlatency-sensitive applications such as neural search. We explore the\nfeasibility and benefits of parameter-efficient contrastive vision-language\nalignment through transfer learning: creating a model such as CLIP by minimally\nupdating an already-trained vision and language model. We find that a minimal\nset of parameter updates ($<$7%) can achieve the same performance as full-model\ntraining, and updating specific components ($<$1% of parameters) can match 75%\nof full-model training. We describe a series of experiments: we show that\nexisting knowledge is conserved more strongly in parameter-efficient training\nand that parameter-efficient scaling scales with model and dataset size. Where\npaired-image text data is scarce but strong multilingual language models exist\n(e.g. low resource languages), parameter-efficient training is even preferable\nto full-model training. Given a fixed compute budget, parameter-efficient\ntraining allows training larger models on the same hardware, achieving\nequivalent performance in less time. Parameter-efficient training hence\nconstitutes an energy-efficient and effective training strategy for contrastive\nvision-language models that may be preferable to the full-model training\nparadigm for common use cases. Code and weights at\nhttps://github.com/codezakh/LilT.\n","authors":["Zaid Khan","Yun Fu"],"pdf_url":"https://arxiv.org/pdf/2303.11866v1.pdf","comment":"Accepted to ICLR 2023"},{"id":"http://arxiv.org/abs/2302.10574v2","updated":"2023-03-21T14:10:33Z","published":"2023-02-21T10:00:58Z","title":"MulGT: Multi-task Graph-Transformer with Task-aware Knowledge Injection\n  and Domain Knowledge-driven Pooling for Whole Slide Image Analysis","summary":"  Whole slide image (WSI) has been widely used to assist automated diagnosis\nunder the deep learning fields. However, most previous works only discuss the\nSINGLE task setting which is not aligned with real clinical setting, where\npathologists often conduct multiple diagnosis tasks simultaneously. Also, it is\ncommonly recognized that the multi-task learning paradigm can improve learning\nefficiency by exploiting commonalities and differences across multiple tasks.\nTo this end, we present a novel multi-task framework (i.e., MulGT) for WSI\nanalysis by the specially designed Graph-Transformer equipped with Task-aware\nKnowledge Injection and Domain Knowledge-driven Graph Pooling modules.\nBasically, with the Graph Neural Network and Transformer as the building\ncommons, our framework is able to learn task-agnostic low-level local\ninformation as well as task-specific high-level global representation.\nConsidering that different tasks in WSI analysis depend on different features\nand properties, we also design a novel Task-aware Knowledge Injection module to\ntransfer the task-shared graph embedding into task-specific feature spaces to\nlearn more accurate representation for different tasks. Further, we elaborately\ndesign a novel Domain Knowledge-driven Graph Pooling module for each task to\nimprove both the accuracy and robustness of different tasks by leveraging\ndifferent diagnosis patterns of multiple tasks. We evaluated our method on two\npublic WSI datasets from TCGA projects, i.e., esophageal carcinoma and kidney\ncarcinoma. Experimental results show that our method outperforms single-task\ncounterparts and the state-of-theart methods on both tumor typing and staging\ntasks.\n","authors":["Weiqin Zhao","Shujun Wang","Maximus Yeung","Tianye Niu","Lequan Yu"],"pdf_url":"https://arxiv.org/pdf/2302.10574v2.pdf","comment":"AAAI 2023"},{"id":"http://arxiv.org/abs/2303.11859v1","updated":"2023-03-21T13:59:32Z","published":"2023-03-21T13:59:32Z","title":"LEAPS: End-to-End One-Step Person Search With Learnable Proposals","summary":"  We propose an end-to-end one-step person search approach with learnable\nproposals, named LEAPS. Given a set of sparse and learnable proposals, LEAPS\nemploys a dynamic person search head to directly perform person detection and\ncorresponding re-id feature generation without non-maximum suppression\npost-processing. The dynamic person search head comprises a detection head and\na novel flexible re-id head. Our flexible re-id head first employs a dynamic\nregion-of-interest (RoI) operation to extract discriminative RoI features of\nthe proposals. Then, it generates re-id features using a plain and a\nhierarchical interaction re-id module. To better guide discriminative re-id\nfeature learning, we introduce a diverse re-id sample matching strategy,\ninstead of bipartite matching in detection head. Comprehensive experiments\nreveal the benefit of the proposed LEAPS, achieving a favorable performance on\ntwo public person search benchmarks: CUHK-SYSU and PRW. When using the same\nResNet50 backbone, our LEAPS obtains a mAP score of 55.0%, outperforming the\nbest reported results in literature by 1.7%, while achieving around a two-fold\nspeedup on the challenging PRW dataset. Our source code and models will be\nreleased.\n","authors":["Zhiqiang Dong","Jiale Cao","Rao Muhammad Anwer","Jin Xie","Fahad Khan","Yanwei Pang"],"pdf_url":"https://arxiv.org/pdf/2303.11859v1.pdf","comment":"11 pages, 9 figures"},{"id":"http://arxiv.org/abs/2303.11855v1","updated":"2023-03-21T13:55:27Z","published":"2023-03-21T13:55:27Z","title":"CLIP-ReIdent: Contrastive Training for Player Re-Identification","summary":"  Sports analytics benefits from recent advances in machine learning providing\na competitive advantage for teams or individuals. One important task in this\ncontext is the performance measurement of individual players to provide reports\nand log files for subsequent analysis. During sport events like basketball,\nthis involves the re-identification of players during a match either from\nmultiple camera viewpoints or from a single camera viewpoint at different\ntimes. In this work, we investigate whether it is possible to transfer the\nout-standing zero-shot performance of pre-trained CLIP models to the domain of\nplayer re-identification. For this purpose we reformulate the contrastive\nlanguage-to-image pre-training approach from CLIP to a contrastive\nimage-to-image training approach using the InfoNCE loss as training objective.\nUnlike previous work, our approach is entirely class-agnostic and benefits from\nlarge-scale pre-training. With a fine-tuned CLIP ViT-L/14 model we achieve\n98.44 % mAP on the MMSports 2022 Player Re-Identification challenge.\nFurthermore we show that the CLIP Vision Transformers have already strong OCR\ncapabilities to identify useful player features like shirt numbers in a\nzero-shot manner without any fine-tuning on the dataset. By applying the\nScore-CAM algorithm we visualise the most important image regions that our\nfine-tuned model identifies when calculating the similarity score between two\nimages of a player.\n","authors":["Konrad Habel","Fabian Deuser","Norbert Oswald"],"pdf_url":"https://arxiv.org/pdf/2303.11855v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11851v1","updated":"2023-03-21T13:49:49Z","published":"2023-03-21T13:49:49Z","title":"Sample4Geo: Hard Negative Sampling For Cross-View Geo-Localisation","summary":"  Cross-View Geo-Localisation is still a challenging task where additional\nmodules, specific pre-processing or zooming strategies are necessary to\ndetermine accurate positions of images. Since different views have different\ngeometries, pre-processing like polar transformation helps to merge them.\nHowever, this results in distorted images which then have to be rectified.\nAdding hard negatives to the training batch could improve the overall\nperformance but with the default loss functions in geo-localisation it is\ndifficult to include them. In this article, we present a simplified but\neffective architecture based on contrastive learning with symmetric InfoNCE\nloss that outperforms current state-of-the-art results. Our framework consists\nof a narrow training pipeline that eliminates the need of using aggregation\nmodules, avoids further pre-processing steps and even increases the\ngeneralisation capability of the model to unknown regions. We introduce two\ntypes of sampling strategies for hard negatives. The first explicitly exploits\ngeographically neighboring locations to provide a good starting point. The\nsecond leverages the visual similarity between the image embeddings in order to\nmine hard negative samples. Our work shows excellent performance on common\ncross-view datasets like CVUSA, CVACT, University-1652 and VIGOR. A comparison\nbetween cross-area and same-area settings demonstrate the good generalisation\ncapability of our model.\n","authors":["Fabian Deuser","Konrad Habel","Norbert Oswald"],"pdf_url":"https://arxiv.org/pdf/2303.11851v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11848v1","updated":"2023-03-21T13:48:53Z","published":"2023-03-21T13:48:53Z","title":"Dens-PU: PU Learning with Density-Based Positive Labeled Augmentation","summary":"  This study proposes a novel approach for solving the PU learning problem\nbased on an anomaly-detection strategy. Latent encodings extracted from\npositive-labeled data are linearly combined to acquire new samples. These new\nsamples are used as embeddings to increase the density of positive-labeled data\nand, thus, define a boundary that approximates the positive class. The further\na sample is from the boundary the more it is considered as a negative sample.\nOnce a set of negative samples is obtained, the PU learning problem reduces to\nbinary classification. The approach, named Dens-PU due to its reliance on the\ndensity of positive-labeled data, was evaluated using benchmark image datasets,\nand state-of-the-art results were attained.\n","authors":["Vasileios Sevetlidis","George Pavlidis","Spyridon Mouroutsos","Antonios Gasteratos"],"pdf_url":"https://arxiv.org/pdf/2303.11848v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10449v2","updated":"2023-03-21T13:41:03Z","published":"2023-03-18T16:22:59Z","title":"Uncertainty-Aware Optimal Transport for Semantically Coherent\n  Out-of-Distribution Detection","summary":"  Semantically coherent out-of-distribution (SCOOD) detection aims to discern\noutliers from the intended data distribution with access to unlabeled extra\nset. The coexistence of in-distribution and out-of-distribution samples will\nexacerbate the model overfitting when no distinction is made. To address this\nproblem, we propose a novel uncertainty-aware optimal transport scheme. Our\nscheme consists of an energy-based transport (ET) mechanism that estimates the\nfluctuating cost of uncertainty to promote the assignment of semantic-agnostic\nrepresentation, and an inter-cluster extension strategy that enhances the\ndiscrimination of semantic property among different clusters by widening the\ncorresponding margin distance. Furthermore, a T-energy score is presented to\nmitigate the magnitude gap between the parallel transport and classifier\nbranches. Extensive experiments on two standard SCOOD benchmarks demonstrate\nthe above-par OOD detection performance, outperforming the state-of-the-art\nmethods by a margin of 27.69% and 34.4% on FPR@95, respectively.\n","authors":["Fan Lu","Kai Zhu","Wei Zhai","Kecheng Zheng","Yang Cao"],"pdf_url":"https://arxiv.org/pdf/2303.10449v2.pdf","comment":"Accepted by CVPR2023"},{"id":"http://arxiv.org/abs/2211.08326v2","updated":"2023-03-21T13:37:04Z","published":"2022-11-14T10:07:30Z","title":"Contrastive learning for regression in multi-site brain age prediction","summary":"  Building accurate Deep Learning (DL) models for brain age prediction is a\nvery relevant topic in neuroimaging, as it could help better understand\nneurodegenerative disorders and find new biomarkers. To estimate accurate and\ngeneralizable models, large datasets have been collected, which are often\nmulti-site and multi-scanner. This large heterogeneity negatively affects the\ngeneralization performance of DL models since they are prone to overfit\nsite-related noise. Recently, contrastive learning approaches have been shown\nto be more robust against noise in data or labels. For this reason, we propose\na novel contrastive learning regression loss for robust brain age prediction\nusing MRI scans. Our method achieves state-of-the-art performance on the\nOpenBHB challenge, yielding the best generalization capability and robustness\nto site-related noise.\n","authors":["Carlo Alberto Barbano","Benoit Dufumier","Edouard Duchesnay","Marco Grangetto","Pietro Gori"],"pdf_url":"https://arxiv.org/pdf/2211.08326v2.pdf","comment":"5 pages"},{"id":"http://arxiv.org/abs/2303.11840v1","updated":"2023-03-21T13:34:12Z","published":"2023-03-21T13:34:12Z","title":"Self-Paced Neutral Expression-Disentangled Learning for Facial\n  Expression Recognition","summary":"  The accuracy of facial expression recognition is typically affected by the\nfollowing factors: high similarities across different expressions, disturbing\nfactors, and micro-facial movement of rapid and subtle changes. One potentially\nviable solution for addressing these barriers is to exploit the neutral\ninformation concealed in neutral expression images. To this end, in this paper\nwe propose a self-Paced Neutral Expression-Disentangled Learning (SPNDL) model.\nSPNDL disentangles neutral information from facial expressions, making it\neasier to extract key and deviation features. Specifically, it allows to\ncapture discriminative information among similar expressions and perceive\nmicro-facial movements. In order to better learn these neutral\nexpression-disentangled features (NDFs) and to alleviate the non-convex\noptimization problem, a self-paced learning (SPL) strategy based on NDFs is\nproposed in the training stage. SPL learns samples from easy to complex by\nincreasing the number of samples selected into the training process, which\nenables to effectively suppress the negative impacts introduced by low-quality\nsamples and inconsistently distributed NDFs. Experiments on three popular\ndatabases (i.e., CK+, Oulu-CASIA, and RAF-DB) show the effectiveness of our\nproposed method.\n","authors":["Zhenqian Wu","Xiaoyuan Li","Yazhou Ren","Xiaorong Pu","Xiaofeng Zhu","Lifang He"],"pdf_url":"https://arxiv.org/pdf/2303.11840v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11420v1","updated":"2023-03-21T13:31:15Z","published":"2023-03-21T13:31:15Z","title":"ADCNet: End-to-end perception with raw radar ADC data","summary":"  There is a renewed interest in radar sensors in the autonomous driving\nindustry. As a relatively mature technology, radars have seen steady\nimprovement over the last few years, making them an appealing alternative or\ncomplement to the commonly used LiDARs. An emerging trend is to leverage rich,\nlow-level radar data for perception. In this work we push this trend to the\nextreme -- we propose a method to perform end-to-end learning on the raw radar\nanalog-to-digital (ADC) data. Specifically, we design a learnable signal\nprocessing module inside the neural network, and a pre-training method guided\nby traditional signal processing algorithms. Experiment results corroborate the\noverall efficacy of the end-to-end learning method, while an ablation study\nvalidates the effectiveness of our individual innovations.\n","authors":["Bo Yang","Ishan Khatri","Michael Happold","Chulong Chen"],"pdf_url":"https://arxiv.org/pdf/2303.11420v1.pdf","comment":"10 pages, 8 figures"},{"id":"http://arxiv.org/abs/2203.04838v3","updated":"2023-03-21T13:30:43Z","published":"2022-03-09T16:12:08Z","title":"CMX: Cross-Modal Fusion for RGB-X Semantic Segmentation with\n  Transformers","summary":"  Scene understanding based on image segmentation is a crucial component for\nautonomous vehicles. Pixel-wise semantic segmentation of RGB images can be\nadvanced by exploiting informative features from the supplementary modality\n(X-modality). In this work, we propose CMX, a transformer-based cross-modal\nfusion framework for RGB-X semantic segmentation. To generalize to different\nsensing modalities encompassing various supplements and uncertainties, we\nconsider that comprehensive cross-modal interactions should be provided. CMX is\nbuilt with two streams to extract features from RGB images and the X-modality.\nIn each feature extraction stage, we design a Cross-Modal Feature Rectification\nModule (CM-FRM) to calibrate the feature of the current modality by combining\nthe feature from the other modality, in spatial- and channel-wise dimensions.\nWith rectified feature pairs, we deploy a Feature Fusion Module (FFM) to mix\nthem for the final semantic prediction. FFM is constructed with a\ncross-attention mechanism, which enables exchange of long-range contexts,\nenhancing bi-modal features globally. Extensive experiments show that CMX\ngeneralizes to diverse multi-modal combinations, achieving state-of-the-art\nperformances on five RGB-Depth benchmarks, as well as RGB-Thermal,\nRGB-Polarization, and RGB-LiDAR datasets. Besides, to investigate the\ngeneralizability to dense-sparse data fusion, we establish an RGB-Event\nsemantic segmentation benchmark based on the EventScape dataset, on which CMX\nsets the new state-of-the-art. The source code of CMX is publicly available at\nhttps://github.com/huaaaliu/RGBX_Semantic_Segmentation.\n","authors":["Jiaming Zhang","Huayao Liu","Kailun Yang","Xinxin Hu","Ruiping Liu","Rainer Stiefelhagen"],"pdf_url":"https://arxiv.org/pdf/2203.04838v3.pdf","comment":"Code is available at\n  https://github.com/huaaaliu/RGBX_Semantic_Segmentation"},{"id":"http://arxiv.org/abs/2303.11837v1","updated":"2023-03-21T13:29:17Z","published":"2023-03-21T13:29:17Z","title":"Self-supervised learning of a tailored Convolutional Auto Encoder for\n  histopathological prostate grading","summary":"  According to GLOBOCAN 2020, prostate cancer is the second most common cancer\nin men worldwide and the fourth most prevalent cancer overall. For\npathologists, grading prostate cancer is challenging, especially when\ndiscriminating between Grade 3 (G3) and Grade 4 (G4). This paper proposes a\nSelf-Supervised Learning (SSL) framework to classify prostate histopathological\nimages when labeled images are scarce. In particular, a tailored Convolutional\nAuto Encoder (CAE) is trained to reconstruct 128x128x3 patches of prostate\ncancer Whole Slide Images (WSIs) as a pretext task. The downstream task of the\nproposed SSL paradigm is the automatic grading of histopathological patches of\nprostate cancer. The presented framework reports promising results on the\nvalidation set, obtaining an overall accuracy of 83% and on the test set,\nachieving an overall accuracy value of 76% with F1-score of 77% in G4.\n","authors":["Zahra Tabatabaei","Adrian colomer","Kjersti Engan","Javier Oliver","Valery Naranjo"],"pdf_url":"https://arxiv.org/pdf/2303.11837v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.05725v2","updated":"2023-03-21T13:28:49Z","published":"2023-03-10T06:12:36Z","title":"CVT-SLR: Contrastive Visual-Textual Transformation for Sign Language\n  Recognition with Variational Alignment","summary":"  Sign language recognition (SLR) is a weakly supervised task that annotates\nsign videos as textual glosses. Recent studies show that insufficient training\ncaused by the lack of large-scale available sign language datasets becomes the\nmain bottleneck for SLR. The majority of SLR works thereby adopt pretrained\nvisual modules and develop two mainstream solutions. The multi-stream\narchitectures extend multi-cue visual features, yielding the current SOTA\nperformances but requiring complex designs and might introduce potential noise.\nAlternatively, the advanced single-cue SLR frameworks using explicit\ncross-modal alignment between visual and textual modalities are simple and\neffective, potentially competitive with the multi-cue framework. In this work,\nwe propose a novel contrastive visual-textual transformation for SLR, CVT-SLR,\nto fully explore the pretrained knowledge of both the visual and language\nmodalities. Based on the single-cue cross-modal alignment framework, we propose\na variational autoencoder (VAE) for pretrained contextual knowledge while\nintroducing the complete pretrained language module. The VAE implicitly aligns\nvisual and textual modalities while benefiting from pretrained contextual\nknowledge as the traditional contextual module. Meanwhile, a contrastive\ncross-modal alignment algorithm is proposed to further enhance the explicit\nconsistency constraints. Extensive experiments conducted on the two most\npopular public datasets, PHOENIX-2014 and PHOENIX-2014T, demonstrate that our\nproposed SLR framework not only consistently outperforms existing single-cue\nmethods but even outperforms SOTA multi-cue methods.\n","authors":["Jiangbin Zheng","Yile Wang","Cheng Tan","Siyuan Li","Ge Wang","Jun Xia","Yidong Chen","Stan Z. Li"],"pdf_url":"https://arxiv.org/pdf/2303.05725v2.pdf","comment":"Accepted to CVPR 2023 (highlight)"},{"id":"http://arxiv.org/abs/2303.11831v1","updated":"2023-03-21T13:19:51Z","published":"2023-03-21T13:19:51Z","title":"GLADE: Gradient Loss Augmented Degradation Enhancement for Unpaired\n  Super-Resolution of Anisotropic MRI","summary":"  We present a novel approach to synthesise high-resolution isotropic 3D\nabdominal MR images, from anisotropic 3D images in an unpaired fashion. Using a\nmodified CycleGAN architecture with a gradient mapping loss, we leverage\ndisjoint patches from the high-resolution (in-plane) data of an anisotropic\nvolume to enforce the network generator to increase the resolution of the\nlow-resolution (through-plane) slices. This will enable accelerated\nwhole-abdomen scanning with high-resolution isotropic images within short\nbreath-hold times.\n","authors":["Michele Pascale","Vivek Muthurangu","Javier Montalt Tordera","Heather E Fitzke","Gauraang Bhatnagar","Stuart Taylor","Jennifer Steeden"],"pdf_url":"https://arxiv.org/pdf/2303.11831v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11828v1","updated":"2023-03-21T13:14:36Z","published":"2023-03-21T13:14:36Z","title":"The Treasure Beneath Multiple Annotations: An Uncertainty-aware Edge\n  Detector","summary":"  Deep learning-based edge detectors heavily rely on pixel-wise labels which\nare often provided by multiple annotators. Existing methods fuse multiple\nannotations using a simple voting process, ignoring the inherent ambiguity of\nedges and labeling bias of annotators. In this paper, we propose a novel\nuncertainty-aware edge detector (UAED), which employs uncertainty to\ninvestigate the subjectivity and ambiguity of diverse annotations.\nSpecifically, we first convert the deterministic label space into a learnable\nGaussian distribution, whose variance measures the degree of ambiguity among\ndifferent annotations. Then we regard the learned variance as the estimated\nuncertainty of the predicted edge maps, and pixels with higher uncertainty are\nlikely to be hard samples for edge detection. Therefore we design an adaptive\nweighting loss to emphasize the learning from those pixels with high\nuncertainty, which helps the network to gradually concentrate on the important\npixels. UAED can be combined with various encoder-decoder backbones, and the\nextensive experiments demonstrate that UAED achieves superior performance\nconsistently across multiple edge detection benchmarks. The source code is\navailable at \\url{https://github.com/ZhouCX117/UAED}\n","authors":["Caixia Zhou","Yaping Huang","Mengyang Pu","Qingji Guan","Li Huang","Haibin Ling"],"pdf_url":"https://arxiv.org/pdf/2303.11828v1.pdf","comment":"CVPR2023"},{"id":"http://arxiv.org/abs/2302.08722v3","updated":"2023-03-21T12:59:20Z","published":"2023-02-17T06:33:06Z","title":"GPT4MIA: Utilizing Generative Pre-trained Transformer (GPT-3) as A\n  Plug-and-Play Transductive Model for Medical Image Analysis","summary":"  In this paper, we propose a novel approach (called GPT4MIA) that utilizes\nGenerative Pre-trained Transformer (GPT) as a plug-and-play transductive\ninference tool for medical image analysis (MIA). We provide theoretical\nanalysis on why a large pre-trained language model such as GPT-3 can be used as\na plug-and-play transductive inference model for MIA. At the methodological\nlevel, we develop several technical treatments to improve the efficiency and\neffectiveness of GPT4MIA, including better prompt structure design, sample\nselection, and prompt ordering of representative samples/features. We present\ntwo concrete use cases (with workflow) of GPT4MIA: (1) detecting prediction\nerrors and (2) improving prediction accuracy, working in conjecture with\nwell-established vision-based models for image classification (e.g., ResNet).\nExperiments validate that our proposed method is effective for these two tasks.\nWe further discuss the opportunities and challenges in utilizing\nTransformer-based large language models for broader MIA applications.\n","authors":["Yizhe Zhang","Danny Z. Chen"],"pdf_url":"https://arxiv.org/pdf/2302.08722v3.pdf","comment":"Version 3: Added appendix with more results and visualizations.\n  Questions and suggestions are welcome"},{"id":"http://arxiv.org/abs/2208.06643v3","updated":"2023-03-21T12:54:36Z","published":"2022-08-13T13:13:41Z","title":"Recent Progress in Transformer-based Medical Image Analysis","summary":"  The transformer is primarily used in the field of natural language\nprocessing. Recently, it has been adopted and shows promise in the computer\nvision (CV) field. Medical image analysis (MIA), as a critical branch of CV,\nalso greatly benefits from this state-of-the-art technique. In this review, we\nfirst recap the core component of the transformer, the attention mechanism, and\nthe detailed structures of the transformer. After that, we depict the recent\nprogress of the transformer in the field of MIA. We organize the applications\nin a sequence of different tasks, including classification, segmentation,\ncaptioning, registration, detection, reconstruction, denoising, localization,\nand synthesis. The mainstream classification and segmentation tasks are further\ndivided into eleven medical image modalities. Finally, We discuss the open\nchallenges and future opportunities in this field. This review with the latest\ncontents, detailed information, and task-modality organization mode may greatly\nbenefit the broad MIA community.\n","authors":["Zhaoshan Liu","Qiujie Lv","Ziduo Yang","Yifan Li","Chau Hung Lee","Lei Shen"],"pdf_url":"https://arxiv.org/pdf/2208.06643v3.pdf","comment":"83 pages, 12 figures"},{"id":"http://arxiv.org/abs/2303.11239v2","updated":"2023-03-21T12:43:11Z","published":"2023-03-20T16:24:06Z","title":"Training Invertible Neural Networks as Autoencoders","summary":"  Autoencoders are able to learn useful data representations in an unsupervised\nmatter and have been widely used in various machine learning and computer\nvision tasks. In this work, we present methods to train Invertible Neural\nNetworks (INNs) as (variational) autoencoders which we call INN (variational)\nautoencoders. Our experiments on MNIST, CIFAR and CelebA show that for low\nbottleneck sizes our INN autoencoder achieves results similar to the classical\nautoencoder. However, for large bottleneck sizes our INN autoencoder\noutperforms its classical counterpart. Based on the empirical results, we\nhypothesize that INN autoencoders might not have any intrinsic information loss\nand thereby are not bounded to a maximal number of layers (depth) after which\nonly suboptimal results can be achieved.\n","authors":["The-Gia Leo Nguyen","Lynton Ardizzone","Ullrich Köthe"],"pdf_url":"https://arxiv.org/pdf/2303.11239v2.pdf","comment":"Conference Paper at GCPR2019"},{"id":"http://arxiv.org/abs/2303.11803v1","updated":"2023-03-21T12:36:58Z","published":"2023-03-21T12:36:58Z","title":"Fighting over-fitting with quantization for learning deep neural\n  networks on noisy labels","summary":"  The rising performance of deep neural networks is often empirically\nattributed to an increase in the available computational power, which allows\ncomplex models to be trained upon large amounts of annotated data. However,\nincreased model complexity leads to costly deployment of modern neural\nnetworks, while gathering such amounts of data requires huge costs to avoid\nlabel noise. In this work, we study the ability of compression methods to\ntackle both of these problems at once. We hypothesize that quantization-aware\ntraining, by restricting the expressivity of neural networks, behaves as a\nregularization. Thus, it may help fighting overfitting on noisy data while also\nallowing for the compression of the model at inference. We first validate this\nclaim on a controlled test with manually introduced label noise. Furthermore,\nwe also test the proposed method on Facial Action Unit detection, where labels\nare typically noisy due to the subtlety of the task. In all cases, our results\nsuggests that quantization significantly improve the results compared with\nexisting baselines, regularization as well as other compression methods.\n","authors":["Gauthier Tallec","Edouard Yvinec","Arnaud Dapogny","Kevin Bailly"],"pdf_url":"https://arxiv.org/pdf/2303.11803v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11797v1","updated":"2023-03-21T12:28:21Z","published":"2023-03-21T12:28:21Z","title":"CAT-Seg: Cost Aggregation for Open-Vocabulary Semantic Segmentation","summary":"  Existing works on open-vocabulary semantic segmentation have utilized\nlarge-scale vision-language models, such as CLIP, to leverage their exceptional\nopen-vocabulary recognition capabilities. However, the problem of transferring\nthese capabilities learned from image-level supervision to the pixel-level task\nof segmentation and addressing arbitrary unseen categories at inference makes\nthis task challenging. To address these issues, we aim to attentively relate\nobjects within an image to given categories by leveraging relational\ninformation among class categories and visual semantics through aggregation,\nwhile also adapting the CLIP representations to the pixel-level task. However,\nwe observe that direct optimization of the CLIP embeddings can harm its\nopen-vocabulary capabilities. In this regard, we propose an alternative\napproach to optimize the image-text similarity map, i.e. the cost map, using a\nnovel cost aggregation-based method. Our framework, namely CAT-Seg, achieves\nstate-of-the-art performance across all benchmarks. We provide extensive\nablation studies to validate our choices. Project page:\nhttps://ku-cvlab.github.io/CAT-Seg/.\n","authors":["Seokju Cho","Heeseong Shin","Sunghwan Hong","Seungjun An","Seungjun Lee","Anurag Arnab","Paul Hongsuck Seo","Seungryong Kim"],"pdf_url":"https://arxiv.org/pdf/2303.11797v1.pdf","comment":"Project page: https://ku-cvlab.github.io/CAT-Seg/"},{"id":"http://arxiv.org/abs/2303.11793v1","updated":"2023-03-21T12:22:59Z","published":"2023-03-21T12:22:59Z","title":"OTJR: Optimal Transport Meets Optimal Jacobian Regularization for\n  Adversarial Robustness","summary":"  Deep neural networks are widely recognized as being vulnerable to adversarial\nperturbation. To overcome this challenge, developing a robust classifier is\ncrucial. So far, two well-known defenses have been adopted to improve the\nlearning of robust classifiers, namely adversarial training (AT) and Jacobian\nregularization. However, each approach behaves differently against adversarial\nperturbations. First, our work carefully analyzes and characterizes these two\nschools of approaches, both theoretically and empirically, to demonstrate how\neach approach impacts the robust learning of a classifier. Next, we propose our\nnovel Optimal Transport with Jacobian regularization method, dubbed OTJR,\njointly incorporating the input-output Jacobian regularization into the AT by\nleveraging the optimal transport theory. In particular, we employ the Sliced\nWasserstein (SW) distance that can efficiently push the adversarial samples'\nrepresentations closer to those of clean samples, regardless of the number of\nclasses within the dataset. The SW distance provides the adversarial samples'\nmovement directions, which are much more informative and powerful for the\nJacobian regularization. Our extensive experiments demonstrate the\neffectiveness of our proposed method, which jointly incorporates Jacobian\nregularization into AT. Furthermore, we demonstrate that our proposed method\nconsistently enhances the model's robustness with CIFAR-100 dataset under\nvarious adversarial attack settings, achieving up to 28.49% under AutoAttack.\n","authors":["Binh M. Le","Shahroz Tariq","Simon S. Woo"],"pdf_url":"https://arxiv.org/pdf/2303.11793v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11791v1","updated":"2023-03-21T12:18:57Z","published":"2023-03-21T12:18:57Z","title":"Propagate And Calibrate: Real-time Passive Non-line-of-sight Tracking","summary":"  Non-line-of-sight (NLOS) tracking has drawn increasing attention in recent\nyears, due to its ability to detect object motion out of sight. Most previous\nworks on NLOS tracking rely on active illumination, e.g., laser, and suffer\nfrom high cost and elaborate experimental conditions. Besides, these techniques\nare still far from practical application due to oversimplified settings. In\ncontrast, we propose a purely passive method to track a person walking in an\ninvisible room by only observing a relay wall, which is more in line with real\napplication scenarios, e.g., security. To excavate imperceptible changes in\nvideos of the relay wall, we introduce difference frames as an essential\ncarrier of temporal-local motion messages. In addition, we propose PAC-Net,\nwhich consists of alternating propagation and calibration, making it capable of\nleveraging both dynamic and static messages on a frame-level granularity. To\nevaluate the proposed method, we build and publish the first dynamic passive\nNLOS tracking dataset, NLOS-Track, which fills the vacuum of realistic NLOS\ndatasets. NLOS-Track contains thousands of NLOS video clips and corresponding\ntrajectories. Both real-shot and synthetic data are included.\n","authors":["Yihao Wang","Zhigang Wang","Bin Zhao","Dong Wang","Mulin Chen","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2303.11791v1.pdf","comment":"CVPR 2023 camera-ready version. Codes and dataset are available at\n  https://againstentropy.github.io/NLOS-Track/"},{"id":"http://arxiv.org/abs/2303.11790v1","updated":"2023-03-21T12:17:21Z","published":"2023-03-21T12:17:21Z","title":"Probabilistic Domain Adaptation for Biomedical Image Segmentation","summary":"  Segmentation is a key analysis tasks in biomedical imaging. Given the many\ndifferent experimental settings in this field, the lack of generalization\nlimits the use of deep learning in practice. Domain adaptation is a promising\nremedy: it trains a model for a given task on a source dataset with labels and\nadapts it to a target dataset without additional labels. We introduce a\nprobabilistic domain adaptation method, building on self-training approaches\nand the Probabilistic UNet. We use the latter to sample multiple segmentation\nhypothesis to implement better pseudo-label filtering. We further study joint\nand separate source-target training strategies and evaluate our method on three\nchallenging domain adaptation tasks for biomedical segmentation.\n","authors":["Anwai Archit","Constantin Pape"],"pdf_url":"https://arxiv.org/pdf/2303.11790v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.09699v2","updated":"2023-03-21T12:10:18Z","published":"2022-11-15T19:07:53Z","title":"PromptCap: Prompt-Guided Image Captioning for VQA with GPT-3","summary":"  Knowledge-based visual question answering (VQA) involves questions that\nrequire world knowledge beyond the image to yield the correct answer. Large\nlanguage models (LMs) like GPT-3 are particularly helpful for this task because\nof their strong knowledge retrieval and reasoning capabilities. To enable LM to\nunderstand images, prior work uses a captioning model to convert images into\ntext. However, when summarizing an image in a single caption sentence, which\nvisual entities to describe are often underspecified. Generic image captions\noften miss visual details essential for the LM to answer visual questions\ncorrectly. To address this challenge, we propose PromptCap (Prompt-guided image\nCaptioning), a captioning model designed to serve as a better connector between\nimages and black-box LMs. Different from generic captions, PromptCap takes a\nnatural-language prompt to control the visual entities to describe in the\ngenerated caption. The prompt contains a question that the caption should aid\nin answering. To avoid extra annotation, PromptCap is trained by examples\nsynthesized with GPT-3 and existing datasets. We demonstrate PromptCap's\neffectiveness on an existing pipeline in which GPT-3 is prompted with image\ncaptions to carry out VQA. PromptCap outperforms generic captions by a large\nmargin and achieves state-of-the-art accuracy on knowledge-based VQA tasks\n(60.4% on OK-VQA and 59.6% on A-OKVQA). Zero-shot results on WebQA show that\nPromptCap generalizes well to unseen domains.\n","authors":["Yushi Hu","Hang Hua","Zhengyuan Yang","Weijia Shi","Noah A. Smith","Jiebo Luo"],"pdf_url":"https://arxiv.org/pdf/2211.09699v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.02625v6","updated":"2023-03-21T11:58:36Z","published":"2022-07-06T12:34:33Z","title":"$L_2$BN: Enhancing Batch Normalization by Equalizing the $L_2$ Norms of\n  Features","summary":"  In this paper, we analyze batch normalization from the perspective of\ndiscriminability and find the disadvantages ignored by previous studies: the\ndifference in $l_2$ norms of sample features can hinder batch normalization\nfrom obtaining more distinguished inter-class features and more compact\nintra-class features. To address this issue, we propose a simple yet effective\nmethod to equalize the $l_2$ norms of sample features. Concretely, we\n$l_2$-normalize each sample feature before feeding them into batch\nnormalization, and therefore the features are of the same magnitude. Since the\nproposed method combines the $l_2$ normalization and batch normalization, we\nname our method $L_2$BN. The $L_2$BN can strengthen the compactness of\nintra-class features and enlarge the discrepancy of inter-class features. The\n$L_2$BN is easy to implement and can exert its effect without any additional\nparameters or hyper-parameters. We evaluate the effectiveness of $L_2$BN\nthrough extensive experiments with various models on image classification and\nacoustic scene classification tasks. The results demonstrate that the $L_2$BN\ncan boost the generalization ability of various neural network models and\nachieve considerable performance improvements.\n","authors":["Zhennan Wang","Kehan Li","Runyi Yu","Yian Zhao","Pengchong Qiao","Chang Liu","Fan Xu","Xiangyang Ji","Guoli Song","Jie Chen"],"pdf_url":"https://arxiv.org/pdf/2207.02625v6.pdf","comment":"12 pages, 8 figures"},{"id":"http://arxiv.org/abs/2303.05933v3","updated":"2023-03-21T11:52:47Z","published":"2023-03-10T14:11:09Z","title":"Self-Paced Learning for Open-Set Domain Adaptation","summary":"  Domain adaptation tackles the challenge of generalizing knowledge acquired\nfrom a source domain to a target domain with different data distributions.\nTraditional domain adaptation methods presume that the classes in the source\nand target domains are identical, which is not always the case in real-world\nscenarios. Open-set domain adaptation (OSDA) addresses this limitation by\nallowing previously unseen classes in the target domain. Open-set domain\nadaptation aims to not only recognize target samples belonging to common\nclasses shared by source and target domains but also perceive unknown class\nsamples. We propose a novel framework based on self-paced learning to\ndistinguish common and unknown class samples precisely, referred to as SPLOS\n(self-paced learning for open-set). To utilize unlabeled target samples for\nself-paced learning, we generate pseudo labels and design a cross-domain mixup\nmethod tailored for OSDA scenarios. This strategy minimizes the noise from\npseudo labels and ensures our model progressively learns common class features\nof the target domain, beginning with simpler examples and advancing to more\ncomplex ones. Furthermore, unlike existing OSDA methods that require manual\nhyperparameter $threshold$ tuning to separate common and unknown classes, our\napproach self-tunes a suitable threshold, eliminating the need for empirical\ntuning during testing. Comprehensive experiments illustrate that our method\nconsistently achieves superior performance on different benchmarks compared\nwith various state-of-the-art methods.\n","authors":["Xinghong Liu","Yi Zhou","Tao Zhou","Jie Qin","Shengcai Liao"],"pdf_url":"https://arxiv.org/pdf/2303.05933v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11771v1","updated":"2023-03-21T11:42:57Z","published":"2023-03-21T11:42:57Z","title":"Self-Sufficient Framework for Continuous Sign Language Recognition","summary":"  The goal of this work is to develop self-sufficient framework for Continuous\nSign Language Recognition (CSLR) that addresses key issues of sign language\nrecognition. These include the need for complex multi-scale features such as\nhands, face, and mouth for understanding, and absence of frame-level\nannotations. To this end, we propose (1) Divide and Focus Convolution (DFConv)\nwhich extracts both manual and non-manual features without the need for\nadditional networks or annotations, and (2) Dense Pseudo-Label Refinement\n(DPLR) which propagates non-spiky frame-level pseudo-labels by combining the\nground truth gloss sequence labels with the predicted sequence. We demonstrate\nthat our model achieves state-of-the-art performance among RGB-based methods on\nlarge-scale CSLR benchmarks, PHOENIX-2014 and PHOENIX-2014-T, while showing\ncomparable results with better efficiency when compared to other approaches\nthat use multi-modality or extra annotations.\n","authors":["Youngjoon Jang","Youngtaek Oh","Jae Won Cho","Myungchul Kim","Dong-Jin Kim","In So Kweon","Joon Son Chung"],"pdf_url":"https://arxiv.org/pdf/2303.11771v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.07389v2","updated":"2023-03-21T11:40:47Z","published":"2023-01-18T09:36:41Z","title":"Towards Models that Can See and Read","summary":"  Visual Question Answering (VQA) and Image Captioning (CAP), which are among\nthe most popular vision-language tasks, have analogous scene-text versions that\nrequire reasoning from the text in the image. Despite their obvious\nresemblance, the two are treated independently and, as we show, yield\ntask-specific methods that can either see or read, but not both. In this work,\nwe conduct an in-depth analysis of this phenomenon and propose UniTNT, a\nUnified Text-Non-Text approach, which grants existing multimodal architectures\nscene-text understanding capabilities. Specifically, we treat scene-text\ninformation as an additional modality, fusing it with any pretrained\nencoder-decoder-based architecture via designated modules. Thorough experiments\nreveal that UniTNT leads to the first single model that successfully handles\nboth task types. Moreover, we show that scene-text understanding capabilities\ncan boost vision-language models' performance on general VQA and CAP by up to\n2.69% and 0.6 CIDEr, respectively.\n","authors":["Roy Ganz","Oren Nuriel","Aviad Aberdam","Yair Kittenplon","Shai Mazor","Ron Litman"],"pdf_url":"https://arxiv.org/pdf/2301.07389v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11759v1","updated":"2023-03-21T11:23:59Z","published":"2023-03-21T11:23:59Z","title":"Simulating Malaria Detection in Laboratories using Deep Learning","summary":"  Malaria is usually diagnosed by a microbiologist by examining a small sample\nof blood smear. Reducing mortality from malaria infection is possible if it is\ndiagnosed early and followed with appropriate treatment. While the WHO has set\naudacious goals of reducing malaria incidence and mortality rates by 90% in\n2030 and eliminating malaria in 35 countries by that time, it still remains a\ndifficult challenge. Computer-assisted diagnostics are on the rise these days\nas they can be used effectively as a primary test in the absence of or\nproviding assistance to a physician or pathologist. The purpose of this paper\nis to describe an approach to detecting, localizing and counting parasitic\ncells in blood sample images towards easing the burden on healthcare workers.\n","authors":["Onyekachukwu R. Okonji"],"pdf_url":"https://arxiv.org/pdf/2303.11759v1.pdf","comment":"16 pages, 8 figures"},{"id":"http://arxiv.org/abs/2303.11755v1","updated":"2023-03-21T11:20:34Z","published":"2023-03-21T11:20:34Z","title":"LIMITR: Leveraging Local Information for Medical Image-Text\n  Representation","summary":"  Medical imaging analysis plays a critical role in the diagnosis and treatment\nof various medical conditions. This paper focuses on chest X-ray images and\ntheir corresponding radiological reports. It presents a new model that learns a\njoint X-ray image & report representation. The model is based on a novel\nalignment scheme between the visual data and the text, which takes into account\nboth local and global information. Furthermore, the model integrates\ndomain-specific information of two types -- lateral images and the consistent\nvisual structure of chest images. Our representation is shown to benefit three\ntypes of retrieval tasks: text-image retrieval, class-based retrieval, and\nphrase-grounding.\n","authors":["Gefen Dawidowicz","Elad Hirsch","Ayellet Tal"],"pdf_url":"https://arxiv.org/pdf/2303.11755v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.15377v3","updated":"2023-03-21T11:19:03Z","published":"2022-11-23T09:57:17Z","title":"Whose Emotion Matters? Speaking Activity Localisation without Prior\n  Knowledge","summary":"  The task of emotion recognition in conversations (ERC) benefits from the\navailability of multiple modalities, as provided, for example, in the\nvideo-based Multimodal EmotionLines Dataset (MELD). However, only a few\nresearch approaches use both acoustic and visual information from the MELD\nvideos. There are two reasons for this: First, label-to-video alignments in\nMELD are noisy, making those videos an unreliable source of emotional speech\ndata. Second, conversations can involve several people in the same scene, which\nrequires the localisation of the utterance source. In this paper, we introduce\nMELD with Fixed Audiovisual Information via Realignment (MELD-FAIR) by using\nrecent active speaker detection and automatic speech recognition models, we are\nable to realign the videos of MELD and capture the facial expressions from\nspeakers in 96.92% of the utterances provided in MELD. Experiments with a\nself-supervised voice recognition model indicate that the realigned MELD-FAIR\nvideos more closely match the transcribed utterances given in the MELD dataset.\nFinally, we devise a model for emotion recognition in conversations trained on\nthe realigned MELD-FAIR videos, which outperforms state-of-the-art models for\nERC based on vision alone. This indicates that localising the source of\nspeaking activities is indeed effective for extracting facial expressions from\nthe uttering speakers and that faces provide more informative visual cues than\nthe visual features state-of-the-art models have been using so far. The\nMELD-FAIR realignment data, and the code of the realignment procedure and of\nthe emotional recognition, are available at\nhttps://github.com/knowledgetechnologyuhh/MELD-FAIR.\n","authors":["Hugo Carneiro","Cornelius Weber","Stefan Wermter"],"pdf_url":"https://arxiv.org/pdf/2211.15377v3.pdf","comment":"17 pages, 8 figures, 7 tables"},{"id":"http://arxiv.org/abs/2303.11749v1","updated":"2023-03-21T11:15:03Z","published":"2023-03-21T11:15:03Z","title":"Detecting Everything in the Open World: Towards Universal Object\n  Detection","summary":"  In this paper, we formally address universal object detection, which aims to\ndetect every scene and predict every category. The dependence on human\nannotations, the limited visual information, and the novel categories in the\nopen world severely restrict the universality of traditional detectors. We\npropose \\textbf{UniDetector}, a universal object detector that has the ability\nto recognize enormous categories in the open world. The critical points for the\nuniversality of UniDetector are: 1) it leverages images of multiple sources and\nheterogeneous label spaces for training through the alignment of image and text\nspaces, which guarantees sufficient information for universal representations.\n2) it generalizes to the open world easily while keeping the balance between\nseen and unseen classes, thanks to abundant information from both vision and\nlanguage modalities. 3) it further promotes the generalization ability to novel\ncategories through our proposed decoupling training manner and probability\ncalibration. These contributions allow UniDetector to detect over 7k\ncategories, the largest measurable category size so far, with only about 500\nclasses participating in training. Our UniDetector behaves the strong zero-shot\ngeneralization ability on large-vocabulary datasets like LVIS, ImageNetBoxes,\nand VisualGenome - it surpasses the traditional supervised baselines by more\nthan 4\\% on average without seeing any corresponding images. On 13 public\ndetection datasets with various scenes, UniDetector also achieves\nstate-of-the-art performance with only a 3\\% amount of training data.\n","authors":["Zhenyu Wang","Yali Li","Xi Chen","Ser-Nam Lim","Antonio Torralba","Hengshuang Zhao","Shengjin Wang"],"pdf_url":"https://arxiv.org/pdf/2303.11749v1.pdf","comment":"Accepted by CVPR2023"},{"id":"http://arxiv.org/abs/2303.05312v2","updated":"2023-03-21T11:01:23Z","published":"2023-03-09T15:00:12Z","title":"3D Video Loops from Asynchronous Input","summary":"  Looping videos are short video clips that can be looped endlessly without\nvisible seams or artifacts. They provide a very attractive way to capture the\ndynamism of natural scenes. Existing methods have been mostly limited to 2D\nrepresentations. In this paper, we take a step forward and propose a practical\nsolution that enables an immersive experience on dynamic 3D looping scenes. The\nkey challenge is to consider the per-view looping conditions from asynchronous\ninput while maintaining view consistency for the 3D representation. We propose\na novel sparse 3D video representation, namely Multi-Tile Video (MTV), which\nnot only provides a view-consistent prior, but also greatly reduces memory\nusage, making the optimization of a 4D volume tractable. Then, we introduce a\ntwo-stage pipeline to construct the 3D looping MTV from completely asynchronous\nmulti-view videos with no time overlap. A novel looping loss based on video\ntemporal retargeting algorithms is adopted during the optimization to loop the\n3D scene. Experiments of our framework have shown promise in successfully\ngenerating and rendering photorealistic 3D looping videos in real time even on\nmobile devices. The code, dataset, and live demos are available in\nhttps://limacv.github.io/VideoLoop3D_web/.\n","authors":["Li Ma","Xiaoyu Li","Jing Liao","Pedro V. Sander"],"pdf_url":"https://arxiv.org/pdf/2303.05312v2.pdf","comment":"For more information, please visit the homepage at\n  https://limacv.github.io/VideoLoop3D_web/"},{"id":"http://arxiv.org/abs/2302.14115v2","updated":"2023-03-21T11:01:09Z","published":"2023-02-27T19:53:49Z","title":"Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense\n  Video Captioning","summary":"  In this work, we introduce Vid2Seq, a multi-modal single-stage dense event\ncaptioning model pretrained on narrated videos which are readily-available at\nscale. The Vid2Seq architecture augments a language model with special time\ntokens, allowing it to seamlessly predict event boundaries and textual\ndescriptions in the same output sequence. Such a unified model requires\nlarge-scale training data, which is not available in current annotated\ndatasets. We show that it is possible to leverage unlabeled narrated videos for\ndense video captioning, by reformulating sentence boundaries of transcribed\nspeech as pseudo event boundaries, and using the transcribed speech sentences\nas pseudo event captions. The resulting Vid2Seq model pretrained on the\nYT-Temporal-1B dataset improves the state of the art on a variety of dense\nvideo captioning benchmarks including YouCook2, ViTT and ActivityNet Captions.\nVid2Seq also generalizes well to the tasks of video paragraph captioning and\nvideo clip captioning, and to few-shot settings. Our code is publicly available\nat https://antoyang.github.io/vid2seq.html.\n","authors":["Antoine Yang","Arsha Nagrani","Paul Hongsuck Seo","Antoine Miech","Jordi Pont-Tuset","Ivan Laptev","Josef Sivic","Cordelia Schmid"],"pdf_url":"https://arxiv.org/pdf/2302.14115v2.pdf","comment":"CVPR 2023 Camera-Ready; Project Webpage:\n  https://antoyang.github.io/vid2seq.html ; 18 pages; 6 figures"},{"id":"http://arxiv.org/abs/2303.11739v1","updated":"2023-03-21T10:56:57Z","published":"2023-03-21T10:56:57Z","title":"Data-efficient Large Scale Place Recognition with Graded Similarity\n  Supervision","summary":"  Visual place recognition (VPR) is a fundamental task of computer vision for\nvisual localization. Existing methods are trained using image pairs that either\ndepict the same place or not. Such a binary indication does not consider\ncontinuous relations of similarity between images of the same place taken from\ndifferent positions, determined by the continuous nature of camera pose. The\nbinary similarity induces a noisy supervision signal into the training of VPR\nmethods, which stall in local minima and require expensive hard mining\nalgorithms to guarantee convergence. Motivated by the fact that two images of\nthe same place only partially share visual cues due to camera pose differences,\nwe deploy an automatic re-annotation strategy to re-label VPR datasets. We\ncompute graded similarity labels for image pairs based on available\nlocalization metadata. Furthermore, we propose a new Generalized Contrastive\nLoss (GCL) that uses graded similarity labels for training contrastive\nnetworks. We demonstrate that the use of the new labels and GCL allow to\ndispense from hard-pair mining, and to train image descriptors that perform\nbetter in VPR by nearest neighbor search, obtaining superior or comparable\nresults than methods that require expensive hard-pair mining and re-ranking\ntechniques. Code and models available at:\nhttps://github.com/marialeyvallina/generalized_contrastive_loss\n","authors":["Maria Leyva-Vallina","Nicola Strisciuglio","Nicolai Petkov"],"pdf_url":"https://arxiv.org/pdf/2303.11739v1.pdf","comment":"Accepted at CVPR 2023"},{"id":"http://arxiv.org/abs/2303.11732v1","updated":"2023-03-21T10:40:13Z","published":"2023-03-21T10:40:13Z","title":"Multi-modal Prompting for Low-Shot Temporal Action Localization","summary":"  In this paper, we consider the problem of temporal action localization under\nlow-shot (zero-shot & few-shot) scenario, with the goal of detecting and\nclassifying the action instances from arbitrary categories within some\nuntrimmed videos, even not seen at training time. We adopt a Transformer-based\ntwo-stage action localization architecture with class-agnostic action proposal,\nfollowed by open-vocabulary classification. We make the following\ncontributions. First, to compensate image-text foundation models with temporal\nmotions, we improve category-agnostic action proposal by explicitly aligning\nembeddings of optical flows, RGB and texts, which has largely been ignored in\nexisting low-shot methods. Second, to improve open-vocabulary action\nclassification, we construct classifiers with strong discriminative power,\ni.e., avoid lexical ambiguities. To be specific, we propose to prompt the\npre-trained CLIP text encoder either with detailed action descriptions\n(acquired from large-scale language models), or visually-conditioned\ninstance-specific prompt vectors. Third, we conduct thorough experiments and\nablation studies on THUMOS14 and ActivityNet1.3, demonstrating the superior\nperformance of our proposed model, outperforming existing state-of-the-art\napproaches by one significant margin.\n","authors":["Chen Ju","Zeqian Li","Peisen Zhao","Ya Zhang","Xiaopeng Zhang","Qi Tian","Yanfeng Wang","Weidi Xie"],"pdf_url":"https://arxiv.org/pdf/2303.11732v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11730v1","updated":"2023-03-21T10:34:39Z","published":"2023-03-21T10:34:39Z","title":"Abstract Visual Reasoning: An Algebraic Approach for Solving Raven's\n  Progressive Matrices","summary":"  We introduce algebraic machine reasoning, a new reasoning framework that is\nwell-suited for abstract reasoning. Effectively, algebraic machine reasoning\nreduces the difficult process of novel problem-solving to routine algebraic\ncomputation. The fundamental algebraic objects of interest are the ideals of\nsome suitably initialized polynomial ring. We shall explain how solving Raven's\nProgressive Matrices (RPMs) can be realized as computational problems in\nalgebra, which combine various well-known algebraic subroutines that include:\nComputing the Gr\\\"obner basis of an ideal, checking for ideal containment, etc.\nCrucially, the additional algebraic structure satisfied by ideals allows for\nmore operations on ideals beyond set-theoretic operations.\n  Our algebraic machine reasoning framework is not only able to select the\ncorrect answer from a given answer set, but also able to generate the correct\nanswer with only the question matrix given. Experiments on the I-RAVEN dataset\nyield an overall $93.2\\%$ accuracy, which significantly outperforms the current\nstate-of-the-art accuracy of $77.0\\%$ and exceeds human performance at $84.4\\%$\naccuracy.\n","authors":["Jingyi Xu","Tushar Vaidya","Yufei Wu","Saket Chandra","Zhangsheng Lai","Kai Fong Ernest Chong"],"pdf_url":"https://arxiv.org/pdf/2303.11730v1.pdf","comment":"Accepted at IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2023. 30 pages, 7 figures (including supplementary\n  material). First three authors contributed equally. Code is available at:\n  https://github.com/Xu-Jingyi/AlgebraicMR"},{"id":"http://arxiv.org/abs/2303.11728v1","updated":"2023-03-21T10:32:27Z","published":"2023-03-21T10:32:27Z","title":"ExtremeNeRF: Few-shot Neural Radiance Fields Under Unconstrained\n  Illumination","summary":"  In this paper, we propose a new challenge that synthesizes a novel view in a\nmore practical environment, where the number of input multi-view images is\nlimited and illumination variations are significant. Despite recent success,\nneural radiance fields (NeRF) require a massive amount of input multi-view\nimages taken under constrained illuminations. To address the problem, we\nsuggest ExtremeNeRF, which utilizes occlusion-aware multiview albedo\nconsistency, supported by geometric alignment and depth consistency. We extract\nintrinsic image components that should be illumination-invariant across\ndifferent views, enabling direct appearance comparison between the input and\nnovel view under unconstrained illumination. We provide extensive experimental\nresults for an evaluation of the task, using the newly built NeRF Extreme\nbenchmark, which is the first in-the-wild novel view synthesis benchmark taken\nunder multiple viewing directions and varying illuminations. The project page\nis at https://seokyeong94.github.io/ExtremeNeRF/\n","authors":["SeokYeong Lee","JunYong Choi","Seungryong Kim","Ig-Jae Kim","Junghyun Cho"],"pdf_url":"https://arxiv.org/pdf/2303.11728v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2111.12273v5","updated":"2023-03-21T10:31:46Z","published":"2021-11-24T05:16:41Z","title":"Sharpness-aware Quantization for Deep Neural Networks","summary":"  Network quantization is a dominant paradigm of model compression. However,\nthe abrupt changes in quantized weights during training often lead to severe\nloss fluctuations and result in a sharp loss landscape, making the gradients\nunstable and thus degrading the performance. Recently, Sharpness-Aware\nMinimization (SAM) has been proposed to smooth the loss landscape and improve\nthe generalization performance of the models. Nevertheless, directly applying\nSAM to the quantized models can lead to perturbation mismatch or diminishment\nissues, resulting in suboptimal performance. In this paper, we propose a novel\nmethod, dubbed Sharpness-Aware Quantization (SAQ), to explore the effect of SAM\nin model compression, particularly quantization for the first time.\nSpecifically, we first provide a unified view of quantization and SAM by\ntreating them as introducing quantization noises and adversarial perturbations\nto the model weights, respectively. According to whether the noise and\nperturbation terms depend on each other, SAQ can be formulated into three\ncases, which are analyzed and compared comprehensively. Furthermore, by\nintroducing an efficient training strategy, SAQ only incurs a little additional\ntraining overhead compared with the default optimizer (e.g., SGD or AdamW).\nExtensive experiments on both convolutional neural networks and Transformers\nacross various datasets (i.e., ImageNet, CIFAR-10/100, Oxford Flowers-102,\nOxford-IIIT Pets) show that SAQ improves the generalization performance of the\nquantized models, yielding the SOTA results in uniform quantization. For\nexample, on ImageNet, SAQ outperforms AdamW by 1.2% on the Top-1 accuracy for\n4-bit ViT-B/16. Our 4-bit ResNet-50 surpasses the previous SOTA method by 0.9%\non the Top-1 accuracy.\n","authors":["Jing Liu","Jianfei Cai","Bohan Zhuang"],"pdf_url":"https://arxiv.org/pdf/2111.12273v5.pdf","comment":"Tech report"},{"id":"http://arxiv.org/abs/2303.11726v1","updated":"2023-03-21T10:30:43Z","published":"2023-03-21T10:30:43Z","title":"3D Human Mesh Estimation from Virtual Markers","summary":"  Inspired by the success of volumetric 3D pose estimation, some recent human\nmesh estimators propose to estimate 3D skeletons as intermediate\nrepresentations, from which, the dense 3D meshes are regressed by exploiting\nthe mesh topology. However, body shape information is lost in extracting\nskeletons, leading to mediocre performance. The advanced motion capture systems\nsolve the problem by placing dense physical markers on the body surface, which\nallows to extract realistic meshes from their non-rigid motions. However, they\ncannot be applied to wild images without markers. In this work, we present an\nintermediate representation, named virtual markers, which learns 64 landmark\nkeypoints on the body surface based on the large-scale mocap data in a\ngenerative style, mimicking the effects of physical markers. The virtual\nmarkers can be accurately detected from wild images and can reconstruct the\nintact meshes with realistic shapes by simple interpolation. Our approach\noutperforms the state-of-the-art methods on three datasets. In particular, it\nsurpasses the existing methods by a notable margin on the SURREAL dataset,\nwhich has diverse body shapes. Code is available at\nhttps://github.com/ShirleyMaxx/VirtualMarker.\n","authors":["Xiaoxuan Ma","Jiajun Su","Chunyu Wang","Wentao Zhu","Yizhou Wang"],"pdf_url":"https://arxiv.org/pdf/2303.11726v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.11724v1","updated":"2023-03-21T10:29:30Z","published":"2023-03-21T10:29:30Z","title":"Task-based Generation of Optimized Projection Sets using Differentiable\n  Ranking","summary":"  We present a method for selecting valuable projections in computed tomography\n(CT) scans to enhance image reconstruction and diagnosis. The approach\nintegrates two important factors, projection-based detectability and data\ncompleteness, into a single feed-forward neural network. The network evaluates\nthe value of projections, processes them through a differentiable ranking\nfunction and makes the final selection using a straight-through estimator. Data\ncompleteness is ensured through the label provided during training. The\napproach eliminates the need for heuristically enforcing data completeness,\nwhich may exclude valuable projections. The method is evaluated on simulated\ndata in a non-destructive testing scenario, where the aim is to maximize the\nreconstruction quality within a specified region of interest. We achieve\ncomparable results to previous methods, laying the foundation for using\nreconstruction-based loss functions to learn the selection of projections.\n","authors":["Linda-Sophie Schneider","Mareike Thies","Christopher Syben","Richard Schielein","Mathias Unberath","Andreas Maier"],"pdf_url":"https://arxiv.org/pdf/2303.11724v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11722v1","updated":"2023-03-21T10:24:29Z","published":"2023-03-21T10:24:29Z","title":"Implicit Neural Representation for Cooperative Low-light Image\n  Enhancement","summary":"  The following three factors restrict the application of existing low-light\nimage enhancement methods: unpredictable brightness degradation and noise,\ninherent gap between metric-favorable and visual-friendly versions, and the\nlimited paired training data. To address these limitations, we propose an\nimplicit Neural Representation method for Cooperative low-light image\nenhancement, dubbed NeRCo. It robustly recovers perceptual-friendly results in\nan unsupervised manner. Concretely, NeRCo unifies the diverse degradation\nfactors of real-world scenes with a controllable fitting function, leading to\nbetter robustness. In addition, for the output results, we introduce\nsemantic-orientated supervision with priors from the pre-trained\nvision-language model. Instead of merely following reference images, it\nencourages results to meet subjective expectations, finding more\nvisual-friendly solutions. Further, to ease the reliance on paired data and\nreduce solution space, we develop a dual-closed-loop constrained enhancement\nmodule. It is trained cooperatively with other affiliated modules in a\nself-supervised manner. Finally, extensive experiments demonstrate the\nrobustness and superior effectiveness of our proposed NeRCo. Our code is\navailable at https://github.com/Ysz2022/NeRCo.\n","authors":["Shuzhou Yang","Moxuan Ding","Yanmin Wu","Zihan Li","Jian Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.11722v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.09069v2","updated":"2023-03-21T10:23:40Z","published":"2022-12-18T11:43:32Z","title":"Masked Wavelet Representation for Compact Neural Radiance Fields","summary":"  Neural radiance fields (NeRF) have demonstrated the potential of\ncoordinate-based neural representation (neural fields or implicit neural\nrepresentation) in neural rendering. However, using a multi-layer perceptron\n(MLP) to represent a 3D scene or object requires enormous computational\nresources and time. There have been recent studies on how to reduce these\ncomputational inefficiencies by using additional data structures, such as grids\nor trees. Despite the promising performance, the explicit data structure\nnecessitates a substantial amount of memory. In this work, we present a method\nto reduce the size without compromising the advantages of having additional\ndata structures. In detail, we propose using the wavelet transform on\ngrid-based neural fields. Grid-based neural fields are for fast convergence,\nand the wavelet transform, whose efficiency has been demonstrated in\nhigh-performance standard codecs, is to improve the parameter efficiency of\ngrids. Furthermore, in order to achieve a higher sparsity of grid coefficients\nwhile maintaining reconstruction quality, we present a novel trainable masking\napproach. Experimental results demonstrate that non-spatial grid coefficients,\nsuch as wavelet coefficients, are capable of attaining a higher level of\nsparsity than spatial grid coefficients, resulting in a more compact\nrepresentation. With our proposed mask and compression pipeline, we achieved\nstate-of-the-art performance within a memory budget of 2 MB. Our code is\navailable at https://github.com/daniel03c1/masked_wavelet_nerf.\n","authors":["Daniel Rho","Byeonghyeon Lee","Seungtae Nam","Joo Chan Lee","Jong Hwan Ko","Eunbyung Park"],"pdf_url":"https://arxiv.org/pdf/2212.09069v2.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2303.11720v1","updated":"2023-03-21T10:14:11Z","published":"2023-03-21T10:14:11Z","title":"Lidar Line Selection with Spatially-Aware Shapley Value for\n  Cost-Efficient Depth Completion","summary":"  Lidar is a vital sensor for estimating the depth of a scene. Typical spinning\nlidars emit pulses arranged in several horizontal lines and the monetary cost\nof the sensor increases with the number of these lines. In this work, we\npresent the new problem of optimizing the positioning of lidar lines to find\nthe most effective configuration for the depth completion task. We propose a\nsolution to reduce the number of lines while retaining the up-to-the-mark\nquality of depth completion. Our method consists of two components, (1) line\nselection based on the marginal contribution of a line computed via the Shapley\nvalue and (2) incorporating line position spread to take into account its need\nto arrive at image-wide depth completion. Spatially-aware Shapley values (SaS)\nsucceed in selecting line subsets that yield a depth accuracy comparable to the\nfull lidar input while using just half of the lines.\n","authors":["Kamil Adamczewski","Christos Sakaridis","Vaishakh Patil","Luc Van Gool"],"pdf_url":"https://arxiv.org/pdf/2303.11720v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09730v2","updated":"2023-03-21T10:11:01Z","published":"2023-03-17T02:19:28Z","title":"ElasticViT: Conflict-aware Supernet Training for Deploying Fast Vision\n  Transformer on Diverse Mobile Devices","summary":"  Neural Architecture Search (NAS) has shown promising performance in the\nautomatic design of vision transformers (ViT) exceeding 1G FLOPs. However,\ndesigning lightweight and low-latency ViT models for diverse mobile devices\nremains a big challenge. In this work, we propose ElasticViT, a two-stage NAS\napproach that trains a high-quality ViT supernet over a very large search space\nthat supports a wide range of mobile devices, and then searches an optimal\nsub-network (subnet) for direct deployment. However, prior supernet training\nmethods that rely on uniform sampling suffer from the gradient conflict issue:\nthe sampled subnets can have vastly different model sizes (e.g., 50M vs. 2G\nFLOPs), leading to different optimization directions and inferior performance.\nTo address this challenge, we propose two novel sampling techniques:\ncomplexity-aware sampling and performance-aware sampling. Complexity-aware\nsampling limits the FLOPs difference among the subnets sampled across adjacent\ntraining steps, while covering different-sized subnets in the search space.\nPerformance-aware sampling further selects subnets that have good accuracy,\nwhich can reduce gradient conflicts and improve supernet quality. Our\ndiscovered models, ElasticViT models, achieve top-1 accuracy from 67.2% to\n80.0% on ImageNet from 60M to 800M FLOPs without extra retraining,\noutperforming all prior CNNs and ViTs in terms of accuracy and latency. Our\ntiny and small models are also the first ViT models that surpass\nstate-of-the-art CNNs with significantly lower latency on mobile devices. For\ninstance, ElasticViT-S1 runs 2.62x faster than EfficientNet-B0 with 0.1% higher\naccuracy.\n","authors":["Chen Tang","Li Lyna Zhang","Huiqiang Jiang","Jiahang Xu","Ting Cao","Quanlu Zhang","Yuqing Yang","Zhi Wang","Mao Yang"],"pdf_url":"https://arxiv.org/pdf/2303.09730v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11717v1","updated":"2023-03-21T10:09:47Z","published":"2023-03-21T10:09:47Z","title":"A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to\n  GPT-5 All You Need?","summary":"  As ChatGPT goes viral, generative AI (AIGC, a.k.a AI-generated content) has\nmade headlines everywhere because of its ability to analyze and create text,\nimages, and beyond. With such overwhelming media coverage, it is almost\nimpossible for us to miss the opportunity to glimpse AIGC from a certain angle.\nIn the era of AI transitioning from pure analysis to creation, it is worth\nnoting that ChatGPT, with its most recent language model GPT-4, is just a tool\nout of numerous AIGC tasks. Impressed by the capability of the ChatGPT, many\npeople are wondering about its limits: can GPT-5 (or other future GPT variants)\nhelp ChatGPT unify all AIGC tasks for diversified content creation? Toward\nanswering this question, a comprehensive review of existing AIGC tasks is\nneeded. As such, our work comes to fill this gap promptly by offering a first\nlook at AIGC, ranging from its techniques to applications. Modern generative AI\nrelies on various technical foundations, ranging from model architecture and\nself-supervised pretraining to generative modeling methods (like GAN and\ndiffusion models). After introducing the fundamental techniques, this work\nfocuses on the technological development of various AIGC tasks based on their\noutput type, including text, images, videos, 3D content, etc., which depicts\nthe full potential of ChatGPT's future. Moreover, we summarize their\nsignificant applications in some mainstream industries, such as education and\ncreativity content. Finally, we discuss the challenges currently faced and\npresent an outlook on how generative AI might evolve in the near future.\n","authors":["Chaoning Zhang","Chenshuang Zhang","Sheng Zheng","Yu Qiao","Chenghao Li","Mengchun Zhang","Sumit Kumar Dam","Chu Myaet Thwal","Ye Lin Tun","Le Luang Huy","Donguk kim","Sung-Ho Bae","Lik-Hang Lee","Yang Yang","Heng Tao Shen","In So Kweon","Choong Seon Hong"],"pdf_url":"https://arxiv.org/pdf/2303.11717v1.pdf","comment":"56 pages, 548 citations"},{"id":"http://arxiv.org/abs/2303.11705v1","updated":"2023-03-21T09:51:19Z","published":"2023-03-21T09:51:19Z","title":"A Single-Step Multiclass SVM based on Quantum Annealing for Remote\n  Sensing Data Classification","summary":"  In recent years, the development of quantum annealers has enabled\nexperimental demonstrations and has increased research interest in applications\nof quantum annealing, such as in quantum machine learning and in particular for\nthe popular quantum SVM. Several versions of the quantum SVM have been\nproposed, and quantum annealing has been shown to be effective in them.\nExtensions to multiclass problems have also been made, which consist of an\nensemble of multiple binary classifiers. This work proposes a novel quantum SVM\nformulation for direct multiclass classification based on quantum annealing,\ncalled Quantum Multiclass SVM (QMSVM). The multiclass classification problem is\nformulated as a single Quadratic Unconstrained Binary Optimization (QUBO)\nproblem solved with quantum annealing. The main objective of this work is to\nevaluate the feasibility, accuracy, and time performance of this approach.\nExperiments have been performed on the D-Wave Advantage quantum annealer for a\nclassification problem on remote sensing data. The results indicate that,\ndespite the memory demands of the quantum annealer, QMSVM can achieve accuracy\nthat is comparable to standard SVM methods and, more importantly, it scales\nmuch more efficiently with the number of training examples, resulting in nearly\nconstant time. This work shows an approach for bringing together classical and\nquantum computation, solving practical problems in remote sensing with current\nhardware.\n","authors":["Amer Delilbasic","Bertrand Le Saux","Morris Riedel","Kristel Michielsen","Gabriele Cavallaro"],"pdf_url":"https://arxiv.org/pdf/2303.11705v1.pdf","comment":"12 pages, 10 figures, 3 tables. Submitted to IEEE JSTARS"},{"id":"http://arxiv.org/abs/2303.11702v1","updated":"2023-03-21T09:42:27Z","published":"2023-03-21T09:42:27Z","title":"Linking generative semi-supervised learning and generative open-set\n  recognition","summary":"  This study investigates the relationship between semi-supervised learning\n(SSL) and open-set recognition (OSR) in the context of generative adversarial\nnetworks (GANs). Although no previous study has formally linked SSL and OSR,\ntheir respective methods share striking similarities. Specifically, SSL-GANs\nand OSR-GANs require generator to produce samples in the complementary space.\nSubsequently, by regularising networks with generated samples, both SSL and OSR\nclassifiers generalize the open space. To demonstrate the connection between\nSSL and OSR, we theoretically and experimentally compare state-of-the-art\nSSL-GAN methods with state-of-the-art OSR-GAN methods. Our results indicate\nthat the SSL optimised margin-GANs, which have a stronger foundation in\nliterature, set the new standard for the combined SSL-OSR task and achieves new\nstate-of-other art results in certain general OSR experiments. However, the OSR\noptimised adversarial reciprocal point (ARP)-GANs still slightly out-performed\nmargin-GANs at other OSR experiments. This result indicates unique insights for\nthe combined optimisation task of SSL-OSR.\n","authors":["Emile Reyn Engelbrecht","Johan du Preez"],"pdf_url":"https://arxiv.org/pdf/2303.11702v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11701v1","updated":"2023-03-21T09:41:13Z","published":"2023-03-21T09:41:13Z","title":"A High-Frequency Focused Network for Lightweight Single Image\n  Super-Resolution","summary":"  Lightweight neural networks for single-image super-resolution (SISR) tasks\nhave made substantial breakthroughs in recent years. Compared to low-frequency\ninformation, high-frequency detail is much more difficult to reconstruct. Most\nSISR models allocate equal computational resources for low-frequency and\nhigh-frequency information, which leads to redundant processing of simple\nlow-frequency information and inadequate recovery of more challenging\nhigh-frequency information. We propose a novel High-Frequency Focused Network\n(HFFN) through High-Frequency Focused Blocks (HFFBs) that selectively enhance\nhigh-frequency information while minimizing redundant feature computation of\nlow-frequency information. The HFFB effectively allocates more computational\nresources to the more challenging reconstruction of high-frequency information.\nMoreover, we propose a Local Feature Fusion Block (LFFB) effectively fuses\nfeatures from multiple HFFBs in a local region, utilizing complementary\ninformation across layers to enhance feature representativeness and reduce\nartifacts in reconstructed images. We assess the efficacy of our proposed HFFN\non five benchmark datasets and show that it significantly enhances the\nsuper-resolution performance of the network. Our experimental results\ndemonstrate state-of-the-art performance in reconstructing high-frequency\ninformation while using a low number of parameters.\n","authors":["Xiaotian Weng","Yi Chen","Zhichao Zheng","Yanhui Gu","Junsheng Zhou","Yudong Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.11701v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11694v1","updated":"2023-03-21T09:28:47Z","published":"2023-03-21T09:28:47Z","title":"Anchor Free remote sensing detector based on solving discrete polar\n  coordinate equation","summary":"  As the rapid development of depth learning, object detection in aviatic\nremote sensing images has become increasingly popular in recent years. Most of\nthe current Anchor Free detectors based on key point detection sampling\ndirectly regression and classification features, with the design of object loss\nfunction based on the horizontal bounding box. It is more challenging for\ncomplex and diverse aviatic remote sensing object. In this paper, we propose an\nAnchor Free aviatic remote sensing object detector (BWP-Det) to detect rotating\nand multi-scale object. Specifically, we design a interactive\ndouble-branch(IDB) up-sampling network, in which one branch gradually\nup-sampling is used for the prediction of Heatmap, and the other branch is used\nfor the regression of boundary box parameters. We improve a weighted\nmulti-scale convolution (WmConv) in order to highlight the difference between\nforeground and background. We extracted Pixel level attention features from the\nmiddle layer to guide the two branches to pay attention to effective object\ninformation in the sampling process. Finally, referring to the calculation idea\nof horizontal IoU, we design a rotating IoU based on the split polar coordinate\nplane, namely JIoU, which is expressed as the intersection ratio following\ndiscretization of the inner ellipse of the rotating bounding box, to solve the\ncorrelation between angle and side length in the regression process of the\nrotating bounding box. Ultimately, BWP-Det, our experiments on DOTA, UCAS-AOD\nand NWPU VHR-10 datasets show, achieves advanced performance with simpler\nmodels and fewer regression parameters.\n","authors":["Linfeng Shi","Yan Li","Xi Zhu"],"pdf_url":"https://arxiv.org/pdf/2303.11694v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.11317v2","updated":"2023-03-21T09:18:20Z","published":"2022-11-21T10:01:03Z","title":"DeSTSeg: Segmentation Guided Denoising Student-Teacher for Anomaly\n  Detection","summary":"  Visual anomaly detection, an important problem in computer vision, is usually\nformulated as a one-class classification and segmentation task. The\nstudent-teacher (S-T) framework has proved to be effective in solving this\nchallenge. However, previous works based on S-T only empirically applied\nconstraints on normal data and fused multi-level information. In this study, we\npropose an improved model called DeSTSeg, which integrates a pre-trained\nteacher network, a denoising student encoder-decoder, and a segmentation\nnetwork into one framework. First, to strengthen the constraints on anomalous\ndata, we introduce a denoising procedure that allows the student network to\nlearn more robust representations. From synthetically corrupted normal images,\nwe train the student network to match the teacher network feature of the same\nimages without corruption. Second, to fuse the multi-level S-T features\nadaptively, we train a segmentation network with rich supervision from\nsynthetic anomaly masks, achieving a substantial performance improvement.\nExperiments on the industrial inspection benchmark dataset demonstrate that our\nmethod achieves state-of-the-art performance, 98.6% on image-level AUC, 75.8%\non pixel-level average precision, and 76.4% on instance-level average\nprecision.\n","authors":["Xuan Zhang","Shiyu Li","Xi Li","Ping Huang","Jiulong Shan","Ting Chen"],"pdf_url":"https://arxiv.org/pdf/2211.11317v2.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2303.11686v1","updated":"2023-03-21T09:08:30Z","published":"2023-03-21T09:08:30Z","title":"Learning a 3D Morphable Face Reflectance Model from Low-cost Data","summary":"  Modeling non-Lambertian effects such as facial specularity leads to a more\nrealistic 3D Morphable Face Model. Existing works build parametric models for\ndiffuse and specular albedo using Light Stage data. However, only diffuse and\nspecular albedo cannot determine the full BRDF. In addition, the requirement of\nLight Stage data is hard to fulfill for the research communities. This paper\nproposes the first 3D morphable face reflectance model with spatially varying\nBRDF using only low-cost publicly-available data. We apply linear shiness\nweighting into parametric modeling to represent spatially varying specular\nintensity and shiness. Then an inverse rendering algorithm is developed to\nreconstruct the reflectance parameters from non-Light Stage data, which are\nused to train an initial morphable reflectance model. To enhance the model's\ngeneralization capability and expressive power, we further propose an\nupdate-by-reconstruction strategy to finetune it on an in-the-wild dataset.\nExperimental results show that our method obtains decent rendering results with\nplausible facial specularities. Our code is released\n\\href{https://yxuhan.github.io/ReflectanceMM/index.html}{\\textcolor{magenta}{here}}.\n","authors":["Yuxuan Han","Zhibo Wang","Feng Xu"],"pdf_url":"https://arxiv.org/pdf/2303.11686v1.pdf","comment":"CVPR 2023. Project page:\n  https://yxuhan.github.io/ReflectanceMM/index.html"},{"id":"http://arxiv.org/abs/2303.11684v1","updated":"2023-03-21T09:00:12Z","published":"2023-03-21T09:00:12Z","title":"SpikeCV: Open a Continuous Computer Vision Era","summary":"  SpikeCV is a new open-source computer vision platform for the spike camera,\nwhich is a neuromorphic visual sensor that has developed rapidly in recent\nyears. In the spike camera, each pixel position directly accumulates the light\nintensity and asynchronously fires spikes. The output binary spikes can reach a\nfrequency of 40,000 Hz. As a new type of visual expression, spike sequence has\nhigh spatiotemporal completeness and preserves the continuous visual\ninformation of the external world. Taking advantage of the low latency and high\ndynamic range of the spike camera, many spike-based algorithms have made\nsignificant progress, such as high-quality imaging and ultra-high-speed target\ndetection.\n  To build up a community ecology for the spike vision to facilitate more users\nto take advantage of the spike camera, SpikeCV provides a variety of\nultra-high-speed scene datasets, hardware interfaces, and an easy-to-use\nmodules library. SpikeCV focuses on encapsulation for spike data,\nstandardization for dataset interfaces, modularization for vision tasks, and\nreal-time applications for challenging scenes. With the advent of the\nopen-source Python ecosystem, modules of SpikeCV can be used as a Python\nlibrary to fulfilled most of the numerical analysis needs of researchers. We\ndemonstrate the efficiency of the SpikeCV on offline inference and real-time\napplications. The project repository address are\n\\url{https://openi.pcl.ac.cn/Cordium/SpikeCV} and\n\\url{https://github.com/Zyj061/SpikeCV\n","authors":["Yajing Zheng","Jiyuan Zhang","Rui Zhao","Jianhao Ding","Shiyan Chen","Ruiqin Xiong","Zhaofei Yu","Tiejun Huang"],"pdf_url":"https://arxiv.org/pdf/2303.11684v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.10930v3","updated":"2023-03-21T08:57:23Z","published":"2022-08-23T13:02:14Z","title":"FS-BAN: Born-Again Networks for Domain Generalization Few-Shot\n  Classification","summary":"  Conventional Few-shot classification (FSC) aims to recognize samples from\nnovel classes given limited labeled data. Recently, domain generalization FSC\n(DG-FSC) has been proposed with the goal to recognize novel class samples from\nunseen domains. DG-FSC poses considerable challenges to many models due to the\ndomain shift between base classes (used in training) and novel classes\n(encountered in evaluation). In this work, we make two novel contributions to\ntackle DG-FSC. Our first contribution is to propose Born-Again Network (BAN)\nepisodic training and comprehensively investigate its effectiveness for DG-FSC.\nAs a specific form of knowledge distillation, BAN has been shown to achieve\nimproved generalization in conventional supervised classification with a\nclosed-set setup. This improved generalization motivates us to study BAN for\nDG-FSC, and we show that BAN is promising to address the domain shift\nencountered in DG-FSC. Building on the encouraging findings, our second (major)\ncontribution is to propose Few-Shot BAN (FS-BAN), a novel BAN approach for\nDG-FSC. Our proposed FS-BAN includes novel multi-task learning objectives:\nMutual Regularization, Mismatched Teacher, and Meta-Control Temperature, each\nof these is specifically designed to overcome central and unique challenges in\nDG-FSC, namely overfitting and domain discrepancy. We analyze different design\nchoices of these techniques. We conduct comprehensive quantitative and\nqualitative analysis and evaluation over six datasets and three baseline\nmodels. The results suggest that our proposed FS-BAN consistently improves the\ngeneralization performance of baseline models and achieves state-of-the-art\naccuracy for DG-FSC. Project Page: https://yunqing-me.github.io/Born-Again-FS/.\n","authors":["Yunqing Zhao","Ngai-Man Cheung"],"pdf_url":"https://arxiv.org/pdf/2208.10930v3.pdf","comment":"15 pages, 9 figures, 15 tables. IEEE Transactions on Image Processing\n  (TIP), 2023"},{"id":"http://arxiv.org/abs/2303.11681v1","updated":"2023-03-21T08:43:15Z","published":"2023-03-21T08:43:15Z","title":"DiffuMask: Synthesizing Images with Pixel-level Annotations for Semantic\n  Segmentation Using Diffusion Models","summary":"  Collecting and annotating images with pixel-wise labels is time-consuming and\nlaborious. In contrast, synthetic data can be freely available using a\ngenerative model (e.g., DALL-E, Stable Diffusion). In this paper, we show that\nit is possible to automatically obtain accurate semantic masks of synthetic\nimages generated by the Off-the-shelf Stable Diffusion model, which uses only\ntext-image pairs during training. Our approach, called DiffuMask, exploits the\npotential of the cross-attention map between text and image, which is natural\nand seamless to extend the text-driven image synthesis to semantic mask\ngeneration. DiffuMask uses text-guided cross-attention information to localize\nclass/word-specific regions, which are combined with practical techniques to\ncreate a novel high-resolution and class-discriminative pixel-wise mask. The\nmethods help to reduce data collection and annotation costs obviously.\nExperiments demonstrate that the existing segmentation methods trained on\nsynthetic data of DiffuMask can achieve a competitive performance over the\ncounterpart of real data (VOC 2012, Cityscapes). For some classes (e.g., bird),\nDiffuMask presents promising performance, close to the stateof-the-art result\nof real data (within 3% mIoU gap). Moreover, in the open-vocabulary\nsegmentation (zero-shot) setting, DiffuMask achieves a new SOTA result on\nUnseen class of VOC 2012. The project website can be found at\nhttps://weijiawu.github.io/DiffusionMask/.\n","authors":["Weijia Wu","Yuzhong Zhao","Mike Zheng Shou","Hong Zhou","Chunhua Shen"],"pdf_url":"https://arxiv.org/pdf/2303.11681v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11678v1","updated":"2023-03-21T08:41:54Z","published":"2023-03-21T08:41:54Z","title":"Full or Weak annotations? An adaptive strategy for budget-constrained\n  annotation campaigns","summary":"  Annotating new datasets for machine learning tasks is tedious,\ntime-consuming, and costly. For segmentation applications, the burden is\nparticularly high as manual delineations of relevant image content are often\nextremely expensive or can only be done by experts with domain-specific\nknowledge. Thanks to developments in transfer learning and training with weak\nsupervision, segmentation models can now also greatly benefit from annotations\nof different kinds. However, for any new domain application looking to use weak\nsupervision, the dataset builder still needs to define a strategy to distribute\nfull segmentation and other weak annotations. Doing so is challenging, however,\nas it is a priori unknown how to distribute an annotation budget for a given\nnew dataset. To this end, we propose a novel approach to determine annotation\nstrategies for segmentation datasets, whereby estimating what proportion of\nsegmentation and classification annotations should be collected given a fixed\nbudget. To do so, our method sequentially determines proportions of\nsegmentation and classification annotations to collect for budget-fractions by\nmodeling the expected improvement of the final segmentation model. We show in\nour experiments that our approach yields annotations that perform very close to\nthe optimal for a number of different annotation budgets and datasets.\n","authors":["Javier Gamazo Tejero","Martin S. Zinkernagel","Sebastian Wolf","Raphael Sznitman","Pablo Márquez Neila"],"pdf_url":"https://arxiv.org/pdf/2303.11678v1.pdf","comment":"CVPR23"},{"id":"http://arxiv.org/abs/2303.11676v1","updated":"2023-03-21T08:37:15Z","published":"2023-03-21T08:37:15Z","title":"Deep Learning Pipeline for Preprocessing and Segmenting Cardiac Magnetic\n  Resonance of Single Ventricle Patients from an Image Registry","summary":"  Purpose: To develop and evaluate an end-to-end deep learning pipeline for\nsegmentation and analysis of cardiac magnetic resonance images to provide\ncore-lab processing for a multi-centre registry of Fontan patients.\n  Materials and Methods: This retrospective study used training (n = 175),\nvalidation (n = 25) and testing (n = 50) cardiac magnetic resonance image exams\ncollected from 13 institutions in the UK, US and Canada. The data was used to\ntrain and evaluate a pipeline containing three deep-learning models. The\npipeline's performance was assessed on the Dice and IoU score between the\nautomated and reference standard manual segmentation. Cardiac function values\nwere calculated from both the automated and manual segmentation and evaluated\nusing Bland-Altman analysis and paired t-tests. The overall pipeline was\nfurther evaluated qualitatively on 475 unseen patient exams.\n  Results: For the 50 testing dataset, the pipeline achieved a median Dice\nscore of 0.91 (0.89-0.94) for end-diastolic volume, 0.86 (0.82-0.89) for\nend-systolic volume, and 0.74 (0.70-0.77) for myocardial mass. The deep\nlearning-derived end-diastolic volume, end-systolic volume, myocardial mass,\nstroke volume and ejection fraction had no statistical difference compared to\nthe same values derived from manual segmentation with p values all greater than\n0.05. For the 475 unseen patient exams, the pipeline achieved 68% adequate\nsegmentation in both systole and diastole, 26% needed minor adjustments in\neither systole or diastole, 5% needed major adjustments, and the cropping model\nonly failed in 0.4%.\n  Conclusion: Deep learning pipeline can provide standardised 'core-lab'\nsegmentation for Fontan patients. This pipeline can now be applied to the >4500\ncardiac magnetic resonance exams currently in the FORCE registry as well as any\nnew patients that are recruited.\n","authors":["Tina Yao","Nicole St. Clair","Gabriel F. Miller","Adam L. Dorfman","Mark A. Fogel","Sunil Ghelani","Rajesh Krishnamurthy","Christopher Z. Lam","Joshua D. Robinson","David Schidlow","Timothy C. Slesnick","Justin Weigand","Michael Quail","Rahul Rathod","Jennifer A. Steeden","Vivek Muthurangu"],"pdf_url":"https://arxiv.org/pdf/2303.11676v1.pdf","comment":"17 pages, 6 figures"},{"id":"http://arxiv.org/abs/2303.11675v1","updated":"2023-03-21T08:36:59Z","published":"2023-03-21T08:36:59Z","title":"BoPR: Body-aware Part Regressor for Human Shape and Pose Estimation","summary":"  This paper presents a novel approach for estimating human body shape and pose\nfrom monocular images that effectively addresses the challenges of occlusions\nand depth ambiguity. Our proposed method BoPR, the Body-aware Part Regressor,\nfirst extracts features of both the body and part regions using an\nattention-guided mechanism. We then utilize these features to encode extra\npart-body dependency for per-part regression, with part features as queries and\nbody feature as a reference. This allows our network to infer the spatial\nrelationship of occluded parts with the body by leveraging visible parts and\nbody reference information. Our method outperforms existing state-of-the-art\nmethods on two benchmark datasets, and our experiments show that it\nsignificantly surpasses existing methods in terms of depth ambiguity and\nocclusion handling. These results provide strong evidence of the effectiveness\nof our approach.\n","authors":["Yongkang Cheng","Shaoli Huang","Jifeng Ning","Ying Shan"],"pdf_url":"https://arxiv.org/pdf/2303.11675v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11674v1","updated":"2023-03-21T08:36:34Z","published":"2023-03-21T08:36:34Z","title":"ALOFT: A Lightweight MLP-like Architecture with Dynamic Low-frequency\n  Transform for Domain Generalization","summary":"  Domain generalization (DG) aims to learn a model that generalizes well to\nunseen target domains utilizing multiple source domains without re-training.\nMost existing DG works are based on convolutional neural networks (CNNs).\nHowever, the local operation of the convolution kernel makes the model focus\ntoo much on local representations (e.g., texture), which inherently causes the\nmodel more prone to overfit to the source domains and hampers its\ngeneralization ability. Recently, several MLP-based methods have achieved\npromising results in supervised learning tasks by learning global interactions\namong different patches of the image. Inspired by this, in this paper, we first\nanalyze the difference between CNN and MLP methods in DG and find that MLP\nmethods exhibit a better generalization ability because they can better capture\nthe global representations (e.g., structure) than CNN methods. Then, based on a\nrecent lightweight MLP method, we obtain a strong baseline that outperforms\nmost state-of-the-art CNN-based methods. The baseline can learn global\nstructure representations with a filter to suppress structure irrelevant\ninformation in the frequency space. Moreover, we propose a dynAmic\nLOw-Frequency spectrum Transform (ALOFT) that can perturb local texture\nfeatures while preserving global structure features, thus enabling the filter\nto remove structure-irrelevant information sufficiently. Extensive experiments\non four benchmarks have demonstrated that our method can achieve great\nperformance improvement with a small number of parameters compared to SOTA\nCNN-based DG methods. Our code is available at\nhttps://github.com/lingeringlight/ALOFT/.\n","authors":["Jintao Guo","Na Wang","Lei Qi","Yinghuan Shi"],"pdf_url":"https://arxiv.org/pdf/2303.11674v1.pdf","comment":"Accepted by CVPR2023. The code is available at\n  https://github.com/lingeringlight/ALOFT/"},{"id":"http://arxiv.org/abs/2303.10904v2","updated":"2023-03-21T08:32:22Z","published":"2023-03-20T06:47:59Z","title":"Actionlet-Dependent Contrastive Learning for Unsupervised Skeleton-Based\n  Action Recognition","summary":"  The self-supervised pretraining paradigm has achieved great success in\nskeleton-based action recognition. However, these methods treat the motion and\nstatic parts equally, and lack an adaptive design for different parts, which\nhas a negative impact on the accuracy of action recognition. To realize the\nadaptive action modeling of both parts, we propose an Actionlet-Dependent\nContrastive Learning method (ActCLR). The actionlet, defined as the\ndiscriminative subset of the human skeleton, effectively decomposes motion\nregions for better action modeling. In detail, by contrasting with the static\nanchor without motion, we extract the motion region of the skeleton data, which\nserves as the actionlet, in an unsupervised manner. Then, centering on\nactionlet, a motion-adaptive data transformation method is built. Different\ndata transformations are applied to actionlet and non-actionlet regions to\nintroduce more diversity while maintaining their own characteristics.\nMeanwhile, we propose a semantic-aware feature pooling method to build feature\nrepresentations among motion and static regions in a distinguished manner.\nExtensive experiments on NTU RGB+D and PKUMMD show that the proposed method\nachieves remarkable action recognition performance. More visualization and\nquantitative experiments demonstrate the effectiveness of our method. Our\nproject website is available at https://langlandslin.github.io/projects/ActCLR/\n","authors":["Lilang Lin","Jiahang Zhang","Jiaying Liu"],"pdf_url":"https://arxiv.org/pdf/2303.10904v2.pdf","comment":"Accepted by CVPR2023 (Highlight). The project page is at\n  https://langlandslin.github.io/projects/ActCLR/"},{"id":"http://arxiv.org/abs/2303.11668v1","updated":"2023-03-21T08:23:05Z","published":"2023-03-21T08:23:05Z","title":"Focus or Not: A Baseline for Anomaly Event Detection On the Open Public\n  Places with Satellite Images","summary":"  In recent years, monitoring the world wide area with satellite images has\nbeen emerged as an important issue.\n  Site monitoring task can be divided into two independent tasks; 1) Change\nDetection and 2) Anomaly Event Detection.\n  Unlike to change detection research is actively conducted based on the\nnumerous datasets(\\eg LEVIR-CD, WHU-CD, S2Looking, xView2 and etc...) to meet\nup the expectations of industries or governments, research on AI models for\ndetecting anomaly events is passively and rarely conducted.\n  In this paper, we introduce a novel satellite imagery dataset(AED-RS) for\ndetecting anomaly events on the open public places.\n  AED-RS Dataset contains satellite images of normal and abnormal situations of\n8 open public places from all over the world.\n  Each places are labeled with different criteria based on the difference of\ncharacteristics of each places.\n  With this dataset, we introduce a baseline model for our dataset TB-FLOW,\nwhich can be trained in weakly-supervised manner and shows reasonable\nperformance on the AED-RS Dataset compared with the other NF(Normalizing-Flow)\nbased anomaly detection models. Our dataset and code will be publicly open in\n\\url{https://github.com/SIAnalytics/RS_AnomalyDetection.git}.\n","authors":["Yongjin Jeon","Youngtack Oh","Doyoung Jeong","Hyunguk Choi","Junsik Kim"],"pdf_url":"https://arxiv.org/pdf/2303.11668v1.pdf","comment":"14 pages, 14 figures"},{"id":"http://arxiv.org/abs/2303.11661v1","updated":"2023-03-21T08:08:13Z","published":"2023-03-21T08:08:13Z","title":"Advanced Multi-Microscopic Views Cell Semi-supervised Segmentation","summary":"  Although deep learning (DL) shows powerful potential in cell segmentation\ntasks, it suffers from poor generalization as DL-based methods originally\nsimplified cell segmentation in detecting cell membrane boundary, lacking\nprominent cellular structures to position overall differentiating. Moreover,\nthe scarcity of annotated cell images limits the performance of DL models.\nSegmentation limitations of a single category of cell make massive practice\ndifficult, much less, with varied modalities. In this paper, we introduce a\nnovel semi-supervised cell segmentation method called Multi-Microscopic-view\nCell semi-supervised Segmentation (MMCS), which can train cell segmentation\nmodels utilizing less labeled multi-posture cell images with different\nmicroscopy well. Technically, MMCS consists of Nucleus-assisted global\nrecognition, Self-adaptive diameter filter, and Temporal-ensembling models.\nNucleus-assisted global recognition adds additional cell nucleus channel to\nimprove the global distinguishing performance of fuzzy cell membrane boundaries\neven when cells aggregate. Besides, self-adapted cell diameter filter can help\nseparate multi-resolution cells with different morphology properly. It further\nleverages the temporal-ensembling models to improve the semi-supervised\ntraining process, achieving effective training with less labeled data.\nAdditionally, optimizing the weight of unlabeled loss contributed to total loss\nalso improve the model performance. Evaluated on the Tuning Set of NeurIPS 2022\nCell Segmentation Challenge (NeurIPS CellSeg), MMCS achieves an F1-score of\n0.8239 and the running time for all cases is within the time tolerance.\n","authors":["Fang Hu","Xuexue Sun","Ke Qing","Fenxi Xiao","Zhi Wang","Xiaolu Fan"],"pdf_url":"https://arxiv.org/pdf/2303.11661v1.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2301.02239v2","updated":"2023-03-21T07:57:46Z","published":"2023-01-05T18:59:51Z","title":"Robust Dynamic Radiance Fields","summary":"  Dynamic radiance field reconstruction methods aim to model the time-varying\nstructure and appearance of a dynamic scene. Existing methods, however, assume\nthat accurate camera poses can be reliably estimated by Structure from Motion\n(SfM) algorithms. These methods, thus, are unreliable as SfM algorithms often\nfail or produce erroneous poses on challenging videos with highly dynamic\nobjects, poorly textured surfaces, and rotating camera motion. We address this\nrobustness issue by jointly estimating the static and dynamic radiance fields\nalong with the camera parameters (poses and focal length). We demonstrate the\nrobustness of our approach via extensive quantitative and qualitative\nexperiments. Our results show favorable performance over the state-of-the-art\ndynamic view synthesis methods.\n","authors":["Yu-Lun Liu","Chen Gao","Andreas Meuleman","Hung-Yu Tseng","Ayush Saraf","Changil Kim","Yung-Yu Chuang","Johannes Kopf","Jia-Bin Huang"],"pdf_url":"https://arxiv.org/pdf/2301.02239v2.pdf","comment":"CVPR 2023. Project page: https://robust-dynrf.github.io/"},{"id":"http://arxiv.org/abs/2303.11654v1","updated":"2023-03-21T07:54:58Z","published":"2023-03-21T07:54:58Z","title":"Mitigating climate and health impact of small-scale kiln industry using\n  multi-spectral classifier and deep learning","summary":"  Industrial air pollution has a direct health impact and is a major\ncontributor to climate change. Small scale industries particularly bull-trench\nbrick kilns are one of the major causes of air pollution in South Asia often\ncreating hazardous levels of smog that is injurious to human health. To\nmitigate the climate and health impact of the kiln industry, fine-grained kiln\nlocalization at different geographic locations is needed. Kiln localization\nusing multi-spectral remote sensing data such as vegetation index results in a\nnoisy estimates whereas use of high-resolution imagery is infeasible due to\ncost and compute complexities. This paper proposes a fusion of spatio-temporal\nmulti-spectral data with high-resolution imagery for detection of brick kilns\nwithin the \"Brick-Kiln-Belt\" of South Asia. We first perform classification\nusing low-resolution spatio-temporal multi-spectral data from Sentinel-2\nimagery by combining vegetation, burn, build up and moisture indices. Then\norientation aware object detector: YOLOv3 (with theta value) is implemented for\nremoval of false detections and fine-grained localization. Our proposed\ntechnique, when compared with other benchmarks, results in a 21x improvement in\nspeed with comparable or higher accuracy when tested over multiple countries.\n","authors":["Usman Nazir","Murtaza Taj","Momin Uppal","Sara Khalid"],"pdf_url":"https://arxiv.org/pdf/2303.11654v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.10206v4","updated":"2023-03-21T07:50:56Z","published":"2022-11-18T12:53:10Z","title":"Multi-view Inverse Rendering for Large-scale Real-world Indoor Scenes","summary":"  We present a efficient multi-view inverse rendering method for large-scale\nreal-world indoor scenes that reconstructs global illumination and\nphysically-reasonable SVBRDFs. Unlike previous representations, where the\nglobal illumination of large scenes is simplified as multiple environment maps,\nwe propose a compact representation called Texture-based Lighting (TBL). It\nconsists of 3D mesh and HDR textures, and efficiently models direct and\ninfinite-bounce indirect lighting of the entire large scene. Based on TBL, we\nfurther propose a hybrid lighting representation with precomputed irradiance,\nwhich significantly improves the efficiency and alleviates the rendering noise\nin the material optimization. To physically disentangle the ambiguity between\nmaterials, we propose a three-stage material optimization strategy based on the\npriors of semantic segmentation and room segmentation. Extensive experiments\nshow that the proposed method outperforms the state-of-the-art quantitatively\nand qualitatively, and enables physically-reasonable mixed-reality applications\nsuch as material editing, editable novel view synthesis and relighting. The\nproject page is at https://lzleejean.github.io/TexIR.\n","authors":["Zhen Li","Lingli Wang","Mofang Cheng","Cihui Pan","Jiaqi Yang"],"pdf_url":"https://arxiv.org/pdf/2211.10206v4.pdf","comment":"Accepted to CVPR 2023. The project page is at:\n  https://lzleejean.github.io/TexIR"},{"id":"http://arxiv.org/abs/2303.11649v1","updated":"2023-03-21T07:49:32Z","published":"2023-03-21T07:49:32Z","title":"CoopInit: Initializing Generative Adversarial Networks via Cooperative\n  Learning","summary":"  Numerous research efforts have been made to stabilize the training of the\nGenerative Adversarial Networks (GANs), such as through regularization and\narchitecture design. However, we identify the instability can also arise from\nthe fragile balance at the early stage of adversarial learning. This paper\nproposes the CoopInit, a simple yet effective cooperative learning-based\ninitialization strategy that can quickly learn a good starting point for GANs,\nwith a very small computation overhead during training. The proposed algorithm\nconsists of two learning stages: (i) Cooperative initialization stage: The\ndiscriminator of GAN is treated as an energy-based model (EBM) and is optimized\nvia maximum likelihood estimation (MLE), with the help of the GAN's generator\nto provide synthetic data to approximate the learning gradients. The EBM also\nguides the MLE learning of the generator via MCMC teaching; (ii) Adversarial\nfinalization stage: After a few iterations of initialization, the algorithm\nseamlessly transits to the regular mini-max adversarial training until\nconvergence. The motivation is that the MLE-based initialization stage drives\nthe model towards mode coverage, which is helpful in alleviating the issue of\nmode dropping during the adversarial learning stage. We demonstrate the\neffectiveness of the proposed approach on image generation and one-sided\nunpaired image-to-image translation tasks through extensive experiments.\n","authors":["Yang Zhao","Jianwen Xie","Ping Li"],"pdf_url":"https://arxiv.org/pdf/2303.11649v1.pdf","comment":"9 pages of main text, 2 pages of references"},{"id":"http://arxiv.org/abs/2302.02871v3","updated":"2023-03-21T07:37:12Z","published":"2023-02-06T15:38:21Z","title":"Top-Down Beats Bottom-Up in 3D Instance Segmentation","summary":"  Most 3D instance segmentation methods exploit a bottom-up strategy, typically\nincluding resource-exhaustive post-processing. For point grouping, bottom-up\nmethods rely on prior assumptions about the objects in the form of\nhyperparameters, which are domain-specific and need to be carefully tuned. On\nthe contrary, we address 3D instance segmentation with a TD3D: top-down, fully\ndata-driven, simple approach trained in an end-to-end manner. With its\nstraightforward fully-convolutional pipeline, it performs surprisingly well on\nthe standard benchmarks: ScanNet v2, its extension ScanNet200, and S3DIS.\nBesides, our method is much faster on inference than the current\nstate-of-the-art grouping-based approaches. Code is available at\nhttps://github.com/SamsungLabs/td3d .\n","authors":["Maksim Kolodiazhnyi","Danila Rukhovich","Anna Vorontsova","Anton Konushin"],"pdf_url":"https://arxiv.org/pdf/2302.02871v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11642v1","updated":"2023-03-21T07:27:37Z","published":"2023-03-21T07:27:37Z","title":"Visibility Constrained Wide-band Illumination Spectrum Design for\n  Seeing-in-the-Dark","summary":"  Seeing-in-the-dark is one of the most important and challenging computer\nvision tasks due to its wide applications and extreme complexities of\nin-the-wild scenarios. Existing arts can be mainly divided into two threads: 1)\nRGB-dependent methods restore information using degraded RGB inputs only (\\eg,\nlow-light enhancement), 2) RGB-independent methods translate images captured\nunder auxiliary near-infrared (NIR) illuminants into RGB domain (\\eg, NIR2RGB\ntranslation). The latter is very attractive since it works in complete darkness\nand the illuminants are visually friendly to naked eyes, but tends to be\nunstable due to its intrinsic ambiguities. In this paper, we try to robustify\nNIR2RGB translation by designing the optimal spectrum of auxiliary illumination\nin the wide-band VIS-NIR range, while keeping visual friendliness. Our core\nidea is to quantify the visibility constraint implied by the human vision\nsystem and incorporate it into the design pipeline. By modeling the formation\nprocess of images in the VIS-NIR range, the optimal multiplexing of a wide\nrange of LEDs is automatically designed in a fully differentiable manner,\nwithin the feasible region defined by the visibility constraint. We also\ncollect a substantially expanded VIS-NIR hyperspectral image dataset for\nexperiments by using a customized 50-band filter wheel. Experimental results\nshow that the task can be significantly improved by using the optimized\nwide-band illumination than using NIR only. Codes Available:\nhttps://github.com/MyNiuuu/VCSD.\n","authors":["Muyao Niu","Zhuoxiao Li","Zhihang Zhong","Yinqiang Zheng"],"pdf_url":"https://arxiv.org/pdf/2303.11642v1.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2303.10883v2","updated":"2023-03-21T07:25:09Z","published":"2023-03-20T06:01:53Z","title":"Explicit Visual Prompting for Low-Level Structure Segmentations","summary":"  We consider the generic problem of detecting low-level structures in images,\nwhich includes segmenting the manipulated parts, identifying out-of-focus\npixels, separating shadow regions, and detecting concealed objects. Whereas\neach such topic has been typically addressed with a domain-specific solution,\nwe show that a unified approach performs well across all of them. We take\ninspiration from the widely-used pre-training and then prompt tuning protocols\nin NLP and propose a new visual prompting model, named Explicit Visual\nPrompting (EVP). Different from the previous visual prompting which is\ntypically a dataset-level implicit embedding, our key insight is to enforce the\ntunable parameters focusing on the explicit visual content from each individual\nimage, i.e., the features from frozen patch embeddings and the input's\nhigh-frequency components. The proposed EVP significantly outperforms other\nparameter-efficient tuning protocols under the same amount of tunable\nparameters (5.7% extra trainable parameters of each task). EVP also achieves\nstate-of-the-art performances on diverse low-level structure segmentation tasks\ncompared to task-specific solutions. Our code is available at:\nhttps://github.com/NiFangBaAGe/Explicit-Visual-Prompt.\n","authors":["Weihuang Liu","Xi Shen","Chi-Man Pun","Xiaodong Cun"],"pdf_url":"https://arxiv.org/pdf/2303.10883v2.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2207.06726v2","updated":"2023-03-21T07:23:13Z","published":"2022-07-14T08:22:58Z","title":"Octuplet Loss: Make Face Recognition Robust to Image Resolution","summary":"  Image resolution, or in general, image quality, plays an essential role in\nthe performance of today's face recognition systems. To address this problem,\nwe propose a novel combination of the popular triplet loss to improve\nrobustness against image resolution via fine-tuning of existing face\nrecognition models. With octuplet loss, we leverage the relationship between\nhigh-resolution images and their synthetically down-sampled variants jointly\nwith their identity labels. Fine-tuning several state-of-the-art approaches\nwith our method proves that we can significantly boost performance for\ncross-resolution (high-to-low resolution) face verification on various datasets\nwithout meaningfully exacerbating the performance on high-to-high resolution\nimages. Our method applied on the FaceTransformer network achieves 95.12% face\nverification accuracy on the challenging XQLFW dataset while reaching 99.73% on\nthe LFW database. Moreover, the low-to-low face verification accuracy benefits\nfrom our method. We release our code to allow seamless integration of the\noctuplet loss into existing frameworks.\n","authors":["Martin Knoche","Mohamed Elkadeem","Stefan Hörmann","Gerhard Rigoll"],"pdf_url":"https://arxiv.org/pdf/2207.06726v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11638v1","updated":"2023-03-21T07:14:18Z","published":"2023-03-21T07:14:18Z","title":"Human Pose as Compositional Tokens","summary":"  Human pose is typically represented by a coordinate vector of body joints or\ntheir heatmap embeddings. While easy for data processing, unrealistic pose\nestimates are admitted due to the lack of dependency modeling between the body\njoints. In this paper, we present a structured representation, named Pose as\nCompositional Tokens (PCT), to explore the joint dependency. It represents a\npose by M discrete tokens with each characterizing a sub-structure with several\ninterdependent joints. The compositional design enables it to achieve a small\nreconstruction error at a low cost. Then we cast pose estimation as a\nclassification task. In particular, we learn a classifier to predict the\ncategories of the M tokens from an image. A pre-learned decoder network is used\nto recover the pose from the tokens without further post-processing. We show\nthat it achieves better or comparable pose estimation results as the existing\nmethods in general scenarios, yet continues to work well when occlusion occurs,\nwhich is ubiquitous in practice. The code and models are publicly available at\nhttps://github.com/Gengzigang/PCT.\n","authors":["Zigang Geng","Chunyu Wang","Yixuan Wei","Ze Liu","Houqiang Li","Han Hu"],"pdf_url":"https://arxiv.org/pdf/2303.11638v1.pdf","comment":"Accepted by CVPR2023"},{"id":"http://arxiv.org/abs/2202.06884v3","updated":"2023-03-21T07:12:46Z","published":"2022-02-14T17:19:23Z","title":"COLA: COarse LAbel pre-training for 3D semantic segmentation of sparse\n  LiDAR datasets","summary":"  Transfer learning is a proven technique in 2D computer vision to leverage the\nlarge amount of data available and achieve high performance with datasets\nlimited in size due to the cost of acquisition or annotation. In 3D, annotation\nis known to be a costly task; nevertheless, pre-training methods have only\nrecently been investigated. Due to this cost, unsupervised pre-training has\nbeen heavily favored. In this work, we tackle the case of real-time 3D semantic\nsegmentation of sparse autonomous driving LiDAR scans. Such datasets have been\nincreasingly released, but each has a unique label set. We propose here an\nintermediate-level label set called coarse labels, which can easily be used on\nany existing and future autonomous driving datasets, thus allowing all the data\navailable to be leveraged at once without any additional manual labeling. This\nway, we have access to a larger dataset, alongside a simple task of semantic\nsegmentation. With it, we introduce a new pre-training task: coarse label\npre-training, also called COLA. We thoroughly analyze the impact of COLA on\nvarious datasets and architectures and show that it yields a noticeable\nperformance improvement, especially when only a small dataset is available for\nthe finetuning task.\n","authors":["Jules Sanchez","Jean-Emmanuel Deschaud","François Goulette"],"pdf_url":"https://arxiv.org/pdf/2202.06884v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11637v1","updated":"2023-03-21T07:08:51Z","published":"2023-03-21T07:08:51Z","title":"Equiangular Basis Vectors","summary":"  We propose Equiangular Basis Vectors (EBVs) for classification tasks. In deep\nneural networks, models usually end with a k-way fully connected layer with\nsoftmax to handle different classification tasks. The learning objective of\nthese methods can be summarized as mapping the learned feature representations\nto the samples' label space. While in metric learning approaches, the main\nobjective is to learn a transformation function that maps training data points\nfrom the original space to a new space where similar points are closer while\ndissimilar points become farther apart. Different from previous methods, our\nEBVs generate normalized vector embeddings as \"predefined classifiers\" which\nare required to not only be with the equal status between each other, but also\nbe as orthogonal as possible. By minimizing the spherical distance of the\nembedding of an input between its categorical EBV in training, the predictions\ncan be obtained by identifying the categorical EBV with the smallest distance\nduring inference. Various experiments on the ImageNet-1K dataset and other\ndownstream tasks demonstrate that our method outperforms the general fully\nconnected classifier while it does not introduce huge additional computation\ncompared with classical metric learning methods. Our EBVs won the first place\nin the 2022 DIGIX Global AI Challenge, and our code is open-source and\navailable at https://github.com/NJUST-VIPGroup/Equiangular-Basis-Vectors.\n","authors":["Yang Shen","Xuhao Sun","Xiu-Shen Wei"],"pdf_url":"https://arxiv.org/pdf/2303.11637v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.11633v1","updated":"2023-03-21T07:00:35Z","published":"2023-03-21T07:00:35Z","title":"Learning Context-aware Classifier for Semantic Segmentation","summary":"  Semantic segmentation is still a challenging task for parsing diverse\ncontexts in different scenes, thus the fixed classifier might not be able to\nwell address varying feature distributions during testing. Different from the\nmainstream literature where the efficacy of strong backbones and effective\ndecoder heads has been well studied, in this paper, additional contextual hints\nare instead exploited via learning a context-aware classifier whose content is\ndata-conditioned, decently adapting to different latent distributions. Since\nonly the classifier is dynamically altered, our method is model-agnostic and\ncan be easily applied to generic segmentation models. Notably, with only\nnegligible additional parameters and +2\\% inference time, decent performance\ngain has been achieved on both small and large models with challenging\nbenchmarks, manifesting substantial practical merits brought by our simple yet\neffective method. The implementation is available at\n\\url{https://github.com/tianzhuotao/CAC}.\n","authors":["Zhuotao Tian","Jiequan Cui","Li Jiang","Xiaojuan Qi","Xin Lai","Yixin Chen","Shu Liu","Jiaya Jia"],"pdf_url":"https://arxiv.org/pdf/2303.11633v1.pdf","comment":"AAAI 2023. Code and models are available at\n  https://github.com/tianzhuotao/CAC"},{"id":"http://arxiv.org/abs/2303.11632v1","updated":"2023-03-21T07:00:13Z","published":"2023-03-21T07:00:13Z","title":"An Embarrassingly Simple Approach for Wafer Feature Extraction and\n  Defect Pattern Recognition","summary":"  Identifying defect patterns in a wafer map during manufacturing is crucial to\nfind the root cause of the underlying issue and provides valuable insights on\nimproving yield in the foundry. Currently used methods use deep neural networks\nto identify the defects. These methods are generally very huge and have\nsignificant inference time. They also require GPU support to efficiently\noperate. All these issues make these models not fit for on-line prediction in\nthe manufacturing foundry. In this paper, we propose an extremely simple yet\neffective technique to extract features from wafer images. The proposed method\nis extremely fast, intuitive, and non-parametric while being explainable. The\nexperiment results show that the proposed pipeline outperforms conventional\ndeep learning models. Our feature extraction requires no training or\nfine-tuning while preserving the relative shape and location of data points as\nrevealed by our interpretability analysis.\n","authors":["Nitish Shukla"],"pdf_url":"https://arxiv.org/pdf/2303.11632v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11630v1","updated":"2023-03-21T06:54:18Z","published":"2023-03-21T06:54:18Z","title":"BoxSnake: Polygonal Instance Segmentation with Box Supervision","summary":"  Box-supervised instance segmentation has gained much attention as it requires\nonly simple box annotations instead of costly mask or polygon annotations.\nHowever, existing box-supervised instance segmentation models mainly focus on\nmask-based frameworks. We propose a new end-to-end training technique, termed\nBoxSnake, to achieve effective polygonal instance segmentation using only box\nannotations for the first time. Our method consists of two loss functions: (1)\na point-based unary loss that constrains the bounding box of predicted polygons\nto achieve coarse-grained segmentation; and (2) a distance-aware pairwise loss\nthat encourages the predicted polygons to fit the object boundaries. Compared\nwith the mask-based weakly-supervised methods, BoxSnake further reduces the\nperformance gap between the predicted segmentation and the bounding box, and\nshows significant superiority on the Cityscapes dataset.\n","authors":["Rui Yang","Lin Song","Yixiao Ge","Xiu Li"],"pdf_url":"https://arxiv.org/pdf/2303.11630v1.pdf","comment":"11 pages, 8 figures"},{"id":"http://arxiv.org/abs/2303.11629v1","updated":"2023-03-21T06:51:31Z","published":"2023-03-21T06:51:31Z","title":"TMA: Temporal Motion Aggregation for Event-based Optical Flow","summary":"  Event cameras have the ability to record continuous and detailed trajectories\nof objects with high temporal resolution, thereby providing intuitive motion\ncues for optical flow estimation. Nevertheless, most existing learning-based\napproaches for event optical flow estimation directly remould the paradigm of\nconventional images by representing the consecutive event stream as static\nframes, ignoring the inherent temporal continuity of event data. In this paper,\nwe argue that temporal continuity is a vital element of event-based optical\nflow and propose a novel Temporal Motion Aggregation (TMA) approach to unlock\nits potential. Technically, TMA comprises three components: an event splitting\nstrategy to incorporate intermediate motion information underlying the temporal\ncontext, a linear lookup strategy to align temporally continuous motion\nfeatures and a novel motion pattern aggregation module to emphasize consistent\npatterns for motion feature enhancement. By incorporating temporally continuous\nmotion information, TMA can derive better flow estimates than existing methods\nat early stages, which not only enables TMA to obtain more accurate final\npredictions, but also greatly reduces the demand for a number of refinements.\nExtensive experiments on DESC-Flow and MVSEC datasets verify the effectiveness\nand superiority of our TMA. Remarkably, compared to E-RAFT, TMA achieves a 6%\nimprovement in accuracy and a 40% reduction in inference time on DSEC-Flow.\n","authors":["Haotian Liu","Guang Chen","Sanqing Qu","Yanping Zhang","Zhijun Li","Alois Knoll","Changjun Jiang"],"pdf_url":"https://arxiv.org/pdf/2303.11629v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11625v1","updated":"2023-03-21T06:48:14Z","published":"2023-03-21T06:48:14Z","title":"Information-containing Adversarial Perturbation for Combating Facial\n  Manipulation Systems","summary":"  With the development of deep learning technology, the facial manipulation\nsystem has become powerful and easy to use. Such systems can modify the\nattributes of the given facial images, such as hair color, gender, and age.\nMalicious applications of such systems pose a serious threat to individuals'\nprivacy and reputation. Existing studies have proposed various approaches to\nprotect images against facial manipulations. Passive defense methods aim to\ndetect whether the face is real or fake, which works for posterior forensics\nbut can not prevent malicious manipulation. Initiative defense methods protect\nimages upfront by injecting adversarial perturbations into images to disrupt\nfacial manipulation systems but can not identify whether the image is fake. To\naddress the limitation of existing methods, we propose a novel two-tier\nprotection method named Information-containing Adversarial Perturbation (IAP),\nwhich provides more comprehensive protection for {facial images}. We use an\nencoder to map a facial image and its identity message to a cross-model\nadversarial example which can disrupt multiple facial manipulation systems to\nachieve initiative protection. Recovering the message in adversarial examples\nwith a decoder serves passive protection, contributing to provenance tracking\nand fake image detection. We introduce a feature-level correlation measurement\nthat is more suitable to measure the difference between the facial images than\nthe commonly used mean squared error. Moreover, we propose a spectral diffusion\nmethod to spread messages to different frequency channels, thereby improving\nthe robustness of the message against facial manipulation. Extensive\nexperimental results demonstrate that our proposed IAP can recover the messages\nfrom the adversarial examples with high average accuracy and effectively\ndisrupt the facial manipulation systems.\n","authors":["Yao Zhu","Yuefeng Chen","Xiaodan Li","Rong Zhang","Xiang Tian","Bolun Zheng","Yaowu Chen"],"pdf_url":"https://arxiv.org/pdf/2303.11625v1.pdf","comment":"\\copyright 20XX IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works"},{"id":"http://arxiv.org/abs/2303.11623v1","updated":"2023-03-21T06:44:02Z","published":"2023-03-21T06:44:02Z","title":"Detecting the open-world objects with the help of the Brain","summary":"  Open World Object Detection (OWOD) is a novel computer vision task with a\nconsiderable challenge, bridging the gap between classic object detection (OD)\nbenchmarks and real-world object detection. In addition to detecting and\nclassifying seen/known objects, OWOD algorithms are expected to detect\nunseen/unknown objects and incrementally learn them. The natural instinct of\nhumans to identify unknown objects in their environments mainly depends on\ntheir brains' knowledge base. It is difficult for a model to do this only by\nlearning from the annotation of several tiny datasets. The large pre-trained\ngrounded language-image models - VL (\\ie GLIP) have rich knowledge about the\nopen world but are limited to the text prompt. We propose leveraging the VL as\nthe ``Brain'' of the open-world detector by simply generating unknown labels.\nLeveraging it is non-trivial because the unknown labels impair the model's\nlearning of known objects. In this paper, we alleviate these problems by\nproposing the down-weight loss function and decoupled detection structure.\nMoreover, our detector leverages the ``Brain'' to learn novel objects beyond VL\nthrough our pseudo-labeling scheme.\n","authors":["Shuailei Ma","Yuefeng Wang","Ying Wei","Peihao Chen","Zhixiang Ye","Jiaqi Fan","Enming Zhang","Thomas H. Li"],"pdf_url":"https://arxiv.org/pdf/2303.11623v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2301.01970"},{"id":"http://arxiv.org/abs/2112.02869v4","updated":"2023-03-21T06:34:09Z","published":"2021-12-06T08:50:47Z","title":"Physics Driven Deep Retinex Fusion for Adaptive Infrared and Visible\n  Image Fusion","summary":"  Convolutional neural networks have turned into an illustrious tool for image\nfusion and super-resolution. However, their excellent performance cannot work\nwithout large fixed-paired datasets; and additionally, these high-demanded\nground truth data always cannot be obtained easily in fusion tasks. In this\nstudy, we show that, the structures of generative networks capture a great deal\nof image feature priors, and then these priors are sufficient to reconstruct\nhigh-quality fused super-resolution result using only low-resolution inputs. By\nthis way, we propose a novel self-supervised dataset-free method for adaptive\ninfrared (IR) and visible (VIS) image super-resolution fusion named Deep\nRetinex Fusion (DRF). The key idea of DRF is first generating component priors\nwhich are disentangled from physical model using our designed generative\nnetworks ZipperNet, LightingNet and AdjustingNet, then combining these priors\nwhich captured by networks via adaptive fusion loss functions based on Retinex\ntheory, and finally reconstructing the super-resolution fusion results.\nFurthermore, in order to verify the effectiveness of our reported DRF, both\nqualitative and quantitative experiments via comparing with other\nstate-of-the-art methods are performed using different test sets. These results\nprove that, comparing with large datasets trained methods, DRF which works\nwithout any dataset achieves the best super-resolution fusion performance; and\nmore importantly, DRF can adaptively balance IR and VIS information and has\ngood noise immunity. DRF codes are open source available at\nhttps://github.com/GuYuanjie/Deep-Retinex-fusion.\n","authors":["Yuanjie Gu","Zhibo Xiao","Yinghan Guan","Haoran Dai","Cheng Liu","Liang Xue","Shouyu Wang"],"pdf_url":"https://arxiv.org/pdf/2112.02869v4.pdf","comment":"20 pages, 9 figures"},{"id":"http://arxiv.org/abs/2303.11616v1","updated":"2023-03-21T06:26:18Z","published":"2023-03-21T06:26:18Z","title":"HRDFuse: Monocular 360°Depth Estimation by Collaboratively Learning\n  Holistic-with-Regional Depth Distributions","summary":"  Depth estimation from a monocular 360{\\deg} image is a burgeoning problem\nowing to its holistic sensing of a scene. Recently, some methods, \\eg,\nOmniFusion, have applied the tangent projection (TP) to represent a\n360{\\deg}image and predicted depth values via patch-wise regressions, which are\nmerged to get a depth map with equirectangular projection (ERP) format.\nHowever, these methods suffer from 1) non-trivial process of merging plenty of\npatches; 2) capturing less holistic-with-regional contextual information by\ndirectly regressing the depth value of each pixel. In this paper, we propose a\nnovel framework, \\textbf{HRDFuse}, that subtly combines the potential of\nconvolutional neural networks (CNNs) and transformers by collaboratively\nlearning the \\textit{holistic} contextual information from the ERP and the\n\\textit{regional} structural information from the TP. Firstly, we propose a\nspatial feature alignment (\\textbf{SFA}) module that learns feature\nsimilarities between the TP and ERP to aggregate the TP features into a\ncomplete ERP feature map in a pixel-wise manner. Secondly, we propose a\ncollaborative depth distribution classification (\\textbf{CDDC}) module that\nlearns the \\textbf{holistic-with-regional} histograms capturing the ERP and TP\ndepth distributions. As such, the final depth values can be predicted as a\nlinear combination of histogram bin centers. Lastly, we adaptively combine the\ndepth predictions from ERP and TP to obtain the final depth map. Extensive\nexperiments show that our method predicts\\textbf{ more smooth and accurate\ndepth} results while achieving \\textbf{favorably better} results than the SOTA\nmethods.\n","authors":["Hao Ai","Zidong cao","Yan-pei Cao","Ying Shan","Lin Wang"],"pdf_url":"https://arxiv.org/pdf/2303.11616v1.pdf","comment":"To appear at CVPR2023, 20 pages"},{"id":"http://arxiv.org/abs/2211.06689v4","updated":"2023-03-21T06:24:38Z","published":"2022-11-12T15:39:07Z","title":"TINC: Tree-structured Implicit Neural Compression","summary":"  Implicit neural representation (INR) can describe the target scenes with high\nfidelity using a small number of parameters, and is emerging as a promising\ndata compression technique. However, limited spectrum coverage is intrinsic to\nINR, and it is non-trivial to remove redundancy in diverse complex data\neffectively. Preliminary studies can only exploit either global or local\ncorrelation in the target data and thus of limited performance. In this paper,\nwe propose a Tree-structured Implicit Neural Compression (TINC) to conduct\ncompact representation for local regions and extract the shared features of\nthese local representations in a hierarchical manner. Specifically, we use\nMulti-Layer Perceptrons (MLPs) to fit the partitioned local regions, and these\nMLPs are organized in tree structure to share parameters according to the\nspatial distance. The parameter sharing scheme not only ensures the continuity\nbetween adjacent regions, but also jointly removes the local and non-local\nredundancy. Extensive experiments show that TINC improves the compression\nfidelity of INR, and has shown impressive compression capabilities over\ncommercial tools and other deep learning based methods. Besides, the approach\nis of high flexibility and can be tailored for different data and parameter\nsettings. The source code can be found at https://github.com/RichealYoung/TINC .\n","authors":["Runzhao Yang","Tingxiong Xiao","Yuxiao Cheng","Jinli Suo","Qionghai Dai"],"pdf_url":"https://arxiv.org/pdf/2211.06689v4.pdf","comment":"Accepted to CVPR2023"},{"id":"http://arxiv.org/abs/2303.11615v1","updated":"2023-03-21T06:20:49Z","published":"2023-03-21T06:20:49Z","title":"Robust Table Structure Recognition with Dynamic Queries Enhanced\n  Detection Transformer","summary":"  We present a new table structure recognition (TSR) approach, called\nTSRFormer, to robustly recognizing the structures of complex tables with\ngeometrical distortions from various table images. Unlike previous methods, we\nformulate table separation line prediction as a line regression problem instead\nof an image segmentation problem and propose a new two-stage dynamic queries\nenhanced DETR based separation line regression approach, named DQ-DETR, to\npredict separation lines from table images directly. Compared to Vallina DETR,\nwe propose three improvements in DQ-DETR to make the two-stage DETR framework\nwork efficiently and effectively for the separation line prediction task: 1) A\nnew query design, named Dynamic Query, to decouple single line query into\nseparable point queries which could intuitively improve the localization\naccuracy for regression tasks; 2) A dynamic queries based progressive line\nregression approach to progressively regressing points on the line which\nfurther enhances localization accuracy for distorted tables; 3) A\nprior-enhanced matching strategy to solve the slow convergence issue of DETR.\nAfter separation line prediction, a simple relation network based cell merging\nmodule is used to recover spanning cells. With these new techniques, our\nTSRFormer achieves state-of-the-art performance on several benchmark datasets,\nincluding SciTSR, PubTabNet, WTW and FinTabNet. Furthermore, we have validated\nthe robustness and high localization accuracy of our approach to tables with\ncomplex structures, borderless cells, large blank spaces, empty or spanning\ncells as well as distorted or even curved shapes on a more challenging\nreal-world in-house dataset.\n","authors":["Jiawei Wang","Weihong Lin","Chixiang Ma","Mingze Li","Zheng Sun","Lei Sun","Qiang Huo"],"pdf_url":"https://arxiv.org/pdf/2303.11615v1.pdf","comment":"18 pages, 11 figures, Preprint. arXiv admin note: substantial text\n  overlap with arXiv:2208.04921"},{"id":"http://arxiv.org/abs/2303.11611v1","updated":"2023-03-21T06:10:47Z","published":"2023-03-21T06:10:47Z","title":"Model Robustness Meets Data Privacy: Adversarial Robustness Distillation\n  without Original Data","summary":"  Large-scale deep learning models have achieved great performance based on\nlarge-scale datasets. Moreover, the existing Adversarial Training (AT) can\nfurther improve the robustness of these large models. However, these large\nmodels are difficult to deploy to mobile devices, and the effect of AT on small\nmodels is very limited. In addition, the data privacy issue (e.g., face data\nand diagnosis report) may lead to the original data being unavailable, which\nrelies on data-free knowledge distillation technology for training. To tackle\nthese issues, we propose a challenging novel task called Data-Free Adversarial\nRobustness Distillation (DFARD), which tries to train small, easily deployable,\nrobust models without relying on the original data. We find the combination of\nexisting techniques resulted in degraded model performance due to fixed\ntraining objectives and scarce information content. First, an interactive\nstrategy is designed for more efficient knowledge transfer to find more\nsuitable training objectives at each epoch. Then, we explore an adaptive\nbalance method to suppress information loss and obtain more data information\nthan previous methods. Experiments show that our method improves baseline\nperformance on the novel task.\n","authors":["Yuzheng Wang","Zhaoyu Chen","Dingkang Yang","Pinxue Guo","Kaixun Jiang","Wenqiang Zhang","Lizhe Qi"],"pdf_url":"https://arxiv.org/pdf/2303.11611v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11610v1","updated":"2023-03-21T06:10:39Z","published":"2023-03-21T06:10:39Z","title":"Novel Class Discovery for 3D Point Cloud Semantic Segmentation","summary":"  Novel class discovery (NCD) for semantic segmentation is the task of learning\na model that can segment unlabelled (novel) classes using only the supervision\nfrom labelled (base) classes. This problem has recently been pioneered for 2D\nimage data, but no work exists for 3D point cloud data. In fact, the\nassumptions made for 2D are loosely applicable to 3D in this case. This paper\nis presented to advance the state of the art on point cloud data analysis in\nfour directions. Firstly, we address the new problem of NCD for point cloud\nsemantic segmentation. Secondly, we show that the transposition of the only\nexisting NCD method for 2D semantic segmentation to 3D data is suboptimal.\nThirdly, we present a new method for NCD based on online clustering that\nexploits uncertainty quantification to produce prototypes for pseudo-labelling\nthe points of the novel classes. Lastly, we introduce a new evaluation protocol\nto assess the performance of NCD for point cloud semantic segmentation. We\nthoroughly evaluate our method on SemanticKITTI and SemanticPOSS datasets,\nshowing that it can significantly outperform the baseline. Project page at this\nlink: https://github.com/LuigiRiz/NOPS.\n","authors":["Luigi Riz","Cristiano Saltori","Elisa Ricci","Fabio Poiesi"],"pdf_url":"https://arxiv.org/pdf/2303.11610v1.pdf","comment":"Paper accepted at CVPR 2023"},{"id":"http://arxiv.org/abs/2111.08913v2","updated":"2023-03-21T06:07:43Z","published":"2021-11-17T05:44:39Z","title":"Hierarchical Knowledge Guided Learning for Real-world Retinal Diseases\n  Recognition","summary":"  In the real world, medical datasets often exhibit a long-tailed data\ndistribution (i.e., a few classes occupy the majority of the data, while most\nclasses have only a limited number of samples), which results in a challenging\nlong-tailed learning scenario. Some recently published datasets in\nophthalmology AI consist of more than 40 kinds of retinal diseases with complex\nabnormalities and variable morbidity. Nevertheless, more than 30 conditions are\nrarely seen in global patient cohorts. From a modeling perspective, most deep\nlearning models trained on these datasets may lack the ability to generalize to\nrare diseases where only a few available samples are presented for training. In\naddition, there may be more than one disease for the presence of the retina,\nresulting in a challenging label co-occurrence scenario, also known as\n\\textit{multi-label}, which can cause problems when some re-sampling strategies\nare applied during training. To address the above two major challenges, this\npaper presents a novel method that enables the deep neural network to learn\nfrom a long-tailed fundus database for various retinal disease recognition.\nFirstly, we exploit the prior knowledge in ophthalmology to improve the feature\nrepresentation using a hierarchy-aware pre-training. Secondly, we adopt an\ninstance-wise class-balanced sampling strategy to address the label\nco-occurrence issue under the long-tailed medical dataset scenario. Thirdly, we\nintroduce a novel hybrid knowledge distillation to train a less biased\nrepresentation and classifier. We conducted extensive experiments on four\ndatabases, including two public datasets and two in-house databases with more\nthan one million fundus images. The experimental results demonstrate the\nsuperiority of our proposed methods with recognition accuracy outperforming the\nstate-of-the-art competitors, especially for these rare diseases.\n","authors":["Lie Ju","Zhen Yu","Lin Wang","Xin Zhao","Xin Wang","Paul Bonnington","Zongyuan Ge"],"pdf_url":"https://arxiv.org/pdf/2111.08913v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11606v1","updated":"2023-03-21T05:56:53Z","published":"2023-03-21T05:56:53Z","title":"CAFS: Class Adaptive Framework for Semi-Supervised Semantic Segmentation","summary":"  Semi-supervised semantic segmentation learns a model for classifying pixels\ninto specific classes using a few labeled samples and numerous unlabeled\nimages. The recent leading approach is consistency regularization by\nselftraining with pseudo-labeling pixels having high confidences for unlabeled\nimages. However, using only highconfidence pixels for self-training may result\nin losing much of the information in the unlabeled datasets due to poor\nconfidence calibration of modern deep learning networks. In this paper, we\npropose a class-adaptive semisupervision framework for semi-supervised semantic\nsegmentation (CAFS) to cope with the loss of most information that occurs in\nexisting high-confidence-based pseudolabeling methods. Unlike existing\nsemi-supervised semantic segmentation frameworks, CAFS constructs a validation\nset on a labeled dataset, to leverage the calibration performance for each\nclass. On this basis, we propose a calibration aware class-wise adaptive\nthresholding and classwise adaptive oversampling using the analysis results\nfrom the validation set. Our proposed CAFS achieves state-ofthe-art performance\non the full data partition of the base PASCAL VOC 2012 dataset and on the 1/4\ndata partition of the Cityscapes dataset with significant margins of 83.0% and\n80.4%, respectively. The code is available at https://github.com/cjf8899/CAFS.\n","authors":["Jingi Ju","Hyeoncheol Noh","Yooseung Wang","Minseok Seo","Dong-Geol Choi"],"pdf_url":"https://arxiv.org/pdf/2303.11606v1.pdf","comment":"13 pages, 9 figures"},{"id":"http://arxiv.org/abs/2303.09026v3","updated":"2023-03-21T05:54:28Z","published":"2023-03-16T01:39:11Z","title":"Commonsense Knowledge Assisted Deep Learning for Resource-constrained\n  and Fine-grained Object Detection","summary":"  In this paper, we consider fine-grained image object detection in\nresource-constrained cases such as edge computing. Deep learning (DL), namely\nlearning with deep neural networks (DNNs), has become the dominating approach\nto object detection. To achieve accurate fine-grained detection, one needs to\nemploy a large enough DNN model and a vast amount of data annotations, which\nbrings a challenge for using modern DL object detectors in resource-constrained\ncases. To this end, we propose an approach, which leverages commonsense\nknowledge to assist a coarse-grained object detector to get accurate\nfine-grained detection results. Specifically, we introduce a commonsense\nknowledge inference module (CKIM) to translate coarse-grained labels given by a\nbackbone lightweight coarse-grained DL detector to fine-grained labels. We\nconsider both crisp-rule and fuzzy-rule based inference in our CKIM; the latter\nis used to handle ambiguity in the target semantic labels. We implement our\nmethod based on several modern DL detectors, namely YOLOv4, Mobilenetv3-SSD and\nYOLOv7-tiny. Experiment results show that our approach outperforms benchmark\ndetectors remarkably in terms of accuracy, model size and processing latency.\n","authors":["Pu Zhang","Bin Liu"],"pdf_url":"https://arxiv.org/pdf/2303.09026v3.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2303.11332v1","updated":"2023-03-21T05:50:53Z","published":"2023-03-21T05:50:53Z","title":"Deep Learning for Video-based Person Re-Identification: A Survey","summary":"  Video-based person re-identification (video re-ID) has lately fascinated\ngrowing attention due to its broad practical applications in various areas,\nsuch as surveillance, smart city, and public safety. Nevertheless, video re-ID\nis quite difficult and is an ongoing stage due to numerous uncertain challenges\nsuch as viewpoint, occlusion, pose variation, and uncertain video sequence,\netc. In the last couple of years, deep learning on video re-ID has continuously\nachieved surprising results on public datasets, with various approaches being\ndeveloped to handle diverse problems in video re-ID. Compared to image-based\nre-ID, video re-ID is much more challenging and complex. To encourage future\nresearch and challenges, this first comprehensive paper introduces a review of\nup-to-date advancements in deep learning approaches for video re-ID. It broadly\ncovers three important aspects, including brief video re-ID methods with their\nlimitations, major milestones with technical challenges, and architectural\ndesign. It offers comparative performance analysis on various available\ndatasets, guidance to improve video re-ID with valuable thoughts, and exciting\nresearch directions.\n","authors":["Khawar Islam"],"pdf_url":"https://arxiv.org/pdf/2303.11332v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2303.11599v1","updated":"2023-03-21T05:34:04Z","published":"2023-03-21T05:34:04Z","title":"Low-complexity Deep Video Compression with A Distributed Coding\n  Architecture","summary":"  Prevalent predictive coding-based video compression methods rely on a heavy\nencoder to reduce the temporal redundancy, which makes it challenging to deploy\nthem on resource-constrained devices. Meanwhile, as early as the 1970s,\ndistributed source coding theory has indicated that independent encoding and\njoint decoding with side information (SI) can achieve high-efficient\ncompression of correlated sources. This has inspired a distributed coding\narchitecture aiming at reducing the encoding complexity. However, traditional\ndistributed coding methods suffer from a substantial performance gap to\npredictive coding ones. Inspired by the great success of learning-based\ncompression, we propose the first end-to-end distributed deep video compression\nframework to improve the rate-distortion performance. A key ingredient is an\neffective SI generation module at the decoder, which helps to effectively\nexploit inter-frame correlations without computation-intensive encoder-side\nmotion estimation and compensation. Experiments show that our method\nsignificantly outperforms conventional distributed video coding and H.264.\nMeanwhile, it enjoys 6-7x encoding speedup against DVC [1] with comparable\ncompression performance. Code is released at\nhttps://github.com/Xinjie-Q/Distributed-DVC.\n","authors":["Xinjie Zhang","Jiawei Shao","Jun Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.11599v1.pdf","comment":"Accepted by ICME 2023"},{"id":"http://arxiv.org/abs/2211.09778v3","updated":"2023-03-21T04:54:55Z","published":"2022-11-17T18:52:19Z","title":"I Can't Believe There's No Images! Learning Visual Tasks Using only\n  Language Data","summary":"  Many high-level skills that are required for computer vision tasks, such as\nparsing questions, comparing and contrasting semantics, and writing\ndescriptions, are also required in other domains such as natural language\nprocessing. In this paper, we ask whether it is possible to learn those skills\nfrom textual data and then transfer them to vision tasks without ever training\non visual training data. Key to our approach is exploiting the joint embedding\nspace of contrastively trained vision and language encoders. In practice, there\ncan be systematic differences between embedding spaces for different modalities\nin contrastive models, and we analyze how these differences affect our approach\nand study strategies to mitigate this concern. We produce models using only\ntext training data on four representative tasks: image captioning, visual\nentailment, visual question answering and visual news, and evaluate them on\nstandard benchmarks using images. We find these models generally perform close\nto models trained on images, while surpassing prior work for captioning and\nvisual entailment in this text only setting by over 9 points, and outperforming\nall prior work on visual news by over 30 points. We also showcase a variety of\nstylistic image captioning models that are trained using no image data and no\nhuman-curated language data, but instead using readily-available text data from\nbooks, the web, or language models.\n","authors":["Sophia Gu","Christopher Clark","Aniruddha Kembhavi"],"pdf_url":"https://arxiv.org/pdf/2211.09778v3.pdf","comment":"website (https://prior.allenai.org/projects/close), code\n  (https://github.com/allenai/close)"},{"id":"http://arxiv.org/abs/2303.11592v1","updated":"2023-03-21T04:42:44Z","published":"2023-03-21T04:42:44Z","title":"Lightweight Hybrid Video Compression Framework Using Reference-Guided\n  Restoration Network","summary":"  Recent deep-learning-based video compression methods brought coding gains\nover conventional codecs such as AVC and HEVC. However, learning-based codecs\ngenerally require considerable computation time and model complexity. In this\npaper, we propose a new lightweight hybrid video codec consisting of a\nconventional video codec(HEVC / VVC), a lossless image codec, and our new\nrestoration network. Precisely, our encoder consists of the conventional video\nencoder and a lossless image encoder, transmitting a lossy-compressed video\nbitstream along with a losslessly-compressed reference frame. The decoder is\nconstructed with corresponding video/image decoders and a new restoration\nnetwork, which enhances the compressed video in two-step processes. In the\nfirst step, a network trained with a large video dataset restores the details\nlost by the conventional encoder. Then, we further boost the video quality with\nthe guidance of a reference image, which is a losslessly compressed video\nframe. The reference image provides video-specific information, which can be\nutilized to better restore the details of a compressed video. Experimental\nresults show that the proposed method achieves comparable performance to\ntop-tier methods, even when applied to HEVC. Nevertheless, our method has lower\ncomplexity, a faster run time, and can be easily integrated into existing\nconventional codecs.\n","authors":["Hochang Rhee","Seyun Kim","Nam Ik Cho"],"pdf_url":"https://arxiv.org/pdf/2303.11592v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11591v1","updated":"2023-03-21T04:42:39Z","published":"2023-03-21T04:42:39Z","title":"SVCNet: Scribble-based Video Colorization Network with Temporal\n  Aggregation","summary":"  In this paper, we propose a scribble-based video colorization network with\ntemporal aggregation called SVCNet. It can colorize monochrome videos based on\ndifferent user-given color scribbles. It addresses three common issues in the\nscribble-based video colorization area: colorization vividness, temporal\nconsistency, and color bleeding. To improve the colorization quality and\nstrengthen the temporal consistency, we adopt two sequential sub-networks in\nSVCNet for precise colorization and temporal smoothing, respectively. The first\nstage includes a pyramid feature encoder to incorporate color scribbles with a\ngrayscale frame, and a semantic feature encoder to extract semantics. The\nsecond stage finetunes the output from the first stage by aggregating the\ninformation of neighboring colorized frames (as short-range connections) and\nthe first colorized frame (as a long-range connection). To alleviate the color\nbleeding artifacts, we learn video colorization and segmentation\nsimultaneously. Furthermore, we set the majority of operations on a fixed small\nimage resolution and use a Super-resolution Module at the tail of SVCNet to\nrecover original sizes. It allows the SVCNet to fit different image resolutions\nat the inference. Finally, we evaluate the proposed SVCNet on DAVIS and Videvo\nbenchmarks. The experimental results demonstrate that SVCNet produces both\nhigher-quality and more temporally consistent videos than other well-known\nvideo colorization approaches. The codes and models can be found at\nhttps://github.com/zhaoyuzhi/SVCNet.\n","authors":["Yuzhi Zhao","Lai-Man Po","Kangcheng Liu","Xuehui Wang","Wing-Yin Yu","Pengfei Xian","Yujia Zhang","Mengyang Liu"],"pdf_url":"https://arxiv.org/pdf/2303.11591v1.pdf","comment":"under revision of IEEE Transactions on Image Processing"},{"id":"http://arxiv.org/abs/2303.11589v1","updated":"2023-03-21T04:41:02Z","published":"2023-03-21T04:41:02Z","title":"LayoutDiffusion: Improving Graphic Layout Generation by Discrete\n  Diffusion Probabilistic Models","summary":"  Creating graphic layouts is a fundamental step in graphic designs. In this\nwork, we present a novel generative model named LayoutDiffusion for automatic\nlayout generation. As layout is typically represented as a sequence of discrete\ntokens, LayoutDiffusion models layout generation as a discrete denoising\ndiffusion process. It learns to reverse a mild forward process, in which\nlayouts become increasingly chaotic with the growth of forward steps and\nlayouts in the neighboring steps do not differ too much. Designing such a mild\nforward process is however very challenging as layout has both categorical\nattributes and ordinal attributes. To tackle the challenge, we summarize three\ncritical factors for achieving a mild forward process for the layout, i.e.,\nlegality, coordinate proximity and type disruption. Based on the factors, we\npropose a block-wise transition matrix coupled with a piece-wise linear noise\nschedule. Experiments on RICO and PubLayNet datasets show that LayoutDiffusion\noutperforms state-of-the-art approaches significantly. Moreover, it enables two\nconditional layout generation tasks in a plug-and-play manner without\nre-training and achieves better performance than existing methods.\n","authors":["Junyi Zhang","Jiaqi Guo","Shizhao Sun","Jian-Guang Lou","Dongmei Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.11589v1.pdf","comment":"27 pages, 20 figures"},{"id":"http://arxiv.org/abs/2206.08920v5","updated":"2023-03-21T04:38:27Z","published":"2022-06-17T17:57:13Z","title":"VectorMapNet: End-to-end Vectorized HD Map Learning","summary":"  Autonomous driving systems require a good understanding of surrounding\nenvironments, including moving obstacles and static High-Definition (HD)\nsemantic map elements. Existing methods approach the semantic map problem by\noffline manual annotation, which suffers from serious scalability issues.\nRecent learning-based methods produce dense rasterized segmentation predictions\nto construct maps. However, these predictions do not include instance\ninformation of individual map elements and require heuristic post-processing to\nobtain vectorized maps. To tackle these challenges, we introduce an end-to-end\nvectorized HD map learning pipeline, termed VectorMapNet. VectorMapNet takes\nonboard sensor observations and predicts a sparse set of polylines in the\nbird's-eye view. This pipeline can explicitly model the spatial relation\nbetween map elements and generate vectorized maps that are friendly to\ndownstream autonomous driving tasks. Extensive experiments show that\nVectorMapNet achieve strong map learning performance on both nuScenes and\nArgoverse2 dataset, surpassing previous state-of-the-art methods by 14.2 mAP\nand 14.6mAP. Qualitatively, we also show that VectorMapNet is capable of\ngenerating comprehensive maps and capturing more fine-grained details of road\ngeometry. To the best of our knowledge, VectorMapNet is the first work designed\ntowards end-to-end vectorized map learning from onboard observations. Our\nproject website is available at\nhttps://tsinghua-mars-lab.github.io/vectormapnet/.\n","authors":["Yicheng Liu","Yuantian Yuan","Yue Wang","Yilun Wang","Hang Zhao"],"pdf_url":"https://arxiv.org/pdf/2206.08920v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12237v1","updated":"2023-03-21T23:44:02Z","published":"2023-03-21T23:44:02Z","title":"Automated deep learning segmentation of high-resolution 7 T ex vivo MRI\n  for quantitative analysis of structure-pathology correlations in\n  neurodegenerative diseases","summary":"  Ex vivo MRI of the brain provides remarkable advantages over in vivo MRI for\nvisualizing and characterizing detailed neuroanatomy, and helps to link\nmicroscale histology studies with morphometric measurements. However, automated\nsegmentation methods for brain mapping in ex vivo MRI are not well developed,\nprimarily due to limited availability of labeled datasets, and heterogeneity in\nscanner hardware and acquisition protocols. In this work, we present a high\nresolution dataset of 37 ex vivo post-mortem human brain tissue specimens\nscanned on a 7T whole-body MRI scanner. We developed a deep learning pipeline\nto segment the cortical mantle by benchmarking the performance of nine deep\nneural architectures. We then segment the four subcortical structures: caudate,\nputamen, globus pallidus, and thalamus; white matter hyperintensities, and the\nnormal appearing white matter. We show excellent generalizing capabilities\nacross whole brain hemispheres in different specimens, and also on unseen\nimages acquired at different magnetic field strengths and different imaging\nsequence. We then compute volumetric and localized cortical thickness\nmeasurements across key regions, and link them with semi-quantitative\nneuropathological ratings. Our code, containerized executables, and the\nprocessed datasets are publicly available at:\nhttps://github.com/Pulkit-Khandelwal/upenn-picsl-brain-ex-vivo.\n","authors":["Pulkit Khandelwal","Michael Tran Duong","Shokufeh Sadaghiani","Sydney Lim","Amanda Denning","Eunice Chung","Sadhana Ravikumar","Sanaz Arezoumandan","Claire Peterson","Madigan Bedard","Noah Capp","Ranjit Ittyerah","Elyse Migdal","Grace Choi","Emily Kopp","Bridget Loja","Eusha Hasan","Jiacheng Li","Karthik Prabhakaran","Gabor Mizsei","Marianna Gabrielyan","Theresa Schuck","Winifred Trotman","John Robinson","Daniel Ohm","Edward B. Lee","John Q. Trojanowski","Corey McMillan","Murray Grossman","David J. Irwin","John Detre","M. Dylan Tisdall","Sandhitsu R. Das","Laura E. M. Wisse","David A. Wolk","Paul A. Yushkevich"],"pdf_url":"https://arxiv.org/pdf/2303.12237v1.pdf","comment":"Preprint submitted to NeuroImage Project website:\n  https://github.com/Pulkit-Khandelwal/upenn-picsl-brain-ex-vivo"},{"id":"http://arxiv.org/abs/2303.12236v1","updated":"2023-03-21T23:43:58Z","published":"2023-03-21T23:43:58Z","title":"SALAD: Part-Level Latent Diffusion for 3D Shape Generation and\n  Manipulation","summary":"  We present a cascaded diffusion model based on a part-level implicit 3D\nrepresentation. Our model achieves state-of-the-art generation quality and also\nenables part-level shape editing and manipulation without any additional\ntraining in conditional setup. Diffusion models have demonstrated impressive\ncapabilities in data generation as well as zero-shot completion and editing via\na guided reverse process. Recent research on 3D diffusion models has focused on\nimproving their generation capabilities with various data representations,\nwhile the absence of structural information has limited their capability in\ncompletion and editing tasks. We thus propose our novel diffusion model using a\npart-level implicit representation. To effectively learn diffusion with\nhigh-dimensional embedding vectors of parts, we propose a cascaded framework,\nlearning diffusion first on a low-dimensional subspace encoding extrinsic\nparameters of parts and then on the other high-dimensional subspace encoding\nintrinsic attributes. In the experiments, we demonstrate the outperformance of\nour method compared with the previous ones both in generation and part-level\ncompletion and manipulation tasks.\n","authors":["Juil Koo","Seungwoo Yoo","Minh Hieu Nguyen","Minhyuk Sung"],"pdf_url":"https://arxiv.org/pdf/2303.12236v1.pdf","comment":"Project page: https://salad3d.github.io"},{"id":"http://arxiv.org/abs/2303.12234v1","updated":"2023-03-21T23:29:38Z","published":"2023-03-21T23:29:38Z","title":"Pre-NeRF 360: Enriching Unbounded Appearances for Neural Radiance Fields","summary":"  Neural radiance fields (NeRF) appeared recently as a powerful tool to\ngenerate realistic views of objects and confined areas. Still, they face\nserious challenges with open scenes, where the camera has unrestricted movement\nand content can appear at any distance. In such scenarios, current\nNeRF-inspired models frequently yield hazy or pixelated outputs, suffer slow\ntraining times, and might display irregularities, because of the challenging\ntask of reconstructing an extensive scene from a limited number of images. We\npropose a new framework to boost the performance of NeRF-based architectures\nyielding significantly superior outcomes compared to the prior work. Our\nsolution overcomes several obstacles that plagued earlier versions of NeRF,\nincluding handling multiple video inputs, selecting keyframes, and extracting\nposes from real-world frames that are ambiguous and symmetrical. Furthermore,\nwe applied our framework, dubbed as \"Pre-NeRF 360\", to enable the use of the\nNutrition5k dataset in NeRF and introduce an updated version of this dataset,\nknown as the N5k360 dataset.\n","authors":["Ahmad AlMughrabi","Umair Haroon","Ricardo Marques","Petia Radeva"],"pdf_url":"https://arxiv.org/pdf/2303.12234v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12218v1","updated":"2023-03-21T22:37:16Z","published":"2023-03-21T22:37:16Z","title":"Compositional 3D Scene Generation using Locally Conditioned Diffusion","summary":"  Designing complex 3D scenes has been a tedious, manual process requiring\ndomain expertise. Emerging text-to-3D generative models show great promise for\nmaking this task more intuitive, but existing approaches are limited to\nobject-level generation. We introduce \\textbf{locally conditioned diffusion} as\nan approach to compositional scene diffusion, providing control over semantic\nparts using text prompts and bounding boxes while ensuring seamless transitions\nbetween these parts. We demonstrate a score distillation sampling--based\ntext-to-3D synthesis pipeline that enables compositional 3D scene generation at\na higher fidelity than relevant baselines.\n","authors":["Ryan Po","Gordon Wetzstein"],"pdf_url":"https://arxiv.org/pdf/2303.12218v1.pdf","comment":"For project page, see https://ryanpo.com/comp3d/"},{"id":"http://arxiv.org/abs/2302.05361v2","updated":"2023-03-21T22:37:15Z","published":"2023-02-10T16:21:07Z","title":"Leveraging Inpainting for Single-Image Shadow Removal","summary":"  Fully-supervised shadow removal methods achieve the best restoration\nqualities on public datasets but still generate some shadow remnants. One of\nthe reasons is the lack of large-scale shadow & shadow-free image pairs.\nUnsupervised methods can alleviate the issue but their restoration qualities\nare much lower than those of fully-supervised methods. In this work, we find\nthat pretraining shadow removal networks on the image inpainting dataset can\nreduce the shadow remnants significantly: a naive encoder-decoder network gets\ncompetitive restoration quality w.r.t. the state-of-the-art methods via only\n10% shadow & shadow-free image pairs. After analyzing networks with/without\ninpainting pre-training via the information stored in the weight (IIW), we find\nthat inpainting pretraining improves restoration quality in non-shadow regions\nand enhances the generalization ability of networks significantly.\nAdditionally, shadow removal fine-tuning enables networks to fill in the\ndetails of shadow regions. Inspired by these observations we formulate shadow\nremoval as an adaptive fusion task that takes advantage of both shadow removal\nand image inpainting. Specifically, we develop an adaptive fusion network\nconsisting of two encoders, an adaptive fusion block, and a decoder. The two\nencoders are responsible for extracting the feature from the shadow image and\nthe shadow-masked image respectively. The adaptive fusion block is responsible\nfor combining these features in an adaptive manner. Finally, the decoder\nconverts the adaptive fused features to the desired shadow-free result. The\nextensive experiments show that our method empowered with inpainting\noutperforms all state-of-the-art methods.\n","authors":["Xiaoguang Li","Qing Guo","Rabab Abdelfattah","Di Lin","Wei Feng","Ivor Tsang","Song Wang"],"pdf_url":"https://arxiv.org/pdf/2302.05361v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12217v1","updated":"2023-03-21T22:35:04Z","published":"2023-03-21T22:35:04Z","title":"Image Reconstruction without Explicit Priors","summary":"  We consider solving ill-posed imaging inverse problems without access to an\nexplicit image prior or ground-truth examples. An overarching challenge in\ninverse problems is that there are many undesired images that fit to the\nobserved measurements, thus requiring image priors to constrain the space of\npossible solutions to more plausible reconstructions. However, in many\napplications it is difficult or potentially impossible to obtain ground-truth\nimages to learn an image prior. Thus, inaccurate priors are often used, which\ninevitably result in biased solutions. Rather than solving an inverse problem\nusing priors that encode the explicit structure of any one image, we propose to\nsolve a set of inverse problems jointly by incorporating prior constraints on\nthe collective structure of the underlying images.The key assumption of our\nwork is that the ground-truth images we aim to reconstruct share common,\nlow-dimensional structure. We show that such a set of inverse problems can be\nsolved simultaneously by learning a shared image generator with a\nlow-dimensional latent space. The parameters of the generator and latent\nembedding are learned by maximizing a proxy for the Evidence Lower Bound\n(ELBO). Once learned, the generator and latent embeddings can be combined to\nprovide reconstructions for each inverse problem. The framework we propose can\nhandle general forward model corruptions, and we show that measurements derived\nfrom only a few ground-truth images (O(10)) are sufficient for image\nreconstruction without explicit priors.\n","authors":["Angela F. Gao","Oscar Leong","He Sun","Katherine L. Bouman"],"pdf_url":"https://arxiv.org/pdf/2303.12217v1.pdf","comment":"ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.12214v1","updated":"2023-03-21T22:24:27Z","published":"2023-03-21T22:24:27Z","title":"Prompt-MIL: Boosting Multi-Instance Learning Schemes via Task-specific\n  Prompt Tuning","summary":"  Whole slide image (WSI) classification is a critical task in computational\npathology, requiring the processing of gigapixel-sized images, which is\nchallenging for current deep-learning methods. Current state of the art methods\nare based on multi-instance learning schemes (MIL), which usually rely on\npretrained features to represent the instances. Due to the lack of\ntask-specific annotated data, these features are either obtained from\nwell-established backbones on natural images, or, more recently from\nself-supervised models pretrained on histopathology. However, both approaches\nyield task-agnostic features, resulting in performance loss compared to the\nappropriate task-related supervision, if available. In this paper, we show that\nwhen task-specific annotations are limited, we can inject such supervision into\ndownstream task training, to reduce the gap between fully task-tuned and task\nagnostic features. We propose Prompt-MIL, an MIL framework that integrates\nprompts into WSI classification. Prompt-MIL adopts a prompt tuning mechanism,\nwhere only a small fraction of parameters calibrates the pretrained features to\nencode task-specific information, rather than the conventional full fine-tuning\napproaches. Extensive experiments on three WSI datasets, TCGA-BRCA, TCGA-CRC,\nand BRIGHT, demonstrate the superiority of Prompt-MIL over conventional MIL\nmethods, achieving a relative improvement of 1.49%-4.03% in accuracy and\n0.25%-8.97% in AUROC while using fewer than 0.3% additional parameters.\nCompared to conventional full fine-tuning approaches, we fine-tune less than\n1.3% of the parameters, yet achieve a relative improvement of 1.29%-13.61% in\naccuracy and 3.22%-27.18% in AUROC and reduce GPU memory consumption by 38%-45%\nwhile training 21%-27% faster.\n","authors":["Jingwei Zhang","Saarthak Kapse","Ke Ma","Prateek Prasanna","Joel Saltz","Maria Vakalopoulou","Dimitris Samaras"],"pdf_url":"https://arxiv.org/pdf/2303.12214v1.pdf","comment":"Submitted to MICCAI 2023"},{"id":"http://arxiv.org/abs/2303.12208v1","updated":"2023-03-21T21:49:39Z","published":"2023-03-21T21:49:39Z","title":"MAGVLT: Masked Generative Vision-and-Language Transformer","summary":"  While generative modeling on multimodal image-text data has been actively\ndeveloped with large-scale paired datasets, there have been limited attempts to\ngenerate both image and text data by a single model rather than a generation of\none fixed modality conditioned on the other modality. In this paper, we explore\na unified generative vision-and-language (VL) model that can produce both\nimages and text sequences. Especially, we propose a generative VL transformer\nbased on the non-autoregressive mask prediction, named MAGVLT, and compare it\nwith an autoregressive generative VL transformer (ARGVLT). In comparison to\nARGVLT, the proposed MAGVLT enables bidirectional context encoding, fast\ndecoding by parallel token predictions in an iterative refinement, and extended\nediting capabilities such as image and text infilling. For rigorous training of\nour MAGVLT with image-text pairs from scratch, we combine the image-to-text,\ntext-to-image, and joint image-and-text mask prediction tasks. Moreover, we\ndevise two additional tasks based on the step-unrolled mask prediction and the\nselective prediction on the mixture of two image-text pairs. Experimental\nresults on various downstream generation tasks of VL benchmarks show that our\nMAGVLT outperforms ARGVLT by a large margin even with significant inference\nspeedup. Particularly, MAGVLT achieves competitive results on both zero-shot\nimage-to-text and text-to-image generation tasks from MS-COCO by one\nmoderate-sized model (fewer than 500M parameters) even without the use of\nmonomodal data and networks.\n","authors":["Sungwoong Kim","Daejin Jo","Donghoon Lee","Jongmin Kim"],"pdf_url":"https://arxiv.org/pdf/2303.12208v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.08241v2","updated":"2023-03-21T21:24:53Z","published":"2023-03-14T21:22:26Z","title":"Subspace Perturbation Analysis for Data-Driven Radar Target Localization","summary":"  Recent works exploring data-driven approaches to classical problems in\nadaptive radar have demonstrated promising results pertaining to the task of\nradar target localization. Via the use of space-time adaptive processing (STAP)\ntechniques and convolutional neural networks, these data-driven approaches to\ntarget localization have helped benchmark the performance of neural networks\nfor matched scenarios. However, the thorough bridging of these topics across\nmismatched scenarios still remains an open problem. As such, in this work, we\naugment our data-driven approach to radar target localization by performing a\nsubspace perturbation analysis, which allows us to benchmark the localization\naccuracy of our proposed deep learning framework across mismatched scenarios.\nTo evaluate this framework, we generate comprehensive datasets by randomly\nplacing targets of variable strengths in mismatched constrained areas via\nRFView, a high-fidelity, site-specific modeling and simulation tool. For the\nradar returns from these constrained areas, we generate heatmap tensors in\nrange, azimuth, and elevation using the normalized adaptive matched filter\n(NAMF) test statistic. We estimate target locations from these heatmap tensors\nusing a convolutional neural network, and demonstrate that the predictive\nperformance of our framework in the presence of mismatches can be\npredetermined.\n","authors":["Shyam Venkatasubramanian","Sandeep Gogineni","Bosung Kang","Ali Pezeshki","Muralidhar Rangaswamy","Vahid Tarokh"],"pdf_url":"https://arxiv.org/pdf/2303.08241v2.pdf","comment":"6 pages, 3 figures. Submitted to 2023 IEEE Radar Conference\n  (RadarConf). Extension of arXiv:2209.02890"},{"id":"http://arxiv.org/abs/2303.12198v1","updated":"2023-03-21T21:10:35Z","published":"2023-03-21T21:10:35Z","title":"Autofluorescence Bronchoscopy Video Analysis for Lesion Frame Detection","summary":"  Because of the significance of bronchial lesions as indicators of early lung\ncancer and squamous cell carcinoma, a critical need exists for early detection\nof bronchial lesions. Autofluorescence bronchoscopy (AFB) is a primary modality\nused for bronchial lesion detection, as it shows high sensitivity to suspicious\nlesions. The physician, however, must interactively browse a long video stream\nto locate lesions, making the search exceedingly tedious and error prone.\nUnfortunately, limited research has explored the use of automated AFB video\nanalysis for efficient lesion detection. We propose a robust automatic AFB\nanalysis approach that distinguishes informative and uninformative AFB video\nframes in a video. In addition, for the informative frames, we determine the\nframes containing potential lesions and delineate candidate lesion regions. Our\napproach draws upon a combination of computer-based image analysis, machine\nlearning, and deep learning. Thus, the analysis of an AFB video stream becomes\nmore tractable. Tests with patient AFB video indicate that $\\ge$97\\% of frames\nwere correctly labeled as informative or uninformative. In addition, $\\ge$97\\%\nof lesion frames were correctly identified, with false positive and false\nnegative rates $\\le$3\\%.\n","authors":["Qi Chang","Rebecca Bascom","Jennifer Toth","Danish Ahmad","William E. Higgins"],"pdf_url":"https://arxiv.org/pdf/2303.12198v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12194v1","updated":"2023-03-21T20:52:02Z","published":"2023-03-21T20:52:02Z","title":"LiDARFormer: A Unified Transformer-based Multi-task Network for LiDAR\n  Perception","summary":"  There is a recent trend in the LiDAR perception field towards unifying\nmultiple tasks in a single strong network with improved performance, as opposed\nto using separate networks for each task. In this paper, we introduce a new\nLiDAR multi-task learning paradigm based on the transformer. The proposed\nLiDARFormer utilizes cross-space global contextual feature information and\nexploits cross-task synergy to boost the performance of LiDAR perception tasks\nacross multiple large-scale datasets and benchmarks. Our novel\ntransformer-based framework includes a cross-space transformer module that\nlearns attentive features between the 2D dense Bird's Eye View (BEV) and 3D\nsparse voxel feature maps. Additionally, we propose a transformer decoder for\nthe segmentation task to dynamically adjust the learned features by leveraging\nthe categorical feature representations. Furthermore, we combine the\nsegmentation and detection features in a shared transformer decoder with\ncross-task attention layers to enhance and integrate the object-level and\nclass-level features. LiDARFormer is evaluated on the large-scale nuScenes and\nthe Waymo Open datasets for both 3D detection and semantic segmentation tasks,\nand it outperforms all previously published methods on both tasks. Notably,\nLiDARFormer achieves the state-of-the-art performance of 76.4% L2 mAPH and\n74.3% NDS on the challenging Waymo and nuScenes detection benchmarks for a\nsingle model LiDAR-only method.\n","authors":["Zixiang Zhou","Dongqiangzi Ye","Weijia Chen","Yufei Xie","Yu Wang","Panqu Wang","Hassan Foroosh"],"pdf_url":"https://arxiv.org/pdf/2303.12194v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.09385v2","updated":"2023-03-21T20:30:25Z","published":"2022-09-19T23:39:15Z","title":"LidarMultiNet: Towards a Unified Multi-Task Network for LiDAR Perception","summary":"  LiDAR-based 3D object detection, semantic segmentation, and panoptic\nsegmentation are usually implemented in specialized networks with distinctive\narchitectures that are difficult to adapt to each other. This paper presents\nLidarMultiNet, a LiDAR-based multi-task network that unifies these three major\nLiDAR perception tasks. Among its many benefits, a multi-task network can\nreduce the overall cost by sharing weights and computation among multiple\ntasks. However, it typically underperforms compared to independently combined\nsingle-task models. The proposed LidarMultiNet aims to bridge the performance\ngap between the multi-task network and multiple single-task networks. At the\ncore of LidarMultiNet is a strong 3D voxel-based encoder-decoder architecture\nwith a Global Context Pooling (GCP) module extracting global contextual\nfeatures from a LiDAR frame. Task-specific heads are added on top of the\nnetwork to perform the three LiDAR perception tasks. More tasks can be\nimplemented simply by adding new task-specific heads while introducing little\nadditional cost. A second stage is also proposed to refine the first-stage\nsegmentation and generate accurate panoptic segmentation results. LidarMultiNet\nis extensively tested on both Waymo Open Dataset and nuScenes dataset,\ndemonstrating for the first time that major LiDAR perception tasks can be\nunified in a single strong network that is trained end-to-end and achieves\nstate-of-the-art performance. Notably, LidarMultiNet reaches the official 1st\nplace in the Waymo Open Dataset 3D semantic segmentation challenge 2022 with\nthe highest mIoU and the best accuracy for most of the 22 classes on the test\nset, using only LiDAR points as input. It also sets the new state-of-the-art\nfor a single model on the Waymo 3D object detection benchmark and three\nnuScenes benchmarks.\n","authors":["Dongqiangzi Ye","Zixiang Zhou","Weijia Chen","Yufei Xie","Yu Wang","Panqu Wang","Hassan Foroosh"],"pdf_url":"https://arxiv.org/pdf/2209.09385v2.pdf","comment":"Accepted to AAAI 2023 (Oral). Full-length paper extending our\n  previous technical report of the 1st place solution of the 2022 Waymo Open\n  Dataset 3D Semantic Segmentation challenge, including evaluations on 5 major\n  benchmarks. arXiv admin note: text overlap with arXiv:2206.11428"},{"id":"http://arxiv.org/abs/2303.12175v1","updated":"2023-03-21T20:21:44Z","published":"2023-03-21T20:21:44Z","title":"Black-box Backdoor Defense via Zero-shot Image Purification","summary":"  Backdoor attacks inject poisoned data into the training set, resulting in\nmisclassification of the poisoned samples during model inference. Defending\nagainst such attacks is challenging, especially in real-world black-box\nsettings where only model predictions are available. In this paper, we propose\na novel backdoor defense framework that can effectively defend against various\nattacks through zero-shot image purification (ZIP). Our proposed framework can\nbe applied to black-box models without requiring any internal information about\nthe poisoned model or any prior knowledge of the clean/poisoned samples. Our\ndefense framework involves a two-step process. First, we apply a linear\ntransformation on the poisoned image to destroy the trigger pattern. Then, we\nuse a pre-trained diffusion model to recover the missing semantic information\nremoved by the transformation. In particular, we design a new reverse process\nusing the transformed image to guide the generation of high-fidelity purified\nimages, which can be applied in zero-shot settings. We evaluate our ZIP\nbackdoor defense framework on multiple datasets with different kinds of\nattacks. Experimental results demonstrate the superiority of our ZIP framework\ncompared to state-of-the-art backdoor defense baselines. We believe that our\nresults will provide valuable insights for future defense methods for black-box\nmodels.\n","authors":["Yucheng Shi","Mengnan Du","Xuansheng Wu","Zihan Guan","Ninghao Liu"],"pdf_url":"https://arxiv.org/pdf/2303.12175v1.pdf","comment":"11 pages, 2 figures"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2303.11991v1","updated":"2023-03-21T16:21:41Z","published":"2023-03-21T16:21:41Z","title":"Application of an ontology for model cards to generate computable\n  artifacts for linking machine learning information from biomedical research","summary":"  Model card reports provide a transparent description of machine learning\nmodels which includes information about their evaluation, limitations, intended\nuse, etc. Federal health agencies have expressed an interest in model cards\nreport for research studies using machine-learning based AI. Previously, we\nhave developed an ontology model for model card reports to structure and\nformalize these reports. In this paper, we demonstrate a Java-based library\n(OWL API, FaCT++) that leverages our ontology to publish computable model card\nreports. We discuss future directions and other use cases that highlight\napplicability and feasibility of ontology-driven systems to support FAIR\nchallenges.\n","authors":["Muhammad Amith","Licong Cui","Kirk Roberts","Cui Tao"],"pdf_url":"https://arxiv.org/pdf/2303.11991v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11916v1","updated":"2023-03-21T15:06:35Z","published":"2023-03-21T15:06:35Z","title":"CompoDiff: Versatile Composed Image Retrieval With Latent Diffusion","summary":"  This paper proposes a novel diffusion-based model, CompoDiff, for solving\nComposed Image Retrieval (CIR) with latent diffusion and presents a newly\ncreated dataset of 18 million reference images, conditions, and corresponding\ntarget image triplets to train the model. CompoDiff not only achieves a new\nzero-shot state-of-the-art on a CIR benchmark such as FashionIQ but also\nenables a more versatile CIR by accepting various conditions, such as negative\ntext and image mask conditions, which are unavailable with existing CIR\nmethods. In addition, the CompoDiff features are on the intact CLIP embedding\nspace so that they can be directly used for all existing models exploiting the\nCLIP space. The code and dataset used for the training, and the pre-trained\nweights are available at https://github.com/navervision/CompoDiff\n","authors":["Geonmo Gu","Sanghyuk Chun","Wonjae Kim","HeeJae Jun","Yoohoon Kang","Sangdoo Yun"],"pdf_url":"https://arxiv.org/pdf/2303.11916v1.pdf","comment":"First two authors contributed equally; 23 pages, 4.8MB"},{"id":"http://arxiv.org/abs/2303.11879v1","updated":"2023-03-21T14:23:46Z","published":"2023-03-21T14:23:46Z","title":"Multimodal Pre-training Framework for Sequential Recommendation via\n  Contrastive Learning","summary":"  Sequential recommendation systems utilize the sequential interactions of\nusers with items as their main supervision signals in learning users'\npreferences. However, existing methods usually generate unsatisfactory results\ndue to the sparsity of user behavior data. To address this issue, we propose a\nnovel pre-training framework, named Multimodal Sequence Mixup for Sequential\nRecommendation (MSM4SR), which leverages both users' sequential behaviors and\nitems' multimodal content (\\ie text and images) for effectively recommendation.\nSpecifically, MSM4SR tokenizes each item image into multiple textual keywords\nand uses the pre-trained BERT model to obtain initial textual and visual\nfeatures of items, for eliminating the discrepancy between the text and image\nmodalities. A novel backbone network, \\ie Multimodal Mixup Sequence Encoder\n(M$^2$SE), is proposed to bridge the gap between the item multimodal content\nand the user behavior, using a complementary sequence mixup strategy. In\naddition, two contrastive learning tasks are developed to assist M$^2$SE in\nlearning generalized multimodal representations of the user behavior sequence.\nExtensive experiments on real-world datasets demonstrate that MSM4SR\noutperforms state-of-the-art recommendation methods. Moreover, we further\nverify the effectiveness of MSM4SR on other challenging tasks including\ncold-start and cross-domain recommendation.\n","authors":["Lingzi Zhang","Xin Zhou","Zhiqi Shen"],"pdf_url":"https://arxiv.org/pdf/2303.11879v1.pdf","comment":"10 pages, 4 figures"},{"id":"http://arxiv.org/abs/2303.01593v2","updated":"2023-03-21T14:22:00Z","published":"2023-03-02T21:35:15Z","title":"QAID: Question Answering Inspired Few-shot Intent Detection","summary":"  Intent detection with semantically similar fine-grained intents is a\nchallenging task. To address it, we reformulate intent detection as a\nquestion-answering retrieval task by treating utterances and intent names as\nquestions and answers. To that end, we utilize a question-answering retrieval\narchitecture and adopt a two stages training schema with batch contrastive\nloss. In the pre-training stage, we improve query representations through\nself-supervised training. Then, in the fine-tuning stage, we increase\ncontextualized token-level similarity scores between queries and answers from\nthe same intent. Our results on three few-shot intent detection benchmarks\nachieve state-of-the-art performance.\n","authors":["Asaf Yehudai","Matan Vetzler","Yosi Mass","Koren Lazar","Doron Cohen","Boaz Carmeli"],"pdf_url":"https://arxiv.org/pdf/2303.01593v2.pdf","comment":"ICLR paper"},{"id":"http://arxiv.org/abs/2303.11780v1","updated":"2023-03-21T11:56:35Z","published":"2023-03-21T11:56:35Z","title":"Debiased Contrastive Learning for Sequential Recommendation","summary":"  Current sequential recommender systems are proposed to tackle the dynamic\nuser preference learning with various neural techniques, such as Transformer\nand Graph Neural Networks (GNNs). However, inference from the highly sparse\nuser behavior data may hinder the representation ability of sequential pattern\nencoding. To address the label shortage issue, contrastive learning (CL)\nmethods are proposed recently to perform data augmentation in two fashions: (i)\nrandomly corrupting the sequence data (e.g. stochastic masking, reordering);\n(ii) aligning representations across pre-defined contrastive views. Although\neffective, we argue that current CL-based methods have limitations in\naddressing popularity bias and disentangling of user conformity and real\ninterest. In this paper, we propose a new Debiased Contrastive learning\nparadigm for Recommendation (DCRec) that unifies sequential pattern encoding\nwith global collaborative relation modeling through adaptive conformity-aware\naugmentation. This solution is designed to tackle the popularity bias issue in\nrecommendation systems. Our debiased contrastive learning framework effectively\ncaptures both the patterns of item transitions within sequences and the\ndependencies between users across sequences. Our experiments on various\nreal-world datasets have demonstrated that DCRec significantly outperforms\nstate-of-the-art baselines, indicating its efficacy for recommendation. To\nfacilitate reproducibility of our results, we make our implementation of DCRec\npublicly available at: https://github.com/HKUDS/DCRec.\n","authors":["Yuhao Yang","Chao Huang","Lianghao Xia","Chunzhen Huang","Da Luo","Kangyi Lin"],"pdf_url":"https://arxiv.org/pdf/2303.11780v1.pdf","comment":"This paper is accepted by WWW'2023"},{"id":"http://arxiv.org/abs/2303.11746v1","updated":"2023-03-21T11:13:01Z","published":"2023-03-21T11:13:01Z","title":"Recommendation Systems in Libraries: an Application with Heterogeneous\n  Data Sources","summary":"  The Reading&Machine project exploits the support of digitalization to\nincrease the attractiveness of libraries and improve the users' experience. The\nproject implements an application that helps the users in their decision-making\nprocess, providing recommendation system (RecSys)-generated lists of books the\nusers might be interested in, and showing them through an interactive Virtual\nReality (VR)-based Graphical User Interface (GUI). In this paper, we focus on\nthe design and testing of the recommendation system, employing data about all\nusers' loans over the past 9 years from the network of libraries located in\nTurin, Italy. In addition, we use data collected by the Anobii online social\ncommunity of readers, who share their feedback and additional information about\nbooks they read. Armed with this heterogeneous data, we build and evaluate\nContent Based (CB) and Collaborative Filtering (CF) approaches. Our results\nshow that the CF outperforms the CB approach, improving by up to 47\\% the\nrelevant recommendations provided to a reader. However, the performance of the\nCB approach is heavily dependent on the number of books the reader has already\nread, and it can work even better than CF for users with a large history.\nFinally, our evaluations highlight that the performances of both approaches are\nsignificantly improved if the system integrates and leverages the information\nfrom the Anobii dataset, which allows us to include more user readings (for CF)\nand richer book metadata (for CB).\n","authors":["Alessandro Speciale","Greta Vallero","Luca Vassio","Marco Mellia"],"pdf_url":"https://arxiv.org/pdf/2303.11746v1.pdf","comment":"Accepted at 7th International workshop on Data Analytics solutions\n  for Real-LIfe APplications - 28th March-31st March, 2023, Ioannina, Greece.\n  The paper will be published in the Proceedings of EDBT/ICDT 2023 Joint\n  Conference"},{"id":"http://arxiv.org/abs/2112.06400v2","updated":"2023-03-21T10:32:23Z","published":"2021-12-13T03:18:04Z","title":"Improving Query Representations for Dense Retrieval with Pseudo\n  Relevance Feedback: A Reproducibility Study","summary":"  Pseudo-Relevance Feedback (PRF) utilises the relevance signals from the top-k\npassages from the first round of retrieval to perform a second round of\nretrieval aiming to improve search effectiveness. A recent research direction\nhas been the study and development of PRF methods for deep language models\nbased rankers, and in particular in the context of dense retrievers. Dense\nretrievers, compared to more complex neural rankers, provide a trade-off\nbetween effectiveness, which is often reduced compared to more complex neural\nrankers, and query latency, which also is reduced making the retrieval pipeline\nmore efficient. The introduction of PRF methods for dense retrievers has been\nmotivated as an attempt to further improve their effectiveness.\n  In this paper, we reproduce and study a recent method for PRF with dense\nretrievers, called ANCE-PRF. This method concatenates the query text and that\nof the top-k feedback passages to form a new query input, which is then encoded\ninto a dense representation using a newly trained query encoder based on the\noriginal dense retriever used for the first round of retrieval. While the\nmethod can potentially be applied to any of the existing dense retrievers,\nprior work has studied it only in the context of the ANCE dense retriever.\n  We study the reproducibility of ANCE-PRF in terms of both its training\n(encoding of the PRF signal) and inference (ranking) steps. We further extend\nthe empirical analysis provided in the original work to investigate the effect\nof the hyper-parameters that govern the training process and the robustness of\nthe method across these different settings. Finally, we contribute a study of\nthe generalisability of the ANCE-PRF method when dense retrievers other than\nANCE are used for the first round of retrieval and for encoding the PRF signal.\n","authors":["Hang Li","Shengyao Zhuang","Ahmed Mourad","Xueguang Ma","Jimmy Lin","Guido Zuccon"],"pdf_url":"https://arxiv.org/pdf/2112.06400v2.pdf","comment":"Accepted at ECIR 2022"},{"id":"http://arxiv.org/abs/2303.11700v1","updated":"2023-03-21T09:37:37Z","published":"2023-03-21T09:37:37Z","title":"Dynamically Expandable Graph Convolution for Streaming Recommendation","summary":"  Personalized recommender systems have been widely studied and deployed to\nreduce information overload and satisfy users' diverse needs. However,\nconventional recommendation models solely conduct a one-time training-test\nfashion and can hardly adapt to evolving demands, considering user preference\nshifts and ever-increasing users and items in the real world. To tackle such\nchallenges, the streaming recommendation is proposed and has attracted great\nattention recently. Among these, continual graph learning is widely regarded as\na promising approach for the streaming recommendation by academia and industry.\nHowever, existing methods either rely on the historical data replay which is\noften not practical under increasingly strict data regulations, or can seldom\nsolve the \\textit{over-stability} issue. To overcome these difficulties, we\npropose a novel \\textbf{D}ynamically \\textbf{E}xpandable \\textbf{G}raph\n\\textbf{C}onvolution (DEGC) algorithm from a \\textit{model isolation}\nperspective for the streaming recommendation which is orthogonal to previous\nmethods. Based on the motivation of disentangling outdated short-term\npreferences from useful long-term preferences, we design a sequence of\noperations including graph convolution pruning, refining, and expanding to only\npreserve beneficial long-term preference-related parameters and extract fresh\nshort-term preferences. Moreover, we model the temporal user preference, which\nis utilized as user embedding initialization, for better capturing the\nindividual-level preference shifts. Extensive experiments on the three most\nrepresentative GCN-based recommendation models and four industrial datasets\ndemonstrate the effectiveness and robustness of our method.\n","authors":["Bowei He","Xu He","Yingxue Zhang","Ruiming Tang","Chen Ma"],"pdf_url":"https://arxiv.org/pdf/2303.11700v1.pdf","comment":"11 pages, 6 figures, published on The Web Conference 2023 (WWW 2023)"},{"id":"http://arxiv.org/abs/2303.11692v1","updated":"2023-03-21T09:27:40Z","published":"2023-03-21T09:27:40Z","title":"ByteCover3: Accurate Cover Song Identification on Short Queries","summary":"  Deep learning based methods have become a paradigm for cover song\nidentification (CSI) in recent years, where the ByteCover systems have achieved\nstate-of-the-art results on all the mainstream datasets of CSI. However, with\nthe burgeon of short videos, many real-world applications require matching\nshort music excerpts to full-length music tracks in the database, which is\nstill under-explored and waiting for an industrial-level solution. In this\npaper, we upgrade the previous ByteCover systems to ByteCover3 that utilizes\nlocal features to further improve the identification performance of short music\nqueries. ByteCover3 is designed with a local alignment loss (LAL) module and a\ntwo-stage feature retrieval pipeline, allowing the system to perform CSI in a\nmore precise and efficient way. We evaluated ByteCover3 on multiple datasets\nwith different benchmark settings, where ByteCover3 beat all the compared\nmethods including its previous versions.\n","authors":["Xingjian Du","Zijie Wang","Xia Liang","Huidong Liang","Bilei Zhu","Zejun Ma"],"pdf_url":"https://arxiv.org/pdf/2303.11692v1.pdf","comment":"Accepeted by ICASSP 2023"},{"id":"http://arxiv.org/abs/2302.04775v2","updated":"2023-03-21T08:48:20Z","published":"2023-02-09T17:09:53Z","title":"Adap-$τ$: Adaptively Modulating Embedding Magnitude for\n  Recommendation","summary":"  Recent years have witnessed the great successes of embedding-based methods in\nrecommender systems. Despite their decent performance, we argue one potential\nlimitation of these methods -- the embedding magnitude has not been explicitly\nmodulated, which may aggravate popularity bias and training instability,\nhindering the model from making a good recommendation. It motivates us to\nleverage the embedding normalization in recommendation. By normalizing\nuser/item embeddings to a specific value, we empirically observe impressive\nperformance gains (9\\% on average) on four real-world datasets. Although\nencouraging, we also reveal a serious limitation when applying normalization in\nrecommendation -- the performance is highly sensitive to the choice of the\ntemperature $\\tau$ which controls the scale of the normalized embeddings.\n  To fully foster the merits of the normalization while circumvent its\nlimitation, this work studied on how to adaptively set the proper $\\tau$.\nTowards this end, we first make a comprehensive analyses of $\\tau$ to fully\nunderstand its role on recommendation. We then accordingly develop an adaptive\nfine-grained strategy Adap-$\\tau$ for the temperature with satisfying four\ndesirable properties including adaptivity, personalized, efficiency and\nmodel-agnostic. Extensive experiments have been conducted to validate the\neffectiveness of the proposal. The code is available at\n\\url{https://github.com/junkangwu/Adap_tau}.\n","authors":["Jiawei Chen","Junkang Wu","Jiancan Wu","Sheng Zhou","Xuezhi Cao","Xiangnan He"],"pdf_url":"https://arxiv.org/pdf/2302.04775v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11666v1","updated":"2023-03-21T08:20:43Z","published":"2023-03-21T08:20:43Z","title":"A Survey on Causal Inference for Recommendation","summary":"  Recently, causal inference has attracted increasing attention from\nresearchers of recommender systems (RS), which analyzes the relationship\nbetween a cause and its effect and has a wide range of real-world applications\nin multiple fields. Causal inference can model the causality in recommender\nsystems like confounding effects and deal with counterfactual problems such as\noffline policy evaluation and data augmentation. Although there are already\nsome valuable surveys on causal recommendations, these surveys introduce\napproaches in a relatively isolated way and lack theoretical analysis of\nexisting methods. Due to the unfamiliarity with causality to RS researchers, it\nis both necessary and challenging to comprehensively review the relevant\nstudies from the perspective of causal theory, which might be instructive for\nthe readers to propose new approaches in practice. This survey attempts to\nprovide a systematic review of up-to-date papers in this area from a\ntheoretical standpoint. Firstly, we introduce the fundamental concepts of\ncausal inference as the basis of the following review. Then we propose a new\ntaxonomy from the perspective of causal techniques and further discuss\ntechnical details about how existing methods apply causal inference to address\nspecific recommender issues. Finally, we highlight some promising directions\nfor future research in this field.\n","authors":["Huishi Luo","Fuzhen Zhuang","Ruobing Xie","Hengshu Zhu","Deqing Wang"],"pdf_url":"https://arxiv.org/pdf/2303.11666v1.pdf","comment":"Under peer review"},{"id":"http://arxiv.org/abs/2303.11648v1","updated":"2023-03-21T07:46:57Z","published":"2023-03-21T07:46:57Z","title":"Improving Content Retrievability in Search with Controllable Query\n  Generation","summary":"  An important goal of online platforms is to enable content discovery, i.e.\nallow users to find a catalog entity they were not familiar with. A\npre-requisite to discover an entity, e.g. a book, with a search engine is that\nthe entity is retrievable, i.e. there are queries for which the system will\nsurface such entity in the top results. However, machine-learned search engines\nhave a high retrievability bias, where the majority of the queries return the\nsame entities. This happens partly due to the predominance of narrow intent\nqueries, where users create queries using the title of an already known entity,\ne.g. in book search 'harry potter'. The amount of broad queries where users\nwant to discover new entities, e.g. in music search 'chill lyrical electronica\nwith an atmospheric feeling to it', and have a higher tolerance to what they\nmight find, is small in comparison. We focus here on two factors that have a\nnegative impact on the retrievability of the entities (I) the training data\nused for dense retrieval models and (II) the distribution of narrow and broad\nintent queries issued in the system. We propose CtrlQGen, a method that\ngenerates queries for a chosen underlying intent-narrow or broad. We can use\nCtrlQGen to improve factor (I) by generating training data for dense retrieval\nmodels comprised of diverse synthetic queries. CtrlQGen can also be used to\ndeal with factor (II) by suggesting queries with broader intents to users. Our\nresults on datasets from the domains of music, podcasts, and books reveal that\nwe can significantly decrease the retrievability bias of a dense retrieval\nmodel when using CtrlQGen. First, by using the generated queries as training\ndata for dense models we make 9% of the entities retrievable (go from zero to\nnon-zero retrievability). Second, by suggesting broader queries to users, we\ncan make 12% of the entities retrievable in the best case.\n","authors":["Gustavo Penha","Enrico Palumbo","Maryam Aziz","Alice Wang","Hugues Bouchard"],"pdf_url":"https://arxiv.org/pdf/2303.11648v1.pdf","comment":"Accepted for publication in the International World Wide Web\n  Conference 2023"},{"id":"http://arxiv.org/abs/2303.07797v2","updated":"2023-03-21T03:54:41Z","published":"2023-03-14T11:12:22Z","title":"Automated Self-Supervised Learning for Recommendation","summary":"  Graph neural networks (GNNs) have emerged as the state-of-the-art paradigm\nfor collaborative filtering (CF). To improve the representation quality over\nlimited labeled data, contrastive learning has attracted attention in\nrecommendation and benefited graph-based CF model recently. However, the\nsuccess of most contrastive methods heavily relies on manually generating\neffective contrastive views for heuristic-based data augmentation. This does\nnot generalize across different datasets and downstream recommendation tasks,\nwhich is difficult to be adaptive for data augmentation and robust to noise\nperturbation. To fill this crucial gap, this work proposes a unified Automated\nCollaborative Filtering (AutoCF) to automatically perform data augmentation for\nrecommendation. Specifically, we focus on the generative self-supervised\nlearning framework with a learnable augmentation paradigm that benefits the\nautomated distillation of important self-supervised signals. To enhance the\nrepresentation discrimination ability, our masked graph autoencoder is designed\nto aggregate global information during the augmentation via reconstructing the\nmasked subgraph structures. Experiments and ablation studies are performed on\nseveral public datasets for recommending products, venues, and locations.\nResults demonstrate the superiority of AutoCF against various baseline methods.\nWe release the model implementation at https://github.com/HKUDS/AutoCF.\n","authors":["Lianghao Xia","Chao Huang","Chunzhen Huang","Kangyi Lin","Tao Yu","Ben Kao"],"pdf_url":"https://arxiv.org/pdf/2303.07797v2.pdf","comment":"Accepted by ACM The Web Conference, 2023"},{"id":"http://arxiv.org/abs/2303.08537v3","updated":"2023-03-21T03:44:25Z","published":"2023-03-15T11:30:16Z","title":"Graph-less Collaborative Filtering","summary":"  Graph neural networks (GNNs) have shown the power in representation learning\nover graph-structured user-item interaction data for collaborative filtering\n(CF) task. However, with their inherently recursive message propagation among\nneighboring nodes, existing GNN-based CF models may generate indistinguishable\nand inaccurate user (item) representations due to the over-smoothing and noise\neffect with low-pass Laplacian smoothing operators. In addition, the recursive\ninformation propagation with the stacked aggregators in the entire graph\nstructures may result in poor scalability in practical applications. Motivated\nby these limitations, we propose a simple and effective collaborative filtering\nmodel (SimRec) that marries the power of knowledge distillation and contrastive\nlearning. In SimRec, adaptive transferring knowledge is enabled between the\nteacher GNN model and a lightweight student network, to not only preserve the\nglobal collaborative signals, but also address the over-smoothing issue with\nrepresentation recalibration. Empirical results on public datasets show that\nSimRec archives better efficiency while maintaining superior recommendation\nperformance compared with various strong baselines. Our implementations are\npublicly available at: https://github.com/HKUDS/SimRec.\n","authors":["Lianghao Xia","Chao Huang","Jiao Shi","Yong Xu"],"pdf_url":"https://arxiv.org/pdf/2303.08537v3.pdf","comment":"Accepted by ACM WWW 2023"},{"id":"http://arxiv.org/abs/2303.11574v1","updated":"2023-03-21T03:43:19Z","published":"2023-03-21T03:43:19Z","title":"Bounding System-Induced Biases in Recommender Systems with A Randomized\n  Dataset","summary":"  Debiased recommendation with a randomized dataset has shown very promising\nresults in mitigating the system-induced biases. However, it still lacks more\ntheoretical insights or an ideal optimization objective function compared with\nthe other more well studied route without a randomized dataset. To bridge this\ngap, we study the debiasing problem from a new perspective and propose to\ndirectly minimize the upper bound of an ideal objective function, which\nfacilitates a better potential solution to the system-induced biases. Firstly,\nwe formulate a new ideal optimization objective function with a randomized\ndataset. Secondly, according to the prior constraints that an adopted loss\nfunction may satisfy, we derive two different upper bounds of the objective\nfunction, i.e., a generalization error bound with the triangle inequality and a\ngeneralization error bound with the separability. Thirdly, we show that most\nexisting related methods can be regarded as the insufficient optimization of\nthese two upper bounds. Fourthly, we propose a novel method called debiasing\napproximate upper bound with a randomized dataset (DUB), which achieves a more\nsufficient optimization of these upper bounds. Finally, we conduct extensive\nexperiments on a public dataset and a real product dataset to verify the\neffectiveness of our DUB.\n","authors":["Dugang Liu","Pengxiang Cheng","Zinan Lin","Xiaolian Zhang","Zhenhua Dong","Rui Zhang","Xiuqiang He","Weike Pan","Zhong Ming"],"pdf_url":"https://arxiv.org/pdf/2303.11574v1.pdf","comment":"Accepted by ACM TOIS"},{"id":"http://arxiv.org/abs/2303.05039v2","updated":"2023-03-21T02:24:25Z","published":"2023-03-09T05:20:17Z","title":"Improving Recommendation Systems with User Personality Inferred from\n  Product Reviews","summary":"  Personality is a psychological factor that reflects people's preferences,\nwhich in turn influences their decision-making. We hypothesize that accurate\nmodeling of users' personalities improves recommendation systems' performance.\nHowever, acquiring such personality profiles is both sensitive and expensive.\nWe address this problem by introducing a novel method to automatically extract\npersonality profiles from public product review text. We then design and assess\nthree context-aware recommendation architectures that leverage the profiles to\ntest our hypothesis.\n  Experiments on our two newly contributed personality datasets --\nAmazon-beauty and Amazon-music -- validate our hypothesis, showing performance\nboosts of 3--28%.Our analysis uncovers that varying personality types\ncontribute differently to recommendation performance: open and extroverted\npersonalities are most helpful in music recommendation, while a conscientious\npersonality is most helpful in beauty product recommendation.\n","authors":["Xinyuan Lu","Min-Yen Kan"],"pdf_url":"https://arxiv.org/pdf/2303.05039v2.pdf","comment":"Accepted by IRS@WSDM'23"},{"id":"http://arxiv.org/abs/2302.11953v2","updated":"2023-03-21T18:38:10Z","published":"2023-02-23T12:02:49Z","title":"MFBE: Leveraging Multi-Field Information of FAQs for Efficient Dense\n  Retrieval","summary":"  In the domain of question-answering in NLP, the retrieval of Frequently Asked\nQuestions (FAQ) is an important sub-area which is well researched and has been\nworked upon for many languages. Here, in response to a user query, a retrieval\nsystem typically returns the relevant FAQs from a knowledge-base. The efficacy\nof such a system depends on its ability to establish semantic match between the\nquery and the FAQs in real-time. The task becomes challenging due to the\ninherent lexical gap between queries and FAQs, lack of sufficient context in\nFAQ titles, scarcity of labeled data and high retrieval latency. In this work,\nwe propose a bi-encoder-based query-FAQ matching model that leverages multiple\ncombinations of FAQ fields (like, question, answer, and category) both during\nmodel training and inference. Our proposed Multi-Field Bi-Encoder (MFBE) model\nbenefits from the additional context resulting from multiple FAQ fields and\nperforms well even with minimal labeled data. We empirically support this claim\nthrough experiments on proprietary as well as open-source public datasets in\nboth unsupervised and supervised settings. Our model achieves around 27% and\n20% better top-1 accuracy for the FAQ retrieval task on internal and open\ndatasets, respectively over the best performing baseline.\n","authors":["Debopriyo Banerjee","Mausam Jain","Ashish Kulkarni"],"pdf_url":"https://arxiv.org/pdf/2302.11953v2.pdf","comment":"The first two authors contributed equally to this work. 12 pages, 3\n  figures, 5 tables. Accepted at the 2023 Pacific-Asia Conference On Knowledge\n  Discovery And Data Mining (PAKDD)"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2303.12076v1","updated":"2023-03-21T17:59:20Z","published":"2023-03-21T17:59:20Z","title":"Dexterity from Touch: Self-Supervised Pre-Training of Tactile\n  Representations with Robotic Play","summary":"  Teaching dexterity to multi-fingered robots has been a longstanding challenge\nin robotics. Most prominent work in this area focuses on learning controllers\nor policies that either operate on visual observations or state estimates\nderived from vision. However, such methods perform poorly on fine-grained\nmanipulation tasks that require reasoning about contact forces or about objects\noccluded by the hand itself. In this work, we present T-Dex, a new approach for\ntactile-based dexterity, that operates in two phases. In the first phase, we\ncollect 2.5 hours of play data, which is used to train self-supervised tactile\nencoders. This is necessary to bring high-dimensional tactile readings to a\nlower-dimensional embedding. In the second phase, given a handful of\ndemonstrations for a dexterous task, we learn non-parametric policies that\ncombine the tactile observations with visual ones. Across five challenging\ndexterous tasks, we show that our tactile-based dexterity models outperform\npurely vision and torque-based models by an average of 1.7X. Finally, we\nprovide a detailed analysis on factors critical to T-Dex including the\nimportance of play data, architectures, and representation learning.\n","authors":["Irmak Guzey","Ben Evans","Soumith Chintala","Lerrel Pinto"],"pdf_url":"https://arxiv.org/pdf/2303.12076v1.pdf","comment":"Video and code can be accessed here:\n  https://tactile-dexterity.github.io/"},{"id":"http://arxiv.org/abs/2110.13311v5","updated":"2023-03-21T17:37:26Z","published":"2021-10-25T22:57:40Z","title":"Physics informed machine learning with Smoothed particle hydrodynamics:\n  Hierarchy of reduced Lagrangian models of turbulence","summary":"  Building efficient, accurate and generalizable reduced order models of\ndeveloped turbulence remains a major challenge. This manuscript approaches this\nproblem by developing a hierarchy of parameterized reduced Lagrangian models\nfor turbulent flows, and investigates the effects of enforcing physical\nstructure through Smoothed Particle Hydrodynamics (SPH) versus relying on\nneural networks (NN)s as universal function approximators. Starting from Neural\nNetwork (NN) parameterizations of a Lagrangian acceleration operator, this\nhierarchy of models gradually incorporates a weakly compressible and\nparameterized SPH framework, which enforces physical symmetries, such as\nGalilean, rotational and translational invariances. Within this hierarchy, two\nnew parameterized smoothing kernels are developed in order to increase the\nflexibility of the learn-able SPH simulators. For each model we experiment with\ndifferent loss functions which are minimized using gradient based optimization,\nwhere efficient computations of gradients are obtained by using Automatic\nDifferentiation (AD) and Sensitivity Analysis (SA). Each model within the\nhierarchy is trained on two data sets associated with weekly compressible\nHomogeneous Isotropic Turbulence (HIT): (1) a validation set using weakly\ncompressible SPH; and (2) a high fidelity set from Direct Numerical Simulations\n(DNS). Numerical evidence shows that encoding more SPH structure improves\ngeneralizability to different turbulent Mach numbers and time shifts, and that\nincluding the novel parameterized smoothing kernels improves the accuracy of\nSPH at the resolved scales.\n","authors":["Michael Woodward","Yifeng Tian","Criston Hyett","Chris Fryer","Daniel Livescu","Mikhail Stepanov","Michael Chertkov"],"pdf_url":"https://arxiv.org/pdf/2110.13311v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12032v1","updated":"2023-03-21T17:19:35Z","published":"2023-03-21T17:19:35Z","title":"The Representational Status of Deep Learning Models","summary":"  This paper aims to clarify the representational status of Deep Learning\nModels (DLMs). While commonly referred to as 'representations', what this\nentails is ambiguous due to a conflation of functional and relational\nconceptions of representation. This paper argues that while DLMs represent\ntheir targets in a relational sense, they are best understood as highly\nidealized models. This result has immediate implications for explainable AI\n(XAI) and directs philosophical attention toward examining the idealized nature\nof DLM representations and their role in future scientific investigation.\n","authors":["Eamon Duede"],"pdf_url":"https://arxiv.org/pdf/2303.12032v1.pdf","comment":"19 pages"},{"id":"http://arxiv.org/abs/2303.12031v1","updated":"2023-03-21T17:16:01Z","published":"2023-03-21T17:16:01Z","title":"Semantic Latent Space Regression of Diffusion Autoencoders for Vertebral\n  Fracture Grading","summary":"  Vertebral fractures are a consequence of osteoporosis, with significant\nhealth implications for affected patients. Unfortunately, grading their\nseverity using CT exams is hard and subjective, motivating automated grading\nmethods. However, current approaches are hindered by imbalance and scarcity of\ndata and a lack of interpretability. To address these challenges, this paper\nproposes a novel approach that leverages unlabelled data to train a generative\nDiffusion Autoencoder (DAE) model as an unsupervised feature extractor. We\nmodel fracture grading as a continuous regression, which is more reflective of\nthe smooth progression of fractures. Specifically, we use a binary, supervised\nfracture classifier to construct a hyperplane in the DAE's latent space. We\nthen regress the severity of the fracture as a function of the distance to this\nhyperplane, calibrating the results to the Genant scale. Importantly, the\ngenerative nature of our method allows us to visualize different grades of a\ngiven vertebra, providing interpretability and insight into the features that\ncontribute to automated grading.\n","authors":["Matthias Keicher","Matan Atad","David Schinz","Alexandra S. Gersing","Sarah C. Foreman","Sophia S. Goller","Juergen Weissinger","Jon Rischewski","Anna-Sophia Dietrich","Benedikt Wiestler","Jan S. Kirschke","Nassir Navab"],"pdf_url":"https://arxiv.org/pdf/2303.12031v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2303.10135v2","updated":"2023-03-21T16:58:19Z","published":"2023-03-17T17:23:14Z","title":"Efficient and Feasible Robotic Assembly Sequence Planning via Graph\n  Representation Learning","summary":"  Automatic Robotic Assembly Sequence Planning (RASP) can significantly improve\nproductivity and resilience in modern manufacturing along with the growing need\nfor greater product customization. One of the main challenges in realizing such\nautomation resides in efficiently finding solutions from a growing number of\npotential sequences for increasingly complex assemblies. Besides, costly\nfeasibility checks are always required for the robotic system. To address this,\nwe propose a holistic graphical approach including a graph representation\ncalled Assembly Graph for product assemblies and a policy architecture, Graph\nAssembly Processing Network, dubbed GRACE for assembly sequence generation.\nSecondly, we use GRACE to extract meaningful information from the graph input\nand predict assembly sequences in a step-by-step manner. In experiments, we\nshow that our approach can predict feasible assembly sequences across product\nvariants of aluminum profiles based on data collected in simulation of a\ndual-armed robotic system. We further demonstrate that our method is capable of\ndetecting infeasible assemblies, substantially alleviating the undesirable\nimpacts from false predictions, and hence facilitating real-world deployment\nsoon. Code and training data will be open-sourced.\n","authors":["Matan Atad","Jianxiang Feng","Ismael Rodríguez","Maximilian Durner","Rudolph Triebel"],"pdf_url":"https://arxiv.org/pdf/2303.10135v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2210.11456v3","updated":"2023-03-21T16:57:57Z","published":"2022-10-20T17:54:03Z","title":"MixMask: Revisiting Masking Strategy for Siamese ConvNets","summary":"  Recent advances in self-supervised learning have integrated Masked Image\nModeling (MIM) and Siamese Networks into a unified framework that leverages the\nbenefits of both techniques. However, several issues remain unaddressed when\napplying conventional erase-based masking with Siamese ConvNets. These include\n(I) the inability to drop uninformative masked regions in ConvNets as they\nprocess data continuously, resulting in low training efficiency compared to ViT\nmodels; and (II) the mismatch between erase-based masking and the\ncontrastive-based objective in Siamese ConvNets, which differs from the MIM\napproach. In this paper, we propose a filling-based masking strategy called\nMixMask to prevent information incompleteness caused by the randomly erased\nregions in an image in the vanilla masking method. Furthermore, we introduce a\nflexible loss function design that considers the semantic distance change\nbetween two different mixed views to adapt the integrated architecture and\nprevent mismatches between the transformed input and objective in Masked\nSiamese ConvNets (MSCN). We conducted extensive experiments on various\ndatasets, including CIFAR-100, Tiny-ImageNet, and ImageNet-1K. The results\ndemonstrate that our proposed framework achieves superior accuracy on linear\nprobing, semi-supervised, and supervised finetuning, outperforming the\nstate-of-the-art MSCN by a significant margin. Additionally, we demonstrate the\nsuperiority of our approach in object detection and segmentation tasks. Our\nsource code is available at https://github.com/LightnessOfBeing/MixMask.\n","authors":["Kirill Vishniakov","Eric Xing","Zhiqiang Shen"],"pdf_url":"https://arxiv.org/pdf/2210.11456v3.pdf","comment":"Technical report. Code is available at\n  https://github.com/LightnessOfBeing/MixMask"},{"id":"http://arxiv.org/abs/2303.12021v1","updated":"2023-03-21T16:55:18Z","published":"2023-03-21T16:55:18Z","title":"Graph Kalman Filters","summary":"  The well-known Kalman filters model dynamical systems by relying on\nstate-space representations with the next state updated, and its uncertainty\ncontrolled, by fresh information associated with newly observed system outputs.\nThis paper generalizes, for the first time in the literature, Kalman and\nextended Kalman filters to discrete-time settings where inputs, states, and\noutputs are represented as attributed graphs whose topology and attributes can\nchange with time. The setup allows us to adapt the framework to cases where the\noutput is a vector or a scalar too (node/graph level tasks). Within the\nproposed theoretical framework, the unknown state-transition and the readout\nfunctions are learned end-to-end along with the downstream prediction task.\n","authors":["Cesare Alippi","Daniele Zambon"],"pdf_url":"https://arxiv.org/pdf/2303.12021v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10780v2","updated":"2023-03-21T16:48:53Z","published":"2023-03-19T22:07:27Z","title":"A Comprehensive Review of Spiking Neural Networks: Interpretation,\n  Optimization, Efficiency, and Best Practices","summary":"  Biological neural networks continue to inspire breakthroughs in neural\nnetwork performance. And yet, one key area of neural computation that has been\nunder-appreciated and under-investigated is biologically plausible,\nenergy-efficient spiking neural networks, whose potential is especially\nattractive for low-power, mobile, or otherwise hardware-constrained settings.\nWe present a literature review of recent developments in the interpretation,\noptimization, efficiency, and accuracy of spiking neural networks. Key\ncontributions include identification, discussion, and comparison of\ncutting-edge methods in spiking neural network optimization, energy-efficiency,\nand evaluation, starting from first principles so as to be accessible to new\npractitioners.\n","authors":["Kai Malcolm","Josue Casco-Rodriguez"],"pdf_url":"https://arxiv.org/pdf/2303.10780v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2111.07632v2","updated":"2023-03-21T16:36:35Z","published":"2021-11-15T09:35:54Z","title":"CoReS: Compatible Representations via Stationarity","summary":"  In this paper, we propose a novel method to learn internal feature\nrepresentation models that are \\textit{compatible} with previously learned\nones. Compatible features enable for direct comparison of old and new learned\nfeatures, allowing them to be used interchangeably over time. This eliminates\nthe need for visual search systems to extract new features for all previously\nseen images in the gallery-set when sequentially upgrading the representation\nmodel. Extracting new features is typically quite expensive or infeasible in\nthe case of very large gallery-sets and/or real time systems (i.e.,\nface-recognition systems, social networks, life-long learning systems, robotics\nand surveillance systems). Our approach, called Compatible Representations via\nStationarity (CoReS), achieves compatibility by encouraging stationarity to the\nlearned representation model without relying on previously learned models.\nStationarity allows features' statistical properties not to change under time\nshift so that the current learned features are inter-operable with the old\nones. We evaluate single and sequential multi-model upgrading in growing\nlarge-scale training datasets and we show that our method improves the\nstate-of-the-art in achieving compatible features by a large margin. In\nparticular, upgrading ten times with training data taken from CASIA-WebFace and\nevaluating in Labeled Face in the Wild (LFW), we obtain a 49\\% increase in\nmeasuring the average number of times compatibility is achieved, which is a\n544\\% relative improvement over previous state-of-the-art.\n","authors":["Niccolo Biondi","Federico Pernici","Matteo Bruni","Alberto Del Bimbo"],"pdf_url":"https://arxiv.org/pdf/2111.07632v2.pdf","comment":"in IEEE Transactions on Pattern Analysis and Machine Intelligence"},{"id":"http://arxiv.org/abs/2303.12002v1","updated":"2023-03-21T16:33:56Z","published":"2023-03-21T16:33:56Z","title":"End-to-End Integration of Speech Separation and Voice Activity Detection\n  for Low-Latency Diarization of Telephone Conversations","summary":"  Recent works show that speech separation guided diarization (SSGD) is an\nincreasingly promising direction, mainly thanks to the recent progress in\nspeech separation. It performs diarization by first separating the speakers and\nthen applying voice activity detection (VAD) on each separated stream. In this\nwork we conduct an in-depth study of SSGD in the conversational telephone\nspeech (CTS) domain, focusing mainly on low-latency streaming diarization\napplications. We consider three state-of-the-art speech separation (SSep)\nalgorithms and study their performance both in online and offline scenarios,\nconsidering non-causal and causal implementations as well as continuous SSep\n(CSS) windowed inference. We compare different SSGD algorithms on two widely\nused CTS datasets: CALLHOME and Fisher Corpus (Part 1 and 2) and evaluate both\nseparation and diarization performance. To improve performance, a novel, causal\nand computationally efficient leakage removal algorithm is proposed, which\nsignificantly decreases false alarms. We also explore, for the first time,\nfully end-to-end SSGD integration between SSep and VAD modules. Crucially, this\nenables fine-tuning on real-world data for which oracle speakers sources are\nnot available. In particular, our best model achieves 8.8% DER on CALLHOME,\nwhich outperforms the current state-of-the-art end-to-end neural diarization\nmodel, despite being trained on an order of magnitude less data and having\nsignificantly lower latency, i.e., 0.1 vs. 1 seconds. Finally, we also show\nthat the separated signals can be readily used also for automatic speech\nrecognition, reaching performance close to using oracle sources in some\nconfigurations.\n","authors":["Giovanni Morrone","Samuele Cornell","Luca Serafini","Enrico Zovato","Alessio Brutti","Stefano Squartini"],"pdf_url":"https://arxiv.org/pdf/2303.12002v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.02601v3","updated":"2023-03-21T16:30:06Z","published":"2023-02-06T07:45:57Z","title":"Learning Representations of Bi-level Knowledge Graphs for Reasoning\n  beyond Link Prediction","summary":"  Knowledge graphs represent known facts using triplets. While existing\nknowledge graph embedding methods only consider the connections between\nentities, we propose considering the relationships between triplets. For\nexample, let us consider two triplets $T_1$ and $T_2$ where $T_1$ is\n(Academy_Awards, Nominates, Avatar) and $T_2$ is (Avatar, Wins,\nAcademy_Awards). Given these two base-level triplets, we see that $T_1$ is a\nprerequisite for $T_2$. In this paper, we define a higher-level triplet to\nrepresent a relationship between triplets, e.g., $\\langle T_1$,\nPrerequisiteFor, $T_2\\rangle$ where PrerequisiteFor is a higher-level relation.\nWe define a bi-level knowledge graph that consists of the base-level and the\nhigher-level triplets. We also propose a data augmentation strategy based on\nthe random walks on the bi-level knowledge graph to augment plausible triplets.\nOur model called BiVE learns embeddings by taking into account the structures\nof the base-level and the higher-level triplets, with additional consideration\nof the augmented triplets. We propose two new tasks: triplet prediction and\nconditional link prediction. Given a triplet $T_1$ and a higher-level relation,\nthe triplet prediction predicts a triplet that is likely to be connected to\n$T_1$ by the higher-level relation, e.g., $\\langle T_1$, PrerequisiteFor,\n?$\\rangle$. The conditional link prediction predicts a missing entity in a\ntriplet conditioned on another triplet, e.g., $\\langle T_1$, PrerequisiteFor,\n(Avatar, Wins, ?)$\\rangle$. Experimental results show that BiVE significantly\noutperforms all other methods in the two new tasks and the typical base-level\nlink prediction in real-world bi-level knowledge graphs.\n","authors":["Chanyoung Chung","Joyce Jiyoung Whang"],"pdf_url":"https://arxiv.org/pdf/2302.02601v3.pdf","comment":"14 pages, 3 figures, 15 tables. 37th AAAI Conference on Artificial\n  Intelligence (AAAI 2023)"},{"id":"http://arxiv.org/abs/2212.07398v4","updated":"2023-03-21T16:16:41Z","published":"2022-12-14T18:31:47Z","title":"Policy Adaptation from Foundation Model Feedback","summary":"  Recent progress on vision-language foundation models have brought significant\nadvancement to building general-purpose robots. By using the pre-trained models\nto encode the scene and instructions as inputs for decision making, the\ninstruction-conditioned policy can generalize across different objects and\ntasks. While this is encouraging, the policy still fails in most cases given an\nunseen task or environment. In this work, we propose Policy Adaptation from\nFoundation model Feedback (PAFF). When deploying the trained policy to a new\ntask or a new environment, we first let the policy play with randomly generated\ninstructions to record the demonstrations. While the execution could be wrong,\nwe can use the pre-trained foundation models to provide feedback to relabel the\ndemonstrations. This automatically provides new pairs of\ndemonstration-instruction data for policy fine-tuning. We evaluate our method\non a broad range of experiments with the focus on generalization on unseen\nobjects, unseen tasks, unseen environments, and sim-to-real transfer. We show\nPAFF improves baselines by a large margin in all cases. Our project page is\navailable at https://geyuying.github.io/PAFF/\n","authors":["Yuying Ge","Annabella Macaluso","Li Erran Li","Ping Luo","Xiaolong Wang"],"pdf_url":"https://arxiv.org/pdf/2212.07398v4.pdf","comment":"Accepted by CVPR 2023; Project page: https://geyuying.github.io/PAFF/"},{"id":"http://arxiv.org/abs/2205.09809v5","updated":"2023-03-21T16:00:59Z","published":"2022-05-19T19:05:15Z","title":"Calibration Matters: Tackling Maximization Bias in Large-scale\n  Advertising Recommendation Systems","summary":"  Calibration is defined as the ratio of the average predicted click rate to\nthe true click rate. The optimization of calibration is essential to many\nonline advertising recommendation systems because it directly affects the\ndownstream bids in ads auctions and the amount of money charged to advertisers.\nDespite its importance, calibration optimization often suffers from a problem\ncalled \"maximization bias\". Maximization bias refers to the phenomenon that the\nmaximum of predicted values overestimates the true maximum. The problem is\nintroduced because the calibration is computed on the set selected by the\nprediction model itself. It persists even if unbiased predictions can be\nachieved on every datapoint and worsens when covariate shifts exist between the\ntraining and test sets. To mitigate this problem, we theorize the\nquantification of maximization bias and propose a variance-adjusting debiasing\n(VAD) meta-algorithm in this paper. The algorithm is efficient, robust, and\npractical as it is able to mitigate maximization bias problems under covariate\nshifts, neither incurring additional online serving costs nor compromising the\nranking performance. We demonstrate the effectiveness of the proposed algorithm\nusing a state-of-the-art recommendation neural network model on a large-scale\nreal-world dataset.\n","authors":["Yewen Fan","Nian Si","Kun Zhang"],"pdf_url":"https://arxiv.org/pdf/2205.09809v5.pdf","comment":"Accepted in ICLR 2023"},{"id":"http://arxiv.org/abs/2303.11954v1","updated":"2023-03-21T15:45:06Z","published":"2023-03-21T15:45:06Z","title":"Bayesian Optimization for Function Compositions with Applications to\n  Dynamic Pricing","summary":"  Bayesian Optimization (BO) is used to find the global optima of black box\nfunctions. In this work, we propose a practical BO method of function\ncompositions where the form of the composition is known but the constituent\nfunctions are expensive to evaluate. By assuming an independent Gaussian\nprocess (GP) model for each of the constituent black-box function, we propose\nEI and UCB based BO algorithms and demonstrate their ability to outperform\nvanilla BO and the current state-of-art algorithms. We demonstrate a novel\napplication of the proposed methods to dynamic pricing in revenue management\nwhen the underlying demand function is expensive to evaluate.\n","authors":["Kunal Jain","Prabuchandran K. J.","Tejas Bodas"],"pdf_url":"https://arxiv.org/pdf/2303.11954v1.pdf","comment":"16 pages, 6 figures, 1 table, To be published in: 17th Learning And\n  Intelligent Optimization Conference (LION17)"},{"id":"http://arxiv.org/abs/2303.10167v2","updated":"2023-03-21T15:42:16Z","published":"2023-03-17T17:54:25Z","title":"Generalized partitioned local depth","summary":"  In this paper we provide a generalization of the concept of cohesion as\nintroduced recently by Berenhaut, Moore and Melvin [Proceedings of the National\nAcademy of Sciences, 119 (4) (2022)]. The formulation presented builds on the\ntechnique of partitioned local depth by distilling two key probabilistic\nconcepts: local relevance and support division. Earlier results are extended\nwithin the new context, and examples of applications to revealing communities\nin data with uncertainty are included.\n","authors":["Kenneth S. Berenhaut","John D. Foley","Liangdongsheng Lyu"],"pdf_url":"https://arxiv.org/pdf/2303.10167v2.pdf","comment":"Improved exposition, examples in 5.1 & 5.3 expanded, 17 pages, 7\n  figures"},{"id":"http://arxiv.org/abs/2206.04928v5","updated":"2023-03-21T15:35:50Z","published":"2022-06-10T07:52:06Z","title":"GAMR: A Guided Attention Model for (visual) Reasoning","summary":"  Humans continue to outperform modern AI systems in their ability to flexibly\nparse and understand complex visual scenes. Here, we present a novel module for\nvisual reasoning, the Guided Attention Model for (visual) Reasoning (GAMR),\nwhich instantiates an active vision theory -- positing that the brain solves\ncomplex visual reasoning problems dynamically -- via sequences of attention\nshifts to select and route task-relevant visual information into memory.\nExperiments on an array of visual reasoning tasks and datasets demonstrate\nGAMR's ability to learn visual routines in a robust and sample-efficient\nmanner. In addition, GAMR is shown to be capable of zero-shot generalization on\ncompletely novel reasoning tasks. Overall, our work provides computational\nsupport for cognitive theories that postulate the need for a critical interplay\nbetween attention and memory to dynamically maintain and manipulate\ntask-relevant visual information to solve complex visual reasoning tasks.\n","authors":["Mohit Vaishnav","Thomas Serre"],"pdf_url":"https://arxiv.org/pdf/2206.04928v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11932v1","updated":"2023-03-21T15:34:50Z","published":"2023-03-21T15:34:50Z","title":"Using Explanations to Guide Models","summary":"  Deep neural networks are highly performant, but might base their decision on\nspurious or background features that co-occur with certain classes, which can\nhurt generalization. To mitigate this issue, the usage of 'model guidance' has\ngained popularity recently: for this, models are guided to be \"right for the\nright reasons\" by regularizing the models' explanations to highlight the right\nfeatures. Experimental validation of these approaches has thus far however been\nlimited to relatively simple and / or synthetic datasets. To gain a better\nunderstanding of which model-guiding approaches actually transfer to more\nchallenging real-world datasets, in this work we conduct an in-depth evaluation\nacross various loss functions, attribution methods, models, and 'guidance\ndepths' on the PASCAL VOC 2007 and MS COCO 2014 datasets, and show that model\nguidance can sometimes even improve model performance. In this context, we\nfurther propose a novel energy loss, show its effectiveness in directing the\nmodel to focus on object features. We also show that these gains can be\nachieved even with a small fraction (e.g. 1%) of bounding box annotations,\nhighlighting the cost effectiveness of this approach. Lastly, we show that this\napproach can also improve generalization under distribution shifts. Code will\nbe made available.\n","authors":["Sukrut Rao","Moritz Böhle","Amin Parchami-Araghi","Bernt Schiele"],"pdf_url":"https://arxiv.org/pdf/2303.11932v1.pdf","comment":"38 pages, 35 figures, 4 tables"},{"id":"http://arxiv.org/abs/2211.14983v2","updated":"2023-03-21T15:18:48Z","published":"2022-11-28T01:11:11Z","title":"Multiagent Reinforcement Learning for Autonomous Routing and Pickup\n  Problem with Adaptation to Variable Demand","summary":"  We derive a learning framework to generate routing/pickup policies for a\nfleet of autonomous vehicles tasked with servicing stochastically appearing\nrequests on a city map. We focus on policies that 1) give rise to coordination\namongst the vehicles, thereby reducing wait times for servicing requests, 2)\nare non-myopic, and consider a-priori potential future requests, 3) can adapt\nto changes in the underlying demand distribution. Specifically, we are\ninterested in policies that are adaptive to fluctuations of actual demand\nconditions in urban environments, such as on-peak vs. off-peak hours. We\nachieve this through a combination of (i) an online play algorithm that\nimproves the performance of an offline-trained policy, and (ii) an offline\napproximation scheme that allows for adapting to changes in the underlying\ndemand model. In particular, we achieve adaptivity of our learned policy to\ndifferent demand distributions by quantifying a region of validity using the\nq-valid radius of a Wasserstein Ambiguity Set. We propose a mechanism for\nswitching the originally trained offline approximation when the current demand\nis outside the original validity region. In this case, we propose to use an\noffline architecture, trained on a historical demand model that is closer to\nthe current demand in terms of Wasserstein distance. We learn routing and\npickup policies over real taxicab requests in San Francisco with high\nvariability between on-peak and off-peak hours, demonstrating the ability of\nour method to adapt to real fluctuation in demand distributions. Our numerical\nresults demonstrate that our method outperforms alternative rollout-based\nreinforcement learning schemes, as well as other classical methods from\noperations research.\n","authors":["Daniel Garces","Sushmita Bhattacharya","Stephanie Gil","Dimitri Bertsekas"],"pdf_url":"https://arxiv.org/pdf/2211.14983v2.pdf","comment":"8 pages, 6 figures, 3 tables, accepted to ICRA 2023"},{"id":"http://arxiv.org/abs/2303.11920v1","updated":"2023-03-21T15:12:01Z","published":"2023-03-21T15:12:01Z","title":"Do intermediate feature coalitions aid explainability of black-box\n  models?","summary":"  This work introduces the notion of intermediate concepts based on levels\nstructure to aid explainability for black-box models. The levels structure is a\nhierarchical structure in which each level corresponds to features of a dataset\n(i.e., a player-set partition). The level of coarseness increases from the\ntrivial set, which only comprises singletons, to the set, which only contains\nthe grand coalition. In addition, it is possible to establish meronomies, i.e.,\npart-whole relationships, via a domain expert that can be utilised to generate\nexplanations at an abstract level. We illustrate the usability of this approach\nin a real-world car model example and the Titanic dataset, where intermediate\nconcepts aid in explainability at different levels of abstraction.\n","authors":["Minal Suresh Patil","Kary Främling"],"pdf_url":"https://arxiv.org/pdf/2303.11920v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2208.09189v3","updated":"2023-03-21T15:06:37Z","published":"2022-08-19T07:28:31Z","title":"Cross-Domain Evaluation of a Deep Learning-Based Type Inference System","summary":"  Optional type annotations allow for enriching dynamic programming languages\nwith static typing features like better Integrated Development Environment\n(IDE) support, more precise program analysis, and early detection and\nprevention of type-related runtime errors. Machine learning-based type\ninference promises interesting results for automating this task. However, the\npractical usage of such systems depends on their ability to generalize across\ndifferent domains, as they are often applied outside their training domain. In\nthis work, we investigate Type4Py as a representative of state-of-the-art deep\nlearning-based type inference systems, by conducting extensive cross-domain\nexperiments. Thereby, we address the following problems: class imbalances,\nout-of-vocabulary words, dataset shifts, and unknown classes. To perform such\nexperiments, we use the datasets ManyTypes4Py and CrossDomainTypes4Py. The\nlatter we introduce in this paper. Our dataset enables the evaluation of type\ninference systems in different domains of software projects and has over\n1,000,000 type annotations mined on the platforms GitHub and Libraries. It\nconsists of data from the two domains web development and scientific\ncalculation. Through our experiments, we detect that the shifts in the dataset\nand the long-tailed distribution with many rare and unknown data types decrease\nthe performance of the deep learning-based type inference system drastically.\nIn this context, we test unsupervised domain adaptation methods and fine-tuning\nto overcome these issues. Moreover, we investigate the impact of\nout-of-vocabulary words.\n","authors":["Bernd Gruner","Tim Sonnekalb","Thomas S. Heinze","Clemens-Alexander Brust"],"pdf_url":"https://arxiv.org/pdf/2208.09189v3.pdf","comment":"Preprint for the MSR'23 technical track"},{"id":"http://arxiv.org/abs/2303.11911v1","updated":"2023-03-21T15:02:50Z","published":"2023-03-21T15:02:50Z","title":"Time Series Contrastive Learning with Information-Aware Augmentations","summary":"  Various contrastive learning approaches have been proposed in recent years\nand achieve significant empirical success. While effective and prevalent,\ncontrastive learning has been less explored for time series data. A key\ncomponent of contrastive learning is to select appropriate augmentations\nimposing some priors to construct feasible positive samples, such that an\nencoder can be trained to learn robust and discriminative representations.\nUnlike image and language domains where ``desired'' augmented samples can be\ngenerated with the rule of thumb guided by prefabricated human priors, the\nad-hoc manual selection of time series augmentations is hindered by their\ndiverse and human-unrecognizable temporal structures. How to find the desired\naugmentations of time series data that are meaningful for given contrastive\nlearning tasks and datasets remains an open question. In this work, we address\nthe problem by encouraging both high \\textit{fidelity} and \\textit{variety}\nbased upon information theory. A theoretical analysis leads to the criteria for\nselecting feasible data augmentations. On top of that, we propose a new\ncontrastive learning approach with information-aware augmentations, InfoTS,\nthat adaptively selects optimal augmentations for time series representation\nlearning. Experiments on various datasets show highly competitive performance\nwith up to 12.0\\% reduction in MSE on forecasting tasks and up to 3.7\\%\nrelative improvement in accuracy on classification tasks over the leading\nbaselines.\n","authors":["Dongsheng Luo","Wei Cheng","Yingheng Wang","Dongkuan Xu","Jingchao Ni","Wenchao Yu","Xuchao Zhang","Yanchi Liu","Yuncong Chen","Haifeng Chen","Xiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.11911v1.pdf","comment":"Published in AAAI 2023"},{"id":"http://arxiv.org/abs/2303.11908v1","updated":"2023-03-21T14:58:16Z","published":"2023-03-21T14:58:16Z","title":"Non-Asymptotic Pointwise and Worst-Case Bounds for Classical Spectrum\n  Estimators","summary":"  Spectrum estimation is a fundamental methodology in the analysis of\ntime-series data, with applications including medicine, speech analysis, and\ncontrol design. The asymptotic theory of spectrum estimation is\nwell-understood, but the theory is limited when the number of samples is fixed\nand finite. This paper gives non-asymptotic error bounds for a broad class of\nspectral estimators, both pointwise (at specific frequencies) and in the worst\ncase over all frequencies. The general method is used to derive error bounds\nfor the classical Blackman-Tukey, Bartlett, and Welch estimators.\n","authors":["Andrew Lamperski"],"pdf_url":"https://arxiv.org/pdf/2303.11908v1.pdf","comment":"12 pages, under review in IEEE Transactions on Signal Processing"},{"id":"http://arxiv.org/abs/2205.00701v3","updated":"2023-03-21T14:39:49Z","published":"2022-05-02T07:45:51Z","title":"DeepGraviLens: a Multi-Modal Architecture for Classifying Gravitational\n  Lensing Data","summary":"  Gravitational lensing is the relativistic effect generated by massive bodies,\nwhich bend the space-time surrounding them. It is a deeply investigated topic\nin astrophysics and allows validating theoretical relativistic results and\nstudying faint astrophysical objects that would not be visible otherwise. In\nrecent years Machine Learning methods have been applied to support the analysis\nof the gravitational lensing phenomena by detecting lensing effects in data\nsets consisting of images associated with brightness variation time series.\nHowever, the state-of-art approaches either consider only images and neglect\ntime-series data or achieve relatively low accuracy on the most difficult data\nsets. This paper introduces DeepGraviLens, a novel multi-modal network that\nclassifies spatio-temporal data belonging to one non-lensed system type and\nthree lensed system types. It surpasses the current state of the art accuracy\nresults by $\\approx$ 19% to $\\approx$ 43%, depending on the considered data\nset. Such an improvement will enable the acceleration of the analysis of lensed\nobjects in upcoming astrophysical surveys, which will exploit the petabytes of\ndata collected, e.g., from the Vera C. Rubin Observatory.\n","authors":["Nicolò Oreste Pinciroli Vago","Piero Fraternali"],"pdf_url":"https://arxiv.org/pdf/2205.00701v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11884v1","updated":"2023-03-21T14:24:58Z","published":"2023-03-21T14:24:58Z","title":"Better Understanding Differences in Attribution Methods via Systematic\n  Evaluations","summary":"  Deep neural networks are very successful on many vision tasks, but hard to\ninterpret due to their black box nature. To overcome this, various post-hoc\nattribution methods have been proposed to identify image regions most\ninfluential to the models' decisions. Evaluating such methods is challenging\nsince no ground truth attributions exist. We thus propose three novel\nevaluation schemes to more reliably measure the faithfulness of those methods,\nto make comparisons between them more fair, and to make visual inspection more\nsystematic. To address faithfulness, we propose a novel evaluation setting\n(DiFull) in which we carefully control which parts of the input can influence\nthe output in order to distinguish possible from impossible attributions. To\naddress fairness, we note that different methods are applied at different\nlayers, which skews any comparison, and so evaluate all methods on the same\nlayers (ML-Att) and discuss how this impacts their performance on quantitative\nmetrics. For more systematic visualizations, we propose a scheme (AggAtt) to\nqualitatively evaluate the methods on complete datasets. We use these\nevaluation schemes to study strengths and shortcomings of some widely used\nattribution methods over a wide range of models. Finally, we propose a\npost-processing smoothing step that significantly improves the performance of\nsome attribution methods, and discuss its applicability.\n","authors":["Sukrut Rao","Moritz Böhle","Bernt Schiele"],"pdf_url":"https://arxiv.org/pdf/2303.11884v1.pdf","comment":"35 pages, 37 figures, 2 tables, extended version of arXiv:2205.10435"},{"id":"http://arxiv.org/abs/2303.11881v1","updated":"2023-03-21T14:24:26Z","published":"2023-03-21T14:24:26Z","title":"Protective Self-Adaptive Pruning to Better Compress DNNs","summary":"  Adaptive network pruning approach has recently drawn significant attention\ndue to its excellent capability to identify the importance and redundancy of\nlayers and filters and customize a suitable pruning solution. However, it\nremains unsatisfactory since current adaptive pruning methods rely mostly on an\nadditional monitor to score layer and filter importance, and thus faces high\ncomplexity and weak interpretability. To tackle these issues, we have deeply\nresearched the weight reconstruction process in iterative prune-train process\nand propose a Protective Self-Adaptive Pruning (PSAP) method. First of all,\nPSAP can utilize its own information, weight sparsity ratio, to adaptively\nadjust pruning ratio of layers before each pruning step. Moreover, we propose a\nprotective reconstruction mechanism to prevent important filters from being\npruned through supervising gradients and to avoid unrecoverable information\nloss as well. Our PSAP is handy and explicit because it merely depends on\nweights and gradients of model itself, instead of requiring an additional\nmonitor as in early works. Experiments on ImageNet and CIFAR-10 also\ndemonstrate its superiority to current works in both accuracy and compression\nratio, especially for compressing with a high ratio or pruning from scratch.\n","authors":["Liang Li","Pengfei Zhao"],"pdf_url":"https://arxiv.org/pdf/2303.11881v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.01593v2","updated":"2023-03-21T14:22:00Z","published":"2023-03-02T21:35:15Z","title":"QAID: Question Answering Inspired Few-shot Intent Detection","summary":"  Intent detection with semantically similar fine-grained intents is a\nchallenging task. To address it, we reformulate intent detection as a\nquestion-answering retrieval task by treating utterances and intent names as\nquestions and answers. To that end, we utilize a question-answering retrieval\narchitecture and adopt a two stages training schema with batch contrastive\nloss. In the pre-training stage, we improve query representations through\nself-supervised training. Then, in the fine-tuning stage, we increase\ncontextualized token-level similarity scores between queries and answers from\nthe same intent. Our results on three few-shot intent detection benchmarks\nachieve state-of-the-art performance.\n","authors":["Asaf Yehudai","Matan Vetzler","Yosi Mass","Koren Lazar","Doron Cohen","Boaz Carmeli"],"pdf_url":"https://arxiv.org/pdf/2303.01593v2.pdf","comment":"ICLR paper"},{"id":"http://arxiv.org/abs/2303.11873v1","updated":"2023-03-21T14:17:29Z","published":"2023-03-21T14:17:29Z","title":"A Tale of Two Circuits: Grokking as Competition of Sparse and Dense\n  Subnetworks","summary":"  Grokking is a phenomenon where a model trained on an algorithmic task first\noverfits but, then, after a large amount of additional training, undergoes a\nphase transition to generalize perfectly. We empirically study the internal\nstructure of networks undergoing grokking on the sparse parity task, and find\nthat the grokking phase transition corresponds to the emergence of a sparse\nsubnetwork that dominates model predictions. On an optimization level, we find\nthat this subnetwork arises when a small subset of neurons undergoes rapid norm\ngrowth, whereas the other neurons in the network decay slowly in norm. Thus, we\nsuggest that the grokking phase transition can be understood to emerge from\ncompetition of two largely distinct subnetworks: a dense one that dominates\nbefore the transition and generalizes poorly, and a sparse one that dominates\nafterwards.\n","authors":["William Merrill","Nikolaos Tsilivis","Aman Shukla"],"pdf_url":"https://arxiv.org/pdf/2303.11873v1.pdf","comment":"Published at the Workshop on Understanding Foundation Models at ICLR\n  2023"},{"id":"http://arxiv.org/abs/2302.11577v2","updated":"2023-03-21T14:16:06Z","published":"2023-01-25T10:34:38Z","title":"Explainable AI does not provide the explanations end-users are asking\n  for","summary":"  Explainable Artificial Intelligence (XAI) techniques are frequently required\nby users in many AI systems with the goal of understanding complex models,\ntheir associated predictions, and gaining trust. While suitable for some\nspecific tasks during development, their adoption by organisations to enhance\ntrust in machine learning systems has unintended consequences. In this paper we\ndiscuss XAI's limitations in deployment and conclude that transparency\nalongside with rigorous validation are better suited to gaining trust in AI\nsystems.\n","authors":["Savio Rozario","George Čevora"],"pdf_url":"https://arxiv.org/pdf/2302.11577v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.01254v2","updated":"2023-03-21T14:11:37Z","published":"2023-02-13T10:33:21Z","title":"Privacy-Preserving Tree-Based Inference with Fully Homomorphic\n  Encryption","summary":"  Privacy enhancing technologies (PETs) have been proposed as a way to protect\nthe privacy of data while still allowing for data analysis. In this work, we\nfocus on Fully Homomorphic Encryption (FHE), a powerful tool that allows for\narbitrary computations to be performed on encrypted data. FHE has received lots\nof attention in the past few years and has reached realistic execution times\nand correctness.\n  More precisely, we explain in this paper how we apply FHE to tree-based\nmodels and get state-of-the-art solutions over encrypted tabular data. We show\nthat our method is applicable to a wide range of tree-based models, including\ndecision trees, random forests, and gradient boosted trees, and has been\nimplemented within the Concrete-ML library, which is open-source at\nhttps://github.com/zama-ai/concrete-ml. With a selected set of use-cases, we\ndemonstrate that our FHE version is very close to the unprotected version in\nterms of accuracy.\n","authors":["Jordan Frery","Andrei Stoian","Roman Bredehoft","Luis Montero","Celia Kherfallah","Benoit Chevallier-Mames","Arthur Meyre"],"pdf_url":"https://arxiv.org/pdf/2303.01254v2.pdf","comment":null},{"id":"http://arxiv.org/abs/1911.02621v3","updated":"2023-03-21T14:10:36Z","published":"2019-11-06T20:29:56Z","title":"The Threat of Adversarial Attacks on Machine Learning in Network\n  Security -- A Survey","summary":"  Machine learning models have made many decision support systems to be faster,\nmore accurate, and more efficient. However, applications of machine learning in\nnetwork security face a more disproportionate threat of active adversarial\nattacks compared to other domains. This is because machine learning\napplications in network security such as malware detection, intrusion\ndetection, and spam filtering are by themselves adversarial in nature. In what\ncould be considered an arm's race between attackers and defenders, adversaries\nconstantly probe machine learning systems with inputs that are explicitly\ndesigned to bypass the system and induce a wrong prediction. In this survey, we\nfirst provide a taxonomy of machine learning techniques, tasks, and depth. We\nthen introduce a classification of machine learning in network security\napplications. Next, we examine various adversarial attacks against machine\nlearning in network security and introduce two classification approaches for\nadversarial attacks in network security. First, we classify adversarial attacks\nin network security based on a taxonomy of network security applications.\nSecondly, we categorize adversarial attacks in network security into a problem\nspace vs feature space dimensional classification model. We then analyze the\nvarious defenses against adversarial attacks on machine learning-based network\nsecurity applications. We conclude by introducing an adversarial risk grid map\nand evaluating several existing adversarial attacks against machine learning in\nnetwork security using the risk grid map. We also identify where each attack\nclassification resides within the adversarial risk grid map.\n","authors":["Olakunle Ibitoye","Rana Abou-Khamis","Mohamed el Shehaby","Ashraf Matrawy","M. Omair Shafiq"],"pdf_url":"https://arxiv.org/pdf/1911.02621v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11863v1","updated":"2023-03-21T14:06:12Z","published":"2023-03-21T14:06:12Z","title":"Continual Learning in the Presence of Spurious Correlation","summary":"  Most continual learning (CL) algorithms have focused on tackling the\nstability-plasticity dilemma, that is, the challenge of preventing the\nforgetting of previous tasks while learning new ones. However, they have\noverlooked the impact of the knowledge transfer when the dataset in a certain\ntask is biased - namely, when some unintended spurious correlations of the\ntasks are learned from the biased dataset. In that case, how would they affect\nlearning future tasks or the knowledge already learned from the past tasks? In\nthis work, we carefully design systematic experiments using one synthetic and\ntwo real-world datasets to answer the question from our empirical findings.\nSpecifically, we first show through two-task CL experiments that standard CL\nmethods, which are unaware of dataset bias, can transfer biases from one task\nto another, both forward and backward, and this transfer is exacerbated\ndepending on whether the CL methods focus on the stability or the plasticity.\nWe then present that the bias transfer also exists and even accumulate in\nlonger sequences of tasks. Finally, we propose a simple, yet strong plug-in\nmethod for debiasing-aware continual learning, dubbed as Group-class Balanced\nGreedy Sampling (BGS). As a result, we show that our BGS can always reduce the\nbias of a CL model, with a slight loss of CL performance at most.\n","authors":["Donggyu Lee","Sangwon Jung","Taesup Moon"],"pdf_url":"https://arxiv.org/pdf/2303.11863v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11860v1","updated":"2023-03-21T13:59:35Z","published":"2023-03-21T13:59:35Z","title":"Online Transformers with Spiking Neurons for Fast Prosthetic Hand\n  Control","summary":"  Transformers are state-of-the-art networks for most sequence processing\ntasks. However, the self-attention mechanism often used in Transformers\nrequires large time windows for each computation step and thus makes them less\nsuitable for online signal processing compared to Recurrent Neural Networks\n(RNNs). In this paper, instead of the self-attention mechanism, we use a\nsliding window attention mechanism. We show that this mechanism is more\nefficient for continuous signals with finite-range dependencies between input\nand target, and that we can use it to process sequences element-by-element,\nthis making it compatible with online processing. We test our model on a finger\nposition regression dataset (NinaproDB8) with Surface Electromyographic (sEMG)\nsignals measured on the forearm skin to estimate muscle activities. Our\napproach sets the new state-of-the-art in terms of accuracy on this dataset\nwhile requiring only very short time windows of 3.5 ms at each inference step.\nMoreover, we increase the sparsity of the network using Leaky-Integrate and\nFire (LIF) units, a bio-inspired neuron model that activates sparsely in time\nsolely when crossing a threshold. We thus reduce the number of synaptic\noperations up to a factor of $\\times5.3$ without loss of accuracy. Our results\nhold great promises for accurate and fast online processing of sEMG signals for\nsmooth prosthetic hand control and is a step towards Transformers and Spiking\nNeural Networks (SNNs) co-integration for energy efficient temporal signal\nprocessing.\n","authors":["Nathan Leroux","Jan Finkbeiner","Emre Neftci"],"pdf_url":"https://arxiv.org/pdf/2303.11860v1.pdf","comment":"Preprint of 9 pages, 4 figures"},{"id":"http://arxiv.org/abs/2303.11848v1","updated":"2023-03-21T13:48:53Z","published":"2023-03-21T13:48:53Z","title":"Dens-PU: PU Learning with Density-Based Positive Labeled Augmentation","summary":"  This study proposes a novel approach for solving the PU learning problem\nbased on an anomaly-detection strategy. Latent encodings extracted from\npositive-labeled data are linearly combined to acquire new samples. These new\nsamples are used as embeddings to increase the density of positive-labeled data\nand, thus, define a boundary that approximates the positive class. The further\na sample is from the boundary the more it is considered as a negative sample.\nOnce a set of negative samples is obtained, the PU learning problem reduces to\nbinary classification. The approach, named Dens-PU due to its reliance on the\ndensity of positive-labeled data, was evaluated using benchmark image datasets,\nand state-of-the-art results were attained.\n","authors":["Vasileios Sevetlidis","George Pavlidis","Spyridon Mouroutsos","Antonios Gasteratos"],"pdf_url":"https://arxiv.org/pdf/2303.11848v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09280v2","updated":"2023-03-21T13:48:00Z","published":"2023-03-13T12:44:32Z","title":"Topology optimization with physics-informed neural networks: application\n  to noninvasive detection of hidden geometries","summary":"  Detecting hidden geometrical structures from surface measurements under\nelectromagnetic, acoustic, or mechanical loading is the goal of noninvasive\nimaging techniques in medical and industrial applications. Solving the inverse\nproblem can be challenging due to the unknown topology and geometry, the\nsparsity of the data, and the complexity of the physical laws. Physics-informed\nneural networks (PINNs) have shown promise as a simple-yet-powerful tool for\nproblem inversion, but they have yet to be applied to general problems with a\npriori unknown topology. Here, we introduce a topology optimization framework\nbased on PINNs that solves geometry detection problems without prior knowledge\nof the number or types of shapes. We allow for arbitrary solution topology by\nrepresenting the geometry using a material density field that approaches binary\nvalues thanks to a novel eikonal regularization. We validate our framework by\ndetecting the number, locations, and shapes of hidden voids and inclusions in\nlinear and nonlinear elastic bodies using measurements of outer surface\ndisplacement from a single mechanical loading experiment. Our methodology opens\na pathway for PINNs to solve various engineering problems targeting geometry\noptimization.\n","authors":["Saviz Mowlavi","Ken Kamrin"],"pdf_url":"https://arxiv.org/pdf/2303.09280v2.pdf","comment":"24 pages, 16 figures including supplementary information. Added\n  supplementary movies"},{"id":"http://arxiv.org/abs/2303.11844v1","updated":"2023-03-21T13:42:43Z","published":"2023-03-21T13:42:43Z","title":"Doubly Regularized Entropic Wasserstein Barycenters","summary":"  We study a general formulation of regularized Wasserstein barycenters that\nenjoys favorable regularity, approximation, stability and (grid-free)\noptimization properties. This barycenter is defined as the unique probability\nmeasure that minimizes the sum of entropic optimal transport (EOT) costs with\nrespect to a family of given probability measures, plus an entropy term. We\ndenote it $(\\lambda,\\tau)$-barycenter, where $\\lambda$ is the inner\nregularization strength and $\\tau$ the outer one. This formulation recovers\nseveral previously proposed EOT barycenters for various choices of\n$\\lambda,\\tau \\geq 0$ and generalizes them. First, in spite of -- and in fact\nowing to -- being \\emph{doubly} regularized, we show that our formulation is\ndebiased for $\\tau=\\lambda/2$: the suboptimality in the (unregularized)\nWasserstein barycenter objective is, for smooth densities, of the order of the\nstrength $\\lambda^2$ of entropic regularization, instead of\n$\\max\\{\\lambda,\\tau\\}$ in general. We discuss this phenomenon for isotropic\nGaussians where all $(\\lambda,\\tau)$-barycenters have closed form. Second, we\nshow that for $\\lambda,\\tau>0$, this barycenter has a smooth density and is\nstrongly stable under perturbation of the marginals. In particular, it can be\nestimated efficiently: given $n$ samples from each of the probability measures,\nit converges in relative entropy to the population barycenter at a rate\n$n^{-1/2}$. And finally, this formulation lends itself naturally to a grid-free\noptimization algorithm: we propose a simple \\emph{noisy particle gradient\ndescent} which, in the mean-field limit, converges globally at an exponential\nrate to the barycenter.\n","authors":["Lénaïc Chizat"],"pdf_url":"https://arxiv.org/pdf/2303.11844v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.08326v2","updated":"2023-03-21T13:37:04Z","published":"2022-11-14T10:07:30Z","title":"Contrastive learning for regression in multi-site brain age prediction","summary":"  Building accurate Deep Learning (DL) models for brain age prediction is a\nvery relevant topic in neuroimaging, as it could help better understand\nneurodegenerative disorders and find new biomarkers. To estimate accurate and\ngeneralizable models, large datasets have been collected, which are often\nmulti-site and multi-scanner. This large heterogeneity negatively affects the\ngeneralization performance of DL models since they are prone to overfit\nsite-related noise. Recently, contrastive learning approaches have been shown\nto be more robust against noise in data or labels. For this reason, we propose\na novel contrastive learning regression loss for robust brain age prediction\nusing MRI scans. Our method achieves state-of-the-art performance on the\nOpenBHB challenge, yielding the best generalization capability and robustness\nto site-related noise.\n","authors":["Carlo Alberto Barbano","Benoit Dufumier","Edouard Duchesnay","Marco Grangetto","Pietro Gori"],"pdf_url":"https://arxiv.org/pdf/2211.08326v2.pdf","comment":"5 pages"},{"id":"http://arxiv.org/abs/2303.11833v1","updated":"2023-03-21T13:21:43Z","published":"2023-03-21T13:21:43Z","title":"Materials Discovery with Extreme Properties via AI-Driven Combinatorial\n  Chemistry","summary":"  The goal of most materials discovery is to discover materials that are\nsuperior to those currently known. Fundamentally, this is close to\nextrapolation, which is a weak point for most machine learning models that\nlearn the probability distribution of data. Herein, we develop AI-driven\ncombinatorial chemistry, which is a rule-based inverse molecular designer that\ndoes not rely on data. Since our model has the potential to generate all\npossible molecular structures that can be obtained from combinations of\nmolecular fragments, unknown materials with superior properties can be\ndiscovered. We theoretically and empirically demonstrate that our model is more\nsuitable for discovering better materials than probability\ndistribution-learning models. In an experiment aimed at discovering molecules\nthat hit seven target properties, our model discovered 1,315 of all\ntarget-hitting molecules and 7,629 of five target-hitting molecules out of\n100,000 trials, whereas the probability distribution-learning models failed. To\nillustrate the performance in actual problems, we also demonstrate that our\nmodels work well on two practical applications: discovering protein docking\nmaterials and HIV inhibitors.\n","authors":["Hyunseung Kim","Haeyeon Choi","Dongju Kang","Won Bo Lee","Jonggeol Na"],"pdf_url":"https://arxiv.org/pdf/2303.11833v1.pdf","comment":"30 pages, 5 figures"},{"id":"http://arxiv.org/abs/2303.11831v1","updated":"2023-03-21T13:19:51Z","published":"2023-03-21T13:19:51Z","title":"GLADE: Gradient Loss Augmented Degradation Enhancement for Unpaired\n  Super-Resolution of Anisotropic MRI","summary":"  We present a novel approach to synthesise high-resolution isotropic 3D\nabdominal MR images, from anisotropic 3D images in an unpaired fashion. Using a\nmodified CycleGAN architecture with a gradient mapping loss, we leverage\ndisjoint patches from the high-resolution (in-plane) data of an anisotropic\nvolume to enforce the network generator to increase the resolution of the\nlow-resolution (through-plane) slices. This will enable accelerated\nwhole-abdomen scanning with high-resolution isotropic images within short\nbreath-hold times.\n","authors":["Michele Pascale","Vivek Muthurangu","Javier Montalt Tordera","Heather E Fitzke","Gauraang Bhatnagar","Stuart Taylor","Jennifer Steeden"],"pdf_url":"https://arxiv.org/pdf/2303.11831v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.07120v3","updated":"2023-03-21T13:09:33Z","published":"2022-03-14T14:08:46Z","title":"Neural Message Passing for Objective-Based Uncertainty Quantification\n  and Optimal Experimental Design","summary":"  Various real-world scientific applications involve the mathematical modeling\nof complex uncertain systems with numerous unknown parameters. Accurate\nparameter estimation is often practically infeasible in such systems, as the\navailable training data may be insufficient and the cost of acquiring\nadditional data may be high. In such cases, based on a Bayesian paradigm, we\ncan design robust operators retaining the best overall performance across all\npossible models and design optimal experiments that can effectively reduce\nuncertainty to enhance the performance of such operators maximally. While\nobjective-based uncertainty quantification (objective-UQ) based on MOCU (mean\nobjective cost of uncertainty) provides an effective means for quantifying\nuncertainty in complex systems, the high computational cost of estimating MOCU\nhas been a challenge in applying it to real-world scientific/engineering\nproblems. In this work, we propose a novel scheme to reduce the computational\ncost for objective-UQ via MOCU based on a data-driven approach. We adopt a\nneural message-passing model for surrogate modeling, incorporating a novel\naxiomatic constraint loss that penalizes an increase in the estimated system\nuncertainty. As an illustrative example, we consider the optimal experimental\ndesign (OED) problem for uncertain Kuramoto models, where the goal is to\npredict the experiments that can most effectively enhance robust\nsynchronization performance through uncertainty reduction. We show that our\nproposed approach can accelerate MOCU-based OED by four to five orders of\nmagnitude, without any visible performance loss compared to the\nstate-of-the-art. The proposed approach applies to general OED tasks, beyond\nthe Kuramoto model.\n","authors":["Qihua Chen","Xuejin Chen","Hyun-Myung Woo","Byung-Jun Yoon"],"pdf_url":"https://arxiv.org/pdf/2203.07120v3.pdf","comment":"14 pages, 5 figures, accepted by Engineering Applications of\n  Artificial Intelligence"},{"id":"http://arxiv.org/abs/2302.02119v2","updated":"2023-03-21T13:05:50Z","published":"2023-02-04T07:31:36Z","title":"Diversity Induced Environment Design via Self-Play","summary":"  Recent work on designing an appropriate distribution of environments has\nshown promise for training effective generally capable agents. Its success is\npartly because of a form of adaptive curriculum learning that generates\nenvironment instances (or levels) at the frontier of the agent's capabilities.\nHowever, such an environment design framework often struggles to find effective\nlevels in challenging design spaces and requires costly interactions with the\nenvironment. In this paper, we aim to introduce diversity in the Unsupervised\nEnvironment Design (UED) framework. Specifically, we propose a task-agnostic\nmethod to identify observed/hidden states that are representative of a given\nlevel. The outcome of this method is then utilized to characterize the\ndiversity between two levels, which as we show can be crucial to effective\nperformance. In addition, to improve sampling efficiency, we incorporate the\nself-play technique that allows the environment generator to automatically\ngenerate environments that are of great benefit to the training agent.\nQuantitatively, our approach, Diversity-induced Environment Design via\nSelf-Play (DivSP), shows compelling performance over existing methods.\n","authors":["Dexun Li","Wenjun Li","Pradeep Varakantham"],"pdf_url":"https://arxiv.org/pdf/2302.02119v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.08722v3","updated":"2023-03-21T12:59:20Z","published":"2023-02-17T06:33:06Z","title":"GPT4MIA: Utilizing Generative Pre-trained Transformer (GPT-3) as A\n  Plug-and-Play Transductive Model for Medical Image Analysis","summary":"  In this paper, we propose a novel approach (called GPT4MIA) that utilizes\nGenerative Pre-trained Transformer (GPT) as a plug-and-play transductive\ninference tool for medical image analysis (MIA). We provide theoretical\nanalysis on why a large pre-trained language model such as GPT-3 can be used as\na plug-and-play transductive inference model for MIA. At the methodological\nlevel, we develop several technical treatments to improve the efficiency and\neffectiveness of GPT4MIA, including better prompt structure design, sample\nselection, and prompt ordering of representative samples/features. We present\ntwo concrete use cases (with workflow) of GPT4MIA: (1) detecting prediction\nerrors and (2) improving prediction accuracy, working in conjecture with\nwell-established vision-based models for image classification (e.g., ResNet).\nExperiments validate that our proposed method is effective for these two tasks.\nWe further discuss the opportunities and challenges in utilizing\nTransformer-based large language models for broader MIA applications.\n","authors":["Yizhe Zhang","Danny Z. Chen"],"pdf_url":"https://arxiv.org/pdf/2302.08722v3.pdf","comment":"Version 3: Added appendix with more results and visualizations.\n  Questions and suggestions are welcome"},{"id":"http://arxiv.org/abs/2303.11809v1","updated":"2023-03-21T12:50:17Z","published":"2023-03-21T12:50:17Z","title":"Addressing Class Variable Imbalance in Federated Semi-supervised\n  Learning","summary":"  Federated Semi-supervised Learning (FSSL) combines techniques from both\nfields of federated and semi-supervised learning to improve the accuracy and\nperformance of models in a distributed environment by using a small fraction of\nlabeled data and a large amount of unlabeled data. Without the need to\ncentralize all data in one place for training, it collect updates of model\ntraining after devices train models at local, and thus can protect the privacy\nof user data. However, during the federal training process, some of the devices\nfail to collect enough data for local training, while new devices will be\nincluded to the group training. This leads to an unbalanced global data\ndistribution and thus affect the performance of the global model training. Most\nof the current research is focusing on class imbalance with a fixed number of\nclasses, while little attention is paid to data imbalance with a variable\nnumber of classes. Therefore, in this paper, we propose Federated\nSemi-supervised Learning for Class Variable Imbalance (FCVI) to solve class\nvariable imbalance. The class-variable learning algorithm is used to mitigate\nthe data imbalance due to changes of the number of classes. Our scheme is\nproved to be significantly better than baseline methods, while maintaining\nclient privacy.\n","authors":["Zehui Dong","Wenjing Liu","Siyuan Liu","Xingzhi Chen"],"pdf_url":"https://arxiv.org/pdf/2303.11809v1.pdf","comment":"12th International Conference on Cloud Computing: Services and\n  Architecture (CLOUD 2023)"},{"id":"http://arxiv.org/abs/2303.11239v2","updated":"2023-03-21T12:43:11Z","published":"2023-03-20T16:24:06Z","title":"Training Invertible Neural Networks as Autoencoders","summary":"  Autoencoders are able to learn useful data representations in an unsupervised\nmatter and have been widely used in various machine learning and computer\nvision tasks. In this work, we present methods to train Invertible Neural\nNetworks (INNs) as (variational) autoencoders which we call INN (variational)\nautoencoders. Our experiments on MNIST, CIFAR and CelebA show that for low\nbottleneck sizes our INN autoencoder achieves results similar to the classical\nautoencoder. However, for large bottleneck sizes our INN autoencoder\noutperforms its classical counterpart. Based on the empirical results, we\nhypothesize that INN autoencoders might not have any intrinsic information loss\nand thereby are not bounded to a maximal number of layers (depth) after which\nonly suboptimal results can be achieved.\n","authors":["The-Gia Leo Nguyen","Lynton Ardizzone","Ullrich Köthe"],"pdf_url":"https://arxiv.org/pdf/2303.11239v2.pdf","comment":"Conference Paper at GCPR2019"},{"id":"http://arxiv.org/abs/2302.09738v2","updated":"2023-03-21T12:42:29Z","published":"2023-02-20T03:31:11Z","title":"Simplifying Momentum-based Riemannian Submanifold Optimization","summary":"  Riemannian submanifold optimization with momentum is computationally\nchallenging because ensuring iterates remain on the submanifold often requires\nsolving difficult differential equations. We simplify such optimization\nalgorithms for the submanifold of symmetric positive-definite matrices with the\naffine invariant metric. We propose a generalized version of the Riemannian\nnormal coordinates which dynamically trivializes the problem into a Euclidean\nunconstrained problem. We use our approach to explain and simplify existing\napproaches for structured covariances and develop efficient second-order\noptimizers for deep learning without explicit matrix inverses.\n","authors":["Wu Lin","Valentin Duruisseaux","Melvin Leok","Frank Nielsen","Mohammad Emtiyaz Khan","Mark Schmidt"],"pdf_url":"https://arxiv.org/pdf/2302.09738v2.pdf","comment":"updated the main text and added more numerical results"},{"id":"http://arxiv.org/abs/2211.11561v2","updated":"2023-03-21T12:34:27Z","published":"2022-11-18T16:58:23Z","title":"SAMSON: Sharpness-Aware Minimization Scaled by Outlier Normalization for\n  Improving DNN Generalization and Robustness","summary":"  Energy-efficient deep neural network (DNN) accelerators are prone to\nnon-idealities that degrade DNN performance at inference time. To mitigate such\ndegradation, existing methods typically add perturbations to the DNN weights\nduring training to simulate inference on noisy hardware. However, this often\nrequires knowledge about the target hardware and leads to a trade-off between\nDNN performance and robustness, decreasing the former to increase the latter.\nIn this work, we show that applying sharpness-aware training, by optimizing for\nboth the loss value and loss sharpness, significantly improves robustness to\nnoisy hardware at inference time without relying on any assumptions about the\ntarget hardware. In particular, we propose a new adaptive sharpness-aware\nmethod that conditions the worst-case perturbation of a given weight not only\non its magnitude but also on the range of the weight distribution. This is\nachieved by performing sharpness-aware minimization scaled by outlier\nminimization (SAMSON). Our approach outperforms existing sharpness-aware\ntraining methods both in terms of model generalization performance in noiseless\nregimes and robustness in noisy settings, as measured on several architectures\nand datasets.\n","authors":["Gonçalo Mordido","Sébastien Henwood","Sarath Chandar","François Leduc-Primeau"],"pdf_url":"https://arxiv.org/pdf/2211.11561v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2303.11774v1","updated":"2023-03-21T11:45:27Z","published":"2023-03-21T11:45:27Z","title":"Exact Non-Oblivious Performance of Rademacher Random Embeddings","summary":"  This paper revisits the performance of Rademacher random projections,\nestablishing novel statistical guarantees that are numerically sharp and\nnon-oblivious with respect to the input data. More specifically, the central\nresult is the Schur-concavity property of Rademacher random projections with\nrespect to the inputs. This offers a novel geometric perspective on the\nperformance of random projections, while improving quantitatively on bounds\nfrom previous works. As a corollary of this broader result, we obtained the\nimproved performance on data which is sparse or is distributed with small\nspread. This non-oblivious analysis is a novelty compared to techniques from\nprevious work, and bridges the frequently observed gap between theory and\npractise. The main result uses an algebraic framework for proving\nSchur-concavity properties, which is a contribution of independent interest and\nan elegant alternative to derivative-based criteria.\n","authors":["Maciej Skorski","Alessandro Temperoni"],"pdf_url":"https://arxiv.org/pdf/2303.11774v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.05599v3","updated":"2023-03-21T11:42:11Z","published":"2023-01-13T15:04:32Z","title":"Short-length SSVEP data extension by a novel generative adversarial\n  networks based framework","summary":"  Steady-state visual evoked potentials (SSVEPs) based brain-computer interface\n(BCI) has received considerable attention due to its high information transfer\nrate (ITR) and available quantity of targets. However, the performance of\nfrequency identification methods heavily hinges on the amount of user\ncalibration data and data length, which hinders the deployment in real-world\napplications. Recently, generative adversarial networks (GANs)-based data\ngeneration methods have been widely adopted to create synthetic\nelectroencephalography (EEG) data, holds promise to address these issues. In\nthis paper, we proposed a GAN-based end-to-end signal transformation network\nfor data length extension, termed as TEGAN. TEGAN transforms short-length SSVEP\nsignals into long-length artificial SSVEP signals. By incorporating a novel\nU-Net generator architecture and an auxiliary classifier into the network\narchitecture, the TEGAN could produce conditioned features in the synthetic\ndata. Additionally, we introduced a two-stage training strategy and the\nLeCam-divergence regularization term to regularize the training process of GAN\nduring the network implementation. The proposed TEGAN was evaluated on two\npublic SSVEP datasets (a 4-class dataset and a 12-class dataset). With the\nassistance of TEGAN, the performance of traditional frequency recognition\nmethods and deep learning-based methods have been significantly improved under\nlimited calibration data. And the classification performance gap of various\nfrequency recognition methods has been narrowed. This study substantiates the\nfeasibility of the proposed method to extend the data length for short-time\nSSVEP signals for developing a high-performance BCI system. The proposed\nGAN-based methods have the great potential of shortening the calibration time\nand cutting down the budget for various real-world BCI-based applications.\n","authors":["Yudong Pan","Ning Li","Yangsong Zhang","Peng Xu","Dezhong Yao"],"pdf_url":"https://arxiv.org/pdf/2301.05599v3.pdf","comment":"16 pages, 10 figures, 2 tables"},{"id":"http://arxiv.org/abs/2207.01955v3","updated":"2023-03-21T11:41:14Z","published":"2022-07-05T10:58:11Z","title":"Ask-AC: An Initiative Advisor-in-the-Loop Actor-Critic Framework","summary":"  Despite the promising results achieved, state-of-the-art interactive\nreinforcement learning schemes rely on passively receiving supervision signals\nfrom advisor experts, in the form of either continuous monitoring or\npre-defined rules, which inevitably result in a cumbersome and expensive\nlearning process. In this paper, we introduce a novel initiative\nadvisor-in-the-loop actor-critic framework, termed as Ask-AC, that replaces the\nunilateral advisor-guidance mechanism with a bidirectional learner-initiative\none, and thereby enables a customized and efficacious message exchange between\nlearner and advisor. At the heart of Ask-AC are two complementary components,\nnamely action requester and adaptive state selector, that can be readily\nincorporated into various discrete actor-critic architectures. The former\ncomponent allows the agent to initiatively seek advisor intervention in the\npresence of uncertain states, while the latter identifies the unstable states\npotentially missed by the former especially when environment changes, and then\nlearns to promote the ask action on such states. Experimental results on both\nstationary and non-stationary environments and across different actor-critic\nbackbones demonstrate that the proposed framework significantly improves the\nlearning efficiency of the agent, and achieves the performances on par with\nthose obtained by continuous advisor monitoring.\n","authors":["Shunyu Liu","Na Yu","Jie Song","Kaixuan Chen","Zunlei Feng","Mingli Song"],"pdf_url":"https://arxiv.org/pdf/2207.01955v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.07389v2","updated":"2023-03-21T11:40:47Z","published":"2023-01-18T09:36:41Z","title":"Towards Models that Can See and Read","summary":"  Visual Question Answering (VQA) and Image Captioning (CAP), which are among\nthe most popular vision-language tasks, have analogous scene-text versions that\nrequire reasoning from the text in the image. Despite their obvious\nresemblance, the two are treated independently and, as we show, yield\ntask-specific methods that can either see or read, but not both. In this work,\nwe conduct an in-depth analysis of this phenomenon and propose UniTNT, a\nUnified Text-Non-Text approach, which grants existing multimodal architectures\nscene-text understanding capabilities. Specifically, we treat scene-text\ninformation as an additional modality, fusing it with any pretrained\nencoder-decoder-based architecture via designated modules. Thorough experiments\nreveal that UniTNT leads to the first single model that successfully handles\nboth task types. Moreover, we show that scene-text understanding capabilities\ncan boost vision-language models' performance on general VQA and CAP by up to\n2.69% and 0.6 CIDEr, respectively.\n","authors":["Roy Ganz","Oren Nuriel","Aviad Aberdam","Yair Kittenplon","Shai Mazor","Ron Litman"],"pdf_url":"https://arxiv.org/pdf/2301.07389v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11761v1","updated":"2023-03-21T11:28:09Z","published":"2023-03-21T11:28:09Z","title":"Reasonable Scale Machine Learning with Open-Source Metaflow","summary":"  As Machine Learning (ML) gains adoption across industries and new use cases,\npractitioners increasingly realize the challenges around effectively developing\nand iterating on ML systems: reproducibility, debugging, scalability, and\ndocumentation are elusive goals for real-world pipelines outside tech-first\ncompanies. In this paper, we review the nature of ML-oriented workloads and\nargue that re-purposing existing tools won't solve the current productivity\nissues, as ML peculiarities warrant specialized development tooling. We then\nintroduce Metaflow, an open-source framework for ML projects explicitly\ndesigned to boost the productivity of data practitioners by abstracting away\nthe execution of ML code from the definition of the business logic. We show how\nour design addresses the main challenges in ML operations (MLOps), and document\nthrough examples, interviews and use cases its practical impact on the field.\n","authors":["Jacopo Tagliabue","Hugo Bowne-Anderson","Ville Tuulos","Savin Goyal","Romain Cledat","David Berg"],"pdf_url":"https://arxiv.org/pdf/2303.11761v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11756v1","updated":"2023-03-21T11:21:31Z","published":"2023-03-21T11:21:31Z","title":"Improving Deep Dynamics Models for Autonomous Vehicles with Multimodal\n  Latent Mapping of Surfaces","summary":"  The safe deployment of autonomous vehicles relies on their ability to\neffectively react to environmental changes. This can require maneuvering on\nvarying surfaces which is still a difficult problem, especially for slippery\nterrains. To address this issue we propose a new approach that learns a\nsurface-aware dynamics model by conditioning it on a latent variable vector\nstoring surface information about the current location. A latent mapper is\ntrained to update these latent variables during inference from multiple\nmodalities on every traversal of the corresponding locations and stores them in\na map. By training everything end-to-end with the loss of the dynamics model,\nwe enforce the latent mapper to learn an update rule for the latent map that is\nuseful for the subsequent dynamics model. We implement and evaluate our\napproach on a real miniature electric car. The results show that the latent map\nis updated to allow more accurate predictions of the dynamics model compared to\na model without this information. We further show that by using this model, the\ndriving performance can be improved on varying and challenging surfaces.\n","authors":["Johan Vertens","Nicolai Dorka","Tim Welschehold","Michael Thompson","Wolfram Burgard"],"pdf_url":"https://arxiv.org/pdf/2303.11756v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11754v1","updated":"2023-03-21T11:20:22Z","published":"2023-03-21T11:20:22Z","title":"Projections of Model Spaces for Latent Graph Inference","summary":"  Graph Neural Networks leverage the connectivity structure of graphs as an\ninductive bias. Latent graph inference focuses on learning an adequate graph\nstructure to diffuse information on and improve the downstream performance of\nthe model. In this work we employ stereographic projections of the hyperbolic\nand spherical model spaces, as well as products of Riemannian manifolds, for\nthe purpose of latent graph inference. Stereographically projected model spaces\nachieve comparable performance to their non-projected counterparts, while\nproviding theoretical guarantees that avoid divergence of the spaces when the\ncurvature tends to zero. We perform experiments on both homophilic and\nheterophilic graphs.\n","authors":["Haitz Sáez de Ocáriz Borde","Álvaro Arroyo","Ingmar Posner"],"pdf_url":"https://arxiv.org/pdf/2303.11754v1.pdf","comment":"Accepted at the ICLR 2023 Workshop on Physics for Machine Learning"},{"id":"http://arxiv.org/abs/2211.15377v3","updated":"2023-03-21T11:19:03Z","published":"2022-11-23T09:57:17Z","title":"Whose Emotion Matters? Speaking Activity Localisation without Prior\n  Knowledge","summary":"  The task of emotion recognition in conversations (ERC) benefits from the\navailability of multiple modalities, as provided, for example, in the\nvideo-based Multimodal EmotionLines Dataset (MELD). However, only a few\nresearch approaches use both acoustic and visual information from the MELD\nvideos. There are two reasons for this: First, label-to-video alignments in\nMELD are noisy, making those videos an unreliable source of emotional speech\ndata. Second, conversations can involve several people in the same scene, which\nrequires the localisation of the utterance source. In this paper, we introduce\nMELD with Fixed Audiovisual Information via Realignment (MELD-FAIR) by using\nrecent active speaker detection and automatic speech recognition models, we are\nable to realign the videos of MELD and capture the facial expressions from\nspeakers in 96.92% of the utterances provided in MELD. Experiments with a\nself-supervised voice recognition model indicate that the realigned MELD-FAIR\nvideos more closely match the transcribed utterances given in the MELD dataset.\nFinally, we devise a model for emotion recognition in conversations trained on\nthe realigned MELD-FAIR videos, which outperforms state-of-the-art models for\nERC based on vision alone. This indicates that localising the source of\nspeaking activities is indeed effective for extracting facial expressions from\nthe uttering speakers and that faces provide more informative visual cues than\nthe visual features state-of-the-art models have been using so far. The\nMELD-FAIR realignment data, and the code of the realignment procedure and of\nthe emotional recognition, are available at\nhttps://github.com/knowledgetechnologyuhh/MELD-FAIR.\n","authors":["Hugo Carneiro","Cornelius Weber","Stefan Wermter"],"pdf_url":"https://arxiv.org/pdf/2211.15377v3.pdf","comment":"17 pages, 8 figures, 7 tables"},{"id":"http://arxiv.org/abs/2303.11746v1","updated":"2023-03-21T11:13:01Z","published":"2023-03-21T11:13:01Z","title":"Recommendation Systems in Libraries: an Application with Heterogeneous\n  Data Sources","summary":"  The Reading&Machine project exploits the support of digitalization to\nincrease the attractiveness of libraries and improve the users' experience. The\nproject implements an application that helps the users in their decision-making\nprocess, providing recommendation system (RecSys)-generated lists of books the\nusers might be interested in, and showing them through an interactive Virtual\nReality (VR)-based Graphical User Interface (GUI). In this paper, we focus on\nthe design and testing of the recommendation system, employing data about all\nusers' loans over the past 9 years from the network of libraries located in\nTurin, Italy. In addition, we use data collected by the Anobii online social\ncommunity of readers, who share their feedback and additional information about\nbooks they read. Armed with this heterogeneous data, we build and evaluate\nContent Based (CB) and Collaborative Filtering (CF) approaches. Our results\nshow that the CF outperforms the CB approach, improving by up to 47\\% the\nrelevant recommendations provided to a reader. However, the performance of the\nCB approach is heavily dependent on the number of books the reader has already\nread, and it can work even better than CF for users with a large history.\nFinally, our evaluations highlight that the performances of both approaches are\nsignificantly improved if the system integrates and leverages the information\nfrom the Anobii dataset, which allows us to include more user readings (for CF)\nand richer book metadata (for CB).\n","authors":["Alessandro Speciale","Greta Vallero","Luca Vassio","Marco Mellia"],"pdf_url":"https://arxiv.org/pdf/2303.11746v1.pdf","comment":"Accepted at 7th International workshop on Data Analytics solutions\n  for Real-LIfe APplications - 28th March-31st March, 2023, Ioannina, Greece.\n  The paper will be published in the Proceedings of EDBT/ICDT 2023 Joint\n  Conference"},{"id":"http://arxiv.org/abs/2303.11742v1","updated":"2023-03-21T11:09:31Z","published":"2023-03-21T11:09:31Z","title":"Beam Management Driven by Radio Environment Maps in O-RAN Architecture","summary":"  The Massive Multiple-Input Multiple-Output (M-MIMO) is considered as one of\nthe key technologies in 5G, and future 6G networks. From the perspective of,\ne.g., channel estimation, especially for high-speed users it is easier to\nimplement an M-MIMO network exploiting a static set of beams, i.e., Grid of\nBeams (GoB). While considering GoB it is important to properly assign users to\nthe beams, i.e., to perform Beam Management (BM). BM can be enhanced by taking\ninto account historical knowledge about the radio environment, e.g., to avoid\nradio link failures. The aim of this paper is to propose such a BM algorithm,\nthat utilizes location-dependent data stored in a Radio Environment Map (REM).\nIt utilizes received power maps, and user mobility patterns to optimize the BM\nprocess in terms of Reinforcement Learning (RL) by using the Policy Iteration\nmethod under different goal functions, e.g., maximization of received power or\nminimization of beam reselections while avoiding radio link failures. The\nproposed solution is compliant with the Open Radio Access Network (O-RAN)\narchitecture, enabling its practical implementation. Simulation studies have\nshown that the proposed BM algorithm can significantly reduce the number of\nbeam reselections or radio link failures compared to the baseline algorithm.\n","authors":["Marcin Hoffmann","Pawel Kryszkiewicz"],"pdf_url":"https://arxiv.org/pdf/2303.11742v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.14115v2","updated":"2023-03-21T11:01:09Z","published":"2023-02-27T19:53:49Z","title":"Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense\n  Video Captioning","summary":"  In this work, we introduce Vid2Seq, a multi-modal single-stage dense event\ncaptioning model pretrained on narrated videos which are readily-available at\nscale. The Vid2Seq architecture augments a language model with special time\ntokens, allowing it to seamlessly predict event boundaries and textual\ndescriptions in the same output sequence. Such a unified model requires\nlarge-scale training data, which is not available in current annotated\ndatasets. We show that it is possible to leverage unlabeled narrated videos for\ndense video captioning, by reformulating sentence boundaries of transcribed\nspeech as pseudo event boundaries, and using the transcribed speech sentences\nas pseudo event captions. The resulting Vid2Seq model pretrained on the\nYT-Temporal-1B dataset improves the state of the art on a variety of dense\nvideo captioning benchmarks including YouCook2, ViTT and ActivityNet Captions.\nVid2Seq also generalizes well to the tasks of video paragraph captioning and\nvideo clip captioning, and to few-shot settings. Our code is publicly available\nat https://antoyang.github.io/vid2seq.html.\n","authors":["Antoine Yang","Arsha Nagrani","Paul Hongsuck Seo","Antoine Miech","Jordi Pont-Tuset","Ivan Laptev","Josef Sivic","Cordelia Schmid"],"pdf_url":"https://arxiv.org/pdf/2302.14115v2.pdf","comment":"CVPR 2023 Camera-Ready; Project Webpage:\n  https://antoyang.github.io/vid2seq.html ; 18 pages; 6 figures"},{"id":"http://arxiv.org/abs/2303.11735v1","updated":"2023-03-21T10:46:56Z","published":"2023-03-21T10:46:56Z","title":"Tensor networks for quantum machine learning","summary":"  Once developed for quantum theory, tensor networks have been established as a\nsuccessful machine learning paradigm. Now, they have been ported back to the\nquantum realm in the emerging field of quantum machine learning to assess\nproblems that classical computers are unable to solve efficiently. Their nature\nat the interface between physics and machine learning makes tensor networks\neasily deployable on quantum computers. In this review article, we shed light\non one of the major architectures considered to be predestined for variational\nquantum machine learning. In particular, we discuss how layouts like MPS, PEPS,\nTTNs and MERA can be mapped to a quantum computer, how they can be used for\nmachine learning and data encoding and which implementation techniques improve\ntheir performance.\n","authors":["Hans-Martin Rieser","Frank Köster","Arne Peter Raulf"],"pdf_url":"https://arxiv.org/pdf/2303.11735v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11734v1","updated":"2023-03-21T10:46:34Z","published":"2023-03-21T10:46:34Z","title":"Unlocking Layer-wise Relevance Propagation for Autoencoders","summary":"  Autoencoders are a powerful and versatile tool often used for various\nproblems such as anomaly detection, image processing and machine translation.\nHowever, their reconstructions are not always trivial to explain. Therefore, we\npropose a fast explainability solution by extending the Layer-wise Relevance\nPropagation method with the help of Deep Taylor Decomposition framework.\nFurthermore, we introduce a novel validation technique for comparing our\nexplainability approach with baseline methods in the case of missing\nground-truth data. Our results highlight computational as well as qualitative\nadvantages of the proposed explainability solution with respect to existing\nmethods.\n","authors":["Kenyu Kobayashi","Renata Khasanova","Arno Schneuwly","Felix Schmidt","Matteo Casserini"],"pdf_url":"https://arxiv.org/pdf/2303.11734v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11733v1","updated":"2023-03-21T10:43:41Z","published":"2023-03-21T10:43:41Z","title":"DIPPM: a Deep Learning Inference Performance Predictive Model using\n  Graph Neural Networks","summary":"  Deep Learning (DL) has developed to become a corner-stone in many everyday\napplications that we are now relying on. However, making sure that the DL model\nuses the underlying hardware efficiently takes a lot of effort. Knowledge about\ninference characteristics can help to find the right match so that enough\nresources are given to the model, but not too much. We have developed a DL\nInference Performance Predictive Model (DIPPM) that predicts the inference\nlatency, energy, and memory usage of a given input DL model on the NVIDIA A100\nGPU. We also devised an algorithm to suggest the appropriate A100\nMulti-Instance GPU profile from the output of DIPPM. We developed a methodology\nto convert DL models expressed in multiple frameworks to a generalized graph\nstructure that is used in DIPPM. It means DIPPM can parse input DL models from\nvarious frameworks. Our DIPPM can be used not only helps to find suitable\nhardware configurations but also helps to perform rapid design-space\nexploration for the inference performance of a model. We constructed a graph\nmulti-regression dataset consisting of 10,508 different DL models to train and\nevaluate the performance of DIPPM, and reached a resulting Mean Absolute\nPercentage Error (MAPE) as low as 1.9%.\n","authors":["Karthick Panner Selvam","Mats Brorsson"],"pdf_url":"https://arxiv.org/pdf/2303.11733v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.07075v3","updated":"2023-03-21T10:41:12Z","published":"2022-04-14T16:13:06Z","title":"Learning and controlling the source-filter representation of speech with\n  a variational autoencoder","summary":"  Understanding and controlling latent representations in deep generative\nmodels is a challenging yet important problem for analyzing, transforming and\ngenerating various types of data. In speech processing, inspiring from the\nanatomical mechanisms of phonation, the source-filter model considers that\nspeech signals are produced from a few independent and physically meaningful\ncontinuous latent factors, among which the fundamental frequency $f_0$ and the\nformants are of primary importance. In this work, we start from a variational\nautoencoder (VAE) trained in an unsupervised manner on a large dataset of\nunlabeled natural speech signals, and we show that the source-filter model of\nspeech production naturally arises as orthogonal subspaces of the VAE latent\nspace. Using only a few seconds of labeled speech signals generated with an\nartificial speech synthesizer, we propose a method to identify the latent\nsubspaces encoding $f_0$ and the first three formant frequencies, we show that\nthese subspaces are orthogonal, and based on this orthogonality, we develop a\nmethod to accurately and independently control the source-filter speech factors\nwithin the latent subspaces. Without requiring additional information such as\ntext or human-labeled data, this results in a deep generative model of speech\nspectrograms that is conditioned on $f_0$ and the formant frequencies, and\nwhich is applied to the transformation speech signals. Finally, we also propose\na robust $f_0$ estimation method that exploits the projection of a speech\nsignal onto the learned latent subspace associated with $f_0$.\n","authors":["Samir Sadok","Simon Leglaive","Laurent Girin","Xavier Alameda-Pineda","Renaud Séguier"],"pdf_url":"https://arxiv.org/pdf/2204.07075v3.pdf","comment":"23 pages, 7 figures, companion website:\n  https://samsad35.github.io/site-sfvae/"},{"id":"http://arxiv.org/abs/2111.12273v5","updated":"2023-03-21T10:31:46Z","published":"2021-11-24T05:16:41Z","title":"Sharpness-aware Quantization for Deep Neural Networks","summary":"  Network quantization is a dominant paradigm of model compression. However,\nthe abrupt changes in quantized weights during training often lead to severe\nloss fluctuations and result in a sharp loss landscape, making the gradients\nunstable and thus degrading the performance. Recently, Sharpness-Aware\nMinimization (SAM) has been proposed to smooth the loss landscape and improve\nthe generalization performance of the models. Nevertheless, directly applying\nSAM to the quantized models can lead to perturbation mismatch or diminishment\nissues, resulting in suboptimal performance. In this paper, we propose a novel\nmethod, dubbed Sharpness-Aware Quantization (SAQ), to explore the effect of SAM\nin model compression, particularly quantization for the first time.\nSpecifically, we first provide a unified view of quantization and SAM by\ntreating them as introducing quantization noises and adversarial perturbations\nto the model weights, respectively. According to whether the noise and\nperturbation terms depend on each other, SAQ can be formulated into three\ncases, which are analyzed and compared comprehensively. Furthermore, by\nintroducing an efficient training strategy, SAQ only incurs a little additional\ntraining overhead compared with the default optimizer (e.g., SGD or AdamW).\nExtensive experiments on both convolutional neural networks and Transformers\nacross various datasets (i.e., ImageNet, CIFAR-10/100, Oxford Flowers-102,\nOxford-IIIT Pets) show that SAQ improves the generalization performance of the\nquantized models, yielding the SOTA results in uniform quantization. For\nexample, on ImageNet, SAQ outperforms AdamW by 1.2% on the Top-1 accuracy for\n4-bit ViT-B/16. Our 4-bit ResNet-50 surpasses the previous SOTA method by 0.9%\non the Top-1 accuracy.\n","authors":["Jing Liu","Jianfei Cai","Bohan Zhuang"],"pdf_url":"https://arxiv.org/pdf/2111.12273v5.pdf","comment":"Tech report"},{"id":"http://arxiv.org/abs/2303.11724v1","updated":"2023-03-21T10:29:30Z","published":"2023-03-21T10:29:30Z","title":"Task-based Generation of Optimized Projection Sets using Differentiable\n  Ranking","summary":"  We present a method for selecting valuable projections in computed tomography\n(CT) scans to enhance image reconstruction and diagnosis. The approach\nintegrates two important factors, projection-based detectability and data\ncompleteness, into a single feed-forward neural network. The network evaluates\nthe value of projections, processes them through a differentiable ranking\nfunction and makes the final selection using a straight-through estimator. Data\ncompleteness is ensured through the label provided during training. The\napproach eliminates the need for heuristically enforcing data completeness,\nwhich may exclude valuable projections. The method is evaluated on simulated\ndata in a non-destructive testing scenario, where the aim is to maximize the\nreconstruction quality within a specified region of interest. We achieve\ncomparable results to previous methods, laying the foundation for using\nreconstruction-based loss functions to learn the selection of projections.\n","authors":["Linda-Sophie Schneider","Mareike Thies","Christopher Syben","Richard Schielein","Mathias Unberath","Andreas Maier"],"pdf_url":"https://arxiv.org/pdf/2303.11724v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11720v1","updated":"2023-03-21T10:14:11Z","published":"2023-03-21T10:14:11Z","title":"Lidar Line Selection with Spatially-Aware Shapley Value for\n  Cost-Efficient Depth Completion","summary":"  Lidar is a vital sensor for estimating the depth of a scene. Typical spinning\nlidars emit pulses arranged in several horizontal lines and the monetary cost\nof the sensor increases with the number of these lines. In this work, we\npresent the new problem of optimizing the positioning of lidar lines to find\nthe most effective configuration for the depth completion task. We propose a\nsolution to reduce the number of lines while retaining the up-to-the-mark\nquality of depth completion. Our method consists of two components, (1) line\nselection based on the marginal contribution of a line computed via the Shapley\nvalue and (2) incorporating line position spread to take into account its need\nto arrive at image-wide depth completion. Spatially-aware Shapley values (SaS)\nsucceed in selecting line subsets that yield a depth accuracy comparable to the\nfull lidar input while using just half of the lines.\n","authors":["Kamil Adamczewski","Christos Sakaridis","Vaishakh Patil","Luc Van Gool"],"pdf_url":"https://arxiv.org/pdf/2303.11720v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11717v1","updated":"2023-03-21T10:09:47Z","published":"2023-03-21T10:09:47Z","title":"A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to\n  GPT-5 All You Need?","summary":"  As ChatGPT goes viral, generative AI (AIGC, a.k.a AI-generated content) has\nmade headlines everywhere because of its ability to analyze and create text,\nimages, and beyond. With such overwhelming media coverage, it is almost\nimpossible for us to miss the opportunity to glimpse AIGC from a certain angle.\nIn the era of AI transitioning from pure analysis to creation, it is worth\nnoting that ChatGPT, with its most recent language model GPT-4, is just a tool\nout of numerous AIGC tasks. Impressed by the capability of the ChatGPT, many\npeople are wondering about its limits: can GPT-5 (or other future GPT variants)\nhelp ChatGPT unify all AIGC tasks for diversified content creation? Toward\nanswering this question, a comprehensive review of existing AIGC tasks is\nneeded. As such, our work comes to fill this gap promptly by offering a first\nlook at AIGC, ranging from its techniques to applications. Modern generative AI\nrelies on various technical foundations, ranging from model architecture and\nself-supervised pretraining to generative modeling methods (like GAN and\ndiffusion models). After introducing the fundamental techniques, this work\nfocuses on the technological development of various AIGC tasks based on their\noutput type, including text, images, videos, 3D content, etc., which depicts\nthe full potential of ChatGPT's future. Moreover, we summarize their\nsignificant applications in some mainstream industries, such as education and\ncreativity content. Finally, we discuss the challenges currently faced and\npresent an outlook on how generative AI might evolve in the near future.\n","authors":["Chaoning Zhang","Chenshuang Zhang","Sheng Zheng","Yu Qiao","Chenghao Li","Mengchun Zhang","Sumit Kumar Dam","Chu Myaet Thwal","Ye Lin Tun","Le Luang Huy","Donguk kim","Sung-Ho Bae","Lik-Hang Lee","Yang Yang","Heng Tao Shen","In So Kweon","Choong Seon Hong"],"pdf_url":"https://arxiv.org/pdf/2303.11717v1.pdf","comment":"56 pages, 548 citations"},{"id":"http://arxiv.org/abs/2303.11716v1","updated":"2023-03-21T10:08:42Z","published":"2023-03-21T10:08:42Z","title":"Style Miner: Find Significant and Stable Explanatory Factors in Time\n  Series with Constrained Reinforcement Learning","summary":"  In high-dimensional time-series analysis, it is essential to have a set of\nkey factors (namely, the style factors) that explain the change of the observed\nvariable. For example, volatility modeling in finance relies on a set of risk\nfactors, and climate change studies in climatology rely on a set of causal\nfactors. The ideal low-dimensional style factors should balance significance\n(with high explanatory power) and stability (consistent, no significant\nfluctuations). However, previous supervised and unsupervised feature extraction\nmethods can hardly address the tradeoff. In this paper, we propose Style Miner,\na reinforcement learning method to generate style factors. We first formulate\nthe problem as a Constrained Markov Decision Process with explanatory power as\nthe return and stability as the constraint. Then, we design fine-grained\nimmediate rewards and costs and use a Lagrangian heuristic to balance them\nadaptively. Experiments on real-world financial data sets show that Style Miner\noutperforms existing learning-based methods by a large margin and achieves a\nrelatively 10% gain in R-squared explanatory power compared to the\nindustry-renowned factors proposed by human experts.\n","authors":["Dapeng Li","Feiyang Pan","Jia He","Zhiwei Xu","Dandan Tu","Guoliang Fan"],"pdf_url":"https://arxiv.org/pdf/2303.11716v1.pdf","comment":"9 pages, 6 figures"},{"id":"http://arxiv.org/abs/2203.03186v2","updated":"2023-03-21T10:04:25Z","published":"2022-03-07T07:44:05Z","title":"Bandits Corrupted by Nature: Lower Bounds on Regret and Robust\n  Optimistic Algorithm","summary":"  We study the corrupted bandit problem, i.e. a stochastic multi-armed bandit\nproblem with $k$ unknown reward distributions, which are heavy-tailed and\ncorrupted by a history-independent adversary or Nature. To be specific, the\nreward obtained by playing an arm comes from corresponding heavy-tailed reward\ndistribution with probability $1-\\varepsilon \\in (0.5,1]$ and an arbitrary\ncorruption distribution of unbounded support with probability $\\varepsilon \\in\n[0,0.5)$.\n  First, we provide $\\textit{a problem-dependent lower bound on the regret}$ of\nany corrupted bandit algorithm. The lower bounds indicate that the corrupted\nbandit problem is harder than the classical stochastic bandit problem with\nsub-Gaussian or heavy-tail rewards.\n  Following that, we propose a novel UCB-type algorithm for corrupted bandits,\nnamely HubUCB, that builds on Huber's estimator for robust mean estimation.\nLeveraging a novel concentration inequality of Huber's estimator, we prove that\nHubUCB achieves a near-optimal regret upper bound.\n  Since computing Huber's estimator has quadratic complexity, we further\nintroduce a sequential version of Huber's estimator that exhibits linear\ncomplexity. We leverage this sequential estimator to design SeqHubUCB that\nenjoys similar regret guarantees while reducing the computational burden.\n  Finally, we experimentally illustrate the efficiency of HubUCB and SeqHubUCB\nin solving corrupted bandits for different reward distributions and different\nlevels of corruptions.\n","authors":["Debabrota Basu","Odalric-Ambrym Maillard","Timothée Mathieu"],"pdf_url":"https://arxiv.org/pdf/2203.03186v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11705v1","updated":"2023-03-21T09:51:19Z","published":"2023-03-21T09:51:19Z","title":"A Single-Step Multiclass SVM based on Quantum Annealing for Remote\n  Sensing Data Classification","summary":"  In recent years, the development of quantum annealers has enabled\nexperimental demonstrations and has increased research interest in applications\nof quantum annealing, such as in quantum machine learning and in particular for\nthe popular quantum SVM. Several versions of the quantum SVM have been\nproposed, and quantum annealing has been shown to be effective in them.\nExtensions to multiclass problems have also been made, which consist of an\nensemble of multiple binary classifiers. This work proposes a novel quantum SVM\nformulation for direct multiclass classification based on quantum annealing,\ncalled Quantum Multiclass SVM (QMSVM). The multiclass classification problem is\nformulated as a single Quadratic Unconstrained Binary Optimization (QUBO)\nproblem solved with quantum annealing. The main objective of this work is to\nevaluate the feasibility, accuracy, and time performance of this approach.\nExperiments have been performed on the D-Wave Advantage quantum annealer for a\nclassification problem on remote sensing data. The results indicate that,\ndespite the memory demands of the quantum annealer, QMSVM can achieve accuracy\nthat is comparable to standard SVM methods and, more importantly, it scales\nmuch more efficiently with the number of training examples, resulting in nearly\nconstant time. This work shows an approach for bringing together classical and\nquantum computation, solving practical problems in remote sensing with current\nhardware.\n","authors":["Amer Delilbasic","Bertrand Le Saux","Morris Riedel","Kristel Michielsen","Gabriele Cavallaro"],"pdf_url":"https://arxiv.org/pdf/2303.11705v1.pdf","comment":"12 pages, 10 figures, 3 tables. Submitted to IEEE JSTARS"},{"id":"http://arxiv.org/abs/2303.11702v1","updated":"2023-03-21T09:42:27Z","published":"2023-03-21T09:42:27Z","title":"Linking generative semi-supervised learning and generative open-set\n  recognition","summary":"  This study investigates the relationship between semi-supervised learning\n(SSL) and open-set recognition (OSR) in the context of generative adversarial\nnetworks (GANs). Although no previous study has formally linked SSL and OSR,\ntheir respective methods share striking similarities. Specifically, SSL-GANs\nand OSR-GANs require generator to produce samples in the complementary space.\nSubsequently, by regularising networks with generated samples, both SSL and OSR\nclassifiers generalize the open space. To demonstrate the connection between\nSSL and OSR, we theoretically and experimentally compare state-of-the-art\nSSL-GAN methods with state-of-the-art OSR-GAN methods. Our results indicate\nthat the SSL optimised margin-GANs, which have a stronger foundation in\nliterature, set the new standard for the combined SSL-OSR task and achieves new\nstate-of-other art results in certain general OSR experiments. However, the OSR\noptimised adversarial reciprocal point (ARP)-GANs still slightly out-performed\nmargin-GANs at other OSR experiments. This result indicates unique insights for\nthe combined optimisation task of SSL-OSR.\n","authors":["Emile Reyn Engelbrecht","Johan du Preez"],"pdf_url":"https://arxiv.org/pdf/2303.11702v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11701v1","updated":"2023-03-21T09:41:13Z","published":"2023-03-21T09:41:13Z","title":"A High-Frequency Focused Network for Lightweight Single Image\n  Super-Resolution","summary":"  Lightweight neural networks for single-image super-resolution (SISR) tasks\nhave made substantial breakthroughs in recent years. Compared to low-frequency\ninformation, high-frequency detail is much more difficult to reconstruct. Most\nSISR models allocate equal computational resources for low-frequency and\nhigh-frequency information, which leads to redundant processing of simple\nlow-frequency information and inadequate recovery of more challenging\nhigh-frequency information. We propose a novel High-Frequency Focused Network\n(HFFN) through High-Frequency Focused Blocks (HFFBs) that selectively enhance\nhigh-frequency information while minimizing redundant feature computation of\nlow-frequency information. The HFFB effectively allocates more computational\nresources to the more challenging reconstruction of high-frequency information.\nMoreover, we propose a Local Feature Fusion Block (LFFB) effectively fuses\nfeatures from multiple HFFBs in a local region, utilizing complementary\ninformation across layers to enhance feature representativeness and reduce\nartifacts in reconstructed images. We assess the efficacy of our proposed HFFN\non five benchmark datasets and show that it significantly enhances the\nsuper-resolution performance of the network. Our experimental results\ndemonstrate state-of-the-art performance in reconstructing high-frequency\ninformation while using a low number of parameters.\n","authors":["Xiaotian Weng","Yi Chen","Zhichao Zheng","Yanhui Gu","Junsheng Zhou","Yudong Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.11701v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11699v1","updated":"2023-03-21T09:37:29Z","published":"2023-03-21T09:37:29Z","title":"Neural networks trained on synthetically generated crystals can extract\n  structural information from ICSD powder X-ray diffractograms","summary":"  Machine learning techniques have successfully been used to extract structural\ninformation such as the crystal space group from powder X-ray diffractograms.\nHowever, training directly on simulated diffractograms from databases such as\nthe ICSD is challenging due to its limited size, class-inhomogeneity, and bias\ntoward certain structure types. We propose an alternative approach of\ngenerating synthetic crystals with random coordinates by using the symmetry\noperations of each space group. Based on this approach, we demonstrate online\ntraining of deep ResNet-like models on up to a few million unique on-the-fly\ngenerated synthetic diffractograms per hour. For our chosen task of space group\nclassification, we achieved a test accuracy of 79.9% on unseen ICSD structure\ntypes from most space groups. This surpasses the 56.1% accuracy of the current\nstate-of-the-art approach of training on ICSD crystals directly. Our results\ndemonstrate that synthetically generated crystals can be used to extract\nstructural information from ICSD powder diffractograms, which makes it possible\nto apply very large state-of-the-art machine learning models in the area of\npowder X-ray diffraction. We further show first steps toward applying our\nmethodology to experimental data, where automated XRD data analysis is crucial,\nespecially in high-throughput settings. While we focused on the prediction of\nthe space group, our approach has the potential to be extended to related tasks\nin the future.\n","authors":["Henrik Schopmans","Patrick Reiser","Pascal Friederich"],"pdf_url":"https://arxiv.org/pdf/2303.11699v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11698v1","updated":"2023-03-21T09:36:58Z","published":"2023-03-21T09:36:58Z","title":"Data Augmentation For Label Enhancement","summary":"  Label distribution (LD) uses the description degree to describe instances,\nwhich provides more fine-grained supervision information when learning with\nlabel ambiguity. Nevertheless, LD is unavailable in many real-world\napplications. To obtain LD, label enhancement (LE) has emerged to recover LD\nfrom logical label. Existing LE approach have the following problems:\n(\\textbf{i}) They use logical label to train mappings to LD, but the\nsupervision information is too loose, which can lead to inaccurate model\nprediction; (\\textbf{ii}) They ignore feature redundancy and use the collected\nfeatures directly. To solve (\\textbf{i}), we use the topology of the feature\nspace to generate more accurate label-confidence. To solve (\\textbf{ii}), we\nproposed a novel supervised LE dimensionality reduction approach, which\nprojects the original data into a lower dimensional feature space. Combining\nthe above two, we obtain the augmented data for LE. Further, we proposed a\nnovel nonlinear LE model based on the label-confidence and reduced features.\nExtensive experiments on 12 real-world datasets are conducted and the results\nshow that our method consistently outperforms the other five comparing\napproaches.\n","authors":["Zhiqiang Kou","Yuheng Jia","Jing Wang","Boyu Shi","Xin Geng"],"pdf_url":"https://arxiv.org/pdf/2303.11698v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11695v1","updated":"2023-03-21T09:32:31Z","published":"2023-03-21T09:32:31Z","title":"Transcriptomics-based matching of drugs to diseases with deep learning","summary":"  In this work we present a deep learning approach to conduct hypothesis-free,\ntranscriptomics-based matching of drugs for diseases. Our proposed neural\nnetwork architecture is trained on approved drug-disease indications, taking as\ninput the relevant disease and drug differential gene expression profiles, and\nlearns to identify novel indications. We assemble an evaluation dataset of\ndisease-drug indications spanning 68 diseases and evaluate in silico our\napproach against the most widely used transcriptomics-based matching baselines,\nCMap and the Characteristic Direction. Our results show a more than 200%\nimprovement over both baselines in terms of standard retrieval metrics. We\nfurther showcase our model's ability to capture different genes' expressions\ninteractions among drugs and diseases. We provide our trained models, data and\ncode to predict with them at https://github.com/healx/dgem-nn-public.\n","authors":["Yannis Papanikolaou","Francesco Tuveri","Misa Ogura","Daniel O'Donovan"],"pdf_url":"https://arxiv.org/pdf/2303.11695v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07356v2","updated":"2023-03-21T09:26:58Z","published":"2022-12-14T17:33:01Z","title":"Scheduling and Aggregation Design for Asynchronous Federated Learning\n  over Wireless Networks","summary":"  Federated Learning (FL) is a collaborative machine learning (ML) framework\nthat combines on-device training and server-based aggregation to train a common\nML model among distributed agents. In this work, we propose an asynchronous FL\ndesign with periodic aggregation to tackle the straggler issue in FL systems.\nConsidering limited wireless communication resources, we investigate the effect\nof different scheduling policies and aggregation designs on the convergence\nperformance. Driven by the importance of reducing the bias and variance of the\naggregated model updates, we propose a scheduling policy that jointly considers\nthe channel quality and training data representation of user devices. The\neffectiveness of our channel-aware data-importance-based scheduling policy,\ncompared with state-of-the-art methods proposed for synchronous FL, is\nvalidated through simulations. Moreover, we show that an ``age-aware''\naggregation weighting design can significantly improve the learning performance\nin an asynchronous FL setting.\n","authors":["Chung-Hsuan Hu","Zheng Chen","Erik G. Larsson"],"pdf_url":"https://arxiv.org/pdf/2212.07356v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11674v1","updated":"2023-03-21T08:36:34Z","published":"2023-03-21T08:36:34Z","title":"ALOFT: A Lightweight MLP-like Architecture with Dynamic Low-frequency\n  Transform for Domain Generalization","summary":"  Domain generalization (DG) aims to learn a model that generalizes well to\nunseen target domains utilizing multiple source domains without re-training.\nMost existing DG works are based on convolutional neural networks (CNNs).\nHowever, the local operation of the convolution kernel makes the model focus\ntoo much on local representations (e.g., texture), which inherently causes the\nmodel more prone to overfit to the source domains and hampers its\ngeneralization ability. Recently, several MLP-based methods have achieved\npromising results in supervised learning tasks by learning global interactions\namong different patches of the image. Inspired by this, in this paper, we first\nanalyze the difference between CNN and MLP methods in DG and find that MLP\nmethods exhibit a better generalization ability because they can better capture\nthe global representations (e.g., structure) than CNN methods. Then, based on a\nrecent lightweight MLP method, we obtain a strong baseline that outperforms\nmost state-of-the-art CNN-based methods. The baseline can learn global\nstructure representations with a filter to suppress structure irrelevant\ninformation in the frequency space. Moreover, we propose a dynAmic\nLOw-Frequency spectrum Transform (ALOFT) that can perturb local texture\nfeatures while preserving global structure features, thus enabling the filter\nto remove structure-irrelevant information sufficiently. Extensive experiments\non four benchmarks have demonstrated that our method can achieve great\nperformance improvement with a small number of parameters compared to SOTA\nCNN-based DG methods. Our code is available at\nhttps://github.com/lingeringlight/ALOFT/.\n","authors":["Jintao Guo","Na Wang","Lei Qi","Yinghuan Shi"],"pdf_url":"https://arxiv.org/pdf/2303.11674v1.pdf","comment":"Accepted by CVPR2023. The code is available at\n  https://github.com/lingeringlight/ALOFT/"},{"id":"http://arxiv.org/abs/2303.11673v1","updated":"2023-03-21T08:34:23Z","published":"2023-03-21T08:34:23Z","title":"A Survey on Class Imbalance in Federated Learning","summary":"  Federated learning, which allows multiple client devices in a network to\njointly train a machine learning model without direct exposure of clients'\ndata, is an emerging distributed learning technique due to its nature of\nprivacy preservation. However, it has been found that models trained with\nfederated learning usually have worse performance than their counterparts\ntrained in the standard centralized learning mode, especially when the training\ndata is imbalanced. In the context of federated learning, data imbalance may\noccur either locally one one client device, or globally across many devices.\nThe complexity of different types of data imbalance has posed challenges to the\ndevelopment of federated learning technique, especially considering the need of\nrelieving data imbalance issue and preserving data privacy at the same time.\nTherefore, in the literature, many attempts have been made to handle class\nimbalance in federated learning. In this paper, we present a detailed review of\nrecent advancements along this line. We first introduce various types of class\nimbalance in federated learning, after which we review existing methods for\nestimating the extent of class imbalance without the need of knowing the actual\ndata to preserve data privacy. After that, we discuss existing methods for\nhandling class imbalance in FL, where the advantages and disadvantages of the\nthese approaches are discussed. We also summarize common evaluation metrics for\nclass imbalanced tasks, and point out potential future directions.\n","authors":["Jing Zhang","Chuanwen Li","Jianzgong Qi","Jiayuan He"],"pdf_url":"https://arxiv.org/pdf/2303.11673v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.06640v2","updated":"2023-03-21T08:24:01Z","published":"2022-10-13T00:40:04Z","title":"Compute-Efficient Deep Learning: Algorithmic Trends and Opportunities","summary":"  Although deep learning has made great progress in recent years, the exploding\neconomic and environmental costs of training neural networks are becoming\nunsustainable. To address this problem, there has been a great deal of research\non *algorithmically-efficient deep learning*, which seeks to reduce training\ncosts not at the hardware or implementation level, but through changes in the\nsemantics of the training program. In this paper, we present a structured and\ncomprehensive overview of the research in this field. First, we formalize the\n*algorithmic speedup* problem, then we use fundamental building blocks of\nalgorithmically efficient training to develop a taxonomy. Our taxonomy\nhighlights commonalities of seemingly disparate methods and reveals current\nresearch gaps. Next, we present evaluation best practices to enable\ncomprehensive, fair, and reliable comparisons of speedup techniques. To further\naid research and applications, we discuss common bottlenecks in the training\npipeline (illustrated via experiments) and offer taxonomic mitigation\nstrategies for them. Finally, we highlight some unsolved research challenges\nand present promising future directions.\n","authors":["Brian R. Bartoldson","Bhavya Kailkhura","Davis Blalock"],"pdf_url":"https://arxiv.org/pdf/2210.06640v2.pdf","comment":"77 pages"},{"id":"http://arxiv.org/abs/2303.11669v1","updated":"2023-03-21T08:23:37Z","published":"2023-03-21T08:23:37Z","title":"Universal Smoothed Score Functions for Generative Modeling","summary":"  We consider the problem of generative modeling based on smoothing an unknown\ndensity of interest in $\\mathbb{R}^d$ using factorial kernels with $M$\nindependent Gaussian channels with equal noise levels introduced by Saremi and\nSrivastava (2022). First, we fully characterize the time complexity of learning\nthe resulting smoothed density in $\\mathbb{R}^{Md}$, called M-density, by\nderiving a universal form for its parametrization in which the score function\nis by construction permutation equivariant. Next, we study the time complexity\nof sampling an M-density by analyzing its condition number for Gaussian\ndistributions. This spectral analysis gives a geometric insight on the \"shape\"\nof M-densities as one increases $M$. Finally, we present results on the sample\nquality in this class of generative models on the CIFAR-10 dataset where we\nreport Fr\\'echet inception distances (14.15), notably obtained with a single\nnoise level on long-run fast-mixing MCMC chains.\n","authors":["Saeed Saremi","Rupesh Kumar Srivastava","Francis Bach"],"pdf_url":"https://arxiv.org/pdf/2303.11669v1.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2303.11650v1","updated":"2023-03-21T07:51:52Z","published":"2023-03-21T07:51:52Z","title":"Uniform Risk Bounds for Learning with Dependent Data Sequences","summary":"  This paper extends standard results from learning theory with independent\ndata to sequences of dependent data. Contrary to most of the literature, we do\nnot rely on mixing arguments or sequential measures of complexity and derive\nuniform risk bounds with classical proof patterns and capacity measures. In\nparticular, we show that the standard classification risk bounds based on the\nVC-dimension hold in the exact same form for dependent data, and further\nprovide Rademacher complexity-based bounds, that remain unchanged compared to\nthe standard results for the identically and independently distributed case.\nFinally, we show how to apply these results in the context of scenario-based\noptimization in order to compute the sample complexity of random programs with\ndependent constraints.\n","authors":["Fabien Lauer"],"pdf_url":"https://arxiv.org/pdf/2303.11650v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11648v1","updated":"2023-03-21T07:46:57Z","published":"2023-03-21T07:46:57Z","title":"Improving Content Retrievability in Search with Controllable Query\n  Generation","summary":"  An important goal of online platforms is to enable content discovery, i.e.\nallow users to find a catalog entity they were not familiar with. A\npre-requisite to discover an entity, e.g. a book, with a search engine is that\nthe entity is retrievable, i.e. there are queries for which the system will\nsurface such entity in the top results. However, machine-learned search engines\nhave a high retrievability bias, where the majority of the queries return the\nsame entities. This happens partly due to the predominance of narrow intent\nqueries, where users create queries using the title of an already known entity,\ne.g. in book search 'harry potter'. The amount of broad queries where users\nwant to discover new entities, e.g. in music search 'chill lyrical electronica\nwith an atmospheric feeling to it', and have a higher tolerance to what they\nmight find, is small in comparison. We focus here on two factors that have a\nnegative impact on the retrievability of the entities (I) the training data\nused for dense retrieval models and (II) the distribution of narrow and broad\nintent queries issued in the system. We propose CtrlQGen, a method that\ngenerates queries for a chosen underlying intent-narrow or broad. We can use\nCtrlQGen to improve factor (I) by generating training data for dense retrieval\nmodels comprised of diverse synthetic queries. CtrlQGen can also be used to\ndeal with factor (II) by suggesting queries with broader intents to users. Our\nresults on datasets from the domains of music, podcasts, and books reveal that\nwe can significantly decrease the retrievability bias of a dense retrieval\nmodel when using CtrlQGen. First, by using the generated queries as training\ndata for dense models we make 9% of the entities retrievable (go from zero to\nnon-zero retrievability). Second, by suggesting broader queries to users, we\ncan make 12% of the entities retrievable in the best case.\n","authors":["Gustavo Penha","Enrico Palumbo","Maryam Aziz","Alice Wang","Hugues Bouchard"],"pdf_url":"https://arxiv.org/pdf/2303.11648v1.pdf","comment":"Accepted for publication in the International World Wide Web\n  Conference 2023"},{"id":"http://arxiv.org/abs/2303.11647v1","updated":"2023-03-21T07:46:28Z","published":"2023-03-21T07:46:28Z","title":"Are uGLAD? Time will tell!","summary":"  We frequently encounter multiple series that are temporally correlated in our\nsurroundings, such as EEG data to examine alterations in brain activity or\nsensors to monitor body movements. Segmentation of multivariate time series\ndata is a technique for identifying meaningful patterns or changes in the time\nseries that can signal a shift in the system's behavior. However, most\nsegmentation algorithms have been designed primarily for univariate time\nseries, and their performance on multivariate data remains largely\nunsatisfactory, making this a challenging problem. In this work, we introduce a\nnovel approach for multivariate time series segmentation using conditional\nindependence (CI) graphs. CI graphs are probabilistic graphical models that\nrepresents the partial correlations between the nodes. We propose a domain\nagnostic multivariate segmentation framework `$\\texttt{tGLAD}$' which draws a\nparallel between the CI graph nodes and the variables of the time series.\nConsider applying a graph recovery model $\\texttt{uGLAD}$ to a short interval\nof the time series, it will result in a CI graph that shows partial\ncorrelations among the variables. We extend this idea to the entire time series\nby utilizing a sliding window to create a batch of time intervals and then run\na single $\\texttt{uGLAD}$ model in multitask learning mode to recover all the\nCI graphs simultaneously. As a result, we obtain a corresponding temporal CI\ngraphs representation. We then designed a first-order and second-order based\ntrajectory tracking algorithms to study the evolution of these graphs across\ndistinct intervals. Finally, an `Allocation' algorithm is used to determine a\nsuitable segmentation of the temporal graph sequence. $\\texttt{tGLAD}$ provides\na competitive time complexity of $O(N)$ for settings where number of variables\n$D<<N$. We demonstrate successful empirical results on a Physical Activity\nMonitoring data.\n","authors":["Shima Imani","Harsh Shrivastava"],"pdf_url":"https://arxiv.org/pdf/2303.11647v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.12465v2","updated":"2023-03-21T07:46:02Z","published":"2023-02-24T05:43:47Z","title":"PaGE-Link: Path-based Graph Neural Network Explanation for Heterogeneous\n  Link Prediction","summary":"  Transparency and accountability have become major concerns for black-box\nmachine learning (ML) models. Proper explanations for the model behavior\nincrease model transparency and help researchers develop more accountable\nmodels. Graph neural networks (GNN) have recently shown superior performance in\nmany graph ML problems than traditional methods, and explaining them has\nattracted increased interest. However, GNN explanation for link prediction (LP)\nis lacking in the literature. LP is an essential GNN task and corresponds to\nweb applications like recommendation and sponsored search on web. Given\nexisting GNN explanation methods only address node/graph-level tasks, we\npropose Path-based GNN Explanation for heterogeneous Link prediction\n(PaGE-Link) that generates explanations with connection interpretability,\nenjoys model scalability, and handles graph heterogeneity. Qualitatively,\nPaGE-Link can generate explanations as paths connecting a node pair, which\nnaturally captures connections between the two nodes and easily transfer to\nhuman-interpretable explanations. Quantitatively, explanations generated by\nPaGE-Link improve AUC for recommendation on citation and user-item graphs by 9\n- 35% and are chosen as better by 78.79% of responses in human evaluation.\n","authors":["Shichang Zhang","Jiani Zhang","Xiang Song","Soji Adeshina","Da Zheng","Christos Faloutsos","Yizhou Sun"],"pdf_url":"https://arxiv.org/pdf/2302.12465v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2201.01652v3","updated":"2023-03-21T07:35:09Z","published":"2022-01-05T15:17:35Z","title":"Stochastic regularized majorization-minimization with weakly convex and\n  multi-convex surrogates","summary":"  Stochastic majorization-minimization (SMM) is a class of stochastic\noptimization algorithms that proceed by sampling new data points and minimizing\na recursive average of surrogate functions of an objective function. The\nsurrogates are required to be strongly convex and convergence rate analysis for\nthe general non-convex setting was not available. In this paper, we propose an\nextension of SMM where surrogates are allowed to be only weakly convex or block\nmulti-convex, and the averaged surrogates are approximately minimized with\nproximal regularization or block-minimized within diminishing radii,\nrespectively. For the general nonconvex constrained setting with non-i.i.d.\ndata samples, we show that the first-order optimality gap of the proposed\nalgorithm decays at the rate $O((\\log n)^{1+\\epsilon}/n^{1/2})$ for the\nempirical loss and $O((\\log n)^{1+\\epsilon}/n^{1/4})$ for the expected loss,\nwhere $n$ denotes the number of data samples processed. Under some additional\nassumption, the latter convergence rate can be improved to $O((\\log\nn)^{1+\\epsilon}/n^{1/2})$. As a corollary, we obtain the first convergence rate\nbounds for various optimization methods under general nonconvex dependent data\nsetting: Double-averaging projected gradient descent and its generalizations,\nproximal point empirical risk minimization, and online matrix/tensor\ndecomposition algorithms. We also provide experimental validation of our\nresults.\n","authors":["Hanbaek Lyu"],"pdf_url":"https://arxiv.org/pdf/2201.01652v3.pdf","comment":"64 pages, 5 figures, 1 table"},{"id":"http://arxiv.org/abs/2303.11643v1","updated":"2023-03-21T07:32:32Z","published":"2023-03-21T07:32:32Z","title":"Manipulating Transfer Learning for Property Inference","summary":"  Transfer learning is a popular method for tuning pretrained (upstream) models\nfor different downstream tasks using limited data and computational resources.\nWe study how an adversary with control over an upstream model used in transfer\nlearning can conduct property inference attacks on a victim's tuned downstream\nmodel. For example, to infer the presence of images of a specific individual in\nthe downstream training set. We demonstrate attacks in which an adversary can\nmanipulate the upstream model to conduct highly effective and specific property\ninference attacks (AUC score $> 0.9$), without incurring significant\nperformance loss on the main task. The main idea of the manipulation is to make\nthe upstream model generate activations (intermediate features) with different\ndistributions for samples with and without a target property, thus enabling the\nadversary to distinguish easily between downstream models trained with and\nwithout training examples that have the target property. Our code is available\nat https://github.com/yulongt23/Transfer-Inference.\n","authors":["Yulong Tian","Fnu Suya","Anshuman Suri","Fengyuan Xu","David Evans"],"pdf_url":"https://arxiv.org/pdf/2303.11643v1.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2201.04207v5","updated":"2023-03-21T07:13:27Z","published":"2022-01-11T21:31:18Z","title":"Fighting Money Laundering with Statistics and Machine Learning","summary":"  Money laundering is a profound global problem. Nonetheless, there is little\nscientific literature on statistical and machine learning methods for\nanti-money laundering. In this paper, we focus on anti-money laundering in\nbanks and provide an introduction and review of the literature. We propose a\nunifying terminology with two central elements: (i) client risk profiling and\n(ii) suspicious behavior flagging. We find that client risk profiling is\ncharacterized by diagnostics, i.e., efforts to find and explain risk factors.\nOn the other hand, suspicious behavior flagging is characterized by\nnon-disclosed features and hand-crafted risk indices. Finally, we discuss\ndirections for future research. One major challenge is the need for more public\ndata sets. This may potentially be addressed by synthetic data generation.\nOther possible research directions include semi-supervised and deep learning,\ninterpretability, and fairness of the results.\n","authors":["Rasmus Jensen","Alexandros Iosifidis"],"pdf_url":"https://arxiv.org/pdf/2201.04207v5.pdf","comment":"Accepted for publication in IEEE Access, vol. 11, pp. 8889-8903,\n  doi:10.1109/ACCESS.2023.3239549"},{"id":"http://arxiv.org/abs/2303.11634v1","updated":"2023-03-21T07:01:22Z","published":"2023-03-21T07:01:22Z","title":"Deep Q-Network Based Decision Making for Autonomous Driving","summary":"  Currently decision making is one of the biggest challenges in autonomous\ndriving. This paper introduces a method for safely navigating an autonomous\nvehicle in highway scenarios by combining deep Q-Networks and insight from\ncontrol theory. A Deep Q-Network is trained in simulation to serve as a central\ndecision-making unit by proposing targets for a trajectory planner. The\ngenerated trajectories in combination with a controller for longitudinal\nmovement are used to execute lane change maneuvers. In order to prove the\nfunctionality of this approach it is evaluated on two different highway traffic\nscenarios. Furthermore, the impact of different state representations on the\nperformance and training process is analyzed. The results show that the\nproposed system can produce efficient and safe driving behavior.\n","authors":["Max Peter Ronecker","Yuan Zhu"],"pdf_url":"https://arxiv.org/pdf/2303.11634v1.pdf","comment":"Accepted at 2019 International Conference on Robotics and Automation\n  Sciences (ICRAS)"},{"id":"http://arxiv.org/abs/2303.11632v1","updated":"2023-03-21T07:00:13Z","published":"2023-03-21T07:00:13Z","title":"An Embarrassingly Simple Approach for Wafer Feature Extraction and\n  Defect Pattern Recognition","summary":"  Identifying defect patterns in a wafer map during manufacturing is crucial to\nfind the root cause of the underlying issue and provides valuable insights on\nimproving yield in the foundry. Currently used methods use deep neural networks\nto identify the defects. These methods are generally very huge and have\nsignificant inference time. They also require GPU support to efficiently\noperate. All these issues make these models not fit for on-line prediction in\nthe manufacturing foundry. In this paper, we propose an extremely simple yet\neffective technique to extract features from wafer images. The proposed method\nis extremely fast, intuitive, and non-parametric while being explainable. The\nexperiment results show that the proposed pipeline outperforms conventional\ndeep learning models. Our feature extraction requires no training or\nfine-tuning while preserving the relative shape and location of data points as\nrevealed by our interpretability analysis.\n","authors":["Nitish Shukla"],"pdf_url":"https://arxiv.org/pdf/2303.11632v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11624v1","updated":"2023-03-21T06:45:14Z","published":"2023-03-21T06:45:14Z","title":"Assessor-Guided Learning for Continual Environments","summary":"  This paper proposes an assessor-guided learning strategy for continual\nlearning where an assessor guides the learning process of a base learner by\ncontrolling the direction and pace of the learning process thus allowing an\nefficient learning of new environments while protecting against the\ncatastrophic interference problem. The assessor is trained in a meta-learning\nmanner with a meta-objective to boost the learning process of the base learner.\nIt performs a soft-weighting mechanism of every sample accepting positive\nsamples while rejecting negative samples. The training objective of a base\nlearner is to minimize a meta-weighted combination of the cross entropy loss\nfunction, the dark experience replay (DER) loss function and the knowledge\ndistillation loss function whose interactions are controlled in such a way to\nattain an improved performance. A compensated over-sampling (COS) strategy is\ndeveloped to overcome the class imbalanced problem of the episodic memory due\nto limited memory budgets. Our approach, Assessor-Guided Learning Approach\n(AGLA), has been evaluated in the class-incremental and task-incremental\nlearning problems. AGLA achieves improved performances compared to its\ncompetitors while the theoretical analysis of the COS strategy is offered.\nSource codes of AGLA, baseline algorithms and experimental logs are shared\npublicly in \\url{https://github.com/anwarmaxsum/AGLA} for further study.\n","authors":["Muhammad Anwar Ma'sum","Mahardhika Pratama","Edwin Lughofer","Weiping Ding","Wisnu Jatmiko"],"pdf_url":"https://arxiv.org/pdf/2303.11624v1.pdf","comment":"Submitted for publication to Information Sciences"},{"id":"http://arxiv.org/abs/2303.11619v1","updated":"2023-03-21T06:40:06Z","published":"2023-03-21T06:40:06Z","title":"Blow-up Algorithm for Sum-of-Products Polynomials and Real Log Canonical\n  Thresholds","summary":"  When considering a real log canonical threshold (RLCT) that gives a Bayesian\ngeneralization error, in general, papers replace a mean error function with a\nrelatively simple polynomial whose RLCT corresponds to that of the mean error\nfunction, and obtain its RLCT by resolving its singularities through an\nalgebraic operation called blow-up. Though it is known that the singularities\nof any polynomial can be resolved by a finite number of blow-up iterations, it\nis not clarified whether or not it is possible to resolve singularities of a\nspecific polynomial by applying a specific blow-up algorithm. Therefore this\npaper considers the blow-up algorithm for the polynomials called\nsum-of-products (sop) polynomials and its RLCT.\n","authors":["Joe Hirose"],"pdf_url":"https://arxiv.org/pdf/2303.11619v1.pdf","comment":"47 pages, 5 figure"},{"id":"http://arxiv.org/abs/2212.07740v2","updated":"2023-03-21T06:06:45Z","published":"2022-12-15T11:44:11Z","title":"Sim-to-Real Transfer for Quadrupedal Locomotion via Terrain Transformer","summary":"  Deep reinforcement learning has recently emerged as an appealing alternative\nfor legged locomotion over multiple terrains by training a policy in physical\nsimulation and then transferring it to the real world (i.e., sim-to-real\ntransfer). Despite considerable progress, the capacity and scalability of\ntraditional neural networks are still limited, which may hinder their\napplications in more complex environments. In contrast, the Transformer\narchitecture has shown its superiority in a wide range of large-scale sequence\nmodeling tasks, including natural language processing and decision-making\nproblems. In this paper, we propose Terrain Transformer (TERT), a high-capacity\nTransformer model for quadrupedal locomotion control on various terrains.\nFurthermore, to better leverage Transformer in sim-to-real scenarios, we\npresent a novel two-stage training framework consisting of an offline\npretraining stage and an online correction stage, which can naturally integrate\nTransformer with privileged training. Extensive experiments in simulation\ndemonstrate that TERT outperforms state-of-the-art baselines on different\nterrains in terms of return, energy consumption and control smoothness. In\nfurther real-world validation, TERT successfully traverses nine challenging\nterrains, including sand pit and stair down, which can not be accomplished by\nstrong baselines.\n","authors":["Hang Lai","Weinan Zhang","Xialin He","Chen Yu","Zheng Tian","Yong Yu","Jun Wang"],"pdf_url":"https://arxiv.org/pdf/2212.07740v2.pdf","comment":"Accepted by ICRA2023"},{"id":"http://arxiv.org/abs/2206.04959v4","updated":"2023-03-21T06:05:16Z","published":"2022-06-10T09:15:48Z","title":"Merak: An Efficient Distributed DNN Training Framework with Automated 3D\n  Parallelism for Giant Foundation Models","summary":"  Foundation models are becoming the dominant deep learning technologies.\nPretraining a foundation model is always time-consumed due to the large scale\nof both the model parameter and training dataset. Besides being\ncomputing-intensive, the training process is extremely memory-intensive and\ncommunication-intensive. These features make it necessary to apply 3D\nparallelism, which integrates data parallelism, pipeline model parallelism and\ntensor model parallelism, to achieve high training efficiency.\n  To achieve this goal, some custom software frameworks such as Megatron-LM and\nDeepSpeed are developed. However, current 3D parallelism frameworks still meet\ntwo issues: i) they are not transparent to model developers, which need to\nmanually modify the model to parallelize training. ii) their utilization of\ncomputation, GPU memory and network bandwidth are not sufficient. We propose\nMerak, an automated 3D parallelism deep learning training framework with high\nresource utilization. Merak automatically deploys with an automatic model\npartitioner, which uses a graph sharding algorithm on a proxy representation of\nthe model. Merak also presents the non-intrusive API for scaling out foundation\nmodel training with minimal code modification. In addition, we design a\nhigh-performance 3D parallel runtime engine in Merak. It uses several\ntechniques to exploit available training resources, including shifted critical\npath pipeline schedule that brings a higher computation utilization,\nstage-aware recomputation that makes use of idle worker memory, and\nsub-pipelined tensor model parallelism that overlaps communication and\ncomputation. Experiments on 64 GPUs show Merak can speedup the training\nperformance over the state-of-the-art 3D parallelism frameworks of models with\n1.5, 2.5, 8.3, and 20 billion parameters by up to 1.42X, 1.39X, 1.43X, and\n1.61X, respectively.\n","authors":["Zhiquan Lai","Shengwei Li","Xudong Tang","Keshi Ge","Weijie Liu","Yabo Duan","Linbo Qiao","Dongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2206.04959v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.05382v2","updated":"2023-03-21T05:47:11Z","published":"2023-03-06T16:36:17Z","title":"ChatGPT Is on the Horizon: Could a Large Language Model Be All We Need\n  for Intelligent Transportation?","summary":"  ChatGPT, developed by OpenAI, is one of the milestone large language models\n(LLMs) with 6 billion parameters. ChatGPT has demonstrated the impressive\nlanguage understanding capability of LLM, particularly in generating\nconversational response. As LLMs start to gain more attention in various\nresearch or engineering domains, it is time to envision how LLM may\nrevolutionize the way we approach intelligent transportation systems. This\npaper explores the future applications of LLM in addressing key transportation\nproblems. By leveraging LLM with cross-modal encoder, an intelligent system can\nalso process traffic data from different modalities and execute transportation\noperations through an LLM. We present and validate these potential\ntransportation applications equipped by LLM. To further demonstrate this\npotential, we also provide a concrete smartphone-based crash report\nauto-generation and analysis framework as a use case. Despite the potential\nbenefits, challenges related to data privacy, data quality, and model bias must\nbe considered. Overall, the use of LLM in intelligent transport systems holds\npromise for more efficient, intelligent, and sustainable transportation systems\nthat further improve daily life around the world.\n","authors":["Ou Zheng","Mohamed Abdel-Aty","Dongdong Wang","Zijin Wang","Shengxuan Ding"],"pdf_url":"https://arxiv.org/pdf/2303.05382v2.pdf","comment":"Submitted to Nature - Machine Intelligence (13 Pages, 8 Figures)"},{"id":"http://arxiv.org/abs/2303.11602v1","updated":"2023-03-21T05:41:24Z","published":"2023-03-21T05:41:24Z","title":"Convergence of stochastic gradient descent on parameterized sphere with\n  applications to variational Monte Carlo simulation","summary":"  We analyze stochastic gradient descent (SGD) type algorithms on a\nhigh-dimensional sphere which is parameterized by a neural network up to a\nnormalization constant. We provide a new algorithm for the setting of\nsupervised learning and show its convergence both theoretically and\nnumerically. We also provide the first proof of convergence for the\nunsupervised setting, which corresponds to the widely used variational Monte\nCarlo (VMC) method in quantum physics.\n","authors":["Nilin Abrahamsen","Zhiyan Ding","Gil Goldshlager","Lin Lin"],"pdf_url":"https://arxiv.org/pdf/2303.11602v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.04319v2","updated":"2023-03-21T05:28:26Z","published":"2022-08-07T13:33:34Z","title":"PhyGNNet: Solving spatiotemporal PDEs with Physics-informed Graph Neural\n  Network","summary":"  Solving partial differential equations (PDEs) is an important research means\nin the fields of physics, biology, and chemistry. As an approximate alternative\nto numerical methods, PINN has received extensive attention and played an\nimportant role in many fields. However, PINN uses a fully connected network as\nits model, which has limited fitting ability and limited extrapolation ability\nin both time and space. In this paper, we propose PhyGNNet for solving partial\ndifferential equations on the basics of a graph neural network which consists\nof encoder, processer, and decoder blocks. In particular, we divide the\ncomputing area into regular grids, define partial differential operators on the\ngrids, then construct pde loss for the network to optimize to build PhyGNNet\nmodel. What's more, we conduct comparative experiments on Burgers equation and\nheat equation to validate our approach, the results show that our method has\nbetter fit ability and extrapolation ability both in time and spatial areas\ncompared with PINN.\n","authors":["Longxiang Jiang","Liyuan Wang","Xinkun Chu","Yonghao Xiao","Hao Zhang"],"pdf_url":"https://arxiv.org/pdf/2208.04319v2.pdf","comment":"there some errors in method describtion"},{"id":"http://arxiv.org/abs/2111.09543v3","updated":"2023-03-21T05:17:08Z","published":"2021-11-18T06:48:00Z","title":"DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with\n  Gradient-Disentangled Embedding Sharing","summary":"  This paper presents a new pre-trained language model, DeBERTaV3, which\nimproves the original DeBERTa model by replacing mask language modeling (MLM)\nwith replaced token detection (RTD), a more sample-efficient pre-training task.\nOur analysis shows that vanilla embedding sharing in ELECTRA hurts training\nefficiency and model performance. This is because the training losses of the\ndiscriminator and the generator pull token embeddings in different directions,\ncreating the \"tug-of-war\" dynamics. We thus propose a new gradient-disentangled\nembedding sharing method that avoids the tug-of-war dynamics, improving both\ntraining efficiency and the quality of the pre-trained model. We have\npre-trained DeBERTaV3 using the same settings as DeBERTa to demonstrate its\nexceptional performance on a wide range of downstream natural language\nunderstanding (NLU) tasks. Taking the GLUE benchmark with eight tasks as an\nexample, the DeBERTaV3 Large model achieves a 91.37% average score, which is\n1.37% over DeBERTa and 1.91% over ELECTRA, setting a new state-of-the-art\n(SOTA) among the models with a similar structure. Furthermore, we have\npre-trained a multi-lingual model mDeBERTa and observed a larger improvement\nover strong baselines compared to English models. For example, the mDeBERTa\nBase achieves a 79.8% zero-shot cross-lingual accuracy on XNLI and a 3.6%\nimprovement over XLM-R Base, creating a new SOTA on this benchmark. We have\nmade our pre-trained models and inference code publicly available at\nhttps://github.com/microsoft/DeBERTa.\n","authors":["Pengcheng He","Jianfeng Gao","Weizhu Chen"],"pdf_url":"https://arxiv.org/pdf/2111.09543v3.pdf","comment":"16 pages, 10 tables, 2 Figures. The DeBERTaV3 model significantly\n  improves performance of the downstream NLU tasks over models with a similar\n  structure, e.g. DeBERTaV3 large achieves 91.37% average GLUE score which is\n  1.37% over DeBERTa large. XSmall has only 22M backbone parameters, but\n  significantly outperforms RoBERTa/XLNet-base. Paper is published as a\n  conference paper at ICLR 2023"},{"id":"http://arxiv.org/abs/2212.09010v2","updated":"2023-03-21T04:48:24Z","published":"2022-12-18T04:44:38Z","title":"Risk-Sensitive Reinforcement Learning with Exponential Criteria","summary":"  While risk-neutral reinforcement learning has shown experimental success in a\nnumber of applications, it is well-known to be non-robust with respect to noise\nand perturbations in the parameters of the system. For this reason,\nrisk-sensitive reinforcement learning algorithms have been studied to introduce\nrobustness and sample efficiency, and lead to better real-life performance. In\nthis work, we introduce new model-free risk-sensitive reinforcement learning\nalgorithms as variations of widely-used Policy Gradient algorithms with similar\nimplementation properties. In particular, we study the effect of exponential\ncriteria on the risk-sensitivity of the policy of a reinforcement learning\nagent, and develop variants of the Monte Carlo Policy Gradient algorithm and\nthe online (temporal-difference) Actor-Critic algorithm. Analytical results\nshowcase that the use of exponential criteria generalize commonly used ad-hoc\nregularization approaches. The implementation, performance, and robustness\nproperties of the proposed methods are evaluated in simulated experiments.\n","authors":["Erfaun Noorani","Christos Mavridis","John Baras"],"pdf_url":"https://arxiv.org/pdf/2212.09010v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11593v1","updated":"2023-03-21T04:47:45Z","published":"2023-03-21T04:47:45Z","title":"Difficulty in learning chirality for Transformer fed with SMILES","summary":"  Recent years have seen development of descriptor generation based on\nrepresentation learning of extremely diverse molecules, especially those that\napply natural language processing (NLP) models to SMILES, a literal\nrepresentation of molecular structure. However, little research has been done\non how these models understand chemical structure. To address this, we\ninvestigated the relationship between the learning progress of SMILES and\nchemical structure using a representative NLP model, the Transformer. The\nresults suggest that while the Transformer learns partial structures of\nmolecules quickly, it requires extended training to understand overall\nstructures. Consistently, the accuracy of molecular property predictions using\ndescriptors generated from models at different learning steps was similar from\nthe beginning to the end of training. Furthermore, we found that the\nTransformer requires particularly long training to learn chirality and\nsometimes stagnates with low translation accuracy due to misunderstanding of\nenantiomers. These findings are expected to deepen understanding of NLP models\nin chemistry.\n","authors":["Yasuhiro Yoshikai","Tadahaya Mizuno","Shumpei Nemoto","Hiroyuki Kusuhara"],"pdf_url":"https://arxiv.org/pdf/2303.11593v1.pdf","comment":"20 pages, 6 figures"},{"id":"http://arxiv.org/abs/2201.12243v2","updated":"2023-03-21T04:41:42Z","published":"2022-01-28T16:53:56Z","title":"Joint Differentiable Optimization and Verification for Certified\n  Reinforcement Learning","summary":"  In model-based reinforcement learning for safety-critical control systems, it\nis important to formally certify system properties (e.g., safety, stability)\nunder the learned controller. However, as existing methods typically apply\nformal verification \\emph{after} the controller has been learned, it is\nsometimes difficult to obtain any certificate, even after many iterations\nbetween learning and verification. To address this challenge, we propose a\nframework that jointly conducts reinforcement learning and formal verification\nby formulating and solving a novel bilevel optimization problem, which is\ndifferentiable by the gradients from the value function and certificates.\nExperiments on a variety of examples demonstrate the significant advantages of\nour framework over the model-based stochastic value gradient (SVG) method and\nthe model-free proximal policy optimization (PPO) method in finding feasible\ncontrollers with barrier functions and Lyapunov functions that ensure system\nsafety and stability.\n","authors":["Yixuan Wang","Simon Zhan","Zhilu Wang","Chao Huang","Zhaoran Wang","Zhuoran Yang","Qi Zhu"],"pdf_url":"https://arxiv.org/pdf/2201.12243v2.pdf","comment":"This paper is accepted to International Conference on Cyber-Physical\n  Systems"},{"id":"http://arxiv.org/abs/2303.10880v2","updated":"2023-03-21T04:41:28Z","published":"2023-03-20T05:38:30Z","title":"Rotating without Seeing: Towards In-hand Dexterity through Touch","summary":"  Tactile information plays a critical role in human dexterity. It reveals\nuseful contact information that may not be inferred directly from vision. In\nfact, humans can even perform in-hand dexterous manipulation without using\nvision. Can we enable the same ability for the multi-finger robot hand? In this\npaper, we present Touch Dexterity, a new system that can perform in-hand object\nrotation using only touching without seeing the object. Instead of relying on\nprecise tactile sensing in a small region, we introduce a new system design\nusing dense binary force sensors (touch or no touch) overlaying one side of the\nwhole robot hand (palm, finger links, fingertips). Such a design is low-cost,\ngiving a larger coverage of the object, and minimizing the Sim2Real gap at the\nsame time. We train an in-hand rotation policy using Reinforcement Learning on\ndiverse objects in simulation. Relying on touch-only sensing, we can directly\ndeploy the policy in a real robot hand and rotate novel objects that are not\npresented in training. Extensive ablations are performed on how tactile\ninformation help in-hand manipulation.Our project is available at\nhttps://touchdexterity.github.io.\n","authors":["Zhao-Heng Yin","Binghao Huang","Yuzhe Qin","Qifeng Chen","Xiaolong Wang"],"pdf_url":"https://arxiv.org/pdf/2303.10880v2.pdf","comment":"Project page: https://touchdexterity.github.io"},{"id":"http://arxiv.org/abs/2303.11582v1","updated":"2023-03-21T04:17:03Z","published":"2023-03-21T04:17:03Z","title":"Adaptive Experimentation at Scale: Bayesian Algorithms for Flexible\n  Batches","summary":"  Standard bandit algorithms that assume continual reallocation of measurement\neffort are challenging to implement due to delayed feedback and\ninfrastructural/organizational difficulties. Motivated by practical instances\ninvolving a handful of reallocation epochs in which outcomes are measured in\nbatches, we develop a new adaptive experimentation framework that can flexibly\nhandle any batch size. Our main observation is that normal approximations\nuniversal in statistical inference can also guide the design of scalable\nadaptive designs. By deriving an asymptotic sequential experiment, we formulate\na dynamic program that can leverage prior information on average rewards. State\ntransitions of the dynamic program are differentiable with respect to the\nsampling allocations, allowing the use of gradient-based methods for planning\nand policy optimization. We propose a simple iterative planning method,\nResidual Horizon Optimization, which selects sampling allocations by optimizing\na planning objective via stochastic gradient-based methods. Our method\nsignificantly improves statistical power over standard adaptive policies, even\nwhen compared to Bayesian bandit algorithms (e.g., Thompson sampling) that\nrequire full distributional knowledge of individual rewards. Overall, we expand\nthe scope of adaptive experimentation to settings which are difficult for\nstandard adaptive policies, including problems with a small number of\nreallocation epochs, low signal-to-noise ratio, and unknown reward\ndistributions.\n","authors":["Ethan Che","Hongseok Namkoong"],"pdf_url":"https://arxiv.org/pdf/2303.11582v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08189v3","updated":"2023-03-21T04:13:48Z","published":"2022-12-15T23:21:49Z","title":"Multi-Resolution Online Deterministic Annealing: A Hierarchical and\n  Progressive Learning Architecture","summary":"  Hierarchical learning algorithms that gradually approximate a solution to a\ndata-driven optimization problem are essential to decision-making systems,\nespecially under limitations on time and computational resources. In this\nstudy, we introduce a general-purpose hierarchical learning architecture that\nis based on the progressive partitioning of a possibly multi-resolution data\nspace. The optimal partition is gradually approximated by solving a sequence of\noptimization sub-problems that yield a sequence of partitions with increasing\nnumber of subsets. We show that the solution of each optimization problem can\nbe estimated online using gradient-free stochastic approximation updates. As a\nconsequence, a function approximation problem can be defined within each subset\nof the partition and solved using the theory of two-timescale stochastic\napproximation algorithms. This simulates an annealing process and defines a\nrobust and interpretable heuristic method to gradually increase the complexity\nof the learning architecture in a task-agnostic manner, giving emphasis to\nregions of the data space that are considered more important according to a\npredefined criterion. Finally, by imposing a tree structure in the progression\nof the partitions, we provide a means to incorporate potential multi-resolution\nstructure of the data space into this approach, significantly reducing its\ncomplexity, while introducing hierarchical variable-rate feature extraction\nproperties similar to certain classes of deep learning architectures.\nAsymptotic convergence analysis and experimental results are provided for\nsupervised and unsupervised learning problems.\n","authors":["Christos Mavridis","John Baras"],"pdf_url":"https://arxiv.org/pdf/2212.08189v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11580v1","updated":"2023-03-21T04:01:55Z","published":"2023-03-21T04:01:55Z","title":"Efficient Multi-stage Inference on Tabular Data","summary":"  Many ML applications and products train on medium amounts of input data but\nget bottlenecked in real-time inference. When implementing ML systems,\nconventional wisdom favors segregating ML code into services queried by product\ncode via Remote Procedure Call (RPC) APIs. This approach clarifies the overall\nsoftware architecture and simplifies product code by abstracting away ML\ninternals. However, the separation adds network latency and entails additional\nCPU overhead. Hence, we simplify inference algorithms and embed them into the\nproduct code to reduce network communication. For public datasets and a\nhigh-performance real-time platform that deals with tabular data, we show that\nover half of the inputs are often amenable to such optimization, while the\nremainder can be handled by the original model. By applying our optimization\nwith AutoML to both training and inference, we reduce inference latency by\n1.3x, CPU resources by 30%, and network communication between application\nfront-end and ML back-end by about 50% for a commercial end-to-end ML platform\nthat serves millions of real-time decisions per second.\n","authors":["Daniel S Johnson","Igor L Markov"],"pdf_url":"https://arxiv.org/pdf/2303.11580v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.04366v2","updated":"2023-03-21T03:52:17Z","published":"2022-10-09T22:46:55Z","title":"Computational Choreography using Human Motion Synthesis","summary":"  Should deep learning models be trained to analyze human performance art? To\nhelp answer this question, we explore an application of deep neural networks to\nsynthesize artistic human motion. Problem tasks in human motion synthesis can\ninclude predicting the motions of humans in-the-wild, as well as generating new\nsequences of motions based on said predictions. We will discuss the potential\nof a less traditional application, where learning models are applied to\npredicting dance movements. There have been notable, recent efforts to analyze\ndance movements in a computational light, such as the Everybody Dance Now (EDN)\nlearning model and a Cal Poly master's thesis, Take The Lead (TTL). We have\neffectively combined these two works along with our own deep neural network to\nproduce a new system for dance motion prediction, image-to-image translation,\nand video generation.\n","authors":["Patrick Perrine","Trevor Kirkby"],"pdf_url":"https://arxiv.org/pdf/2210.04366v2.pdf","comment":"6 pages, 7 figures, to be submitted to AIVR 2023"},{"id":"http://arxiv.org/abs/2303.11577v1","updated":"2023-03-21T03:51:15Z","published":"2023-03-21T03:51:15Z","title":"Feature-adjacent multi-fidelity physics-informed machine learning for\n  partial differential equations","summary":"  Physics-informed neural networks have emerged as an alternative method for\nsolving partial differential equations. However, for complex problems, the\ntraining of such networks can still require high-fidelity data which can be\nexpensive to generate. To reduce or even eliminate the dependency on\nhigh-fidelity data, we propose a novel multi-fidelity architecture which is\nbased on a feature space shared by the low- and high-fidelity solutions. In the\nfeature space, the projections of the low-fidelity and high-fidelity solutions\nare adjacent by constraining their relative distance. The feature space is\nrepresented with an encoder and its mapping to the original solution space is\neffected through a decoder. The proposed multi-fidelity approach is validated\non forward and inverse problems for steady and unsteady problems described by\npartial differential equations.\n","authors":["Wenqian Chen","Panos Stinis"],"pdf_url":"https://arxiv.org/pdf/2303.11577v1.pdf","comment":"12 figures"},{"id":"http://arxiv.org/abs/2303.07397v2","updated":"2023-03-21T03:27:35Z","published":"2023-03-13T18:25:46Z","title":"Fast exploration and learning of latent graphs with aliased observations","summary":"  Consider this scenario: an agent navigates a latent graph by performing\nactions that take it from one node to another. The chosen action determines the\nprobability distribution over the next visited node. At each node, the agent\nreceives an observation, but this observation is not unique, so it does not\nidentify the node, making the problem aliased. The purpose of this work is to\nprovide a policy that approximately maximizes exploration efficiency (i.e., how\nwell the graph is recovered for a given exploration budget). In the unaliased\ncase, we show improved performance w.r.t. state-of-the-art reinforcement\nlearning baselines. For the aliased case we are not aware of suitable baselines\nand instead show faster recovery w.r.t. a random policy for a wide variety of\ntopologies, and exponentially faster recovery than a random policy for\nchallenging topologies. We dub the algorithm eFeX (from eFficient eXploration).\n","authors":["Miguel Lazaro-Gredilla","Ishan Deshpande","Sivaramakrishnan Swaminathan","Meet Dave","Dileep George"],"pdf_url":"https://arxiv.org/pdf/2303.07397v2.pdf","comment":"v2: Added extra figure and fixed typos"},{"id":"http://arxiv.org/abs/2303.11563v1","updated":"2023-03-21T03:14:15Z","published":"2023-03-21T03:14:15Z","title":"Dynamic Healthcare Embeddings for Improving Patient Care","summary":"  As hospitals move towards automating and integrating their computing systems,\nmore fine-grained hospital operations data are becoming available. These data\ninclude hospital architectural drawings, logs of interactions between patients\nand healthcare professionals, prescription data, procedures data, and data on\npatient admission, discharge, and transfers. This has opened up many\nfascinating avenues for healthcare-related prediction tasks for improving\npatient care. However, in order to leverage off-the-shelf machine learning\nsoftware for these tasks, one needs to learn structured representations of\nentities involved from heterogeneous, dynamic data streams. Here, we propose\nDECENT, an auto-encoding heterogeneous co-evolving dynamic neural network, for\nlearning heterogeneous dynamic embeddings of patients, doctors, rooms, and\nmedications from diverse data streams. These embeddings capture similarities\namong doctors, rooms, patients, and medications based on static attributes and\ndynamic interactions. DECENT enables several applications in healthcare\nprediction, such as predicting mortality risk and case severity of patients,\nadverse events (e.g., transfer back into an intensive care unit), and future\nhealthcare-associated infections. The results of using the learned patient\nembeddings in predictive modeling show that DECENT has a gain of up to 48.1% on\nthe mortality risk prediction task, 12.6% on the case severity prediction task,\n6.4% on the medical intensive care unit transfer task, and 3.8% on the\nClostridioides difficile (C.diff) Infection (CDI) prediction task over the\nstate-of-the-art baselines. In addition, case studies on the learned doctor,\nmedication, and room embeddings show that our approach learns meaningful and\ninterpretable embeddings.\n","authors":["Hankyu Jang","Sulyun Lee","D. M. Hasibul Hasan","Philip M. Polgreen","Sriram V. Pemmaraju","Bijaya Adhikari"],"pdf_url":"https://arxiv.org/pdf/2303.11563v1.pdf","comment":"To be published in IEEE/ACM ASONAM 2022"},{"id":"http://arxiv.org/abs/2303.11562v1","updated":"2023-03-21T03:05:21Z","published":"2023-03-21T03:05:21Z","title":"Dynamic-Aware Loss for Learning with Label Noise","summary":"  Label noise poses a serious threat to deep neural networks (DNNs). Employing\nrobust loss function which reconciles fitting ability with robustness is a\nsimple but effective strategy to handle this problem. However, the widely-used\nstatic trade-off between these two factors contradicts the dynamic nature of\nDNNs learning with label noise, leading to inferior performance. Therefore, we\npropose a dynamics-aware loss (DAL) to solve this problem. Considering that\nDNNs tend to first learn generalized patterns, then gradually overfit label\nnoise, DAL strengthens the fitting ability initially, then gradually increases\nthe weight of robustness. Moreover, at the later stage, we let DNNs put more\nemphasis on easy examples which are more likely to be correctly labeled than\nhard ones and introduce a bootstrapping term to further reduce the negative\nimpact of label noise. Both the detailed theoretical analyses and extensive\nexperimental results demonstrate the superiority of our method.\n","authors":["Xiu-Chuan Li","Xiaobo Xia","Fei Zhu","Tongliang Liu","Xu-Yao Zhang","Cheng-Lin Liu"],"pdf_url":"https://arxiv.org/pdf/2303.11562v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.04386v2","updated":"2023-03-21T02:50:48Z","published":"2023-03-08T05:19:08Z","title":"Policy Mirror Descent Inherently Explores Action Space","summary":"  Explicit exploration in the action space was assumed to be indispensable for\nonline policy gradient methods to avoid a drastic degradation in sample\ncomplexity, for solving general reinforcement learning problems over finite\nstate and action spaces. In this paper, we establish for the first time an\n$\\tilde{\\mathcal{O}}(1/\\epsilon^2)$ sample complexity for online policy\ngradient methods without incorporating any exploration strategies. The\nessential development consists of two new on-policy evaluation operators and a\nnovel analysis of the stochastic policy mirror descent method (SPMD). SPMD with\nthe first evaluation operator, called value-based estimation, tailors to the\nKullback-Leibler divergence. Provided the Markov chains on the state space of\ngenerated policies are uniformly mixing with non-diminishing minimal visitation\nmeasure, an $\\tilde{\\mathcal{O}}(1/\\epsilon^2)$ sample complexity is obtained\nwith a linear dependence on the size of the action space. SPMD with the second\nevaluation operator, namely truncated on-policy Monte Carlo (TOMC), attains an\n$\\tilde{\\mathcal{O}}(\\mathcal{H}_{\\mathcal{D}}/\\epsilon^2)$ sample complexity,\nwhere $\\mathcal{H}_{\\mathcal{D}}$ mildly depends on the effective horizon and\nthe size of the action space with properly chosen Bregman divergence (e.g.,\nTsallis divergence). SPMD with TOMC also exhibits stronger convergence\nproperties in that it controls the optimality gap with high probability rather\nthan in expectation. In contrast to explicit exploration, these new policy\ngradient methods can prevent repeatedly committing to potentially high-risk\nactions when searching for optimal policies.\n","authors":["Yan Li","Guanghui Lan"],"pdf_url":"https://arxiv.org/pdf/2303.04386v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11553v1","updated":"2023-03-21T02:44:15Z","published":"2023-03-21T02:44:15Z","title":"Dynamic Vertex Replacement Grammars","summary":"  Context-free graph grammars have shown a remarkable ability to model\nstructures in real-world relational data. However, graph grammars lack the\nability to capture time-changing phenomena since the left-to-right transitions\nof a production rule do not represent temporal change. In the present work, we\ndescribe dynamic vertex-replacement grammars (DyVeRG), which generalize vertex\nreplacement grammars in the time domain by providing a formal framework for\nupdating a learned graph grammar in accordance with modifications to its\nunderlying data. We show that DyVeRG grammars can be learned from, and used to\ngenerate, real-world dynamic graphs faithfully while remaining\nhuman-interpretable. We also demonstrate their ability to forecast by computing\ndyvergence scores, a novel graph similarity measurement exposed by this\nframework.\n","authors":["Daniel Gonzalez Cedre","Justus Isaiah Hibshman","Timothy La Fond","Grant Boquet","Tim Weninger"],"pdf_url":"https://arxiv.org/pdf/2303.11553v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11551v1","updated":"2023-03-21T02:37:46Z","published":"2023-03-21T02:37:46Z","title":"ModEFormer: Modality-Preserving Embedding for Audio-Video\n  Synchronization using Transformers","summary":"  Lack of audio-video synchronization is a common problem during television\nbroadcasts and video conferencing, leading to an unsatisfactory viewing\nexperience. A widely accepted paradigm is to create an error detection\nmechanism that identifies the cases when audio is leading or lagging. We\npropose ModEFormer, which independently extracts audio and video embeddings\nusing modality-specific transformers. Different from the other\ntransformer-based approaches, ModEFormer preserves the modality of the input\nstreams which allows us to use a larger batch size with more negative audio\nsamples for contrastive learning. Further, we propose a trade-off between the\nnumber of negative samples and number of unique samples in a batch to\nsignificantly exceed the performance of previous methods. Experimental results\nshow that ModEFormer achieves state-of-the-art performance, 94.5% for LRS2 and\n90.9% for LRS3. Finally, we demonstrate how ModEFormer can be used for offset\ndetection for test clips.\n","authors":["Akash Gupta","Rohun Tripathi","Wondong Jang"],"pdf_url":"https://arxiv.org/pdf/2303.11551v1.pdf","comment":"Paper accepted at ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.11545v1","updated":"2023-03-21T02:19:48Z","published":"2023-03-21T02:19:48Z","title":"Fix the Noise: Disentangling Source Feature for Controllable Domain\n  Translation","summary":"  Recent studies show strong generative performance in domain translation\nespecially by using transfer learning techniques on the unconditional\ngenerator. However, the control between different domain features using a\nsingle model is still challenging. Existing methods often require additional\nmodels, which is computationally demanding and leads to unsatisfactory visual\nquality. In addition, they have restricted control steps, which prevents a\nsmooth transition. In this paper, we propose a new approach for high-quality\ndomain translation with better controllability. The key idea is to preserve\nsource features within a disentangled subspace of a target feature space. This\nallows our method to smoothly control the degree to which it preserves source\nfeatures while generating images from an entirely new domain using only a\nsingle model. Our extensive experiments show that the proposed method can\nproduce more consistent and realistic images than previous works and maintain\nprecise controllability over different levels of transformation. The code is\navailable at https://github.com/LeeDongYeun/FixNoise.\n","authors":["Dongyeun Lee","Jae Young Lee","Doyeon Kim","Jaehyun Choi","Jaejun Yoo","Junmo Kim"],"pdf_url":"https://arxiv.org/pdf/2303.11545v1.pdf","comment":"Accepted by CVPR 2023. The code is available at\n  https://github.com/LeeDongYeun/FixNoise. Extended from arXiv:2204.14079 (AICC\n  workshop at CVPR 2022)"},{"id":"http://arxiv.org/abs/2303.09065v3","updated":"2023-03-21T02:15:48Z","published":"2023-03-16T03:45:46Z","title":"Maximum margin learning of t-SPNs for cell classification with filtered\n  input","summary":"  An algorithm based on a deep probabilistic architecture referred to as a\ntree-structured sum-product network (t-SPN) is considered for cell\nclassification. The t-SPN is constructed such that the unnormalized probability\nis represented as conditional probabilities of a subset of most similar cell\nclasses. The constructed t-SPN architecture is learned by maximizing the\nmargin, which is the difference in the conditional probability between the true\nand the most competitive false label. To enhance the generalization ability of\nthe architecture, L2-regularization (REG) is considered along with the maximum\nmargin (MM) criterion in the learning process. To highlight cell features, this\npaper investigates the effectiveness of two generic high-pass filters: ideal\nhigh-pass filtering and the Laplacian of Gaussian (LOG) filtering. On both\nHEp-2 and Feulgen benchmark datasets, the t-SPN architecture learned based on\nthe max-margin criterion with regularization produced the highest accuracy rate\ncompared to other state-of-the-art algorithms that include convolutional neural\nnetwork (CNN) based algorithms. The ideal high-pass filter was more effective\non the HEp-2 dataset, which is based on immunofluorescence staining, while the\nLOG was more effective on the Feulgen dataset, which is based on Feulgen\nstaining.\n","authors":["Haeyong Kang","Chang D. Yoo","Yongcheon Na"],"pdf_url":"https://arxiv.org/pdf/2303.09065v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11540v1","updated":"2023-03-21T02:11:37Z","published":"2023-03-21T02:11:37Z","title":"MSTFormer: Motion Inspired Spatial-temporal Transformer with\n  Dynamic-aware Attention for long-term Vessel Trajectory Prediction","summary":"  Incorporating the dynamics knowledge into the model is critical for achieving\naccurate trajectory prediction while considering the spatial and temporal\ncharacteristics of the vessel. However, existing methods rarely consider the\nunderlying dynamics knowledge and directly use machine learning algorithms to\npredict the trajectories. Intuitively, the vessel's motions are following the\nlaws of dynamics, e.g., the speed of a vessel decreases when turning a corner.\nYet, it is challenging to combine dynamic knowledge and neural networks due to\ntheir inherent heterogeneity. Against this background, we propose MSTFormer, a\nmotion inspired vessel trajectory prediction method based on Transformer. The\ncontribution of this work is threefold. First, we design a data augmentation\nmethod to describe the spatial features and motion features of the trajectory.\nSecond, we propose a Multi-headed Dynamic-aware Self-attention mechanism to\nfocus on trajectory points with frequent motion transformations. Finally, we\nconstruct a knowledge-inspired loss function to further boost the performance\nof the model. Experimental results on real-world datasets show that our\nstrategy not only effectively improves long-term predictive capability but also\noutperforms backbones on cornering data.The ablation analysis further confirms\nthe efficacy of the proposed method. To the best of our knowledge, MSTFormer is\nthe first neural network model for trajectory prediction fused with vessel\nmotion dynamics, providing a worthwhile direction for future research.The\nsource code is available at https://github.com/simple316/MSTFormer.\n","authors":["Huimin Qiang","Zhiyuan Guo","Shiyuan Xie","Xiaodong Peng"],"pdf_url":"https://arxiv.org/pdf/2303.11540v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2303.11536v1","updated":"2023-03-21T01:57:40Z","published":"2023-03-21T01:57:40Z","title":"Indeterminate Probability Neural Network","summary":"  We propose a new general model called IPNN - Indeterminate Probability Neural\nNetwork, which combines neural network and probability theory together. In the\nclassical probability theory, the calculation of probability is based on the\noccurrence of events, which is hardly used in current neural networks. In this\npaper, we propose a new general probability theory, which is an extension of\nclassical probability theory, and makes classical probability theory a special\ncase to our theory. Besides, for our proposed neural network framework, the\noutput of neural network is defined as probability events, and based on the\nstatistical analysis of these events, the inference model for classification\ntask is deduced. IPNN shows new property: It can perform unsupervised\nclustering while doing classification. Besides, IPNN is capable of making very\nlarge classification with very small neural network, e.g. model with 100 output\nnodes can classify 10 billion categories. Theoretical advantages are reflected\nin experimental results.\n","authors":["Tao Yang","Chuang Liu","Xiaofeng Ma","Weijia Lu","Ning Wu","Bingyang Li","Zhifei Yang","Peng Liu","Lin Sun","Xiaodong Zhang","Can Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.11536v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2205.14230v2","updated":"2023-03-21T01:55:06Z","published":"2022-05-27T20:50:36Z","title":"Semi-supervised Semantics-guided Adversarial Training for Trajectory\n  Prediction","summary":"  Predicting the trajectories of surrounding objects is a critical task for\nself-driving vehicles and many other autonomous systems. Recent works\ndemonstrate that adversarial attacks on trajectory prediction, where small\ncrafted perturbations are introduced to history trajectories, may significantly\nmislead the prediction of future trajectories and induce unsafe planning.\nHowever, few works have addressed enhancing the robustness of this important\nsafety-critical task.In this paper, we present a novel adversarial training\nmethod for trajectory prediction. Compared with typical adversarial training on\nimage tasks, our work is challenged by more random input with rich context and\na lack of class labels. To address these challenges, we propose a method based\non a semi-supervised adversarial autoencoder, which models disentangled\nsemantic features with domain knowledge and provides additional latent labels\nfor the adversarial training. Extensive experiments with different types of\nattacks demonstrate that our Semisupervised Semantics-guided Adversarial\nTraining (SSAT) method can effectively mitigate the impact of adversarial\nattacks by up to 73% and outperform other popular defense methods. In addition,\nexperiments show that our method can significantly improve the system's robust\ngeneralization to unseen patterns of attacks. We believe that such\nsemantics-guided architecture and advancement on robust generalization is an\nimportant step for developing robust prediction models and enabling safe\ndecision-making.\n","authors":["Ruochen Jiao","Xiangguo Liu","Takami Sato","Qi Alfred Chen","Qi Zhu"],"pdf_url":"https://arxiv.org/pdf/2205.14230v2.pdf","comment":"11 pages, adversarial training for trajectory prediction"},{"id":"http://arxiv.org/abs/2302.12426v3","updated":"2023-03-21T01:49:46Z","published":"2023-02-24T03:13:12Z","title":"Statistical Analysis of Karcher Means for Random Restricted PSD Matrices","summary":"  Non-asymptotic statistical analysis is often missing for modern\ngeometry-aware machine learning algorithms due to the possibly intricate\nnon-linear manifold structure. This paper studies an intrinsic mean model on\nthe manifold of restricted positive semi-definite matrices and provides a\nnon-asymptotic statistical analysis of the Karcher mean. We also consider a\ngeneral extrinsic signal-plus-noise model, under which a deterministic error\nbound of the Karcher mean is provided. As an application, we show that the\ndistributed principal component analysis algorithm, LRC-dPCA, achieves the same\nperformance as the full sample PCA algorithm. Numerical experiments lend strong\nsupport to our theories.\n","authors":["Hengchao Chen","Xiang Li","Qiang Sun"],"pdf_url":"https://arxiv.org/pdf/2302.12426v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11529v1","updated":"2023-03-21T01:28:23Z","published":"2023-03-21T01:28:23Z","title":"Counterfactually Fair Regression with Double Machine Learning","summary":"  Counterfactual fairness is an approach to AI fairness that tries to make\ndecisions based on the outcomes that an individual with some kind of sensitive\nstatus would have had without this status. This paper proposes Double Machine\nLearning (DML) Fairness which analogises this problem of counterfactual\nfairness in regression problems to that of estimating counterfactual outcomes\nin causal inference under the Potential Outcomes framework. It uses arbitrary\nmachine learning methods to partial out the effect of sensitive variables on\nnonsensitive variables and outcomes. Assuming that the effects of the two sets\nof variables are additively separable, outcomes will be approximately equalised\nand individual-level outcomes will be counterfactually fair. This paper\ndemonstrates the approach in a simulation study pertaining to discrimination in\nworkplace hiring and an application on real data estimating the GPAs of law\nschool students. It then discusses when it is appropriate to apply such a\nmethod to problems of real-world discrimination where constructs are\nconceptually complex and finally, whether DML Fairness can achieve justice in\nthese settings.\n","authors":["Patrick Rehill"],"pdf_url":"https://arxiv.org/pdf/2303.11529v1.pdf","comment":"21 pages, 16 figures"},{"id":"http://arxiv.org/abs/1703.01347v3","updated":"2023-03-21T01:19:15Z","published":"2017-03-03T21:39:56Z","title":"Contextual Linear Bandits under Noisy Features: Towards Bayesian Oracles","summary":"  We study contextual linear bandit problems under feature uncertainty; they\nare noisy with missing entries. To address the challenges of the noise, we\nanalyze Bayesian oracles given observed noisy features. Our Bayesian analysis\nfinds that the optimal hypothesis can be far from the underlying realizability\nfunction, depending on the noise characteristics, which are highly\nnon-intuitive and do not occur for classical noiseless setups. This implies\nthat classical approaches cannot guarantee a non-trivial regret bound.\nTherefore, we propose an algorithm that aims at the Bayesian oracle from\nobserved information under this model, achieving $\\tilde{O}(d\\sqrt{T})$ regret\nbound when there is a large number of arms. We demonstrate the proposed\nalgorithm using synthetic and real-world datasets.\n","authors":["Jung-hun Kim","Se-Young Yun","Minchan Jeong","Jun Hyun Nam","Jinwoo Shin","Richard Combes"],"pdf_url":"https://arxiv.org/pdf/1703.01347v3.pdf","comment":"30 pages; accepted at AISTATS2023; minor corrections to Bayesian\n  features"},{"id":"http://arxiv.org/abs/2303.11525v1","updated":"2023-03-21T01:06:37Z","published":"2023-03-21T01:06:37Z","title":"SIFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency","summary":"  Recent works have explored the use of weight sparsity to improve the training\nefficiency (test accuracy w.r.t training FLOPs) of deep neural networks (DNNs).\nThese works aim to reduce training FLOPs but training with sparse weights often\nleads to accuracy loss or requires longer train schedules, making the resulting\ntraining efficiency less clear. In contrast, we focus on using sparsity to\nincrease accuracy while using the same FLOPS as the dense model and show\ntraining efficiency gains through higher accuracy. In this work, we introduce\nSIFT, a family of Sparse Iso-FLOP Transformations which are used as drop-in\nreplacements for dense layers to improve their representational capacity and\nFLOP efficiency. Each transformation is parameterized by a single parameter\n(sparsity level) and provides a larger search space to find optimal sparse\nmasks. Without changing any training hyperparameters, replacing dense layers\nwith SIFT leads to significant improvements across computer vision (CV) and\nnatural language processing (NLP) tasks, including ResNet-18 on ImageNet\n(+3.5%) and GPT-3 Small on WikiText-103 (-0.4 PPL), both matching larger dense\nmodel variants with 2x or more FLOPs. To the best of our knowledge, this is the\nfirst work to demonstrate the use of sparsity for improving accuracy of dense\nmodels via a simple-to-use set of sparse transformations. Code is available at:\nhttps://github.com/CerebrasResearch/SIFT.\n","authors":["Shreyas Saxena","Vithursan Thangarasa","Abhay Gupta","Sean Lie"],"pdf_url":"https://arxiv.org/pdf/2303.11525v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.10381v3","updated":"2023-03-21T00:53:55Z","published":"2022-11-18T17:25:14Z","title":"Environmental Sensor Placement with Convolutional Gaussian Neural\n  Processes","summary":"  Environmental sensors are crucial for monitoring weather conditions and the\nimpacts of climate change. However, it is challenging to maximise measurement\ninformativeness and place sensors efficiently, particularly in remote regions\nlike Antarctica. Probabilistic machine learning models can evaluate placement\ninformativeness by predicting the uncertainty reduction provided by a new\nsensor. Gaussian process (GP) models are widely used for this purpose, but they\nstruggle with capturing complex non-stationary behaviour and scaling to large\ndatasets. This paper proposes using a convolutional Gaussian neural process\n(ConvGNP) to address these issues. A ConvGNP uses neural networks to\nparameterise a joint Gaussian distribution at arbitrary target locations,\nenabling flexibility and scalability. Using simulated surface air temperature\nanomaly over Antarctica as ground truth, the ConvGNP learns spatial and\nseasonal non-stationarities, outperforming a non-stationary GP baseline. In a\nsimulated sensor placement experiment, the ConvGNP better predicts the\nperformance boost obtained from new observations than GP baselines, leading to\nmore informative sensor placements. We connect our work with similar machine\nlearning and physics-based approaches and discuss steps towards an operational\nsensor placement recommendation system.\n","authors":["Tom R. Andersson","Wessel P. Bruinsma","Stratis Markou","James Requeima","Alejandro Coca-Castro","Anna Vaughan","Anna-Louise Ellis","Matthew Lazzara","Daniel C. Jones","J. Scott Hosking","Richard E. Turner"],"pdf_url":"https://arxiv.org/pdf/2211.10381v3.pdf","comment":"In review for the Climate Informatics 2023 special issue of\n  Environmental Data Science"},{"id":"http://arxiv.org/abs/2303.11522v1","updated":"2023-03-21T00:53:37Z","published":"2023-03-21T00:53:37Z","title":"Online Learning for Equilibrium Pricing in Markets under Incomplete\n  Information","summary":"  The study of market equilibria is central to economic theory, particularly in\nefficiently allocating scarce resources. However, the computation of\nequilibrium prices at which the supply of goods matches their demand typically\nrelies on having access to complete information on private attributes of\nagents, e.g., suppliers' cost functions, which are often unavailable in\npractice. Motivated by this practical consideration, we consider the problem of\nsetting equilibrium prices in the incomplete information setting wherein a\nmarket operator seeks to satisfy the customer demand for a commodity by\npurchasing the required amount from competing suppliers with privately known\ncost functions unknown to the market operator. In this incomplete information\nsetting, we consider the online learning problem of learning equilibrium prices\nover time while jointly optimizing three performance metrics -- unmet demand,\ncost regret, and payment regret -- pertinent in the context of equilibrium\npricing over a horizon of $T$ periods. We first consider the setting when\nsuppliers' cost functions are fixed and develop algorithms that achieve a\nregret of $O(\\log \\log T)$ when the customer demand is constant over time, or\n$O(\\sqrt{T} \\log \\log T)$ when the demand is variable over time. Next, we\nconsider the setting when the suppliers' cost functions can vary over time and\nillustrate that no online algorithm can achieve sublinear regret on all three\nmetrics when the market operator has no information about how the cost\nfunctions change over time. Thus, we consider an augmented setting wherein the\noperator has access to hints/contexts that, without revealing the complete\nspecification of the cost functions, reflect the variation in the cost\nfunctions over time and propose an algorithm with sublinear regret in this\naugmented setting.\n","authors":["Devansh Jalota","Haoyuan Sun","Navid Azizan"],"pdf_url":"https://arxiv.org/pdf/2303.11522v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06075v2","updated":"2023-03-21T00:36:17Z","published":"2023-03-10T16:53:51Z","title":"Long-tailed Classification from a Bayesian-decision-theory Perspective","summary":"  Long-tailed classification poses a challenge due to its heavy imbalance in\nclass probabilities and tail-sensitivity risks with asymmetric misprediction\ncosts. Recent attempts have used re-balancing loss and ensemble methods, but\nthey are largely heuristic and depend heavily on empirical results, lacking\ntheoretical explanation. Furthermore, existing methods overlook the decision\nloss, which characterizes different costs associated with tailed classes. This\npaper presents a general and principled framework from a\nBayesian-decision-theory perspective, which unifies existing techniques\nincluding re-balancing and ensemble methods, and provides theoretical\njustifications for their effectiveness. From this perspective, we derive a\nnovel objective based on the integrated risk and a Bayesian deep-ensemble\napproach to improve the accuracy of all classes, especially the \"tail\".\nBesides, our framework allows for task-adaptive decision loss which provides\nprovably optimal decisions in varying task scenarios, along with the capability\nto quantify uncertainty. Finally, We conduct comprehensive experiments,\nincluding standard classification, tail-sensitive classification with a new\nFalse Head Rate metric, calibration, and ablation studies. Our framework\nsignificantly improves the current SOTA even on large-scale real-world datasets\nlike ImageNet.\n","authors":["Bolian Li","Ruqi Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.06075v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.05949v2","updated":"2023-03-21T00:28:23Z","published":"2022-08-11T17:40:45Z","title":"Valid Inference after Causal Discovery","summary":"  Causal discovery and causal effect estimation are two fundamental tasks in\ncausal inference. While many methods have been developed for each task\nindividually, statistical challenges arise when applying these methods jointly:\nestimating causal effects after running causal discovery algorithms on the same\ndata leads to \"double dipping,\" invalidating the coverage guarantees of\nclassical confidence intervals. To this end, we develop tools for valid\npost-causal-discovery inference. Across empirical studies, we show that a naive\ncombination of causal discovery and subsequent inference algorithms leads to\nhighly inflated miscoverage rates; on the other hand, applying our method\nprovides reliable coverage while achieving more accurate causal discovery than\ndata splitting.\n","authors":["Paula Gradu","Tijana Zrnic","Yixin Wang","Michael I. Jordan"],"pdf_url":"https://arxiv.org/pdf/2208.05949v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11511v1","updated":"2023-03-21T00:15:53Z","published":"2023-03-21T00:15:53Z","title":"STDLens: Model Hijacking-resilient Federated Learning for Object\n  Detection","summary":"  Federated Learning (FL) has been gaining popularity as a collaborative\nlearning framework to train deep learning-based object detection models over a\ndistributed population of clients. Despite its advantages, FL is vulnerable to\nmodel hijacking. The attacker can control how the object detection system\nshould misbehave by implanting Trojaned gradients using only a small number of\ncompromised clients in the collaborative learning process. This paper\nintroduces STDLens, a principled approach to safeguarding FL against such\nattacks. We first investigate existing mitigation mechanisms and analyze their\nfailures caused by the inherent errors in spatial clustering analysis on\ngradients. Based on the insights, we introduce a three-tier forensic framework\nto identify and expel Trojaned gradients and reclaim the performance over the\ncourse of FL. We consider three types of adaptive attacks and demonstrate the\nrobustness of STDLens against advanced adversaries. Extensive experiments show\nthat STDLens can protect FL against different model hijacking attacks and\noutperform existing methods in identifying and removing Trojaned gradients with\nsignificantly higher precision and much lower false-positive rates.\n","authors":["Ka-Ho Chow","Ling Liu","Wenqi Wei","Fatih Ilhan","Yanzhao Wu"],"pdf_url":"https://arxiv.org/pdf/2303.11511v1.pdf","comment":"CVPR 2023. Source Code: https://github.com/git-disl/STDLens"},{"id":"http://arxiv.org/abs/2303.11508v1","updated":"2023-03-21T00:04:33Z","published":"2023-03-21T00:04:33Z","title":"AI-in-the-Loop -- The impact of HMI in AI-based Application","summary":"  Artificial intelligence (AI) and human-machine interaction (HMI) are two\nkeywords that usually do not fit embedded applications. Within the steps needed\nbefore applying AI to solve a specific task, HMI is usually missing during the\nAI architecture design and the training of an AI model. The human-in-the-loop\nconcept is prevalent in all other steps of developing AI, from data analysis\nvia data selection and cleaning to performance evaluation. During AI\narchitecture design, HMI can immediately highlight unproductive layers of the\narchitecture so that lightweight network architecture for embedded applications\ncan be created easily. We show that by using this HMI, users can instantly\ndistinguish which AI architecture should be trained and evaluated first since a\nhigh accuracy on the task could be expected. This approach reduces the\nresources needed for AI development by avoiding training and evaluating AI\narchitectures with unproductive layers and leads to lightweight AI\narchitectures. These resulting lightweight AI architectures will enable HMI\nwhile running the AI on an edge device. By enabling HMI during an AI uses\ninference, we will introduce the AI-in-the-loop concept that combines AI's and\nhumans' strengths. In our AI-in-the-loop approach, the AI remains the working\nhorse and primarily solves the task. If the AI is unsure whether its inference\nsolves the task correctly, it asks the user to use an appropriate HMI.\nConsequently, AI will become available in many applications soon since HMI will\nmake AI more reliable and explainable.\n","authors":["Julius Schöning","Clemens Westerkamp"],"pdf_url":"https://arxiv.org/pdf/2303.11508v1.pdf","comment":"12 pages; 9 figures; 1 table;"},{"id":"http://arxiv.org/abs/2302.10873v2","updated":"2023-03-21T00:02:34Z","published":"2023-02-21T18:42:24Z","title":"Context-Aware Timewise VAEs for Real-Time Vehicle Trajectory Prediction","summary":"  Real-time, accurate prediction of human steering behaviors has wide\napplications, from developing intelligent traffic systems to deploying\nautonomous driving systems in both real and simulated worlds. In this paper, we\npresent ContextVAE, a context-aware approach for multi-modal vehicle trajectory\nprediction. Built upon the backbone architecture of a timewise variational\nautoencoder, ContextVAE employs a dual attention mechanism for observation\nencoding that accounts for the environmental context information and the\ndynamic agents' states in a unified way. By utilizing features extracted from\nsemantic maps during agent state encoding, our approach takes into account both\nthe social features exhibited by agents on the scene and the physical\nenvironment constraints to generate map-compliant and socially-aware\ntrajectories. We perform extensive testing on the nuScenes prediction\nchallenge, Lyft Level 5 dataset and Waymo Open Motion Dataset to show the\neffectiveness of our approach and its state-of-the-art performance. In all\ntested datasets, ContextVAE models are fast to train and provide high-quality\nmulti-modal predictions in real-time.\n","authors":["Pei Xu","Jean-Bernard Hayet","Ioannis Karamouzas"],"pdf_url":"https://arxiv.org/pdf/2302.10873v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12238v1","updated":"2023-03-21T23:44:09Z","published":"2023-03-21T23:44:09Z","title":"DG-Trans: Dual-level Graph Transformer for Spatiotemporal Incident\n  Impact Prediction on Traffic Networks","summary":"  The prompt estimation of traffic incident impacts can guide commuters in\ntheir trip planning and improve the resilience of transportation agencies'\ndecision-making on resilience. However, it is more challenging than node-level\nand graph-level forecasting tasks, as it requires extracting the anomaly\nsubgraph or sub-time-series from dynamic graphs. In this paper, we propose\nDG-Trans, a novel traffic incident impact prediction framework, to foresee the\nimpact of traffic incidents through dynamic graph learning. The proposed\nframework contains a dual-level spatial transformer and an\nimportance-score-based temporal transformer, and the performance of this\nframework is justified by two newly constructed benchmark datasets. The\ndual-level spatial transformer removes unnecessary edges between nodes to\nisolate the affected subgraph from the other nodes. Meanwhile, the\nimportance-score-based temporal transformer identifies abnormal changes in node\nfeatures, causing the predictions to rely more on measurement changes after the\nincident occurs. Therefore, DG-Trans is equipped with dual abilities that\nextract spatiotemporal dependency and identify anomaly nodes affected by\nincidents while removing noise introduced by benign nodes. Extensive\nexperiments on real-world datasets verify that DG-Trans outperforms the\nexisting state-of-the-art methods, especially in extracting spatiotemporal\ndependency patterns and predicting traffic accident impacts. It offers\npromising potential for traffic incident management systems.\n","authors":["Yanshen Sun","Kaiqun Fu","Chang-Tien Lu"],"pdf_url":"https://arxiv.org/pdf/2303.12238v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12233v1","updated":"2023-03-21T23:29:35Z","published":"2023-03-21T23:29:35Z","title":"Secure Aggregation in Federated Learning is not Private: Leaking User\n  Data at Large Scale through Model Modification","summary":"  Security and privacy are important concerns in machine learning. End user\ndevices often contain a wealth of data and this information is sensitive and\nshould not be shared with servers or enterprises. As a result, federated\nlearning was introduced to enable machine learning over large decentralized\ndatasets while promising privacy by eliminating the need for data sharing.\nHowever, prior work has shown that shared gradients often contain private\ninformation and attackers can gain knowledge either through malicious\nmodification of the architecture and parameters or by using optimization to\napproximate user data from the shared gradients. Despite this, most attacks\nhave so far been limited in scale of number of clients, especially failing when\nclient gradients are aggregated together using secure model aggregation. The\nattacks that still function are strongly limited in the number of clients\nattacked, amount of training samples they leak, or number of iterations they\ntake to be trained. In this work, we introduce MANDRAKE, an attack that\novercomes previous limitations to directly leak large amounts of client data\neven under secure aggregation across large numbers of clients. Furthermore, we\nbreak the anonymity of aggregation as the leaked data is identifiable and\ndirectly tied back to the clients they come from. We show that by sending\nclients customized convolutional parameters, the weight gradients of data\npoints between clients will remain separate through aggregation. With an\naggregation across many clients, prior work could only leak less than 1% of\nimages. With the same number of non-zero parameters, and using only a single\ntraining iteration, MANDRAKE leaks 70-80% of data samples.\n","authors":["Joshua C. Zhao","Atul Sharma","Ahmed Roushdy Elkordy","Yahya H. Ezzeldin","Salman Avestimehr","Saurabh Bagchi"],"pdf_url":"https://arxiv.org/pdf/2303.12233v1.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2212.09668v2","updated":"2023-03-21T23:01:40Z","published":"2022-12-19T17:54:36Z","title":"Task-Oriented Communications for NextG: End-to-End Deep Learning and AI\n  Security Aspects","summary":"  Communications systems to date are primarily designed with the goal of\nreliable transfer of digital sequences (bits). Next generation (NextG)\ncommunication systems are beginning to explore shifting this design paradigm to\nreliably executing a given task such as in task-oriented communications. In\nthis paper, wireless signal classification is considered as the task for the\nNextG Radio Access Network (RAN), where edge devices collect wireless signals\nfor spectrum awareness and communicate with the NextG base station (gNodeB)\nthat needs to identify the signal label. Edge devices may not have sufficient\nprocessing power and may not be trusted to perform the signal classification\ntask, whereas the transfer of signals to the gNodeB may not be feasible due to\nstringent delay, rate, and energy restrictions. Task-oriented communications is\nconsidered by jointly training the transmitter, receiver and classifier\nfunctionalities as an encoder-decoder pair for the edge device and the gNodeB.\nThis approach improves the accuracy compared to the separated case of signal\ntransfer followed by classification. Adversarial machine learning poses a major\nsecurity threat to the use of deep learning for task-oriented communications. A\nmajor performance loss is shown when backdoor (Trojan) and adversarial\n(evasion) attacks target the training and test processes of task-oriented\ncommunications.\n","authors":["Yalin E. Sagduyu","Sennur Ulukus","Aylin Yener"],"pdf_url":"https://arxiv.org/pdf/2212.09668v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12224v1","updated":"2023-03-21T22:55:51Z","published":"2023-03-21T22:55:51Z","title":"Infrastructure-based End-to-End Learning and Prevention of Driver\n  Failure","summary":"  Intelligent intersection managers can improve safety by detecting dangerous\ndrivers or failure modes in autonomous vehicles, warning oncoming vehicles as\nthey approach an intersection. In this work, we present FailureNet, a recurrent\nneural network trained end-to-end on trajectories of both nominal and reckless\ndrivers in a scaled miniature city. FailureNet observes the poses of vehicles\nas they approach an intersection and detects whether a failure is present in\nthe autonomy stack, warning cross-traffic of potentially dangerous drivers.\nFailureNet can accurately identify control failures, upstream perception\nerrors, and speeding drivers, distinguishing them from nominal driving. The\nnetwork is trained and deployed with autonomous vehicles in the MiniCity.\nCompared to speed or frequency-based predictors, FailureNet's recurrent neural\nnetwork structure provides improved predictive power, yielding upwards of 84%\naccuracy when deployed on hardware.\n","authors":["Noam Buckman","Shiva Sreeram","Mathias Lechner","Yutong Ban","Ramin Hasani","Sertac Karaman","Daniela Rus"],"pdf_url":"https://arxiv.org/pdf/2303.12224v1.pdf","comment":"8 pages. Accepted to ICRA 2023"},{"id":"http://arxiv.org/abs/2303.00848v2","updated":"2023-03-21T22:23:51Z","published":"2023-03-01T22:36:05Z","title":"Understanding the Diffusion Objective as a Weighted Integral of ELBOs","summary":"  Diffusion models in the literature are optimized with various objectives that\nare special cases of a weighted loss, where the weighting function specifies\nthe weight per noise level. Uniform weighting corresponds to maximizing the\nELBO, a principled approximation of maximum likelihood. In current practice\ndiffusion models are optimized with non-uniform weighting due to better results\nin terms of sample quality. In this work we expose a direct relationship\nbetween the weighted loss (with any weighting) and the ELBO objective.\n  We show that the weighted loss can be written as a weighted integral of\nELBOs, with one ELBO per noise level. If the weighting function is monotonic,\nthen the weighted loss is a likelihood-based objective: it maximizes the ELBO\nunder simple data augmentation, namely Gaussian noise perturbation. Our main\ncontribution is a deeper theoretical understanding of the diffusion objective,\nbut we also performed some experiments comparing monotonic with non-monotonic\nweightings, finding that monotonic weighting performs competitively with the\nbest published results.\n","authors":["Diederik P. Kingma","Ruiqi Gao"],"pdf_url":"https://arxiv.org/pdf/2303.00848v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.11173v3","updated":"2023-03-21T22:20:47Z","published":"2022-08-23T20:02:09Z","title":"The Alberta Plan for AI Research","summary":"  Herein we describe our approach to artificial intelligence research, which we\ncall the Alberta Plan. The Alberta Plan is pursued within our research groups\nin Alberta and by others who are like minded throughout the world. We welcome\nall who would join us in this pursuit.\n","authors":["Richard S. Sutton","Michael Bowling","Patrick M. Pilarski"],"pdf_url":"https://arxiv.org/pdf/2208.11173v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12212v1","updated":"2023-03-21T22:12:53Z","published":"2023-03-21T22:12:53Z","title":"Community detection in complex networks via node similarity, graph\n  representation learning, and hierarchical clustering","summary":"  Community detection is a critical challenge in the analysis of real-world\ngraphs and complex networks, including social, transportation, citation,\ncybersecurity networks, and food webs. Motivated by many similarities between\ncommunity detection and clustering in Euclidean spaces, we propose three\nalgorithm frameworks to apply hierarchical clustering methods for community\ndetection in graphs. We show that using our methods, it is possible to apply\nvarious linkage-based (single-, complete-, average- linkage, Ward, Genie)\nclustering algorithms to find communities based on vertex similarity matrices,\neigenvector matrices thereof, and Euclidean vector representations of nodes. We\nconvey a comprehensive analysis of choices for each framework, including\nstate-of-the-art graph representation learning algorithms, such as Deep Neural\nGraph Representation, and a vertex proximity matrix known to yield high-quality\nresults in machine learning -- Positive Pointwise Mutual Information. Overall,\nwe test over a hundred combinations of framework components and show that some\n-- including Wasserman-Faust and PPMI proximity, DNGR representation -- can\ncompete with algorithms such as state-of-the-art Leiden and Louvain and easily\noutperform other known community detection algorithms. Notably, our algorithms\nremain hierarchical and allow the user to specify any number of clusters a\npriori.\n","authors":["Łukasz Brzozowski","Grzegorz Siudem","Marek Gagolewski"],"pdf_url":"https://arxiv.org/pdf/2303.12212v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12210v1","updated":"2023-03-21T21:58:59Z","published":"2023-03-21T21:58:59Z","title":"A Random Projection k Nearest Neighbours Ensemble for Classification via\n  Extended Neighbourhood Rule","summary":"  Ensembles based on k nearest neighbours (kNN) combine a large number of base\nlearners, each constructed on a sample taken from a given training data.\nTypical kNN based ensembles determine the k closest observations in the\ntraining data bounded to a test sample point by a spherical region to predict\nits class. In this paper, a novel random projection extended neighbourhood rule\n(RPExNRule) ensemble is proposed where bootstrap samples from the given\ntraining data are randomly projected into lower dimensions for additional\nrandomness in the base models and to preserve features information. It uses the\nextended neighbourhood rule (ExNRule) to fit kNN as base learners on randomly\nprojected bootstrap samples.\n","authors":["Amjad Ali","Muhammad Hamraz","Dost Muhammad Khan","Wajdan Deebani","Zardad Khan"],"pdf_url":"https://arxiv.org/pdf/2303.12210v1.pdf","comment":"23 pages, 8 diagrams, 69 references"},{"id":"http://arxiv.org/abs/2303.12208v1","updated":"2023-03-21T21:49:39Z","published":"2023-03-21T21:49:39Z","title":"MAGVLT: Masked Generative Vision-and-Language Transformer","summary":"  While generative modeling on multimodal image-text data has been actively\ndeveloped with large-scale paired datasets, there have been limited attempts to\ngenerate both image and text data by a single model rather than a generation of\none fixed modality conditioned on the other modality. In this paper, we explore\na unified generative vision-and-language (VL) model that can produce both\nimages and text sequences. Especially, we propose a generative VL transformer\nbased on the non-autoregressive mask prediction, named MAGVLT, and compare it\nwith an autoregressive generative VL transformer (ARGVLT). In comparison to\nARGVLT, the proposed MAGVLT enables bidirectional context encoding, fast\ndecoding by parallel token predictions in an iterative refinement, and extended\nediting capabilities such as image and text infilling. For rigorous training of\nour MAGVLT with image-text pairs from scratch, we combine the image-to-text,\ntext-to-image, and joint image-and-text mask prediction tasks. Moreover, we\ndevise two additional tasks based on the step-unrolled mask prediction and the\nselective prediction on the mixture of two image-text pairs. Experimental\nresults on various downstream generation tasks of VL benchmarks show that our\nMAGVLT outperforms ARGVLT by a large margin even with significant inference\nspeedup. Particularly, MAGVLT achieves competitive results on both zero-shot\nimage-to-text and text-to-image generation tasks from MS-COCO by one\nmoderate-sized model (fewer than 500M parameters) even without the use of\nmonomodal data and networks.\n","authors":["Sungwoong Kim","Daejin Jo","Donghoon Lee","Jongmin Kim"],"pdf_url":"https://arxiv.org/pdf/2303.12208v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.12206v1","updated":"2023-03-21T21:42:03Z","published":"2023-03-21T21:42:03Z","title":"Policy Optimization for Personalized Interventions in Behavioral Health","summary":"  Problem definition: Behavioral health interventions, delivered through\ndigital platforms, have the potential to significantly improve health outcomes,\nthrough education, motivation, reminders, and outreach. We study the problem of\noptimizing personalized interventions for patients to maximize some long-term\noutcome, in a setting where interventions are costly and capacity-constrained.\n  Methodology/results: This paper provides a model-free approach to solving\nthis problem. We find that generic model-free approaches from the reinforcement\nlearning literature are too data intensive for healthcare applications, while\nsimpler bandit approaches make progress at the expense of ignoring long-term\npatient dynamics. We present a new algorithm we dub DecompPI that approximates\none step of policy iteration. Implementing DecompPI simply consists of a\nprediction task from offline data, alleviating the need for online\nexperimentation. Theoretically, we show that under a natural set of structural\nassumptions on patient dynamics, DecompPI surprisingly recovers at least 1/2 of\nthe improvement possible between a naive baseline policy and the optimal\npolicy. At the same time, DecompPI is both robust to estimation errors and\ninterpretable. Through an empirical case study on a mobile health platform for\nimproving treatment adherence for tuberculosis, we find that DecompPI can\nprovide the same efficacy as the status quo with approximately half the\ncapacity of interventions.\n  Managerial implications: DecompPI is general and is easily implementable for\norganizations aiming to improve long-term behavior through targeted\ninterventions. Our case study suggests that the platform's costs of deploying\ninterventions can potentially be cut by 50%, which facilitates the ability to\nscale up the system in a cost-efficient fashion.\n","authors":["Jackie Baek","Justin J. Boutilier","Vivek F. Farias","Jonas Oddur Jonasson","Erez Yoeli"],"pdf_url":"https://arxiv.org/pdf/2303.12206v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12199v1","updated":"2023-03-21T21:12:29Z","published":"2023-03-21T21:12:29Z","title":"Analytical Conjugate Priors for Subclasses of Generalized Pareto\n  Distributions","summary":"  This article is written for pedagogical purposes aiming at practitioners\ntrying to estimate the finite support of continuous probability distributions,\ni.e., the minimum and the maximum of a distribution defined on a finite domain.\nGeneralized Pareto distribution GP({\\theta}, {\\sigma}, {\\xi}) is a\nthree-parameter distribution which plays a key role in Peaks-Over-Threshold\nframework for tail estimation in Extreme Value Theory. Estimators for GP often\nlack analytical solutions and the best known Bayesian methods for GP involves\nnumerical methods. Moreover, existing literature focuses on estimating the\nscale {\\sigma} and the shape {\\xi}, lacking discussion of the estimation of the\nlocation {\\theta} which is the lower support of (minimum value possible in) a\nGP. To fill the gap, we analyze four two-parameter subclasses of GP whose\nconjugate priors can be obtained analytically, although some of the results are\nknown. Namely, we prove the conjugacy for {\\xi} > 0 (Pareto), {\\xi} = 0\n(Shifted Exponential), {\\xi} < 0 (Power), and {\\xi} = -1 (Two-parameter\nUniform).\n","authors":["Masataro Asai"],"pdf_url":"https://arxiv.org/pdf/2303.12199v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.13349v2","updated":"2023-03-21T21:01:46Z","published":"2023-01-31T00:52:14Z","title":"Unconstrained Dynamic Regret via Sparse Coding","summary":"  Motivated by time series forecasting, we study Online Linear Optimization\n(OLO) under the coupling of two problem structures: the domain is unbounded,\nand the performance of an algorithm is measured by its dynamic regret. Handling\neither of them requires the regret bound to depend on certain complexity\nmeasure of the comparator sequence -- specifically, the comparator norm in\nunconstrained OLO, and the path length in dynamic regret. In contrast to a\nrecent work (Jacobsen & Cutkosky, 2022) that adapts to the combination of these\ntwo complexity measures, we propose an alternative complexity measure by\nrecasting the problem into sparse coding. Adaptivity can be achieved by a\nsimple modular framework, which naturally exploits more intricate prior\nknowledge of the environment. Along the way, we also present a new gradient\nadaptive algorithm for static unconstrained OLO, designed using novel\ncontinuous time machinery. This could be of independent interest.\n","authors":["Zhiyu Zhang","Ashok Cutkosky","Ioannis Ch. Paschalidis"],"pdf_url":"https://arxiv.org/pdf/2301.13349v2.pdf","comment":"Added experiments"},{"id":"http://arxiv.org/abs/2208.00277v4","updated":"2023-03-21T20:05:37Z","published":"2022-07-30T17:14:14Z","title":"MobileNeRF: Exploiting the Polygon Rasterization Pipeline for Efficient\n  Neural Field Rendering on Mobile Architectures","summary":"  Neural Radiance Fields (NeRFs) have demonstrated amazing ability to\nsynthesize images of 3D scenes from novel views. However, they rely upon\nspecialized volumetric rendering algorithms based on ray marching that are\nmismatched to the capabilities of widely deployed graphics hardware. This paper\nintroduces a new NeRF representation based on textured polygons that can\nsynthesize novel images efficiently with standard rendering pipelines. The NeRF\nis represented as a set of polygons with textures representing binary opacities\nand feature vectors. Traditional rendering of the polygons with a z-buffer\nyields an image with features at every pixel, which are interpreted by a small,\nview-dependent MLP running in a fragment shader to produce a final pixel color.\nThis approach enables NeRFs to be rendered with the traditional polygon\nrasterization pipeline, which provides massive pixel-level parallelism,\nachieving interactive frame rates on a wide range of compute platforms,\nincluding mobile phones.\n","authors":["Zhiqin Chen","Thomas Funkhouser","Peter Hedman","Andrea Tagliasacchi"],"pdf_url":"https://arxiv.org/pdf/2208.00277v4.pdf","comment":"CVPR 2023. Project page: https://mobile-nerf.github.io, code:\n  https://github.com/google-research/jax3d/tree/main/jax3d/projects/mobilenerf"},{"id":"http://arxiv.org/abs/2211.15387v2","updated":"2023-03-21T19:47:52Z","published":"2022-11-24T10:02:38Z","title":"AIREPAIR: A Repair Platform for Neural Networks","summary":"  We present AIREPAIR, a platform for repairing neural networks. It features\nthe integration of existing network repair tools. Based on AIREPAIR, one can\nrun different repair methods on the same model, thus enabling the fair\ncomparison of different repair techniques. We evaluate AIREPAIR with three\nstate-of-the-art repair tools on popular deep-learning datasets and models. Our\nevaluation confirms the utility of AIREPAIR, by comparing and analyzing the\nresults from different repair techniques. A demonstration is available at\nhttps://youtu.be/UkKw5neeWhw.\n","authors":["Xidan Song","Youcheng Sun","Mustafa A. Mustafa","Lucas Cordeiro"],"pdf_url":"https://arxiv.org/pdf/2211.15387v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12164v1","updated":"2023-03-21T19:45:59Z","published":"2023-03-21T19:45:59Z","title":"Viscoelastic Constitutive Artificial Neural Networks (vCANNs) $-$ a\n  framework for data-driven anisotropic nonlinear finite viscoelasticity","summary":"  The constitutive behavior of polymeric materials is often modeled by finite\nlinear viscoelastic (FLV) or quasi-linear viscoelastic (QLV) models. These\npopular models are simplifications that typically cannot accurately capture the\nnonlinear viscoelastic behavior of materials. For example, the success of\nattempts to capture strain rate-dependent behavior has been limited so far. To\novercome this problem, we introduce viscoelastic Constitutive Artificial Neural\nNetworks (vCANNs), a novel physics-informed machine learning framework for\nanisotropic nonlinear viscoelasticity at finite strains. vCANNs rely on the\nconcept of generalized Maxwell models enhanced with nonlinear strain\n(rate)-dependent properties represented by neural networks. The flexibility of\nvCANNs enables them to automatically identify accurate and sparse constitutive\nmodels of a broad range of materials. To test vCANNs, we trained them on\nstress-strain data from Polyvinyl Butyral, the electro-active polymers VHB 4910\nand 4905, and a biological tissue, the rectus abdominis muscle. Different\nloading conditions were considered, including relaxation tests, cyclic\ntension-compression tests, and blast loads. We demonstrate that vCANNs can\nlearn to capture the behavior of all these materials accurately and\ncomputationally efficiently without human guidance.\n","authors":["Kian P. Abdolazizi","Kevin Linka","Christian J. Cyron"],"pdf_url":"https://arxiv.org/pdf/2303.12164v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12157v1","updated":"2023-03-21T19:34:20Z","published":"2023-03-21T19:34:20Z","title":"Learning a Depth Covariance Function","summary":"  We propose learning a depth covariance function with applications to\ngeometric vision tasks. Given RGB images as input, the covariance function can\nbe flexibly used to define priors over depth functions, predictive\ndistributions given observations, and methods for active point selection. We\nleverage these techniques for a selection of downstream tasks: depth\ncompletion, bundle adjustment, and monocular dense visual odometry.\n","authors":["Eric Dexheimer","Andrew J. Davison"],"pdf_url":"https://arxiv.org/pdf/2303.12157v1.pdf","comment":"CVPR 2023. Project page: https://edexheim.github.io/depth_cov/"},{"id":"http://arxiv.org/abs/2210.12239v3","updated":"2023-03-21T19:33:46Z","published":"2022-10-21T20:41:44Z","title":"Auto-Encoder Neural Network Incorporating X-Ray Fluorescence Fundamental\n  Parameters with Machine Learning","summary":"  We consider energy-dispersive X-ray Fluorescence (EDXRF) applications where\nthe fundamental parameters method is impractical such as when instrument\nparameters are unavailable. For example, on a mining shovel or conveyor belt,\nrocks are constantly moving (leading to varying angles of incidence and\ndistances) and there may be other factors not accounted for (like dust). Neural\nnetworks do not require instrument and fundamental parameters but training\nneural networks requires XRF spectra labelled with elemental composition, which\nis often limited because of its expense. We develop a neural network model that\nlearns from limited labelled data and also benefits from domain knowledge by\nlearning to invert a forward model. The forward model uses transition energies\nand probabilities of all elements and parameterized distributions to\napproximate other fundamental and instrument parameters. We evaluate the model\nand baseline models on a rock dataset from a lithium mineral exploration\nproject. Our model works particularly well for some low-Z elements (Li, Mg, Al,\nand K) as well as some high-Z elements (Sn and Pb) despite these elements being\noutside the suitable range for common spectrometers to directly measure, likely\nowing to the ability of neural networks to learn correlations and non-linear\nrelationships.\n","authors":["Matthew Dirks","David Poole"],"pdf_url":"https://arxiv.org/pdf/2210.12239v3.pdf","comment":"X-Ray Spectrometry 2023"},{"id":"http://arxiv.org/abs/2302.05185v3","updated":"2023-03-21T19:25:54Z","published":"2023-02-10T11:30:19Z","title":"On Penalty-based Bilevel Gradient Descent Method","summary":"  Bilevel optimization enjoys a wide range of applications in hyper-parameter\noptimization, meta-learning and reinforcement learning. However, bilevel\noptimization problems are difficult to solve. Recent progress on scalable\nbilevel algorithms mainly focuses on bilevel optimization problems where the\nlower-level objective is either strongly convex or unconstrained. In this work,\nwe tackle the bilevel problem through the lens of the penalty method. We show\nthat under certain conditions, the penalty reformulation recovers the solutions\nof the original bilevel problem. Further, we propose the penalty-based bilevel\ngradient descent (PBGD) algorithm and establish its finite-time convergence for\nthe constrained bilevel problem without lower-level strong convexity.\nExperiments showcase the efficiency of the proposed PBGD algorithm.\n","authors":["Han Shen","Quan Xiao","Tianyi Chen"],"pdf_url":"https://arxiv.org/pdf/2302.05185v3.pdf","comment":"Improved Section 4 by removing a critical assumption; Added citations"},{"id":"http://arxiv.org/abs/2303.12148v1","updated":"2023-03-21T19:10:21Z","published":"2023-03-21T19:10:21Z","title":"Neural Pre-Processing: A Learning Framework for End-to-end Brain MRI\n  Pre-processing","summary":"  Head MRI pre-processing involves converting raw images to an\nintensity-normalized, skull-stripped brain in a standard coordinate space. In\nthis paper, we propose an end-to-end weakly supervised learning approach,\ncalled Neural Pre-processing (NPP), for solving all three sub-tasks\nsimultaneously via a neural network, trained on a large dataset without\nindividual sub-task supervision. Because the overall objective is highly\nunder-constrained, we explicitly disentangle geometric-preserving intensity\nmapping (skull-stripping and intensity normalization) and spatial\ntransformation (spatial normalization). Quantitative results show that our\nmodel outperforms state-of-the-art methods which tackle only a single sub-task.\nOur ablation experiments demonstrate the importance of the architecture design\nwe chose for NPP. Furthermore, NPP affords the user the flexibility to control\neach of these tasks at inference time. The code and model are freely-available\nat \\url{https://github.com/Novestars/Neural-Pre-processing}.\n","authors":["Xinzi He","Alan Wang","Mert R. Sabuncu"],"pdf_url":"https://arxiv.org/pdf/2303.12148v1.pdf","comment":"8"},{"id":"http://arxiv.org/abs/2303.12147v1","updated":"2023-03-21T19:10:09Z","published":"2023-03-21T19:10:09Z","title":"Universal Approximation Property of Hamiltonian Deep Neural Networks","summary":"  This paper investigates the universal approximation capabilities of\nHamiltonian Deep Neural Networks (HDNNs) that arise from the discretization of\nHamiltonian Neural Ordinary Differential Equations. Recently, it has been shown\nthat HDNNs enjoy, by design, non-vanishing gradients, which provide numerical\nstability during training. However, although HDNNs have demonstrated\nstate-of-the-art performance in several applications, a comprehensive study to\nquantify their expressivity is missing. In this regard, we provide a universal\napproximation theorem for HDNNs and prove that a portion of the flow of HDNNs\ncan approximate arbitrary well any continuous function over a compact domain.\nThis result provides a solid theoretical foundation for the practical use of\nHDNNs.\n","authors":["Muhammad Zakwan","Massimiliano d'Angelo","Giancarlo Ferrari-Trecate"],"pdf_url":"https://arxiv.org/pdf/2303.12147v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12136v1","updated":"2023-03-21T18:51:17Z","published":"2023-03-21T18:51:17Z","title":"Improving Fabrication Fidelity of Integrated Nanophotonic Devices Using\n  Deep Learning","summary":"  Next-generation integrated nanophotonic device designs leverage advanced\noptimization techniques such as inverse design and topology optimization which\nachieve high performance and extreme miniaturization by optimizing a massively\ncomplex design space enabled by small feature sizes. However, unless the\noptimization is heavily constrained, the generated small features are not\nreliably fabricated, leading to optical performance degradation. Even for\nsimpler, conventional designs, fabrication-induced performance degradation\nstill occurs. The degree of deviation from the original design not only depends\non the size and shape of its features, but also on the distribution of features\nand the surrounding environment, presenting complex, proximity-dependent\nbehavior. Without proprietary fabrication process specifications, design\ncorrections can only be made after calibrating fabrication runs take place. In\nthis work, we introduce a general deep machine learning model that\nautomatically corrects photonic device design layouts prior to first\nfabrication. Only a small set of scanning electron microscopy images of\nengineered training features are required to create the deep learning model.\nWith correction, the outcome of the fabricated layout is closer to what is\nintended, and thus so too is the performance of the design. Without modifying\nthe nanofabrication process, adding significant computation in design, or\nrequiring proprietary process specifications, we believe our model opens the\ndoor to new levels of reliability and performance in next-generation photonic\ncircuits.\n","authors":["Dusan Gostimirovic","Yuri Grinberg","Dan-Xia Xu","Odile Liboiron-Ladouceur"],"pdf_url":"https://arxiv.org/pdf/2303.12136v1.pdf","comment":"13 pages, 8 figures"},{"id":"http://arxiv.org/abs/2303.12132v1","updated":"2023-03-21T18:45:09Z","published":"2023-03-21T18:45:09Z","title":"Fundamentals of Generative Large Language Models and Perspectives in\n  Cyber-Defense","summary":"  Generative Language Models gained significant attention in late 2022 / early\n2023, notably with the introduction of models refined to act consistently with\nusers' expectations of interactions with AI (conversational models). Arguably\nthe focal point of public attention has been such a refinement of the GPT3\nmodel -- the ChatGPT and its subsequent integration with auxiliary\ncapabilities, including search as part of Microsoft Bing. Despite extensive\nprior research invested in their development, their performance and\napplicability to a range of daily tasks remained unclear and niche. However,\ntheir wider utilization without a requirement for technical expertise, made in\nlarge part possible through conversational fine-tuning, revealed the extent of\ntheir true capabilities in a real-world environment. This has garnered both\npublic excitement for their potential applications and concerns about their\ncapabilities and potential malicious uses. This review aims to provide a brief\noverview of the history, state of the art, and implications of Generative\nLanguage Models in terms of their principles, abilities, limitations, and\nfuture prospects -- especially in the context of cyber-defense, with a focus on\nthe Swiss operational environment.\n","authors":["Andrei Kucharavy","Zachary Schillaci","Loïc Maréchal","Maxime Würsch","Ljiljana Dolamic","Remi Sabonnadiere","Dimitri Percia David","Alain Mermoud","Vincent Lenders"],"pdf_url":"https://arxiv.org/pdf/2303.12132v1.pdf","comment":"41 pages (without references), 13 figures; public report of\n  Cyber-Defence Campus"},{"id":"http://arxiv.org/abs/2302.11953v2","updated":"2023-03-21T18:38:10Z","published":"2023-02-23T12:02:49Z","title":"MFBE: Leveraging Multi-Field Information of FAQs for Efficient Dense\n  Retrieval","summary":"  In the domain of question-answering in NLP, the retrieval of Frequently Asked\nQuestions (FAQ) is an important sub-area which is well researched and has been\nworked upon for many languages. Here, in response to a user query, a retrieval\nsystem typically returns the relevant FAQs from a knowledge-base. The efficacy\nof such a system depends on its ability to establish semantic match between the\nquery and the FAQs in real-time. The task becomes challenging due to the\ninherent lexical gap between queries and FAQs, lack of sufficient context in\nFAQ titles, scarcity of labeled data and high retrieval latency. In this work,\nwe propose a bi-encoder-based query-FAQ matching model that leverages multiple\ncombinations of FAQ fields (like, question, answer, and category) both during\nmodel training and inference. Our proposed Multi-Field Bi-Encoder (MFBE) model\nbenefits from the additional context resulting from multiple FAQ fields and\nperforms well even with minimal labeled data. We empirically support this claim\nthrough experiments on proprietary as well as open-source public datasets in\nboth unsupervised and supervised settings. Our model achieves around 27% and\n20% better top-1 accuracy for the FAQ retrieval task on internal and open\ndatasets, respectively over the best performing baseline.\n","authors":["Debopriyo Banerjee","Mausam Jain","Ashish Kulkarni"],"pdf_url":"https://arxiv.org/pdf/2302.11953v2.pdf","comment":"The first two authors contributed equally to this work. 12 pages, 3\n  figures, 5 tables. Accepted at the 2023 Pacific-Asia Conference On Knowledge\n  Discovery And Data Mining (PAKDD)"},{"id":"http://arxiv.org/abs/2206.14057v3","updated":"2023-03-21T18:26:14Z","published":"2022-06-28T15:00:45Z","title":"Safe Exploration Incurs Nearly No Additional Sample Complexity for\n  Reward-free RL","summary":"  Reward-free reinforcement learning (RF-RL), a recently introduced RL\nparadigm, relies on random action-taking to explore the unknown environment\nwithout any reward feedback information. While the primary goal of the\nexploration phase in RF-RL is to reduce the uncertainty in the estimated model\nwith minimum number of trajectories, in practice, the agent often needs to\nabide by certain safety constraint at the same time. It remains unclear how\nsuch safe exploration requirement would affect the corresponding sample\ncomplexity in order to achieve the desired optimality of the obtained policy in\nplanning. In this work, we make a first attempt to answer this question. In\nparticular, we consider the scenario where a safe baseline policy is known\nbeforehand, and propose a unified Safe reWard-frEe ExploraTion (SWEET)\nframework. We then particularize the SWEET framework to the tabular and the\nlow-rank MDP settings, and develop algorithms coined Tabular-SWEET and\nLow-rank-SWEET, respectively. Both algorithms leverage the concavity and\ncontinuity of the newly introduced truncated value functions, and are\nguaranteed to achieve zero constraint violation during exploration with high\nprobability. Furthermore, both algorithms can provably find a near-optimal\npolicy subject to any constraint in the planning phase. Remarkably, the sample\ncomplexities under both algorithms match or even outperform the state of the\nart in their constraint-free counterparts up to some constant factors, proving\nthat safety constraint hardly increases the sample complexity for RF-RL.\n","authors":["Ruiquan Huang","Jing Yang","Yingbin Liang"],"pdf_url":"https://arxiv.org/pdf/2206.14057v3.pdf","comment":"Accepted by ICLR 2023"},{"id":"http://arxiv.org/abs/2107.02266v3","updated":"2023-03-21T18:18:30Z","published":"2021-07-05T21:05:11Z","title":"Near-optimal inference in adaptive linear regression","summary":"  When data is collected in an adaptive manner, even simple methods like\nordinary least squares can exhibit non-normal asymptotic behavior. As an\nundesirable consequence, hypothesis tests and confidence intervals based on\nasymptotic normality can lead to erroneous results. We propose a family of\nonline debiasing estimators to correct these distributional anomalies in least\nsquares estimation. Our proposed methods take advantage of the covariance\nstructure present in the dataset and provide sharper estimates in directions\nfor which more information has accrued. We establish an asymptotic normality\nproperty for our proposed online debiasing estimators under mild conditions on\nthe data collection process and provide asymptotically exact confidence\nintervals. We additionally prove a minimax lower bound for the adaptive linear\nregression problem, thereby providing a baseline by which to compare\nestimators. There are various conditions under which our proposed estimators\nachieve the minimax lower bound. We demonstrate the usefulness of our theory\nvia applications to multi-armed bandit, autoregressive time series estimation,\nand active learning with exploration.\n","authors":["Koulik Khamaru","Yash Deshpande","Tor Lattimore","Lester Mackey","Martin J. Wainwright"],"pdf_url":"https://arxiv.org/pdf/2107.02266v3.pdf","comment":"51 pages, 7 figures"},{"id":"http://arxiv.org/abs/2303.12513v1","updated":"2023-03-21T17:30:40Z","published":"2023-03-21T17:30:40Z","title":"Is BERT Blind? Exploring the Effect of Vision-and-Language Pretraining\n  on Visual Language Understanding","summary":"  Most humans use visual imagination to understand and reason about language,\nbut models such as BERT reason about language using knowledge acquired during\ntext-only pretraining. In this work, we investigate whether vision-and-language\npretraining can improve performance on text-only tasks that involve implicit\nvisual reasoning, focusing primarily on zero-shot probing methods. We propose a\nsuite of visual language understanding (VLU) tasks for probing the visual\nreasoning abilities of text encoder models, as well as various non-visual\nnatural language understanding (NLU) tasks for comparison. We also contribute a\nnovel zero-shot knowledge probing method, Stroop probing, for applying models\nsuch as CLIP to text-only tasks without needing a prediction head such as the\nmasked language modelling head of models like BERT. We show that SOTA\nmultimodally trained text encoders outperform unimodally trained text encoders\non the VLU tasks while being underperformed by them on the NLU tasks, lending\nnew context to previously mixed results regarding the NLU capabilities of\nmultimodal models. We conclude that exposure to images during pretraining\naffords inherent visual reasoning knowledge that is reflected in language-only\ntasks that require implicit visual reasoning. Our findings bear importance in\nthe broader context of multimodal learning, providing principled guidelines for\nthe choice of text encoders used in such contexts.\n","authors":["Morris Alper","Michael Fiman","Hadar Averbuch-Elor"],"pdf_url":"https://arxiv.org/pdf/2303.12513v1.pdf","comment":"To be presented in CVPR 2023. Project webpage:\n  https://isbertblind.github.io/"},{"id":"http://arxiv.org/abs/2303.12097v1","updated":"2023-03-21T15:57:46Z","published":"2023-03-21T15:57:46Z","title":"CLSA: Contrastive Learning-based Survival Analysis for Popularity\n  Prediction in MEC Networks","summary":"  Mobile Edge Caching (MEC) integrated with Deep Neural Networks (DNNs) is an\ninnovative technology with significant potential for the future generation of\nwireless networks, resulting in a considerable reduction in users' latency. The\nMEC network's effectiveness, however, heavily relies on its capacity to predict\nand dynamically update the storage of caching nodes with the most popular\ncontents. To be effective, a DNN-based popularity prediction model needs to\nhave the ability to understand the historical request patterns of content,\nincluding their temporal and spatial correlations. Existing state-of-the-art\ntime-series DNN models capture the latter by simultaneously inputting the\nsequential request patterns of multiple contents to the network, considerably\nincreasing the size of the input sample. This motivates us to address this\nchallenge by proposing a DNN-based popularity prediction framework based on the\nidea of contrasting input samples against each other, designed for the Unmanned\nAerial Vehicle (UAV)-aided MEC networks. Referred to as the Contrastive\nLearning-based Survival Analysis (CLSA), the proposed architecture consists of\na self-supervised Contrastive Learning (CL) model, where the temporal\ninformation of sequential requests is learned using a Long Short Term Memory\n(LSTM) network as the encoder of the CL architecture. Followed by a Survival\nAnalysis (SA) network, the output of the proposed CLSA architecture is\nprobabilities for each content's future popularity, which are then sorted in\ndescending order to identify the Top-K popular contents. Based on the\nsimulation results, the proposed CLSA architecture outperforms its counterparts\nacross the classification accuracy and cache-hit ratio.\n","authors":["Zohreh Hajiakhondi-Meybodi","Arash Mohammadi","Jamshid Abouei","Konstantinos N. Plataniotis"],"pdf_url":"https://arxiv.org/pdf/2303.12097v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2303.11879v1","updated":"2023-03-21T14:23:46Z","published":"2023-03-21T14:23:46Z","title":"Multimodal Pre-training Framework for Sequential Recommendation via\n  Contrastive Learning","summary":"  Sequential recommendation systems utilize the sequential interactions of\nusers with items as their main supervision signals in learning users'\npreferences. However, existing methods usually generate unsatisfactory results\ndue to the sparsity of user behavior data. To address this issue, we propose a\nnovel pre-training framework, named Multimodal Sequence Mixup for Sequential\nRecommendation (MSM4SR), which leverages both users' sequential behaviors and\nitems' multimodal content (\\ie text and images) for effectively recommendation.\nSpecifically, MSM4SR tokenizes each item image into multiple textual keywords\nand uses the pre-trained BERT model to obtain initial textual and visual\nfeatures of items, for eliminating the discrepancy between the text and image\nmodalities. A novel backbone network, \\ie Multimodal Mixup Sequence Encoder\n(M$^2$SE), is proposed to bridge the gap between the item multimodal content\nand the user behavior, using a complementary sequence mixup strategy. In\naddition, two contrastive learning tasks are developed to assist M$^2$SE in\nlearning generalized multimodal representations of the user behavior sequence.\nExtensive experiments on real-world datasets demonstrate that MSM4SR\noutperforms state-of-the-art recommendation methods. Moreover, we further\nverify the effectiveness of MSM4SR on other challenging tasks including\ncold-start and cross-domain recommendation.\n","authors":["Lingzi Zhang","Xin Zhou","Zhiqi Shen"],"pdf_url":"https://arxiv.org/pdf/2303.11879v1.pdf","comment":"10 pages, 4 figures"},{"id":"http://arxiv.org/abs/2302.08722v3","updated":"2023-03-21T12:59:20Z","published":"2023-02-17T06:33:06Z","title":"GPT4MIA: Utilizing Generative Pre-trained Transformer (GPT-3) as A\n  Plug-and-Play Transductive Model for Medical Image Analysis","summary":"  In this paper, we propose a novel approach (called GPT4MIA) that utilizes\nGenerative Pre-trained Transformer (GPT) as a plug-and-play transductive\ninference tool for medical image analysis (MIA). We provide theoretical\nanalysis on why a large pre-trained language model such as GPT-3 can be used as\na plug-and-play transductive inference model for MIA. At the methodological\nlevel, we develop several technical treatments to improve the efficiency and\neffectiveness of GPT4MIA, including better prompt structure design, sample\nselection, and prompt ordering of representative samples/features. We present\ntwo concrete use cases (with workflow) of GPT4MIA: (1) detecting prediction\nerrors and (2) improving prediction accuracy, working in conjecture with\nwell-established vision-based models for image classification (e.g., ResNet).\nExperiments validate that our proposed method is effective for these two tasks.\nWe further discuss the opportunities and challenges in utilizing\nTransformer-based large language models for broader MIA applications.\n","authors":["Yizhe Zhang","Danny Z. Chen"],"pdf_url":"https://arxiv.org/pdf/2302.08722v3.pdf","comment":"Version 3: Added appendix with more results and visualizations.\n  Questions and suggestions are welcome"},{"id":"http://arxiv.org/abs/2303.11717v1","updated":"2023-03-21T10:09:47Z","published":"2023-03-21T10:09:47Z","title":"A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to\n  GPT-5 All You Need?","summary":"  As ChatGPT goes viral, generative AI (AIGC, a.k.a AI-generated content) has\nmade headlines everywhere because of its ability to analyze and create text,\nimages, and beyond. With such overwhelming media coverage, it is almost\nimpossible for us to miss the opportunity to glimpse AIGC from a certain angle.\nIn the era of AI transitioning from pure analysis to creation, it is worth\nnoting that ChatGPT, with its most recent language model GPT-4, is just a tool\nout of numerous AIGC tasks. Impressed by the capability of the ChatGPT, many\npeople are wondering about its limits: can GPT-5 (or other future GPT variants)\nhelp ChatGPT unify all AIGC tasks for diversified content creation? Toward\nanswering this question, a comprehensive review of existing AIGC tasks is\nneeded. As such, our work comes to fill this gap promptly by offering a first\nlook at AIGC, ranging from its techniques to applications. Modern generative AI\nrelies on various technical foundations, ranging from model architecture and\nself-supervised pretraining to generative modeling methods (like GAN and\ndiffusion models). After introducing the fundamental techniques, this work\nfocuses on the technological development of various AIGC tasks based on their\noutput type, including text, images, videos, 3D content, etc., which depicts\nthe full potential of ChatGPT's future. Moreover, we summarize their\nsignificant applications in some mainstream industries, such as education and\ncreativity content. Finally, we discuss the challenges currently faced and\npresent an outlook on how generative AI might evolve in the near future.\n","authors":["Chaoning Zhang","Chenshuang Zhang","Sheng Zheng","Yu Qiao","Chenghao Li","Mengchun Zhang","Sumit Kumar Dam","Chu Myaet Thwal","Ye Lin Tun","Le Luang Huy","Donguk kim","Sung-Ho Bae","Lik-Hang Lee","Yang Yang","Heng Tao Shen","In So Kweon","Choong Seon Hong"],"pdf_url":"https://arxiv.org/pdf/2303.11717v1.pdf","comment":"56 pages, 548 citations"},{"id":"http://arxiv.org/abs/2303.11599v1","updated":"2023-03-21T05:34:04Z","published":"2023-03-21T05:34:04Z","title":"Low-complexity Deep Video Compression with A Distributed Coding\n  Architecture","summary":"  Prevalent predictive coding-based video compression methods rely on a heavy\nencoder to reduce the temporal redundancy, which makes it challenging to deploy\nthem on resource-constrained devices. Meanwhile, as early as the 1970s,\ndistributed source coding theory has indicated that independent encoding and\njoint decoding with side information (SI) can achieve high-efficient\ncompression of correlated sources. This has inspired a distributed coding\narchitecture aiming at reducing the encoding complexity. However, traditional\ndistributed coding methods suffer from a substantial performance gap to\npredictive coding ones. Inspired by the great success of learning-based\ncompression, we propose the first end-to-end distributed deep video compression\nframework to improve the rate-distortion performance. A key ingredient is an\neffective SI generation module at the decoder, which helps to effectively\nexploit inter-frame correlations without computation-intensive encoder-side\nmotion estimation and compensation. Experiments show that our method\nsignificantly outperforms conventional distributed video coding and H.264.\nMeanwhile, it enjoys 6-7x encoding speedup against DVC [1] with comparable\ncompression performance. Code is released at\nhttps://github.com/Xinjie-Q/Distributed-DVC.\n","authors":["Xinjie Zhang","Jiawei Shao","Jun Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.11599v1.pdf","comment":"Accepted by ICME 2023"},{"id":"http://arxiv.org/abs/2303.11591v1","updated":"2023-03-21T04:42:39Z","published":"2023-03-21T04:42:39Z","title":"SVCNet: Scribble-based Video Colorization Network with Temporal\n  Aggregation","summary":"  In this paper, we propose a scribble-based video colorization network with\ntemporal aggregation called SVCNet. It can colorize monochrome videos based on\ndifferent user-given color scribbles. It addresses three common issues in the\nscribble-based video colorization area: colorization vividness, temporal\nconsistency, and color bleeding. To improve the colorization quality and\nstrengthen the temporal consistency, we adopt two sequential sub-networks in\nSVCNet for precise colorization and temporal smoothing, respectively. The first\nstage includes a pyramid feature encoder to incorporate color scribbles with a\ngrayscale frame, and a semantic feature encoder to extract semantics. The\nsecond stage finetunes the output from the first stage by aggregating the\ninformation of neighboring colorized frames (as short-range connections) and\nthe first colorized frame (as a long-range connection). To alleviate the color\nbleeding artifacts, we learn video colorization and segmentation\nsimultaneously. Furthermore, we set the majority of operations on a fixed small\nimage resolution and use a Super-resolution Module at the tail of SVCNet to\nrecover original sizes. It allows the SVCNet to fit different image resolutions\nat the inference. Finally, we evaluate the proposed SVCNet on DAVIS and Videvo\nbenchmarks. The experimental results demonstrate that SVCNet produces both\nhigher-quality and more temporally consistent videos than other well-known\nvideo colorization approaches. The codes and models can be found at\nhttps://github.com/zhaoyuzhi/SVCNet.\n","authors":["Yuzhi Zhao","Lai-Man Po","Kangcheng Liu","Xuehui Wang","Wing-Yin Yu","Pengfei Xian","Yujia Zhang","Mengyang Liu"],"pdf_url":"https://arxiv.org/pdf/2303.11591v1.pdf","comment":"under revision of IEEE Transactions on Image Processing"},{"id":"http://arxiv.org/abs/2303.12112v1","updated":"2023-03-21T18:03:14Z","published":"2023-03-21T18:03:14Z","title":"Positive-Augmented Constrastive Learning for Image and Video Captioning\n  Evaluation","summary":"  The CLIP model has been recently proven to be very effective for a variety of\ncross-modal tasks, including the evaluation of captions generated from\nvision-and-language architectures. In this paper, we propose a new recipe for a\ncontrastive-based evaluation metric for image captioning, namely\nPositive-Augmented Contrastive learning Score (PAC-S), that in a novel way\nunifies the learning of a contrastive visual-semantic space with the addition\nof generated images and text on curated data. Experiments spanning several\ndatasets demonstrate that our new metric achieves the highest correlation with\nhuman judgments on both images and videos, outperforming existing\nreference-based metrics like CIDEr and SPICE and reference-free metrics like\nCLIP-Score. Finally, we test the system-level correlation of the proposed\nmetric when considering popular image captioning approaches, and assess the\nimpact of employing different cross-modal features. Our source code and trained\nmodels are publicly available at: https://github.com/aimagelab/pacscore.\n","authors":["Sara Sarto","Manuele Barraco","Marcella Cornia","Lorenzo Baraldi","Rita Cucchiara"],"pdf_url":"https://arxiv.org/pdf/2303.12112v1.pdf","comment":"CVPR 2023 (highlight paper)"}]},"2023-03-22T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2303.12788v1","updated":"2023-03-22T17:57:47Z","published":"2023-03-22T17:57:47Z","title":"Open-source Frame Semantic Parsing","summary":"  While the state-of-the-art for frame semantic parsing has progressed\ndramatically in recent years, it is still difficult for end-users to apply\nstate-of-the-art models in practice. To address this, we present Frame Semantic\nTransformer, an open-source Python library which achieves near state-of-the-art\nperformance on FrameNet 1.7, while focusing on ease-of-use. We use a T5 model\nfine-tuned on Propbank and FrameNet exemplars as a base, and improve\nperformance by using FrameNet lexical units to provide hints to T5 at inference\ntime. We enhance robustness to real-world data by using textual data\naugmentations during training.\n","authors":["David Chanin"],"pdf_url":"https://arxiv.org/pdf/2303.12788v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11117v2","updated":"2023-03-22T17:48:31Z","published":"2023-03-20T13:58:35Z","title":"EmotionIC: Emotional Inertia and Contagion-driven Dependency Modelling\n  for Emotion Recognition in Conversation","summary":"  Emotion Recognition in Conversation (ERC) has attracted growing attention in\nrecent years as a result of the advancement and implementation of\nhuman-computer interface technologies. However, previous approaches to modeling\nglobal and local context dependencies lost the diversity of dependency\ninformation and do not take the context dependency into account at the\nclassification level. In this paper, we propose a novel approach to dependency\nmodeling driven by Emotional Inertia and Contagion (EmotionIC) for\nconversational emotion recognition at the feature extraction and classification\nlevels. At the feature extraction level, our designed Identity Masked\nMulti-head Attention (IM-MHA) captures the identity-based long-distant context\nin the dialogue to contain the diverse influence of different participants and\nconstruct the global emotional atmosphere, while the devised Dialogue-based\nGate Recurrent Unit (DialogGRU) that aggregates the emotional tendencies of\ndyadic dialogue is applied to refine the contextual features with inter- and\nintra-speaker dependencies. At the classification level, by introducing skip\nconnections in Conditional Random Field (CRF), we elaborate the Skip-chain CRF\n(SkipCRF) to capture the high-order dependencies within and between speakers,\nand to emulate the emotional flow of distant participants. Experimental results\nshow that our method can significantly outperform the state-of-the-art models\non four benchmark datasets. The ablation studies confirm that our modules can\neffectively model emotional inertia and contagion.\n","authors":["Yingjian Liu","Jiang Li","Xiaoping Wang","Zhigang Zeng"],"pdf_url":"https://arxiv.org/pdf/2303.11117v2.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2303.12057v2","updated":"2023-03-22T17:35:51Z","published":"2023-03-21T17:48:00Z","title":"Large Language Models Can Be Used to Estimate the Ideologies of\n  Politicians in a Zero-Shot Learning Setting","summary":"  The mass aggregation of knowledge embedded in large language models (LLMs)\nholds the promise of new solutions to problems of observability and measurement\nin the social sciences. We examine the utility of one such model for a\nparticularly difficult measurement task: measuring the latent ideology of\nlawmakers, which allows us to better understand functions that are core to\ndemocracy, such as how politics shape policy and how political actors represent\ntheir constituents. We scale the senators of the 116th United States Congress\nalong the liberal-conservative spectrum by prompting ChatGPT to select the more\nliberal (or conservative) senator in pairwise comparisons. We show that the LLM\nproduced stable answers across repeated iterations, did not hallucinate, and\nwas not simply regurgitating information from a single source. This new scale\nstrongly correlates with pre-existing liberal-conservative scales such as\nNOMINATE, but also differs in several important ways, such as correctly placing\nsenators who vote against their party for far-left or far-right ideological\nreasons on the extreme ends. The scale also highly correlates with ideological\nmeasures based on campaign giving and political activists' perceptions of these\nsenators. In addition to the potential for better-automated data collection and\ninformation retrieval, our results suggest LLMs are likely to open new avenues\nfor measuring latent constructs like ideology that rely on aggregating large\nquantities of data from public sources.\n","authors":["Patrick Y. Wu","Joshua A. Tucker","Jonathan Nagler","Solomon Messing"],"pdf_url":"https://arxiv.org/pdf/2303.12057v2.pdf","comment":"18 pages, 4 figures; V2: fixed graphical error on Figure 2"},{"id":"http://arxiv.org/abs/2303.12772v1","updated":"2023-03-22T17:35:35Z","published":"2023-03-22T17:35:35Z","title":"Interpretable Bangla Sarcasm Detection using BERT and Explainable AI","summary":"  A positive phrase or a sentence with an underlying negative motive is usually\ndefined as sarcasm that is widely used in today's social media platforms such\nas Facebook, Twitter, Reddit, etc. In recent times active users in social media\nplatforms are increasing dramatically which raises the need for an automated\nNLP-based system that can be utilized in various tasks such as determining\nmarket demand, sentiment analysis, threat detection, etc. However, since\nsarcasm usually implies the opposite meaning and its detection is frequently a\nchallenging issue, data meaning extraction through an NLP-based model becomes\nmore complicated. As a result, there has been a lot of study on sarcasm\ndetection in English over the past several years, and there's been a noticeable\nimprovement and yet sarcasm detection in the Bangla language's state remains\nthe same. In this article, we present a BERT-based system that can achieve\n99.60\\% while the utilized traditional machine learning algorithms are only\ncapable of achieving 89.93\\%. Additionally, we have employed Local\nInterpretable Model-Agnostic Explanations that introduce explainability to our\nsystem. Moreover, we have utilized a newly collected bangla sarcasm dataset,\nBanglaSarc that was constructed specifically for the evaluation of this study.\nThis dataset consists of fresh records of sarcastic and non-sarcastic comments,\nthe majority of which are acquired from Facebook and YouTube comment sections.\n","authors":["Ramisa Anan","Tasnim Sakib Apon","Zeba Tahsin Hossain","Elizabeth Antora Modhu","Sudipta Mondal","MD. Golam Rabiul Alam"],"pdf_url":"https://arxiv.org/pdf/2303.12772v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12767v1","updated":"2023-03-22T17:32:56Z","published":"2023-03-22T17:32:56Z","title":"Can we trust the evaluation on ChatGPT?","summary":"  ChatGPT, the first large language model (LLM) with mass adoption, has\ndemonstrated remarkable performance in numerous natural language tasks. Despite\nits evident usefulness, evaluating ChatGPT's performance in diverse problem\ndomains remains challenging due to the closed nature of the model and its\ncontinuous updates via Reinforcement Learning from Human Feedback (RLHF). We\nhighlight the issue of data contamination in ChatGPT evaluations, with a case\nstudy of the task of stance detection. We discuss the challenge of preventing\ndata contamination and ensuring fair model evaluation in the age of closed and\ncontinuously trained models.\n","authors":["Rachith Aiyappa","Jisun An","Haewoon Kwak","Yong-Yeol Ahn"],"pdf_url":"https://arxiv.org/pdf/2303.12767v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12712v1","updated":"2023-03-22T16:51:28Z","published":"2023-03-22T16:51:28Z","title":"Sparks of Artificial General Intelligence: Early experiments with GPT-4","summary":"  Artificial intelligence (AI) researchers have been developing and refining\nlarge language models (LLMs) that exhibit remarkable capabilities across a\nvariety of domains and tasks, challenging our understanding of learning and\ncognition. The latest model developed by OpenAI, GPT-4, was trained using an\nunprecedented scale of compute and data. In this paper, we report on our\ninvestigation of an early version of GPT-4, when it was still in active\ndevelopment by OpenAI. We contend that (this early version of) GPT-4 is part of\na new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that\nexhibit more general intelligence than previous AI models. We discuss the\nrising capabilities and implications of these models. We demonstrate that,\nbeyond its mastery of language, GPT-4 can solve novel and difficult tasks that\nspan mathematics, coding, vision, medicine, law, psychology and more, without\nneeding any special prompting. Moreover, in all of these tasks, GPT-4's\nperformance is strikingly close to human-level performance, and often vastly\nsurpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's\ncapabilities, we believe that it could reasonably be viewed as an early (yet\nstill incomplete) version of an artificial general intelligence (AGI) system.\nIn our exploration of GPT-4, we put special emphasis on discovering its\nlimitations, and we discuss the challenges ahead for advancing towards deeper\nand more comprehensive versions of AGI, including the possible need for\npursuing a new paradigm that moves beyond next-word prediction. We conclude\nwith reflections on societal influences of the recent technological leap and\nfuture research directions.\n","authors":["Sébastien Bubeck","Varun Chandrasekaran","Ronen Eldan","Johannes Gehrke","Eric Horvitz","Ece Kamar","Peter Lee","Yin Tat Lee","Yuanzhi Li","Scott Lundberg","Harsha Nori","Hamid Palangi","Marco Tulio Ribeiro","Yi Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.12712v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12671v1","updated":"2023-03-22T15:49:33Z","published":"2023-03-22T15:49:33Z","title":"Integrating Image Features with Convolutional Sequence-to-sequence\n  Network for Multilingual Visual Question Answering","summary":"  Visual Question Answering (VQA) is a task that requires computers to give\ncorrect answers for the input questions based on the images. This task can be\nsolved by humans with ease but is a challenge for computers. The\nVLSP2022-EVJVQA shared task carries the Visual Question Answering task in the\nmultilingual domain on a newly released dataset: UIT-EVJVQA, in which the\nquestions and answers are written in three different languages: English,\nVietnamese and Japanese. We approached the challenge as a sequence-to-sequence\nlearning task, in which we integrated hints from pre-trained state-of-the-art\nVQA models and image features with Convolutional Sequence-to-Sequence network\nto generate the desired answers. Our results obtained up to 0.3442 by F1 score\non the public test set, 0.4210 on the private test set, and placed 3rd in the\ncompetition.\n","authors":["Triet Minh Thai","Son T. Luu"],"pdf_url":"https://arxiv.org/pdf/2303.12671v1.pdf","comment":"VLSP2022-EVJVQA"},{"id":"http://arxiv.org/abs/2303.12665v1","updated":"2023-03-22T15:44:15Z","published":"2023-03-22T15:44:15Z","title":"Evaluating the Role of Target Arguments in Rumour Stance Classification","summary":"  Considering a conversation thread, stance classification aims to identify the\nopinion (e.g. agree or disagree) of replies towards a given target. The target\nof the stance is expected to be an essential component in this task, being one\nof the main factors that make it different from sentiment analysis. However, a\nrecent study shows that a target-oblivious model outperforms target-aware\nmodels, suggesting that targets are not useful when predicting stance. This\npaper re-examines this phenomenon for rumour stance classification (RSC) on\nsocial media, where a target is a rumour story implied by the source tweet in\nthe conversation. We propose adversarial attacks in the test data, aiming to\nassess the models robustness and evaluate the role of the data in the models\nperformance. Results show that state-of-the-art models, including approaches\nthat use the entire conversation thread, overly relying on superficial signals.\nOur hypothesis is that the naturally high occurrence of target-independent\ndirect replies in RSC (e.g. \"this is fake\" or just \"fake\") results in the\nimpressive performance of target-oblivious models, highlighting the risk of\ntarget instances being treated as noise during training.\n","authors":["Yue Li","Carolina Scarton"],"pdf_url":"https://arxiv.org/pdf/2303.12665v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.00363v3","updated":"2023-03-22T15:42:50Z","published":"2022-04-30T23:03:49Z","title":"Visual Spatial Reasoning","summary":"  Spatial relations are a basic part of human cognition. However, they are\nexpressed in natural language in a variety of ways, and previous work has\nsuggested that current vision-and-language models (VLMs) struggle to capture\nrelational information. In this paper, we present Visual Spatial Reasoning\n(VSR), a dataset containing more than 10k natural text-image pairs with 66\ntypes of spatial relations in English (such as: under, in front of, and\nfacing). While using a seemingly simple annotation format, we show how the\ndataset includes challenging linguistic phenomena, such as varying reference\nframes. We demonstrate a large gap between human and model performance: the\nhuman ceiling is above 95%, while state-of-the-art models only achieve around\n70%. We observe that VLMs' by-relation performances have little correlation\nwith the number of training examples and the tested models are in general\nincapable of recognising relations concerning the orientations of objects.\n","authors":["Fangyu Liu","Guy Emerson","Nigel Collier"],"pdf_url":"https://arxiv.org/pdf/2205.00363v3.pdf","comment":"TACL camera-ready version; code and data available at\n  https://github.com/cambridgeltl/visual-spatial-reasoning"},{"id":"http://arxiv.org/abs/2303.10659v2","updated":"2023-03-22T15:27:08Z","published":"2023-03-19T13:47:56Z","title":"COVID-19 event extraction from Twitter via extractive question answering\n  with continuous prompts","summary":"  As COVID-19 ravages the world, social media analytics could augment\ntraditional surveys in assessing how the pandemic evolves and capturing\nconsumer chatter that could help healthcare agencies in addressing it. This\ntypically involves mining disclosure events that mention testing positive for\nthe disease or discussions surrounding perceptions and beliefs in preventative\nor treatment options. The 2020 shared task on COVID-19 event extraction\n(conducted as part of the W-NUT workshop during the EMNLP conference)\nintroduced a new Twitter dataset for benchmarking event extraction from\nCOVID-19 tweets. In this paper, we cast the problem of event extraction as\nextractive question answering using recent advances in continuous prompting in\nlanguage models. On the shared task test dataset, our approach leads to over 5%\nabsolute micro-averaged F1-score improvement over prior best results, across\nall COVID-19 event slots. Our ablation study shows that continuous prompts have\na major impact on the eventual performance.\n","authors":["Yuhang Jiang","Ramakanth Kavuluru"],"pdf_url":"https://arxiv.org/pdf/2303.10659v2.pdf","comment":"Accepted to appear in MEDINFO 2023. Code:\n  https://github.com/bionlproc/twitter-covid-QA-extraction"},{"id":"http://arxiv.org/abs/2303.11331v2","updated":"2023-03-22T14:10:37Z","published":"2023-03-20T17:59:59Z","title":"EVA-02: A Visual Representation for Neon Genesis","summary":"  We launch EVA-02, a next-generation Transformer-based visual representation\npre-trained to reconstruct strong and robust language-aligned vision features\nvia masked image modeling. With an updated plain Transformer architecture as\nwell as extensive pre-training from an open & accessible giant CLIP vision\nencoder, EVA-02 demonstrates superior performance compared to prior\nstate-of-the-art approaches across various representative vision tasks, while\nutilizing significantly fewer parameters and compute budgets. Notably, using\nexclusively publicly accessible training data, EVA-02 with only 304M parameters\nachieves a phenomenal 90.0 fine-tuning top-1 accuracy on ImageNet-1K val set.\nAdditionally, our EVA-02-CLIP can reach up to 80.4 zero-shot top-1 on\nImageNet-1K, outperforming the previous largest & best open-sourced CLIP with\nonly ~1/6 parameters and ~1/6 image-text training data. We offer four EVA-02\nvariants in various model sizes, ranging from 6M to 304M parameters, all with\nimpressive performance. To facilitate open access and open research, we release\nthe complete suite of EVA-02 to the community at\nhttps://github.com/baaivision/EVA/tree/master/EVA-02.\n","authors":["Yuxin Fang","Quan Sun","Xinggang Wang","Tiejun Huang","Xinlong Wang","Yue Cao"],"pdf_url":"https://arxiv.org/pdf/2303.11331v2.pdf","comment":"v2: Fix some known issues & typos. v1: To Asuka. Code & Models:\n  https://github.com/baaivision/EVA/tree/master/EVA-02"},{"id":"http://arxiv.org/abs/2303.12582v1","updated":"2023-03-22T14:09:20Z","published":"2023-03-22T14:09:20Z","title":"AfroDigits: A Community-Driven Spoken Digit Dataset for African\n  Languages","summary":"  The advancement of speech technologies has been remarkable, yet its\nintegration with African languages remains limited due to the scarcity of\nAfrican speech corpora. To address this issue, we present AfroDigits, a\nminimalist, community-driven dataset of spoken digits for African languages,\ncurrently covering 38 African languages. As a demonstration of the practical\napplications of AfroDigits, we conduct audio digit classification experiments\non six African languages [Igbo (ibo), Yoruba (yor), Rundi (run), Oshiwambo\n(kua), Shona (sna), and Oromo (gax)] using the Wav2Vec2.0-Large and XLS-R\nmodels. Our experiments reveal a useful insight on the effect of mixing African\nspeech corpora during finetuning. AfroDigits is the first published audio digit\ndataset for African languages and we believe it will, among other things, pave\nthe way for Afro-centric speech applications such as the recognition of\ntelephone numbers, and street numbers. We release the dataset and platform\npublicly at https://huggingface.co/datasets/chrisjay/crowd-speech-africa and\nhttps://huggingface.co/spaces/chrisjay/afro-speech respectively.\n","authors":["Chris Chinenye Emezue","Sanchit Gandhi","Lewis Tunstall","Abubakar Abid","Joshua Meyer","Quentin Lhoest","Pete Allen","Patrick Von Platen","Douwe Kiela","Yacine Jernite","Julien Chaumond","Merve Noyan","Omar Sanseviero"],"pdf_url":"https://arxiv.org/pdf/2303.12582v1.pdf","comment":"Accepted to the AfricaNLP Workshop at ICLR 2023"},{"id":"http://arxiv.org/abs/2303.12570v1","updated":"2023-03-22T13:54:46Z","published":"2023-03-22T13:54:46Z","title":"RepoCoder: Repository-Level Code Completion Through Iterative Retrieval\n  and Generation","summary":"  The task of repository-level code completion is to continue writing the\nunfinished code based on a broader context of the repository. While for\nautomated code completion tools, it is difficult to utilize the useful\ninformation scattered in different files. We propose RepoCoder, a simple,\ngeneric, and effective framework to address the challenge. It streamlines the\nrepository-level code completion process by incorporating a similarity-based\nretriever and a pre-trained code language model, which allows for the effective\nutilization of repository-level information for code completion and grants the\nability to generate code at various levels of granularity. Furthermore,\nRepoCoder utilizes a novel iterative retrieval-generation paradigm that bridges\nthe gap between retrieval context and the intended completion target. We also\npropose a new benchmark RepoEval, which consists of the latest and high-quality\nreal-world repositories covering line, API invocation, and function body\ncompletion scenarios. We test the performance of RepoCoder by using various\ncombinations of code retrievers and generators. Experimental results indicate\nthat RepoCoder significantly improves the zero-shot code completion baseline by\nover 10% in all settings and consistently outperforms the vanilla\nretrieval-augmented code completion approach. Furthermore, we validate the\neffectiveness of RepoCoder through comprehensive analysis, providing valuable\ninsights for future research.\n","authors":["Fengji Zhang","Bei Chen","Yue Zhang","Jin Liu","Daoguang Zan","Yi Mao","Jian-Guang Lou","Weizhu Chen"],"pdf_url":"https://arxiv.org/pdf/2303.12570v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12528v1","updated":"2023-03-22T13:03:10Z","published":"2023-03-22T13:03:10Z","title":"MEGA: Multilingual Evaluation of Generative AI","summary":"  Generative AI models have impressive performance on many Natural Language\nProcessing tasks such as language understanding, reasoning and language\ngeneration. One of the most important questions that is being asked by the AI\ncommunity today is about the capabilities and limits of these models, and it is\nclear that evaluating generative AI is very challenging. Most studies on\ngenerative Large Language Models (LLMs) are restricted to English and it is\nunclear how capable these models are at understanding and generating other\nlanguages. We present the first comprehensive benchmarking of generative LLMs -\nMEGA, which evaluates models on standard NLP benchmarks, covering 8 diverse\ntasks and 33 typologically diverse languages. We also compare the performance\nof generative LLMs to State of the Art (SOTA) non-autoregressive models on\nthese tasks to determine how well generative models perform compared to the\nprevious generation of LLMs. We present a thorough analysis of the performance\nof models across languages and discuss some of the reasons why generative LLMs\nare currently not optimal for all languages. We create a framework for\nevaluating generative LLMs in the multilingual setting and provide directions\nfor future progress in the field.\n","authors":["Kabir Ahuja","Rishav Hada","Millicent Ochieng","Prachi Jain","Harshita Diddee","Samuel Maina","Tanuja Ganu","Sameer Segal","Maxamed Axmed","Kalika Bali","Sunayana Sitaram"],"pdf_url":"https://arxiv.org/pdf/2303.12528v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09461v2","updated":"2023-03-22T11:30:41Z","published":"2023-03-08T15:46:14Z","title":"ChatGPT Participates in a Computer Science Exam","summary":"  We asked ChatGPT to participate in an undergraduate computer science exam on\n''Algorithms and Data Structures''. The program was evaluated on the entire\nexam as posed to the students. We hand-copied its answers onto an exam sheet,\nwhich was subsequently graded in a blind setup alongside those of 200\nparticipating students. We find that ChatGPT narrowly passed the exam,\nobtaining 20.5 out of 40 points. This impressive performance indicates that\nChatGPT can indeed succeed in challenging tasks like university exams. At the\nsame time, the questions in our exam are structurally similar to those of other\nexams, solved homework problems, and teaching materials that can be found\nonline and might have been part of ChatGPT's training data. Therefore, it would\nbe inadequate to conclude from this experiment that ChatGPT has any\nunderstanding of computer science. We also assess the improvements brought by\nGPT-4. We find that GPT-4 would have obtained about 17\\% more exam points than\nGPT-3.5, reaching the performance of the average student. The transcripts of\nour conversations with ChatGPT are available at\n\\url{https://github.com/tml-tuebingen/chatgpt-algorithm-exam}, and the entire\ngraded exam is in the appendix of this paper.\n","authors":["Sebastian Bordt","Ulrike von Luxburg"],"pdf_url":"https://arxiv.org/pdf/2303.09461v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08518v2","updated":"2023-03-22T11:29:48Z","published":"2023-03-15T10:53:49Z","title":"UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation","summary":"  Large Language Models (LLMs) are popular for their impressive abilities, but\nthe need for model-specific fine-tuning or task-specific prompt engineering can\nhinder their generalization. We propose UPRISE (Universal Prompt Retrieval for\nImproving zero-Shot Evaluation), which tunes a lightweight and versatile\nretriever that automatically retrieves prompts for a given zero-shot task\ninput. Specifically, we demonstrate universality in a cross-task and\ncross-model scenario: the retriever is tuned on a diverse set of tasks, but\ntested on unseen task types; we use a small frozen LLM, GPT-Neo-2.7B, for\ntuning the retriever, but test the retriever on different LLMs of much larger\nscales, such as BLOOM-7.1B, OPT-66B and GPT3-175B. Additionally, we show that\nUPRISE mitigates the hallucination problem in our experiments with ChatGPT,\nsuggesting its potential to improve even the strongest LLMs. Our model and code\nare available at https://github.com/microsoft/LMOps.\n","authors":["Daixuan Cheng","Shaohan Huang","Junyu Bi","Yuefeng Zhan","Jianfeng Liu","Yujing Wang","Hao Sun","Furu Wei","Denvy Deng","Qi Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.08518v2.pdf","comment":"Update link to our model and code"},{"id":"http://arxiv.org/abs/2206.06807v2","updated":"2023-03-22T11:22:29Z","published":"2022-06-14T12:56:34Z","title":"The Causal Structure of Semantic Ambiguities","summary":"  Ambiguity is a natural language phenomenon occurring at different levels of\nsyntax, semantics, and pragmatics. It is widely studied; in Psycholinguistics,\nfor instance, we have a variety of competing studies for the human\ndisambiguation processes. These studies are empirical and based on eyetracking\nmeasurements. Here we take first steps towards formalizing these processes for\nsemantic ambiguities where we identified the presence of two features: (1)\njoint plausibility degrees of different possible interpretations, (2) causal\nstructures according to which certain words play a more substantial role in the\nprocesses. The novel sheaf-theoretic model of definite causality developed by\nGogioso and Pinzani in QPL 2021 offers tools to model and reason about these\nfeatures. We applied this theory to a dataset of ambiguous phrases extracted\nfrom Psycholinguistics literature and their human plausibility judgements\ncollected by us using the Amazon Mechanical Turk engine. We measured the causal\nfractions of different disambiguation orders within the phrases and discovered\ntwo prominent orders: from subject to verb in the subject-verb and from object\nto verb in the verb object phrases. We also found evidence for delay in the\ndisambiguation of polysemous vs homonymous verbs, again compatible with\nPsycholinguistic findings.\n","authors":["Daphne Wang","Mehrnoosh Sadrzadeh"],"pdf_url":"https://arxiv.org/pdf/2206.06807v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.00690v3","updated":"2023-03-22T07:20:37Z","published":"2022-08-01T08:58:02Z","title":"Generative Bias for Robust Visual Question Answering","summary":"  The task of Visual Question Answering (VQA) is known to be plagued by the\nissue of VQA models exploiting biases within the dataset to make its final\nprediction. Various previous ensemble based debiasing methods have been\nproposed where an additional model is purposefully trained to be biased in\norder to train a robust target model. However, these methods compute the bias\nfor a model simply from the label statistics of the training data or from\nsingle modal branches. In this work, in order to better learn the bias a target\nVQA model suffers from, we propose a generative method to train the bias model\ndirectly from the target model, called GenB. In particular, GenB employs a\ngenerative network to learn the bias in the target model through a combination\nof the adversarial objective and knowledge distillation. We then debias our\ntarget model with GenB as a bias model, and show through extensive experiments\nthe effects of our method on various VQA bias datasets including VQA-CP2,\nVQA-CP1, GQA-OOD, and VQA-CE, and show state-of-the-art results with the LXMERT\narchitecture on VQA-CP2.\n","authors":["Jae Won Cho","Dong-jin Kim","Hyeonggon Ryu","In So Kweon"],"pdf_url":"https://arxiv.org/pdf/2208.00690v3.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2211.12824v2","updated":"2023-03-22T07:20:06Z","published":"2022-11-23T10:14:12Z","title":"Tell Me What Happened: Unifying Text-guided Video Completion via\n  Multimodal Masked Video Generation","summary":"  Generating a video given the first several static frames is challenging as it\nanticipates reasonable future frames with temporal coherence. Besides video\nprediction, the ability to rewind from the last frame or infilling between the\nhead and tail is also crucial, but they have rarely been explored for video\ncompletion. Since there could be different outcomes from the hints of just a\nfew frames, a system that can follow natural language to perform video\ncompletion may significantly improve controllability. Inspired by this, we\nintroduce a novel task, text-guided video completion (TVC), which requests the\nmodel to generate a video from partial frames guided by an instruction. We then\npropose Multimodal Masked Video Generation (MMVG) to address this TVC task.\nDuring training, MMVG discretizes the video frames into visual tokens and masks\nmost of them to perform video completion from any time point. At inference\ntime, a single MMVG model can address all 3 cases of TVC, including video\nprediction, rewind, and infilling, by applying corresponding masking\nconditions. We evaluate MMVG in various video scenarios, including egocentric,\nanimation, and gaming. Extensive experimental results indicate that MMVG is\neffective in generating high-quality visual appearances with text guidance for\nTVC.\n","authors":["Tsu-Jui Fu","Licheng Yu","Ning Zhang","Cheng-Yang Fu","Jong-Chyi Su","William Yang Wang","Sean Bell"],"pdf_url":"https://arxiv.org/pdf/2211.12824v2.pdf","comment":"CVPR'23"},{"id":"http://arxiv.org/abs/2303.12320v1","updated":"2023-03-22T05:35:29Z","published":"2023-03-22T05:35:29Z","title":"GrapeQA: GRaph Augmentation and Pruning to Enhance Question-Answering","summary":"  Commonsense question-answering (QA) methods combine the power of pre-trained\nLanguage Models (LM) with the reasoning provided by Knowledge Graphs (KG). A\ntypical approach collects nodes relevant to the QA pair from a KG to form a\nWorking Graph (WG) followed by reasoning using Graph Neural Networks(GNNs).\nThis faces two major challenges: (i) it is difficult to capture all the\ninformation from the QA in the WG, and (ii) the WG contains some irrelevant\nnodes from the KG. To address these, we propose GrapeQA with two simple\nimprovements on the WG: (i) Prominent Entities for Graph Augmentation\nidentifies relevant text chunks from the QA pair and augments the WG with\ncorresponding latent representations from the LM, and (ii) Context-Aware Node\nPruning removes nodes that are less relevant to the QA pair. We evaluate our\nresults on OpenBookQA, CommonsenseQA and MedQA-USMLE and see that GrapeQA shows\nconsistent improvements over its LM + KG predecessor (QA-GNN in particular) and\nlarge improvements on OpenBookQA.\n","authors":["Dhaval Taunk","Lakshya Khanna","Pavan Kandru","Vasudeva Varma","Charu Sharma","Makarand Tapaswi"],"pdf_url":"https://arxiv.org/pdf/2303.12320v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12314v1","updated":"2023-03-22T05:04:21Z","published":"2023-03-22T05:04:21Z","title":"Self-supervised Meta-Prompt Learning with Meta-Gradient Regularization\n  for Few-shot Generalization","summary":"  Prompt tuning is a parameter-efficient method, which learns soft prompts and\nconditions frozen language models to perform specific downstream tasks. Though\neffective, prompt tuning under few-shot settings on the one hand heavily relies\non a good initialization of soft prompts. On the other hand, it can easily\nresult in overfitting. Existing works leverage pre-training or supervised\nmeta-learning to initialize soft prompts but they cannot data-efficiently\ngeneralize to unseen downstream tasks. To address the above problems, this\npaper proposes a novel Self-sUpervised meta-Prompt learning framework with\nmeta-gradient Regularization for few-shot generalization (SUPMER). We first\ndesign a set of self-supervised anchor meta-training tasks with different task\nformats and further enrich the task distribution with curriculum-based task\naugmentation. Then a novel meta-gradient regularization method is integrated\ninto meta-prompt learning. It meta-learns to transform the raw gradients during\nfew-shot learning into a domain-generalizable direction, thus alleviating the\nproblem of overfitting. Extensive experiments show that SUPMER achieves better\nperformance for different few-shot downstream tasks, and also exhibits a\nstronger domain generalization ability.\n","authors":["Kaihang Pan","Juncheng Li","Hongye Song","Jun Lin","Xiaozhong Liu","Siliang Tang"],"pdf_url":"https://arxiv.org/pdf/2303.12314v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12308v1","updated":"2023-03-22T04:52:43Z","published":"2023-03-22T04:52:43Z","title":"XWikiGen: Cross-lingual Summarization for Encyclopedic Text Generation\n  in Low Resource Languages","summary":"  Lack of encyclopedic text contributors, especially on Wikipedia, makes\nautomated text generation for \\emph{low resource (LR) languages} a critical\nproblem. Existing work on Wikipedia text generation has focused on\n\\emph{English only} where English reference articles are summarized to generate\nEnglish Wikipedia pages. But, for low-resource languages, the scarcity of\nreference articles makes monolingual summarization ineffective in solving this\nproblem. Hence, in this work, we propose \\task{}, which is the task of\ncross-lingual multi-document summarization of text from multiple reference\narticles, written in various languages, to generate Wikipedia-style text.\nAccordingly, we contribute a benchmark dataset, \\data{}, spanning $\\sim$69K\nWikipedia articles covering five domains and eight languages. We harness this\ndataset to train a two-stage system where the input is a set of citations and a\nsection title and the output is a section-specific LR summary. The proposed\nsystem is based on a novel idea of neural unsupervised extractive summarization\nto coarsely identify salient information followed by a neural abstractive model\nto generate the section-specific text. Extensive experiments show that\nmulti-domain training is better than the multi-lingual setup on average.\n","authors":["Dhaval Taunk","Shivprasad Sagare","Anupam Patil","Shivansh Subramanian","Manish Gupta","Vasudeva Varma"],"pdf_url":"https://arxiv.org/pdf/2303.12308v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12300v1","updated":"2023-03-22T04:11:35Z","published":"2023-03-22T04:11:35Z","title":"Exploring Turkish Speech Recognition via Hybrid CTC/Attention\n  Architecture and Multi-feature Fusion Network","summary":"  In recent years, End-to-End speech recognition technology based on deep\nlearning has developed rapidly. Due to the lack of Turkish speech data, the\nperformance of Turkish speech recognition system is poor. Firstly, this paper\nstudies a series of speech recognition tuning technologies. The results show\nthat the performance of the model is the best when the data enhancement\ntechnology combining speed perturbation with noise addition is adopted and the\nbeam search width is set to 16. Secondly, to maximize the use of effective\nfeature information and improve the accuracy of feature extraction, this paper\nproposes a new feature extractor LSPC. LSPC and LiGRU network are combined to\nform a shared encoder structure, and model compression is realized. The results\nshow that the performance of LSPC is better than MSPC and VGGnet when only\nusing Fbank features, and the WER is improved by 1.01% and 2.53% respectively.\nFinally, based on the above two points, a new multi-feature fusion network is\nproposed as the main structure of the encoder. The results show that the WER of\nthe proposed feature fusion network based on LSPC is improved by 0.82% and\n1.94% again compared with the single feature (Fbank feature and Spectrogram\nfeature) extraction using LSPC. Our model achieves performance comparable to\nthat of advanced End-to-End models.\n","authors":["Zeyu Ren","Nurmement Yolwas","Huiru Wang","Wushour Slamu"],"pdf_url":"https://arxiv.org/pdf/2303.12300v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12294v1","updated":"2023-03-22T03:58:38Z","published":"2023-03-22T03:58:38Z","title":"Evaluating Transformer Models and Human Behaviors on Chinese Character\n  Naming","summary":"  Neural network models have been proposed to explain the grapheme-phoneme\nmapping process in humans for many alphabet languages. These models not only\nsuccessfully learned the correspondence of the letter strings and their\npronunciation, but also captured human behavior in nonce word naming tasks. How\nwould the neural models perform for a non-alphabet language (e.g., Chinese)\nunknown character task? How well would the model capture human behavior? In\nthis study, we evaluate a set of transformer models and compare their\nperformances with human behaviors on an unknown Chinese character naming task.\nWe found that the models and humans behaved very similarly, that they had\nsimilar accuracy distribution for each character, and had a substantial overlap\nin answers. In addition, the models' answers are highly correlated with humans'\nanswers. These results suggested that the transformer models can well capture\nhuman's character naming behavior.\n","authors":["Xiaomeng Ma","Lingyu Gao"],"pdf_url":"https://arxiv.org/pdf/2303.12294v1.pdf","comment":"Accepted by TACL"},{"id":"http://arxiv.org/abs/2303.10893v2","updated":"2023-03-22T03:20:27Z","published":"2023-03-20T06:20:03Z","title":"Character, Word, or Both? Revisiting the Segmentation Granularity for\n  Chinese Pre-trained Language Models","summary":"  Pretrained language models (PLMs) have shown marvelous improvements across\nvarious NLP tasks. Most Chinese PLMs simply treat an input text as a sequence\nof characters, and completely ignore word information. Although Whole Word\nMasking can alleviate this, the semantics in words is still not well\nrepresented. In this paper, we revisit the segmentation granularity of Chinese\nPLMs. We propose a mixed-granularity Chinese BERT (MigBERT) by considering both\ncharacters and words. To achieve this, we design objective functions for\nlearning both character and word-level representations. We conduct extensive\nexperiments on various Chinese NLP tasks to evaluate existing PLMs as well as\nthe proposed MigBERT. Experimental results show that MigBERT achieves new SOTA\nperformance on all these tasks. Further analysis demonstrates that words are\nsemantically richer than characters. More interestingly, we show that MigBERT\nalso works with Japanese. Our code and model have been released\nhere~\\footnote{https://github.com/xnliang98/MigBERT}.\n","authors":["Xinnian Liang","Zefan Zhou","Hui Huang","Shuangzhi Wu","Tong Xiao","Muyun Yang","Zhoujun Li","Chao Bian"],"pdf_url":"https://arxiv.org/pdf/2303.10893v2.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2211.12764v3","updated":"2023-03-22T02:36:52Z","published":"2022-11-23T08:20:29Z","title":"VoP: Text-Video Co-operative Prompt Tuning for Cross-Modal Retrieval","summary":"  Many recent studies leverage the pre-trained CLIP for text-video cross-modal\nretrieval by tuning the backbone with additional heavy modules, which not only\nbrings huge computational burdens with much more parameters, but also leads to\nthe knowledge forgetting from upstream models. In this work, we propose the\nVoP: Text-Video Co-operative Prompt Tuning for efficient tuning on the\ntext-video retrieval task. The proposed VoP is an end-to-end framework with\nboth video & text prompts introducing, which can be regarded as a powerful\nbaseline with only 0.1% trainable parameters. Further, based on the\nspatio-temporal characteristics of videos, we develop three novel video prompt\nmechanisms to improve the performance with different scales of trainable\nparameters. The basic idea of the VoP enhancement is to model the frame\nposition, frame context, and layer function with specific trainable prompts,\nrespectively. Extensive experiments show that compared to full fine-tuning, the\nenhanced VoP achieves a 1.4% average R@1 gain across five text-video retrieval\nbenchmarks with 6x less parameter overhead. The code will be available at\nhttps://github.com/bighuang624/VoP.\n","authors":["Siteng Huang","Biao Gong","Yulin Pan","Jianwen Jiang","Yiliang Lv","Yuyuan Li","Donglin Wang"],"pdf_url":"https://arxiv.org/pdf/2211.12764v3.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2303.12810v1","updated":"2023-03-22T22:53:44Z","published":"2023-03-22T22:53:44Z","title":"Are LLMs the Master of All Trades? : Exploring Domain-Agnostic Reasoning\n  Skills of LLMs","summary":"  The potential of large language models (LLMs) to reason like humans has been\na highly contested topic in Machine Learning communities. However, the\nreasoning abilities of humans are multifaceted and can be seen in various\nforms, including analogical, spatial and moral reasoning, among others. This\nfact raises the question whether LLMs can perform equally well across all these\ndifferent domains. This research work aims to investigate the performance of\nLLMs on different reasoning tasks by conducting experiments that directly use\nor draw inspirations from existing datasets on analogical and spatial\nreasoning. Additionally, to evaluate the ability of LLMs to reason like human,\ntheir performance is evaluted on more open-ended, natural language questions.\nMy findings indicate that LLMs excel at analogical and moral reasoning, yet\nstruggle to perform as proficiently on spatial reasoning tasks. I believe these\nexperiments are crucial for informing the future development of LLMs,\nparticularly in contexts that require diverse reasoning proficiencies. By\nshedding light on the reasoning abilities of LLMs, this study aims to push\nforward our understanding of how they can better emulate the cognitive\nabilities of humans.\n","authors":["Shrivats Agrawal"],"pdf_url":"https://arxiv.org/pdf/2303.12810v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12936v1","updated":"2023-03-22T22:31:09Z","published":"2023-03-22T22:31:09Z","title":"Analyzing the Generalizability of Deep Contextualized Language\n  Representations For Text Classification","summary":"  This study evaluates the robustness of two state-of-the-art deep contextual\nlanguage representations, ELMo and DistilBERT, on supervised learning of binary\nprotest news classification and sentiment analysis of product reviews. A\n\"cross-context\" setting is enabled using test sets that are distinct from the\ntraining data. Specifically, in the news classification task, the models are\ndeveloped on local news from India and tested on the local news from China. In\nthe sentiment analysis task, the models are trained on movie reviews and tested\non customer reviews. This comparison is aimed at exploring the limits of the\nrepresentative power of today's Natural Language Processing systems on the path\nto the systems that are generalizable to real-life scenarios. The models are\nfine-tuned and fed into a Feed-Forward Neural Network and a Bidirectional Long\nShort Term Memory network. Multinomial Naive Bayes and Linear Support Vector\nMachine are used as traditional baselines. The results show that, in binary\ntext classification, DistilBERT is significantly better than ELMo on\ngeneralizing to the cross-context setting. ELMo is observed to be significantly\nmore robust to the cross-context test data than both baselines. On the other\nhand, the baselines performed comparably well to ELMo when the training and\ntest data are subsets of the same corpus (no cross-context). DistilBERT is also\nfound to be 30% smaller and 83% faster than ELMo. The results suggest that\nDistilBERT can transfer generic semantic knowledge to other domains better than\nELMo. DistilBERT is also favorable in incorporating into real-life systems for\nit requires a smaller computational training budget. When generalization is not\nthe utmost preference and test domain is similar to the training domain, the\ntraditional ML algorithms can still be considered as more economic alternatives\nto deep language representations.\n","authors":["Berfu Buyukoz"],"pdf_url":"https://arxiv.org/pdf/2303.12936v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2111.03837v2","updated":"2023-03-22T22:18:01Z","published":"2021-11-06T09:04:16Z","title":"Focusing on Potential Named Entities During Active Label Acquisition","summary":"  Named entity recognition (NER) aims to identify mentions of named entities in\nan unstructured text and classify them into predefined named entity classes.\nWhile deep learning-based pre-trained language models help to achieve good\npredictive performances in NER, many domain-specific NER applications still\ncall for a substantial amount of labeled data. Active learning (AL), a general\nframework for the label acquisition problem, has been used for NER tasks to\nminimize the annotation cost without sacrificing model performance. However,\nthe heavily imbalanced class distribution of tokens introduces challenges in\ndesigning effective AL querying methods for NER. We propose several AL sentence\nquery evaluation functions that pay more attention to potential positive\ntokens, and evaluate these proposed functions with both sentence-based and\ntoken-based cost evaluation strategies. We also propose a better data-driven\nnormalization approach to penalize sentences that are too long or too short.\nOur experiments on three datasets from different domains reveal that the\nproposed approach reduces the number of annotated tokens while achieving better\nor comparable prediction performance with conventional methods.\n","authors":["Ali Osman Berk Sapci","Oznur Tastan","Reyyan Yeniterzi"],"pdf_url":"https://arxiv.org/pdf/2111.03837v2.pdf","comment":"20 pages, 8 figures"},{"id":"http://arxiv.org/abs/2301.09209v2","updated":"2023-03-22T21:35:42Z","published":"2023-01-22T21:30:12Z","title":"Summarize the Past to Predict the Future: Natural Language Descriptions\n  of Context Boost Multimodal Object Interaction","summary":"  We study object interaction anticipation in egocentric videos. This task\nrequires an understanding of the spatiotemporal context formed by past actions\non objects, coined action context. We propose TransFusion, a multimodal\ntransformer-based architecture. It exploits the representational power of\nlanguage by summarising the action context. TransFusion leverages pre-trained\nimage captioning and vision-language models to extract the action context from\npast video frames. This action context together with the next video frame is\nprocessed by the multimodal fusion module to forecast the next object\ninteraction. Our model enables more efficient end-to-end learning. The large\npre-trained language models add common sense and a generalisation capability.\nExperiments on Ego4D and EPIC-KITCHENS-100 show the effectiveness of our\nmultimodal fusion model. They also highlight the benefits of using\nlanguage-based context summaries in a task where vision seems to suffice. Our\nmethod outperforms state-of-the-art approaches by 40.4% in relative terms in\noverall mAP on the Ego4D test set. We validate the effectiveness of TransFusion\nvia experiments on EPIC-KITCHENS-100. Video and code are available at:\nhttps://eth-ait.github.io/transfusion-proj/.\n","authors":["Razvan-George Pasca","Alexey Gavryushin","Yen-Ling Kuo","Luc Van Gool","Otmar Hilliges","Xi Wang"],"pdf_url":"https://arxiv.org/pdf/2301.09209v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.09229v3","updated":"2023-03-22T21:10:42Z","published":"2022-05-18T22:15:20Z","title":"PromptDA: Label-guided Data Augmentation for Prompt-based Few-shot\n  Learners","summary":"  Recent advances in large pre-trained language models (PLMs) lead to\nimpressive gains in natural language understanding (NLU) tasks with\ntask-specific fine-tuning. However, directly fine-tuning PLMs heavily relies on\nsufficient labeled training instances, which are usually hard to obtain.\nPrompt-based tuning on PLMs has shown to be powerful for various downstream\nfew-shot tasks. Existing works studying prompt-based tuning for few-shot NLU\ntasks mainly focus on deriving proper label words with a verbalizer or\ngenerating prompt templates to elicit semantics from PLMs. In addition,\nconventional data augmentation strategies such as synonym substitution, though\nwidely adopted in low-resource scenarios, only bring marginal improvements for\nprompt-based few-shot learning. Thus, an important research question arises:\nhow to design effective data augmentation methods for prompt-based few-shot\ntuning? To this end, considering the label semantics are essential in\nprompt-based tuning, we propose a novel label-guided data augmentation\nframework PromptDA, which exploits the enriched label semantic information for\ndata augmentation. Extensive experiment results on few-shot text classification\ntasks demonstrate the superior performance of the proposed framework by\neffectively leveraging label semantics and data augmentation for natural\nlanguage understanding. Our code is available at\nhttps://github.com/canyuchen/PromptDA.\n","authors":["Canyu Chen","Kai Shu"],"pdf_url":"https://arxiv.org/pdf/2205.09229v3.pdf","comment":"Accepted to Proceedings of EACL 2023 main conference. Code is\n  available at https://github.com/canyuchen/PromptDA"},{"id":"http://arxiv.org/abs/2303.12898v1","updated":"2023-03-22T20:26:30Z","published":"2023-03-22T20:26:30Z","title":"Towards Understanding the Generalization of Medical Text-to-SQL Models\n  and Datasets","summary":"  Electronic medical records (EMRs) are stored in relational databases. It can\nbe challenging to access the required information if the user is unfamiliar\nwith the database schema or general database fundamentals. Hence, researchers\nhave explored text-to-SQL generation methods that provide healthcare\nprofessionals direct access to EMR data without needing a database expert.\nHowever, currently available datasets have been essentially \"solved\" with\nstate-of-the-art models achieving accuracy greater than or near 90%. In this\npaper, we show that there is still a long way to go before solving text-to-SQL\ngeneration in the medical domain. To show this, we create new splits of the\nexisting medical text-to-SQL dataset MIMICSQL that better measure the\ngeneralizability of the resulting models. We evaluate state-of-the-art language\nmodels on our new split showing substantial drops in performance with accuracy\ndropping from up to 92% to 28%, thus showing substantial room for improvement.\nMoreover, we introduce a novel data augmentation approach to improve the\ngeneralizability of the language models. Overall, this paper is the first step\ntowards developing more robust text-to-SQL models in the medical\ndomain.\\footnote{The dataset and code will be released upon acceptance.\n","authors":["Richard Tarbell","Kim-Kwang Raymond Choo","Glenn Dietrich","Anthony Rios"],"pdf_url":"https://arxiv.org/pdf/2303.12898v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12024v2","updated":"2023-03-22T20:16:47Z","published":"2023-03-21T17:04:44Z","title":"cTBL: Augmenting Large Language Models for Conversational Tables","summary":"  An open challenge in multimodal conversational AI requires augmenting large\nlanguage models with information from textual and non-textual sources for\nmulti-turn dialogue. To address this problem, this paper introduces\nConversational Tables (cTBL), a three-step encoder-decoder approach to retrieve\ntabular information and generate dialogue responses grounded on the retrieved\ninformation. cTBL uses Transformer encoder embeddings for Dense Table Retrieval\nand obtains up to 5% relative improvement in Top-1 and Top-3 accuracy over\nsparse retrieval on the HyrbiDialogue dataset. Additionally, cTBL performs\ntabular knowledge retrieval using both encoder and decoder models, resulting in\nup to 46% relative improvement in ROUGE scores and better human evaluation for\nresponse generation on HyrbiDialogue.\n","authors":["Anirudh S Sundar","Larry Heck"],"pdf_url":"https://arxiv.org/pdf/2303.12024v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12892v1","updated":"2023-03-22T20:10:29Z","published":"2023-03-22T20:10:29Z","title":"A Small-Scale Switch Transformer and NLP-based Model for Clinical\n  Narratives Classification","summary":"  In recent years, Transformer-based models such as the Switch Transformer have\nachieved remarkable results in natural language processing tasks. However,\nthese models are often too complex and require extensive pre-training, which\nlimits their effectiveness for small clinical text classification tasks with\nlimited data. In this study, we propose a simplified Switch Transformer\nframework and train it from scratch on a small French clinical text\nclassification dataset at CHU Sainte-Justine hospital. Our results demonstrate\nthat the simplified small-scale Transformer models outperform pre-trained\nBERT-based models, including DistillBERT, CamemBERT, FlauBERT, and FrALBERT.\nAdditionally, using a mixture of expert mechanisms from the Switch Transformer\nhelps capture diverse patterns; hence, the proposed approach achieves better\nresults than a conventional Transformer with the self-attention mechanism.\nFinally, our proposed framework achieves an accuracy of 87\\%, precision at\n87\\%, and recall at 85\\%, compared to the third-best pre-trained BERT-based\nmodel, FlauBERT, which achieved an accuracy of 84\\%, precision at 84\\%, and\nrecall at 84\\%. However, Switch Transformers have limitations, including a\ngeneralization gap and sharp minima. We compare it with a multi-layer\nperceptron neural network for small French clinical narratives classification\nand show that the latter outperforms all other models.\n","authors":["Thanh-Dung Le","Philippe Jouvet","Rita Noumeir"],"pdf_url":"https://arxiv.org/pdf/2303.12892v1.pdf","comment":"Submitted to IEEE Journal of Biomedical and Health Informatics"},{"id":"http://arxiv.org/abs/2210.15387v2","updated":"2023-03-22T19:38:02Z","published":"2022-10-27T12:48:10Z","title":"Automatic Severity Assessment of Dysarthric speech by using\n  Self-supervised Model with Multi-task Learning","summary":"  Automatic assessment of dysarthric speech is essential for sustained\ntreatments and rehabilitation. However, obtaining atypical speech is\nchallenging, often leading to data scarcity issues. To tackle the problem, we\npropose a novel automatic severity assessment method for dysarthric speech,\nusing the self-supervised model in conjunction with multi-task learning.\nWav2vec 2.0 XLS-R is jointly trained for two different tasks: severity\nclassification and auxiliary automatic speech recognition (ASR). For the\nbaseline experiments, we employ hand-crafted acoustic features and machine\nlearning classifiers such as SVM, MLP, and XGBoost. Explored on the Korean\ndysarthric speech QoLT database, our model outperforms the traditional baseline\nmethods, with a relative percentage increase of 1.25% for F1-score. In\naddition, the proposed model surpasses the model trained without ASR head,\nachieving 10.61% relative percentage improvements. Furthermore, we present how\nmulti-task learning affects the severity classification performance by\nanalyzing the latent representations and regularization effect.\n","authors":["Eun Jung Yeo","Kwanghee Choi","Sunhee Kim","Minhwa Chung"],"pdf_url":"https://arxiv.org/pdf/2210.15387v2.pdf","comment":"Accepted to ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.12869v1","updated":"2023-03-22T19:01:25Z","published":"2023-03-22T19:01:25Z","title":"JaCoText: A Pretrained Model for Java Code-Text Generation","summary":"  Pretrained transformer-based models have shown high performance in natural\nlanguage generation task. However, a new wave of interest has surged: automatic\nprogramming language generation. This task consists of translating natural\nlanguage instructions to a programming code. Despite the fact that well-known\npretrained models on language generation have achieved good performance in\nlearning programming languages, effort is still needed in automatic code\ngeneration. In this paper, we introduce JaCoText, a model based on Transformers\nneural network. It aims to generate java source code from natural language\ntext. JaCoText leverages advantages of both natural language and code\ngeneration models. More specifically, we study some findings from the state of\nthe art and use them to (1) initialize our model from powerful pretrained\nmodels, (2) explore additional pretraining on our java dataset, (3) carry out\nexperiments combining the unimodal and bimodal data in the training, and (4)\nscale the input and output length during the fine-tuning of the model.\nConducted experiments on CONCODE dataset show that JaCoText achieves new\nstate-of-the-art results.\n","authors":["Jessica López Espejel","Mahaman Sanoussi Yahaya Alassan","Walid Dahhane","El Hassane Ettifouri"],"pdf_url":"https://arxiv.org/pdf/2303.12869v1.pdf","comment":"International Conference on Code Generation and Implementation\n  Volume: 17"},{"id":"http://arxiv.org/abs/2303.12860v1","updated":"2023-03-22T18:49:43Z","published":"2023-03-22T18:49:43Z","title":"Salient Span Masking for Temporal Understanding","summary":"  Salient Span Masking (SSM) has shown itself to be an effective strategy to\nimprove closed-book question answering performance. SSM extends general masked\nlanguage model pretraining by creating additional unsupervised training\nsentences that mask a single entity or date span, thus oversampling factual\ninformation. Despite the success of this paradigm, the span types and sampling\nstrategies are relatively arbitrary and not widely studied for other tasks.\nThus, we investigate SSM from the perspective of temporal tasks, where learning\na good representation of various temporal expressions is important. To that\nend, we introduce Temporal Span Masking (TSM) intermediate training. First, we\nfind that SSM alone improves the downstream performance on three temporal tasks\nby an avg. +5.8 points. Further, we are able to achieve additional improvements\n(avg. +0.29 points) by adding the TSM task. These comprise the new best\nreported results on the targeted tasks. Our analysis suggests that the\neffectiveness of SSM stems from the sentences chosen in the training data\nrather than the mask choice: sentences with entities frequently also contain\ntemporal expressions. Nonetheless, the additional targeted spans of TSM can\nstill improve performance, especially in a zero-shot context.\n","authors":["Jeremy R. Cole","Aditi Chaudhary","Bhuwan Dhingra","Partha Talukdar"],"pdf_url":"https://arxiv.org/pdf/2303.12860v1.pdf","comment":"5 pages 1 figure, to appear in EACL 2023"},{"id":"http://arxiv.org/abs/2210.14868v2","updated":"2023-03-22T18:37:20Z","published":"2022-10-26T17:17:06Z","title":"Multi-lingual Evaluation of Code Generation Models","summary":"  We present new benchmarks on evaluation code generation models: MBXP and\nMultilingual HumanEval, and MathQA-X. These datasets cover over 10 programming\nlanguages and are generated using a scalable conversion framework that\ntranspiles prompts and test cases from the original Python datasets into the\ncorresponding data in the target language. Using these benchmarks, we are able\nto assess the performance of code generation models in a multi-lingual fashion,\nand discovered generalization ability of language models on out-of-domain\nlanguages, advantages of multi-lingual models over mono-lingual, the ability of\nfew-shot prompting to teach the model new languages, and zero-shot translation\nabilities even on mono-lingual settings. Furthermore, we use our code\ngeneration model to perform large-scale bootstrapping to obtain synthetic\ncanonical solutions in several languages, which can be used for other\ncode-related evaluations such as code insertion, robustness, or summarization\ntasks. Overall, our benchmarks represents a significant step towards a deeper\nunderstanding of language models' code generation abilities. We publicly\nrelease our code and datasets at https://github.com/amazon-research/mxeval.\n","authors":["Ben Athiwaratkun","Sanjay Krishna Gouda","Zijian Wang","Xiaopeng Li","Yuchen Tian","Ming Tan","Wasi Uddin Ahmad","Shiqi Wang","Qing Sun","Mingyue Shang","Sujan Kumar Gonugondla","Hantian Ding","Varun Kumar","Nathan Fulton","Arash Farahani","Siddhartha Jain","Robert Giaquinto","Haifeng Qian","Murali Krishna Ramanathan","Ramesh Nallapati","Baishakhi Ray","Parminder Bhatia","Sudipta Sengupta","Dan Roth","Bing Xiang"],"pdf_url":"https://arxiv.org/pdf/2210.14868v2.pdf","comment":"Code and data release: https://github.com/amazon-research/mxeval"},{"id":"http://arxiv.org/abs/2303.13463v1","updated":"2023-03-22T15:32:40Z","published":"2023-03-22T15:32:40Z","title":"W2KPE: Keyphrase Extraction with Word-Word Relation","summary":"  This paper describes our submission to ICASSP 2023 MUG Challenge Track 4,\nKeyphrase Extraction, which aims to extract keyphrases most relevant to the\nconference theme from conference materials. We model the challenge as a\nsingle-class Named Entity Recognition task and developed techniques for better\nperformance on the challenge: For the data preprocessing, we encode the split\nkeyphrases after word segmentation. In addition, we increase the amount of\ninput information that the model can accept at one time by fusing multiple\npreprocessed sentences into one segment. We replace the loss function with the\nmulti-class focal loss to address the sparseness of keyphrases. Besides, we\nscore each appearance of keyphrases and add an extra output layer to fit the\nscore to rank keyphrases. Exhaustive evaluations are performed to find the best\ncombination of the word segmentation tool, the pre-trained embedding model, and\nthe corresponding hyperparameters. With these proposals, we scored 45.04 on the\nfinal test set.\n","authors":["Wen Cheng","Shichen Dong","Wei Wang"],"pdf_url":"https://arxiv.org/pdf/2303.13463v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13466v1","updated":"2023-03-22T13:46:16Z","published":"2023-03-22T13:46:16Z","title":"Extracting Physical Rehabilitation Exercise Information from Clinical\n  Notes: a Comparison of Rule-Based and Machine Learning Natural Language\n  Processing Techniques","summary":"  Physical rehabilitation plays a crucial role in the recovery process of\npost-stroke patients. By personalizing therapies for patients leveraging\npredictive modeling and electronic health records (EHRs), healthcare providers\ncan make the rehabilitation process more efficient. Before predictive modeling\ncan provide decision support for the assignment of treatment plans, automated\nmethods are necessary to extract physical rehabilitation exercise information\nfrom unstructured EHRs. We introduce a rule-based natural language processing\nalgorithm to annotate therapeutic procedures for stroke patients and compare it\nto several small machine learning models. We find that our algorithm\noutperforms these models in extracting half of the concepts where sufficient\ndata is available, and individual exercise descriptions can be assigned binary\nlabels with an f-score of no less than 0.75 per concept. More research needs to\nbe done before these algorithms can be deployed on unlabeled documents, but\ncurrent progress gives promise to the potential of precision rehabilitation\nresearch.\n","authors":["Stephen W. Shaffran","Fengyi Gao","Parker E. Denny","Bayan M. Aldhahwani","Allyn Bove","Shyam Visweswaran","Yanshan Wang"],"pdf_url":"https://arxiv.org/pdf/2303.13466v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13465v1","updated":"2023-03-22T09:29:22Z","published":"2023-03-22T09:29:22Z","title":"Deep RL with Hierarchical Action Exploration for Dialogue Generation","summary":"  Conventionally, since the natural language action space is astronomical,\napproximate dynamic programming applied to dialogue generation involves policy\nimprovement with action sampling. However, such a practice is inefficient for\nreinforcement learning (RL) because the eligible (high action value) responses\nare very sparse, and the greedy policy sustained by the random sampling is\nflabby. This paper shows that the performance of dialogue policy positively\ncorrelated with sampling size by theoretical and experimental. We introduce a\nnovel dual-granularity Q-function to alleviate this limitation by exploring the\nmost promising response category to intervene in the sampling. It extracts the\nactions following the grained hierarchy, which can achieve the optimum with\nfewer policy iterations. Our approach learns in the way of offline RL from\nmultiple reward functions designed to recognize human emotional details.\nEmpirical studies demonstrate that our algorithm outperforms the baseline\nmethods. Further verification presents that ours can generate responses with\nhigher expected rewards and controllability.\n","authors":["Itsugun Cho","Ryota Takahashi","Yusaku Yanase","Hiroaki Saito"],"pdf_url":"https://arxiv.org/pdf/2303.13465v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12816v1","updated":"2023-03-22T07:34:33Z","published":"2023-03-22T07:34:33Z","title":"From Wide to Deep: Dimension Lifting Network for Parameter-efficient\n  Knowledge Graph Embedding","summary":"  Knowledge graph embedding (KGE) that maps entities and relations into vector\nrepresentations is essential for downstream tasks. Conventional KGE methods\nrequire relatively high-dimensional entity representations to preserve the\nstructural information of knowledge graph, but lead to oversized model\nparameters. Recent methods reduce model parameters by adopting low-dimensional\nentity representations, while developing techniques (e.g., knowledge\ndistillation) to compensate for the reduced dimension. However, such operations\nproduce degraded model accuracy and limited reduction of model parameters.\nSpecifically, we view the concatenation of all entity representations as an\nembedding layer, and then conventional KGE methods that adopt high-dimensional\nentity representations equal to enlarging the width of the embedding layer to\ngain expressiveness. To achieve parameter efficiency without sacrificing\naccuracy, we instead increase the depth and propose a deeper embedding network\nfor entity representations, i.e., a narrow embedding layer and a multi-layer\ndimension lifting network (LiftNet). Experiments on three public datasets show\nthat the proposed method (implemented based on TransE and DistMult) with\n4-dimensional entity representations achieves more accurate link prediction\nresults than counterpart parameter-efficient KGE methods and strong KGE\nbaselines, including TransE and DistMult with 512-dimensional entity\nrepresentations.\n","authors":["Borui Cai","Yong Xiang","Longxiang Gao","Di Wu","He Zhang","Jiong Jin","Tom Luan"],"pdf_url":"https://arxiv.org/pdf/2303.12816v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2303.12793v1","updated":"2023-03-22T17:59:59Z","published":"2023-03-22T17:59:59Z","title":"CiCo: Domain-Aware Sign Language Retrieval via Cross-Lingual Contrastive\n  Learning","summary":"  This work focuses on sign language retrieval-a recently proposed task for\nsign language understanding. Sign language retrieval consists of two sub-tasks:\ntext-to-sign-video (T2V) retrieval and sign-video-to-text (V2T) retrieval.\nDifferent from traditional video-text retrieval, sign language videos, not only\ncontain visual signals but also carry abundant semantic meanings by themselves\ndue to the fact that sign languages are also natural languages. Considering\nthis character, we formulate sign language retrieval as a cross-lingual\nretrieval problem as well as a video-text retrieval task. Concretely, we take\ninto account the linguistic properties of both sign languages and natural\nlanguages, and simultaneously identify the fine-grained cross-lingual (i.e.,\nsign-to-word) mappings while contrasting the texts and the sign videos in a\njoint embedding space. This process is termed as cross-lingual contrastive\nlearning. Another challenge is raised by the data scarcity issue-sign language\ndatasets are orders of magnitude smaller in scale than that of speech\nrecognition. We alleviate this issue by adopting a domain-agnostic sign encoder\npre-trained on large-scale sign videos into the target domain via\npseudo-labeling. Our framework, termed as domain-aware sign language retrieval\nvia Cross-lingual Contrastive learning or CiCo for short, outperforms the\npioneering method by large margins on various datasets, e.g., +22.4 T2V and\n+28.0 V2T R@1 improvements on How2Sign dataset, and +13.7 T2V and +17.1 V2T R@1\nimprovements on PHOENIX-2014T dataset. Code and models are available at:\nhttps://github.com/FangyunWei/SLRT.\n","authors":["Yiting Cheng","Fangyun Wei","Jianmin Bao","Dong Chen","Wenqiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.12793v1.pdf","comment":"Accepted by CVPR 2023. Code and models are available at:\n  https://github.com/FangyunWei/SLRT"},{"id":"http://arxiv.org/abs/2303.12791v1","updated":"2023-03-22T17:59:12Z","published":"2023-03-22T17:59:12Z","title":"SHERF: Generalizable Human NeRF from a Single Image","summary":"  Existing Human NeRF methods for reconstructing 3D humans typically rely on\nmultiple 2D images from multi-view cameras or monocular videos captured from\nfixed camera views. However, in real-world scenarios, human images are often\ncaptured from random camera angles, presenting challenges for high-quality 3D\nhuman reconstruction. In this paper, we propose SHERF, the first generalizable\nHuman NeRF model for recovering animatable 3D humans from a single input image.\nSHERF extracts and encodes 3D human representations in canonical space,\nenabling rendering and animation from free views and poses. To achieve\nhigh-fidelity novel view and pose synthesis, the encoded 3D human\nrepresentations should capture both global appearance and local fine-grained\ntextures. To this end, we propose a bank of 3D-aware hierarchical features,\nincluding global, point-level, and pixel-aligned features, to facilitate\ninformative encoding. Global features enhance the information extracted from\nthe single input image and complement the information missing from the partial\n2D observation. Point-level features provide strong clues of 3D human\nstructure, while pixel-aligned features preserve more fine-grained details. To\neffectively integrate the 3D-aware hierarchical feature bank, we design a\nfeature fusion transformer. Extensive experiments on THuman, RenderPeople,\nZJU_MoCap, and HuMMan datasets demonstrate that SHERF achieves state-of-the-art\nperformance, with better generalizability for novel view and pose synthesis.\n","authors":["Shoukang Hu","Fangzhou Hong","Liang Pan","Haiyi Mei","Lei Yang","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2303.12791v1.pdf","comment":"Project webpage: https://skhu101.github.io/SHERF/"},{"id":"http://arxiv.org/abs/2211.09703v2","updated":"2023-03-22T17:58:09Z","published":"2022-11-17T17:38:55Z","title":"EfficientTrain: Exploring Generalized Curriculum Learning for Training\n  Visual Backbones","summary":"  The superior performance of modern deep networks usually comes with a costly\ntraining procedure. This paper presents a new curriculum learning approach for\nthe efficient training of visual backbones (e.g., vision Transformers). Our\nwork is inspired by the inherent learning dynamics of deep networks: we\nexperimentally show that at an earlier training stage, the model mainly learns\nto recognize some 'easier-to-learn' discriminative patterns within each\nexample, e.g., the lower-frequency components of images and the original\ninformation before data augmentation. Driven by this phenomenon, we propose a\ncurriculum where the model always leverages all the training data at each\nepoch, while the curriculum starts with only exposing the 'easier-to-learn'\npatterns of each example, and introduces gradually more difficult patterns. To\nimplement this idea, we 1) introduce a cropping operation in the Fourier\nspectrum of the inputs, which enables the model to learn from only the\nlower-frequency components efficiently, 2) demonstrate that exposing the\nfeatures of original images amounts to adopting weaker data augmentation, and\n3) integrate 1) and 2) and design a curriculum learning schedule with a\ngreedy-search algorithm. The resulting approach, EfficientTrain, is simple,\ngeneral, yet surprisingly effective. In the absence of hyper-parameter tuning,\nit reduces the training wall-time of a wide variety of popular models (e.g.,\nResNet, ConvNeXt, DeiT, PVT, Swin, and CSWin) by >1.5x on ImageNet-1K/22K\nwithout sacrificing the accuracy. It is also effective for self-supervised\nlearning (e.g., MAE). Code is available at\nhttps://github.com/LeapLabTHU/EfficientTrain.\n","authors":["Yulin Wang","Yang Yue","Rui Lu","Tianjiao Liu","Zhao Zhong","Shiji Song","Gao Huang"],"pdf_url":"https://arxiv.org/pdf/2211.09703v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12790v1","updated":"2023-03-22T17:58:01Z","published":"2023-03-22T17:58:01Z","title":"Diffuse-Denoise-Count: Accurate Crowd-Counting with Diffusion Models","summary":"  Crowd counting is a key aspect of crowd analysis and has been typically\naccomplished by estimating a crowd-density map and summing over the density\nvalues. However, this approach suffers from background noise accumulation and\nloss of density due to the use of broad Gaussian kernels to create the ground\ntruth density maps. This issue can be overcome by narrowing the Gaussian\nkernel. However, existing approaches perform poorly when trained with such\nground truth density maps. To overcome this limitation, we propose using\nconditional diffusion models to predict density maps, as diffusion models are\nknown to model complex distributions well and show high fidelity to training\ndata during crowd-density map generation. Furthermore, as the intermediate time\nsteps of the diffusion process are noisy, we incorporate a regression branch\nfor direct crowd estimation only during training to improve the feature\nlearning. In addition, owing to the stochastic nature of the diffusion model,\nwe introduce producing multiple density maps to improve the counting\nperformance contrary to the existing crowd counting pipelines. Further, we also\ndiffer from the density summation and introduce contour detection followed by\nsummation as the counting operation, which is more immune to background noise.\nWe conduct extensive experiments on public datasets to validate the\neffectiveness of our method. Specifically, our novel crowd-counting pipeline\nimproves the error of crowd-counting by up to $6\\%$ on JHU-CROWD++ and up to\n$7\\%$ on UCF-QNRF.\n","authors":["Yasiru Ranasinghe","Nithin Gopalakrishnan Nair","Wele Gedara Chaminda Bandara","Vishal M. Patel"],"pdf_url":"https://arxiv.org/pdf/2303.12790v1.pdf","comment":"The project is available at\n  https://github.com/dylran/DiffuseDenoiseCount"},{"id":"http://arxiv.org/abs/2303.12789v1","updated":"2023-03-22T17:57:57Z","published":"2023-03-22T17:57:57Z","title":"Instruct-NeRF2NeRF: Editing 3D Scenes with Instructions","summary":"  We propose a method for editing NeRF scenes with text-instructions. Given a\nNeRF of a scene and the collection of images used to reconstruct it, our method\nuses an image-conditioned diffusion model (InstructPix2Pix) to iteratively edit\nthe input images while optimizing the underlying scene, resulting in an\noptimized 3D scene that respects the edit instruction. We demonstrate that our\nproposed method is able to edit large-scale, real-world scenes, and is able to\naccomplish more realistic, targeted edits than prior work.\n","authors":["Ayaan Haque","Matthew Tancik","Alexei A. Efros","Aleksander Holynski","Angjoo Kanazawa"],"pdf_url":"https://arxiv.org/pdf/2303.12789v1.pdf","comment":"Project website: https://instruct-nerf2nerf.github.io"},{"id":"http://arxiv.org/abs/2303.12787v1","updated":"2023-03-22T17:57:36Z","published":"2023-03-22T17:57:36Z","title":"EPro-PnP: Generalized End-to-End Probabilistic Perspective-n-Points for\n  Monocular Object Pose Estimation","summary":"  Locating 3D objects from a single RGB image via Perspective-n-Point (PnP) is\na long-standing problem in computer vision. Driven by end-to-end deep learning,\nrecent studies suggest interpreting PnP as a differentiable layer, allowing for\npartial learning of 2D-3D point correspondences by backpropagating the\ngradients of pose loss. Yet, learning the entire correspondences from scratch\nis highly challenging, particularly for ambiguous pose solutions, where the\nglobally optimal pose is theoretically non-differentiable w.r.t. the points. In\nthis paper, we propose the EPro-PnP, a probabilistic PnP layer for general\nend-to-end pose estimation, which outputs a distribution of pose with\ndifferentiable probability density on the SE(3) manifold. The 2D-3D coordinates\nand corresponding weights are treated as intermediate variables learned by\nminimizing the KL divergence between the predicted and target pose\ndistribution. The underlying principle generalizes previous approaches, and\nresembles the attention mechanism. EPro-PnP can enhance existing correspondence\nnetworks, closing the gap between PnP-based method and the task-specific\nleaders on the LineMOD 6DoF pose estimation benchmark. Furthermore, EPro-PnP\nhelps to explore new possibilities of network design, as we demonstrate a novel\ndeformable correspondence network with the state-of-the-art pose accuracy on\nthe nuScenes 3D object detection benchmark. Our code is available at\nhttps://github.com/tjiiv-cprg/EPro-PnP-v2.\n","authors":["Hansheng Chen","Wei Tian","Pichao Wang","Fan Wang","Lu Xiong","Hao Li"],"pdf_url":"https://arxiv.org/pdf/2303.12787v1.pdf","comment":"Code available at https://github.com/tjiiv-cprg/EPro-PnP-v2. arXiv\n  admin note: substantial text overlap with arXiv:2203.13254"},{"id":"http://arxiv.org/abs/2303.12786v1","updated":"2023-03-22T17:57:01Z","published":"2023-03-22T17:57:01Z","title":"FeatureNeRF: Learning Generalizable NeRFs by Distilling Foundation\n  Models","summary":"  Recent works on generalizable NeRFs have shown promising results on novel\nview synthesis from single or few images. However, such models have rarely been\napplied on other downstream tasks beyond synthesis such as semantic\nunderstanding and parsing. In this paper, we propose a novel framework named\nFeatureNeRF to learn generalizable NeRFs by distilling pre-trained vision\nfoundation models (e.g., DINO, Latent Diffusion). FeatureNeRF leverages 2D\npre-trained foundation models to 3D space via neural rendering, and then\nextract deep features for 3D query points from NeRF MLPs. Consequently, it\nallows to map 2D images to continuous 3D semantic feature volumes, which can be\nused for various downstream tasks. We evaluate FeatureNeRF on tasks of 2D/3D\nsemantic keypoint transfer and 2D/3D object part segmentation. Our extensive\nexperiments demonstrate the effectiveness of FeatureNeRF as a generalizable 3D\nsemantic feature extractor. Our project page is available at\nhttps://jianglongye.com/featurenerf/ .\n","authors":["Jianglong Ye","Naiyan Wang","Xiaolong Wang"],"pdf_url":"https://arxiv.org/pdf/2303.12786v1.pdf","comment":"Project page: https://jianglongye.com/featurenerf/"},{"id":"http://arxiv.org/abs/2303.12782v1","updated":"2023-03-22T17:52:11Z","published":"2023-03-22T17:52:11Z","title":"Tube-Link: A Flexible Cross Tube Baseline for Universal Video\n  Segmentation","summary":"  The goal of video segmentation is to accurately segment and track every pixel\nin diverse scenarios. In this paper, we present Tube-Link, a versatile\nframework that addresses multiple core tasks of video segmentation with a\nunified architecture. Our framework is a near-online approach that takes a\nshort subclip as input and outputs the corresponding spatial-temporal tube\nmasks. To enhance the modeling of cross-tube relationships, we propose an\neffective way to perform tube-level linking via attention along the queries. In\naddition, we introduce temporal contrastive learning to instance-wise\ndiscriminative features for tube-level association. Our approach offers\nflexibility and efficiency for both short and long video inputs, as the length\nof each subclip can be varied according to the needs of datasets or scenarios.\nTube-Link outperforms existing specialized architectures by a significant\nmargin on five video segmentation datasets. Specifically, it achieves almost\n13% relative improvements on VIPSeg and 4% improvements on KITTI-STEP over the\nstrong baseline Video K-Net. When using a ResNet50 backbone on Youtube-VIS-2019\nand 2021, Tube-Link boosts IDOL by 3% and 4%, respectively. Code will be\navailable.\n","authors":["Xiangtai Li","Haobo Yuan","Wenwei Zhang","Guangliang Cheng","Jiangmiao Pang","Chen Change Loy"],"pdf_url":"https://arxiv.org/pdf/2303.12782v1.pdf","comment":"Project page: https://github.com/lxtGH/Tube-Link"},{"id":"http://arxiv.org/abs/2303.12779v1","updated":"2023-03-22T17:46:27Z","published":"2023-03-22T17:46:27Z","title":"LFM-3D: Learnable Feature Matching Across Wide Baselines Using 3D\n  Signals","summary":"  Finding localized correspondences across different images of the same object\nis crucial to understand its geometry. In recent years, this problem has seen\nremarkable progress with the advent of deep learning based local image features\nand learnable matchers. Still, learnable matchers often underperform when there\nexists only small regions of co-visibility between image pairs (i.e. wide\ncamera baselines). To address this problem, we leverage recent progress in\ncoarse single-view geometry estimation methods. We propose LFM-3D, a Learnable\nFeature Matching framework that uses models based on graph neural networks, and\nenhances their capabilities by integrating noisy, estimated 3D signals to boost\ncorrespondence estimation. When integrating 3D signals into the matcher model,\nwe show that a suitable positional encoding is critical to effectively make use\nof the low-dimensional 3D information. We experiment with two different 3D\nsignals - normalized object coordinates and monocular depth estimates - and\nevaluate our method on large-scale (synthetic and real) datasets containing\nobject-centric image pairs across wide baselines. We observe strong feature\nmatching improvements compared to 2D-only methods, with up to +6% total recall\nand +28% precision at fixed recall. We additionally demonstrate that the\nresulting improved correspondences lead to much higher relative posing accuracy\nfor in-the-wild image pairs, with a more than 8% boost compared to the 2D-only\napproach.\n","authors":["Arjun Karpur","Guilherme Perrotta","Ricardo Martin-Brualla","Howard Zhou","Andre Araujo"],"pdf_url":"https://arxiv.org/pdf/2303.12779v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.06514v3","updated":"2023-03-22T17:46:15Z","published":"2023-02-13T16:49:27Z","title":"Multiple Appropriate Facial Reaction Generation in Dyadic Interaction\n  Settings: What, Why and How?","summary":"  According to the Stimulus Organism Response (SOR) theory, all human\nbehavioral reactions are stimulated by context, where people will process the\nreceived stimulus and produce an appropriate reaction. This implies that in a\nspecific context for a given input stimulus, a person can react differently\naccording to their internal state and other contextual factors. Analogously, in\ndyadic interactions, humans communicate using verbal and nonverbal cues, where\na broad spectrum of listeners' non-verbal reactions might be appropriate for\nresponding to a specific speaker behaviour. There already exists a body of work\nthat investigated the problem of automatically generating an appropriate\nreaction for a given input. However, none attempted to automatically generate\nmultiple appropriate reactions in the context of dyadic interactions and\nevaluate the appropriateness of those reactions using objective measures. This\npaper starts by defining the facial Multiple Appropriate Reaction Generation\n(fMARG) task for the first time in the literature and proposes a new set of\nobjective evaluation metrics to evaluate the appropriateness of the generated\nreactions. The paper subsequently introduces a framework to predict, generate,\nand evaluate multiple appropriate facial reactions.\n","authors":["Siyang Song","Micol Spitale","Yiming Luo","Batuhan Bal","Hatice Gunes"],"pdf_url":"https://arxiv.org/pdf/2302.06514v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12776v1","updated":"2023-03-22T17:42:22Z","published":"2023-03-22T17:42:22Z","title":"Dense Distinct Query for End-to-End Object Detection","summary":"  One-to-one label assignment in object detection has successfully obviated the\nneed for non-maximum suppression (NMS) as postprocessing and makes the pipeline\nend-to-end. However, it triggers a new dilemma as the widely used sparse\nqueries cannot guarantee a high recall, while dense queries inevitably bring\nmore similar queries and encounter optimization difficulties. As both sparse\nand dense queries are problematic, then what are the expected queries in\nend-to-end object detection? This paper shows that the solution should be Dense\nDistinct Queries (DDQ). Concretely, we first lay dense queries like traditional\ndetectors and then select distinct ones for one-to-one assignments. DDQ blends\nthe advantages of traditional and recent end-to-end detectors and significantly\nimproves the performance of various detectors including FCN, R-CNN, and DETRs.\nMost impressively, DDQ-DETR achieves 52.1 AP on MS-COCO dataset within 12\nepochs using a ResNet-50 backbone, outperforming all existing detectors in the\nsame setting. DDQ also shares the benefit of end-to-end detectors in crowded\nscenes and achieves 93.8 AP on CrowdHuman. We hope DDQ can inspire researchers\nto consider the complementarity between traditional methods and end-to-end\ndetectors. The source code can be found at\n\\url{https://github.com/jshilong/DDQ}.\n","authors":["Shilong Zhang","Wang xinjiang","Jiaqi Wang","Jiangmiao Pang","Chengqi Lyu","Wenwei Zhang","Ping Luo","Kai Chen"],"pdf_url":"https://arxiv.org/pdf/2303.12776v1.pdf","comment":"Accepted to CVPR2023. Code has been released at\n  https://github.com/jshilong/DDQ"},{"id":"http://arxiv.org/abs/2303.12766v1","updated":"2023-03-22T17:30:14Z","published":"2023-03-22T17:30:14Z","title":"Spherical Transformer for LiDAR-based 3D Recognition","summary":"  LiDAR-based 3D point cloud recognition has benefited various applications.\nWithout specially considering the LiDAR point distribution, most current\nmethods suffer from information disconnection and limited receptive field,\nespecially for the sparse distant points. In this work, we study the\nvarying-sparsity distribution of LiDAR points and present SphereFormer to\ndirectly aggregate information from dense close points to the sparse distant\nones. We design radial window self-attention that partitions the space into\nmultiple non-overlapping narrow and long windows. It overcomes the\ndisconnection issue and enlarges the receptive field smoothly and dramatically,\nwhich significantly boosts the performance of sparse distant points. Moreover,\nto fit the narrow and long windows, we propose exponential splitting to yield\nfine-grained position encoding and dynamic feature selection to increase model\nrepresentation ability. Notably, our method ranks 1st on both nuScenes and\nSemanticKITTI semantic segmentation benchmarks with 81.9% and 74.8% mIoU,\nrespectively. Also, we achieve the 3rd place on nuScenes object detection\nbenchmark with 72.8% NDS and 68.5% mAP. Code is available at\nhttps://github.com/dvlab-research/SphereFormer.git.\n","authors":["Xin Lai","Yukang Chen","Fanbin Lu","Jianhui Liu","Jiaya Jia"],"pdf_url":"https://arxiv.org/pdf/2303.12766v1.pdf","comment":"Accepted to CVPR 2023. Code is available at\n  https://github.com/dvlab-research/SphereFormer.git"},{"id":"http://arxiv.org/abs/2303.09540v3","updated":"2023-03-22T17:22:35Z","published":"2023-03-16T17:53:24Z","title":"SemDeDup: Data-efficient learning at web-scale through semantic\n  deduplication","summary":"  Progress in machine learning has been driven in large part by massive\nincreases in data. However, large web-scale datasets such as LAION are largely\nuncurated beyond searches for exact duplicates, potentially leaving much\nredundancy. Here, we introduce SemDeDup, a method which leverages embeddings\nfrom pre-trained models to identify and remove semantic duplicates: data pairs\nwhich are semantically similar, but not exactly identical. Removing semantic\nduplicates preserves performance and speeds up learning. Analyzing a subset of\nLAION, we show that SemDeDup can remove 50% of the data with minimal\nperformance loss, effectively halving training time. Moreover, performance\nincreases out of distribution. Also, analyzing language models trained on C4, a\npartially curated dataset, we show that SemDeDup improves over prior approaches\nwhile providing efficiency gains. SemDeDup provides an example of how simple\nways of leveraging quality embeddings can be used to make models learn faster\nwith less data.\n","authors":["Amro Abbas","Kushal Tirumala","Dániel Simig","Surya Ganguli","Ari S. Morcos"],"pdf_url":"https://arxiv.org/pdf/2303.09540v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12760v1","updated":"2023-03-22T17:14:10Z","published":"2023-03-22T17:14:10Z","title":"Uncertainty Aware Active Learning for Reconfiguration of Pre-trained\n  Deep Object-Detection Networks for New Target Domains","summary":"  Object detection is one of the most important and fundamental aspects of\ncomputer vision tasks, which has been broadly utilized in pose estimation,\nobject tracking and instance segmentation models. To obtain training data for\nobject detection model efficiently, many datasets opt to obtain their\nunannotated data in video format and the annotator needs to draw a bounding box\naround each object in the images. Annotating every frame from a video is costly\nand inefficient since many frames contain very similar information for the\nmodel to learn from. How to select the most informative frames from a video to\nannotate has become a highly practical task to solve but attracted little\nattention in research. In this paper, we proposed a novel active learning\nalgorithm for object detection models to tackle this problem. In the proposed\nactive learning algorithm, both classification and localization informativeness\nof unlabelled data are measured and aggregated. Utilizing the temporal\ninformation from video frames, two novel localization informativeness\nmeasurements are proposed. Furthermore, a weight curve is proposed to avoid\nquerying adjacent frames. Proposed active learning algorithm with multiple\nconfigurations was evaluated on the MuPoTS dataset and FootballPD dataset.\n","authors":["Jiaming Na","Varuna De-Silva"],"pdf_url":"https://arxiv.org/pdf/2303.12760v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12756v1","updated":"2023-03-22T17:08:31Z","published":"2023-03-22T17:08:31Z","title":"MaskCon: Masked Contrastive Learning for Coarse-Labelled Dataset","summary":"  Deep learning has achieved great success in recent years with the aid of\nadvanced neural network structures and large-scale human-annotated datasets.\nHowever, it is often costly and difficult to accurately and efficiently\nannotate large-scale datasets, especially for some specialized domains where\nfine-grained labels are required. In this setting, coarse labels are much\neasier to acquire as they do not require expert knowledge. In this work, we\npropose a contrastive learning method, called $\\textbf{Mask}$ed\n$\\textbf{Con}$trastive learning~($\\textbf{MaskCon}$) to address the\nunder-explored problem setting, where we learn with a coarse-labelled dataset\nin order to address a finer labelling problem. More specifically, within the\ncontrastive learning framework, for each sample our method generates\nsoft-labels with the aid of coarse labels against other samples and another\naugmented view of the sample in question. By contrast to self-supervised\ncontrastive learning where only the sample's augmentations are considered hard\npositives, and in supervised contrastive learning where only samples with the\nsame coarse labels are considered hard positives, we propose soft labels based\non sample distances, that are masked by the coarse labels. This allows us to\nutilize both inter-sample relations and coarse labels. We demonstrate that our\nmethod can obtain as special cases many existing state-of-the-art works and\nthat it provides tighter bounds on the generalization error. Experimentally,\nour method achieves significant improvement over the current state-of-the-art\nin various datasets, including CIFAR10, CIFAR100, ImageNet-1K, Standford Online\nProducts and Stanford Cars196 datasets. Code and annotations are available at\nhttps://github.com/MrChenFeng/MaskCon_CVPR2023.\n","authors":["Chen Feng","Ioannis Patras"],"pdf_url":"https://arxiv.org/pdf/2303.12756v1.pdf","comment":"CVPR 2023 camera-ready version. Codes are available at\n  https://github.com/MrChenFeng/MaskCon_CVPR2023"},{"id":"http://arxiv.org/abs/2302.02005v2","updated":"2023-03-22T17:03:51Z","published":"2023-02-03T21:20:58Z","title":"DeepAstroUDA: Semi-Supervised Universal Domain Adaptation for\n  Cross-Survey Galaxy Morphology Classification and Anomaly Detection","summary":"  Artificial intelligence methods show great promise in increasing the quality\nand speed of work with large astronomical datasets, but the high complexity of\nthese methods leads to the extraction of dataset-specific, non-robust features.\nTherefore, such methods do not generalize well across multiple datasets. We\npresent a universal domain adaptation method, \\textit{DeepAstroUDA}, as an\napproach to overcome this challenge. This algorithm performs semi-supervised\ndomain adaptation and can be applied to datasets with different data\ndistributions and class overlaps. Non-overlapping classes can be present in any\nof the two datasets (the labeled source domain, or the unlabeled target\ndomain), and the method can even be used in the presence of unknown classes. We\napply our method to three examples of galaxy morphology classification tasks of\ndifferent complexities ($3$-class and $10$-class problems), with anomaly\ndetection: 1) datasets created after different numbers of observing years from\na single survey (LSST mock data of $1$ and $10$ years of observations); 2) data\nfrom different surveys (SDSS and DECaLS); and 3) data from observing fields\nwith different depths within one survey (wide field and Stripe 82 deep field of\nSDSS). For the first time, we demonstrate the successful use of domain\nadaptation between very discrepant observational datasets.\n\\textit{DeepAstroUDA} is capable of bridging the gap between two astronomical\nsurveys, increasing classification accuracy in both domains (up to $40\\%$ on\nthe unlabeled data), and making model performance consistent across datasets.\nFurthermore, our method also performs well as an anomaly detection algorithm\nand successfully clusters unknown class samples even in the unlabeled target\ndataset.\n","authors":["A. Ćiprijanović","A. Lewis","K. Pedro","S. Madireddy","B. Nord","G. N. Perdue","S. M. Wild"],"pdf_url":"https://arxiv.org/pdf/2302.02005v2.pdf","comment":"Accepted in Machine Learning Science and Technology (MLST); 24 pages,\n  14 figures"},{"id":"http://arxiv.org/abs/2201.11097v2","updated":"2023-03-22T17:02:59Z","published":"2022-01-26T18:06:33Z","title":"Adaptive Instance Distillation for Object Detection in Autonomous\n  Driving","summary":"  In recent years, knowledge distillation (KD) has been widely used to derive\nefficient models. Through imitating a large teacher model, a lightweight\nstudent model can achieve comparable performance with more efficiency. However,\nmost existing knowledge distillation methods are focused on classification\ntasks. Only a limited number of studies have applied knowledge distillation to\nobject detection, especially in time-sensitive autonomous driving scenarios. In\nthis paper, we propose Adaptive Instance Distillation (AID) to selectively\nimpart teacher's knowledge to the student to improve the performance of\nknowledge distillation. Unlike previous KD methods that treat all instances\nequally, our AID can attentively adjust the distillation weights of instances\nbased on the teacher model's prediction loss. We verified the effectiveness of\nour AID method through experiments on the KITTI and the COCO traffic datasets.\nThe results show that our method improves the performance of state-of-the-art\nattention-guided and non-local distillation methods and achieves better\ndistillation results on both single-stage and two-stage detectors. Compared to\nthe baseline, our AID led to an average of 2.7% and 2.1% mAP increases for\nsingle-stage and two-stage detectors, respectively. Furthermore, our AID is\nalso shown to be useful for self-distillation to improve the teacher model's\nperformance.\n","authors":["Qizhen Lan","Qing Tian"],"pdf_url":"https://arxiv.org/pdf/2201.11097v2.pdf","comment":"6 pages, 3 figures"},{"id":"http://arxiv.org/abs/2303.12696v1","updated":"2023-03-22T16:42:26Z","published":"2023-03-22T16:42:26Z","title":"Dense Network Expansion for Class Incremental Learning","summary":"  The problem of class incremental learning (CIL) is considered.\nState-of-the-art approaches use a dynamic architecture based on network\nexpansion (NE), in which a task expert is added per task. While effective from\na computational standpoint, these methods lead to models that grow quickly with\nthe number of tasks. A new NE method, dense network expansion (DNE), is\nproposed to achieve a better trade-off between accuracy and model complexity.\nThis is accomplished by the introduction of dense connections between the\nintermediate layers of the task expert networks, that enable the transfer of\nknowledge from old to new tasks via feature sharing and reusing. This sharing\nis implemented with a cross-task attention mechanism, based on a new task\nattention block (TAB), that fuses information across tasks. Unlike traditional\nattention mechanisms, TAB operates at the level of the feature mixing and is\ndecoupled with spatial attentions. This is shown more effective than a joint\nspatial-and-task attention for CIL. The proposed DNE approach can strictly\nmaintain the feature space of old classes while growing the network and feature\nscale at a much slower rate than previous methods. In result, it outperforms\nthe previous SOTA methods by a margin of 4\\% in terms of accuracy, with similar\nor even smaller model scale.\n","authors":["Zhiyuan Hu","Yunsheng Li","Jiancheng Lyu","Dashan Gao","Nuno Vasconcelos"],"pdf_url":"https://arxiv.org/pdf/2303.12696v1.pdf","comment":"Accepted by CVPR2023"},{"id":"http://arxiv.org/abs/2303.12688v1","updated":"2023-03-22T16:36:10Z","published":"2023-03-22T16:36:10Z","title":"Pix2Video: Video Editing using Image Diffusion","summary":"  Image diffusion models, trained on massive image collections, have emerged as\nthe most versatile image generator model in terms of quality and diversity.\nThey support inverting real images and conditional (e.g., text) generation,\nmaking them attractive for high-quality image editing applications. We\ninvestigate how to use such pre-trained image models for text-guided video\nediting. The critical challenge is to achieve the target edits while still\npreserving the content of the source video. Our method works in two simple\nsteps: first, we use a pre-trained structure-guided (e.g., depth) image\ndiffusion model to perform text-guided edits on an anchor frame; then, in the\nkey step, we progressively propagate the changes to the future frames via\nself-attention feature injection to adapt the core denoising step of the\ndiffusion model. We then consolidate the changes by adjusting the latent code\nfor the frame before continuing the process. Our approach is training-free and\ngeneralizes to a wide range of edits. We demonstrate the effectiveness of the\napproach by extensive experimentation and compare it against four different\nprior and parallel efforts (on ArXiv). We demonstrate that realistic\ntext-guided video edits are possible, without any compute-intensive\npreprocessing or video-specific finetuning.\n","authors":["Duygu Ceylan","Chun-Hao Paul Huang","Niloy J. Mitra"],"pdf_url":"https://arxiv.org/pdf/2303.12688v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09799v2","updated":"2023-03-22T16:34:57Z","published":"2023-03-17T07:02:59Z","title":"Style Transfer for 2D Talking Head Animation","summary":"  Audio-driven talking head animation is a challenging research topic with many\nreal-world applications. Recent works have focused on creating photo-realistic\n2D animation, while learning different talking or singing styles remains an\nopen problem. In this paper, we present a new method to generate talking head\nanimation with learnable style references. Given a set of style reference\nframes, our framework can reconstruct 2D talking head animation based on a\nsingle input image and an audio stream. Our method first produces facial\nlandmarks motion from the audio stream and constructs the intermediate style\npatterns from the style reference images. We then feed both outputs into a\nstyle-aware image generator to generate the photo-realistic and fidelity 2D\nanimation. In practice, our framework can extract the style information of a\nspecific character and transfer it to any new static image for talking head\nanimation. The intensive experimental results show that our method achieves\nbetter results than recent state-of-the-art approaches qualitatively and\nquantitatively.\n","authors":["Trong-Thang Pham","Nhat Le","Tuong Do","Hung Nguyen","Erman Tjiputra","Quang D. Tran","Anh Nguyen"],"pdf_url":"https://arxiv.org/pdf/2303.09799v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12678v1","updated":"2023-03-22T16:21:44Z","published":"2023-03-22T16:21:44Z","title":"Uni-Fusion: Universal Continuous Mapping","summary":"  We introduce Uni-Fusion, an universal continuous mapping framework for\nsurfaces, surface properties (color, infrared, etc.) and more (latent features\nin CLIP embedding space, etc.). We propose the first Universal Implicit\nEncoding model that supports encoding of both geometry and various types of\nproperties (RGB, infrared, feature and etc.) without the need for any training.\nBased on that, our framework divides the point cloud into regular grid voxels\nand produces a latent feature in each voxel to form a Latent Implicit Map (LIM)\nfor geometries and arbitrary properties. Then, by fusing a Local LIM of new\nframe to Global LIM, an incremental reconstruction is approached. Encoded with\ncorresponding types of data, our Latent Implicit Map is capable to generate\ncontinuous surfaces, surface properties fields, surface feature fields and any\nother possible options. To demonstrate the capabilities of our model, we\nimplement three applications: (1) incremental reconstruction for surfaces and\ncolor (2) 2D-to-3D fabricated properties transfers (3) open-vocabulary scene\nunderstanding by producing a text CLIP feature field on surfaces. We evaluate\nUni-Fusion by comparing in corresponding applications, from which, Uni-Fusion\nshows high flexibility to various of application while performing best or\ncompetitive. The project page of Uni-Fusion is available at\nhttps://jarrome.github.io/Uni-Fusion/\n","authors":["Yijun Yuan","Andreas Nuechter"],"pdf_url":"https://arxiv.org/pdf/2303.12678v1.pdf","comment":"Project page: https://jarrome.github.io/Uni-Fusion/"},{"id":"http://arxiv.org/abs/2303.12675v1","updated":"2023-03-22T16:14:39Z","published":"2023-03-22T16:14:39Z","title":"VecFontSDF: Learning to Reconstruct and Synthesize High-quality Vector\n  Fonts via Signed Distance Functions","summary":"  Font design is of vital importance in the digital content design and modern\nprinting industry. Developing algorithms capable of automatically synthesizing\nvector fonts can significantly facilitate the font design process. However,\nexisting methods mainly concentrate on raster image generation, and only a few\napproaches can directly synthesize vector fonts. This paper proposes an\nend-to-end trainable method, VecFontSDF, to reconstruct and synthesize\nhigh-quality vector fonts using signed distance functions (SDFs). Specifically,\nbased on the proposed SDF-based implicit shape representation, VecFontSDF\nlearns to model each glyph as shape primitives enclosed by several parabolic\ncurves, which can be precisely converted to quadratic B\\'ezier curves that are\nwidely used in vector font products. In this manner, most image generation\nmethods can be easily extended to synthesize vector fonts. Qualitative and\nquantitative experiments conducted on a publicly-available dataset demonstrate\nthat our method obtains high-quality results on several tasks, including vector\nfont reconstruction, interpolation, and few-shot vector font synthesis,\nmarkedly outperforming the state of the art.\n","authors":["Zeqing Xia","Bojun Xiong","Zhouhui Lian"],"pdf_url":"https://arxiv.org/pdf/2303.12675v1.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2303.12671v1","updated":"2023-03-22T15:49:33Z","published":"2023-03-22T15:49:33Z","title":"Integrating Image Features with Convolutional Sequence-to-sequence\n  Network for Multilingual Visual Question Answering","summary":"  Visual Question Answering (VQA) is a task that requires computers to give\ncorrect answers for the input questions based on the images. This task can be\nsolved by humans with ease but is a challenge for computers. The\nVLSP2022-EVJVQA shared task carries the Visual Question Answering task in the\nmultilingual domain on a newly released dataset: UIT-EVJVQA, in which the\nquestions and answers are written in three different languages: English,\nVietnamese and Japanese. We approached the challenge as a sequence-to-sequence\nlearning task, in which we integrated hints from pre-trained state-of-the-art\nVQA models and image features with Convolutional Sequence-to-Sequence network\nto generate the desired answers. Our results obtained up to 0.3442 by F1 score\non the public test set, 0.4210 on the private test set, and placed 3rd in the\ncompetition.\n","authors":["Triet Minh Thai","Son T. Luu"],"pdf_url":"https://arxiv.org/pdf/2303.12671v1.pdf","comment":"VLSP2022-EVJVQA"},{"id":"http://arxiv.org/abs/2303.12670v1","updated":"2023-03-22T15:48:23Z","published":"2023-03-22T15:48:23Z","title":"Correlational Image Modeling for Self-Supervised Visual Pre-Training","summary":"  We introduce Correlational Image Modeling (CIM), a novel and surprisingly\neffective approach to self-supervised visual pre-training. Our CIM performs a\nsimple pretext task: we randomly crop image regions (exemplars) from an input\nimage (context) and predict correlation maps between the exemplars and the\ncontext. Three key designs enable correlational image modeling as a nontrivial\nand meaningful self-supervisory task. First, to generate useful\nexemplar-context pairs, we consider cropping image regions with various scales,\nshapes, rotations, and transformations. Second, we employ a bootstrap learning\nframework that involves online and target encoders. During pre-training, the\nformer takes exemplars as inputs while the latter converts the context. Third,\nwe model the output correlation maps via a simple cross-attention block, within\nwhich the context serves as queries and the exemplars offer values and keys. We\nshow that CIM performs on par or better than the current state of the art on\nself-supervised and transfer benchmarks.\n","authors":["Wei Li","Jiahao Xie","Chen Change Loy"],"pdf_url":"https://arxiv.org/pdf/2303.12670v1.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2303.12669v1","updated":"2023-03-22T15:47:16Z","published":"2023-03-22T15:47:16Z","title":"An Extended Study of Human-like Behavior under Adversarial Training","summary":"  Neural networks have a number of shortcomings. Amongst the severest ones is\nthe sensitivity to distribution shifts which allows models to be easily fooled\ninto wrong predictions by small perturbations to inputs that are often\nimperceivable to humans and do not have to carry semantic meaning. Adversarial\ntraining poses a partial solution to address this issue by training models on\nworst-case perturbations. Yet, recent work has also pointed out that the\nreasoning in neural networks is different from humans. Humans identify objects\nby shape, while neural nets mainly employ texture cues. Exemplarily, a model\ntrained on photographs will likely fail to generalize to datasets containing\nsketches. Interestingly, it was also shown that adversarial training seems to\nfavorably increase the shift toward shape bias. In this work, we revisit this\nobservation and provide an extensive analysis of this effect on various\narchitectures, the common $\\ell_2$- and $\\ell_\\infty$-training, and\nTransformer-based models. Further, we provide a possible explanation for this\nphenomenon from a frequency perspective.\n","authors":["Paul Gavrikov","Janis Keuper","Margret Keuper"],"pdf_url":"https://arxiv.org/pdf/2303.12669v1.pdf","comment":"6 pages, accepted at the CVPR 2023 Workshop \"The 3rd Workshop of\n  Adversarial Machine Learning on Computer Vision: Art of Robustness\""},{"id":"http://arxiv.org/abs/2205.00363v3","updated":"2023-03-22T15:42:50Z","published":"2022-04-30T23:03:49Z","title":"Visual Spatial Reasoning","summary":"  Spatial relations are a basic part of human cognition. However, they are\nexpressed in natural language in a variety of ways, and previous work has\nsuggested that current vision-and-language models (VLMs) struggle to capture\nrelational information. In this paper, we present Visual Spatial Reasoning\n(VSR), a dataset containing more than 10k natural text-image pairs with 66\ntypes of spatial relations in English (such as: under, in front of, and\nfacing). While using a seemingly simple annotation format, we show how the\ndataset includes challenging linguistic phenomena, such as varying reference\nframes. We demonstrate a large gap between human and model performance: the\nhuman ceiling is above 95%, while state-of-the-art models only achieve around\n70%. We observe that VLMs' by-relation performances have little correlation\nwith the number of training examples and the tested models are in general\nincapable of recognising relations concerning the orientations of objects.\n","authors":["Fangyu Liu","Guy Emerson","Nigel Collier"],"pdf_url":"https://arxiv.org/pdf/2205.00363v3.pdf","comment":"TACL camera-ready version; code and data available at\n  https://github.com/cambridgeltl/visual-spatial-reasoning"},{"id":"http://arxiv.org/abs/2303.12658v1","updated":"2023-03-22T15:36:19Z","published":"2023-03-22T15:36:19Z","title":"Reliable and Efficient Evaluation of Adversarial Robustness for Deep\n  Hashing-Based Retrieval","summary":"  Deep hashing has been extensively applied to massive image retrieval due to\nits efficiency and effectiveness. Recently, several adversarial attacks have\nbeen presented to reveal the vulnerability of deep hashing models against\nadversarial examples. However, existing attack methods suffer from degraded\nperformance or inefficiency because they underutilize the semantic relations\nbetween original samples or spend a lot of time learning these relations with a\ndeep neural network. In this paper, we propose a novel Pharos-guided Attack,\ndubbed PgA, to evaluate the adversarial robustness of deep hashing networks\nreliably and efficiently. Specifically, we design pharos code to represent the\nsemantics of the benign image, which preserves the similarity to semantically\nrelevant samples and dissimilarity to irrelevant ones. It is proven that we can\nquickly calculate the pharos code via a simple math formula. Accordingly, PgA\ncan directly conduct a reliable and efficient attack on deep hashing-based\nretrieval by maximizing the similarity between the hash code of the adversarial\nexample and the pharos code. Extensive experiments on the benchmark datasets\nverify that the proposed algorithm outperforms the prior state-of-the-arts in\nboth attack strength and speed.\n","authors":["Xunguang Wang","Jiawang Bai","Xinyue Xu","Xiaomeng Li"],"pdf_url":"https://arxiv.org/pdf/2303.12658v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2204.10779"},{"id":"http://arxiv.org/abs/2303.12649v1","updated":"2023-03-22T15:30:44Z","published":"2023-03-22T15:30:44Z","title":"MI-SegNet: Mutual Information-Based US Segmentation for Unseen Domain\n  Generalization","summary":"  Generalization capabilities of learning-based medical image segmentation\nacross domains are currently limited by the performance degradation caused by\nthe domain shift, particularly for ultrasound (US) imaging. The quality of US\nimages heavily relies on carefully tuned acoustic parameters, which vary across\nsonographers, machines, and settings. To improve the generalizability on US\nimages across domains, we propose MI-SegNet, a novel mutual information (MI)\nbased framework to explicitly disentangle the anatomical and domain feature\nrepresentations; therefore, robust domain-independent segmentation can be\nexpected. Two encoders are employed to extract the relevant features for the\ndisentanglement. The segmentation only uses the anatomical feature map for its\nprediction. In order to force the encoders to learn meaningful feature\nrepresentations a cross-reconstruction method is used during training.\nTransformations, specific to either domain or anatomy are applied to guide the\nencoders in their respective feature extraction task. Additionally, any MI\npresent in both feature maps is punished to further promote separate feature\nspaces. We validate the generalizability of the proposed domain-independent\nsegmentation approach on several datasets with varying parameters and machines.\nFurthermore, we demonstrate the effectiveness of the proposed MI-SegNet serving\nas a pre-trained model by comparing it with state-of-the-art networks.\n","authors":["Yuan Bi","Zhongliang Jiang","Ricarda Clarenbach","Reza Ghotbi","Angelos Karlas","Nassir Navab"],"pdf_url":"https://arxiv.org/pdf/2303.12649v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12644v1","updated":"2023-03-22T15:26:22Z","published":"2023-03-22T15:26:22Z","title":"Feature-Conditioned Cascaded Video Diffusion Models for Precise\n  Echocardiogram Synthesis","summary":"  Image synthesis is expected to provide value for the translation of machine\nlearning methods into clinical practice. Fundamental problems like model\nrobustness, domain transfer, causal modelling, and operator training become\napproachable through synthetic data. Especially, heavily operator-dependant\nmodalities like Ultrasound imaging require robust frameworks for image and\nvideo generation. So far, video generation has only been possible by providing\ninput data that is as rich as the output data, e.g., image sequence plus\nconditioning in, video out. However, clinical documentation is usually scarce\nand only single images are reported and stored, thus retrospective\npatient-specific analysis or the generation of rich training data becomes\nimpossible with current approaches. In this paper, we extend elucidated\ndiffusion models for video modelling to generate plausible video sequences from\nsingle images and arbitrary conditioning with clinical parameters. We explore\nthis idea within the context of echocardiograms by looking into the variation\nof the Left Ventricle Ejection Fraction, the most essential clinical metric\ngained from these examinations. We use the publicly available EchoNet-Dynamic\ndataset for all our experiments. Our image to sequence approach achieves an R2\nscore of 93%, which is 38 points higher than recently proposed sequence to\nsequence generation methods. A public demo is available here: bit.ly/3HTskPF.\nCode and models will be available at:\nhttps://github.com/HReynaud/EchoDiffusion.\n","authors":["Hadrien Reynaud","Mengyun Qiao","Mischa Dombrowski","Thomas Day","Reza Razavi","Alberto Gomez","Paul Leeson","Bernhard Kainz"],"pdf_url":"https://arxiv.org/pdf/2303.12644v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2303.12641v1","updated":"2023-03-22T15:23:09Z","published":"2023-03-22T15:23:09Z","title":"Reveal to Revise: An Explainable AI Life Cycle for Iterative Bias\n  Correction of Deep Models","summary":"  State-of-the-art machine learning models often learn spurious correlations\nembedded in the training data. This poses risks when deploying these models for\nhigh-stake decision-making, such as in medical applications like skin cancer\ndetection. To tackle this problem, we propose Reveal to Revise (R2R), a\nframework entailing the entire eXplainable Artificial Intelligence (XAI) life\ncycle, enabling practitioners to iteratively identify, mitigate, and\n(re-)evaluate spurious model behavior with a minimal amount of human\ninteraction. In the first step (1), R2R reveals model weaknesses by finding\noutliers in attributions or through inspection of latent concepts learned by\nthe model. Secondly (2), the responsible artifacts are detected and spatially\nlocalized in the input data, which is then leveraged to (3) revise the model\nbehavior. Concretely, we apply the methods of RRR, CDEP and ClArC for model\ncorrection, and (4) (re-)evaluate the model's performance and remaining\nsensitivity towards the artifact. Using two medical benchmark datasets for\nMelanoma detection and bone age estimation, we apply our R2R framework to VGG,\nResNet and EfficientNet architectures and thereby reveal and correct real\ndataset-intrinsic artifacts, as well as synthetic variants in a controlled\nsetting. Completing the XAI life cycle, we demonstrate multiple R2R iterations\nto mitigate different biases. Code is available on\nhttps://github.com/maxdreyer/Reveal2Revise.\n","authors":["Frederik Pahde","Maximilian Dreyer","Wojciech Samek","Sebastian Lapuschkin"],"pdf_url":"https://arxiv.org/pdf/2303.12641v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2102.05368v2","updated":"2023-03-22T15:21:10Z","published":"2021-02-10T10:13:39Z","title":"RoBIC: A benchmark suite for assessing classifiers robustness","summary":"  Many defenses have emerged with the development of adversarial attacks.\nModels must be objectively evaluated accordingly. This paper systematically\ntackles this concern by proposing a new parameter-free benchmark we coin RoBIC.\nRoBIC fairly evaluates the robustness of image classifiers using a new\nhalf-distortion measure. It gauges the robustness of the network against white\nand black box attacks, independently of its accuracy. RoBIC is faster than the\nother available benchmarks. We present the significant differences in the\nrobustness of 16 recent models as assessed by RoBIC.\n","authors":["Thibault Maho","Benoît Bonnet","Teddy Furon","Erwan Le Merrer"],"pdf_url":"https://arxiv.org/pdf/2102.05368v2.pdf","comment":"4 pages, accepted to ICIP 2021"},{"id":"http://arxiv.org/abs/2303.12621v1","updated":"2023-03-22T15:01:20Z","published":"2023-03-22T15:01:20Z","title":"OcTr: Octree-based Transformer for 3D Object Detection","summary":"  A key challenge for LiDAR-based 3D object detection is to capture sufficient\nfeatures from large scale 3D scenes especially for distant or/and occluded\nobjects. Albeit recent efforts made by Transformers with the long sequence\nmodeling capability, they fail to properly balance the accuracy and efficiency,\nsuffering from inadequate receptive fields or coarse-grained holistic\ncorrelations. In this paper, we propose an Octree-based Transformer, named\nOcTr, to address this issue. It first constructs a dynamic octree on the\nhierarchical feature pyramid through conducting self-attention on the top level\nand then recursively propagates to the level below restricted by the octants,\nwhich captures rich global context in a coarse-to-fine manner while maintaining\nthe computational complexity under control. Furthermore, for enhanced\nforeground perception, we propose a hybrid positional embedding, composed of\nthe semantic-aware positional embedding and attention mask, to fully exploit\nsemantic and geometry clues. Extensive experiments are conducted on the Waymo\nOpen Dataset and KITTI Dataset, and OcTr reaches newly state-of-the-art\nresults.\n","authors":["Chao Zhou","Yanan Zhang","Jiaxin Chen","Di Huang"],"pdf_url":"https://arxiv.org/pdf/2303.12621v1.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2303.12618v1","updated":"2023-03-22T14:59:49Z","published":"2023-03-22T14:59:49Z","title":"A Perceptual Quality Assessment Exploration for AIGC Images","summary":"  \\underline{AI} \\underline{G}enerated \\underline{C}ontent (\\textbf{AIGC}) has\ngained widespread attention with the increasing efficiency of deep learning in\ncontent creation. AIGC, created with the assistance of artificial intelligence\ntechnology, includes various forms of content, among which the AI-generated\nimages (AGIs) have brought significant impact to society and have been applied\nto various fields such as entertainment, education, social media, etc. However,\ndue to hardware limitations and technical proficiency, the quality of AIGC\nimages (AGIs) varies, necessitating refinement and filtering before practical\nuse. Consequently, there is an urgent need for developing objective models to\nassess the quality of AGIs. Unfortunately, no research has been carried out to\ninvestigate the perceptual quality assessment for AGIs specifically. Therefore,\nin this paper, we first discuss the major evaluation aspects such as technical\nissues, AI artifacts, unnaturalness, discrepancy, and aesthetics for AGI\nquality assessment. Then we present the first perceptual AGI quality assessment\ndatabase, AGIQA-1K, which consists of 1,080 AGIs generated from diffusion\nmodels. A well-organized subjective experiment is followed to collect the\nquality labels of the AGIs. Finally, we conduct a benchmark experiment to\nevaluate the performance of current image quality assessment (IQA) models.\n","authors":["Zicheng Zhang","Chunyi Li","Wei Sun","Xiaohong Liu","Xiongkuo Min","Guangtao Zhai"],"pdf_url":"https://arxiv.org/pdf/2303.12618v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.10779v5","updated":"2023-03-22T14:57:41Z","published":"2022-04-18T04:51:08Z","title":"CgAT: Center-Guided Adversarial Training for Deep Hashing-Based\n  Retrieval","summary":"  Deep hashing has been extensively utilized in massive image retrieval because\nof its efficiency and effectiveness. However, deep hashing models are\nvulnerable to adversarial examples, making it essential to develop adversarial\ndefense methods for image retrieval. Existing solutions achieved limited\ndefense performance because of using weak adversarial samples for training and\nlacking discriminative optimization objectives to learn robust features. In\nthis paper, we present a min-max based Center-guided Adversarial Training,\nnamely CgAT, to improve the robustness of deep hashing networks through worst\nadversarial examples. Specifically, we first formulate the center code as a\nsemantically-discriminative representative of the input image content, which\npreserves the semantic similarity with positive samples and dissimilarity with\nnegative examples. We prove that a mathematical formula can calculate the\ncenter code immediately. After obtaining the center codes in each optimization\niteration of the deep hashing network, they are adopted to guide the\nadversarial training process. On the one hand, CgAT generates the worst\nadversarial examples as augmented data by maximizing the Hamming distance\nbetween the hash codes of the adversarial examples and the center codes. On the\nother hand, CgAT learns to mitigate the effects of adversarial samples by\nminimizing the Hamming distance to the center codes. Extensive experiments on\nthe benchmark datasets demonstrate the effectiveness of our adversarial\ntraining algorithm in defending against adversarial attacks for deep\nhashing-based retrieval. Compared with the current state-of-the-art defense\nmethod, we significantly improve the defense performance by an average of\n18.61\\%, 12.35\\%, and 11.56\\% on FLICKR-25K, NUS-WIDE, and MS-COCO,\nrespectively. The code is available at https://github.com/xunguangwang/CgAT.\n","authors":["Xunguang Wang","Yiqun Lin","Xiaomeng Li"],"pdf_url":"https://arxiv.org/pdf/2204.10779v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12615v1","updated":"2023-03-22T14:56:51Z","published":"2023-03-22T14:56:51Z","title":"Multi-view Feature Extraction based on Triple Contrastive Heads","summary":"  Multi-view feature extraction is an efficient approach for alleviating the\nissue of dimensionality in highdimensional multi-view data. Contrastive\nlearning (CL), which is a popular self-supervised learning method, has recently\nattracted considerable attention. In this study, we propose a novel multi-view\nfeature extraction method based on triple contrastive heads, which combines the\nsample-, recovery- , and feature-level contrastive losses to extract the\nsufficient yet minimal subspace discriminative information in compliance with\ninformation bottleneck principle. In MFETCH, we construct the feature-level\ncontrastive loss, which removes the redundent information in the consistency\ninformation to achieve the minimality of the subspace discriminative\ninformation. Moreover, the recovery-level contrastive loss is also constructed\nin MFETCH, which captures the view-specific discriminative information to\nachieve the sufficiency of the subspace discriminative information.The\nnumerical experiments demonstrate that the proposed method offers a strong\nadvantage for multi-view feature extraction.\n","authors":["Hongjie Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.12615v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2302.03932"},{"id":"http://arxiv.org/abs/2211.11277v3","updated":"2023-03-22T14:20:14Z","published":"2022-11-21T09:13:53Z","title":"DrapeNet: Garment Generation and Self-Supervised Draping","summary":"  Recent approaches to drape garments quickly over arbitrary human bodies\nleverage self-supervision to eliminate the need for large training sets.\nHowever, they are designed to train one network per clothing item, which\nseverely limits their generalization abilities. In our work, we rely on\nself-supervision to train a single network to drape multiple garments. This is\nachieved by predicting a 3D deformation field conditioned on the latent codes\nof a generative network, which models garments as unsigned distance fields. Our\npipeline can generate and drape previously unseen garments of any topology,\nwhose shape can be edited by manipulating their latent codes. Being fully\ndifferentiable, our formulation makes it possible to recover accurate 3D models\nof garments from partial observations -- images or 3D scans -- via gradient\ndescent. Our code is publicly available at\nhttps://github.com/liren2515/DrapeNet .\n","authors":["Luca De Luigi","Ren Li","Benoît Guillard","Mathieu Salzmann","Pascal Fua"],"pdf_url":"https://arxiv.org/pdf/2211.11277v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11331v2","updated":"2023-03-22T14:10:37Z","published":"2023-03-20T17:59:59Z","title":"EVA-02: A Visual Representation for Neon Genesis","summary":"  We launch EVA-02, a next-generation Transformer-based visual representation\npre-trained to reconstruct strong and robust language-aligned vision features\nvia masked image modeling. With an updated plain Transformer architecture as\nwell as extensive pre-training from an open & accessible giant CLIP vision\nencoder, EVA-02 demonstrates superior performance compared to prior\nstate-of-the-art approaches across various representative vision tasks, while\nutilizing significantly fewer parameters and compute budgets. Notably, using\nexclusively publicly accessible training data, EVA-02 with only 304M parameters\nachieves a phenomenal 90.0 fine-tuning top-1 accuracy on ImageNet-1K val set.\nAdditionally, our EVA-02-CLIP can reach up to 80.4 zero-shot top-1 on\nImageNet-1K, outperforming the previous largest & best open-sourced CLIP with\nonly ~1/6 parameters and ~1/6 image-text training data. We offer four EVA-02\nvariants in various model sizes, ranging from 6M to 304M parameters, all with\nimpressive performance. To facilitate open access and open research, we release\nthe complete suite of EVA-02 to the community at\nhttps://github.com/baaivision/EVA/tree/master/EVA-02.\n","authors":["Yuxin Fang","Quan Sun","Xinggang Wang","Tiejun Huang","Xinlong Wang","Yue Cao"],"pdf_url":"https://arxiv.org/pdf/2303.11331v2.pdf","comment":"v2: Fix some known issues & typos. v1: To Asuka. Code & Models:\n  https://github.com/baaivision/EVA/tree/master/EVA-02"},{"id":"http://arxiv.org/abs/2211.05568v3","updated":"2023-03-22T14:08:17Z","published":"2022-11-10T13:44:57Z","title":"Unbiased Supervised Contrastive Learning","summary":"  Many datasets are biased, namely they contain easy-to-learn features that are\nhighly correlated with the target class only in the dataset but not in the true\nunderlying distribution of the data. For this reason, learning unbiased models\nfrom biased data has become a very relevant research topic in the last years.\nIn this work, we tackle the problem of learning representations that are robust\nto biases. We first present a margin-based theoretical framework that allows us\nto clarify why recent contrastive losses (InfoNCE, SupCon, etc.) can fail when\ndealing with biased data. Based on that, we derive a novel formulation of the\nsupervised contrastive loss (epsilon-SupInfoNCE), providing more accurate\ncontrol of the minimal distance between positive and negative samples.\nFurthermore, thanks to our theoretical framework, we also propose FairKL, a new\ndebiasing regularization loss, that works well even with extremely biased data.\nWe validate the proposed losses on standard vision datasets including CIFAR10,\nCIFAR100, and ImageNet, and we assess the debiasing capability of FairKL with\nepsilon-SupInfoNCE, reaching state-of-the-art performance on a number of biased\ndatasets, including real instances of biases in the wild.\n","authors":["Carlo Alberto Barbano","Benoit Dufumier","Enzo Tartaglione","Marco Grangetto","Pietro Gori"],"pdf_url":"https://arxiv.org/pdf/2211.05568v3.pdf","comment":"Accepted at ICLR 2023"},{"id":"http://arxiv.org/abs/2303.09998v2","updated":"2023-03-22T13:58:12Z","published":"2023-03-17T14:20:28Z","title":"TBP-Former: Learning Temporal Bird's-Eye-View Pyramid for Joint\n  Perception and Prediction in Vision-Centric Autonomous Driving","summary":"  Vision-centric joint perception and prediction (PnP) has become an emerging\ntrend in autonomous driving research. It predicts the future states of the\ntraffic participants in the surrounding environment from raw RGB images.\nHowever, it is still a critical challenge to synchronize features obtained at\nmultiple camera views and timestamps due to inevitable geometric distortions\nand further exploit those spatial-temporal features. To address this issue, we\npropose a temporal bird's-eye-view pyramid transformer (TBP-Former) for\nvision-centric PnP, which includes two novel designs. First, a\npose-synchronized BEV encoder is proposed to map raw image inputs with any\ncamera pose at any time to a shared and synchronized BEV space for better\nspatial-temporal synchronization. Second, a spatial-temporal pyramid\ntransformer is introduced to comprehensively extract multi-scale BEV features\nand predict future BEV states with the support of spatial-temporal priors.\nExtensive experiments on nuScenes dataset show that our proposed framework\noverall outperforms all state-of-the-art vision-based prediction methods.\n","authors":["Shaoheng Fang","Zi Wang","Yiqi Zhong","Junhao Ge","Siheng Chen","Yanfeng Wang"],"pdf_url":"https://arxiv.org/pdf/2303.09998v2.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.12564v1","updated":"2023-03-22T13:46:15Z","published":"2023-03-22T13:46:15Z","title":"RaBit: Parametric Modeling of 3D Biped Cartoon Characters with a\n  Topological-consistent Dataset","summary":"  Assisting people in efficiently producing visually plausible 3D characters\nhas always been a fundamental research topic in computer vision and computer\ngraphics. Recent learning-based approaches have achieved unprecedented accuracy\nand efficiency in the area of 3D real human digitization. However, none of the\nprior works focus on modeling 3D biped cartoon characters, which are also in\ngreat demand in gaming and filming. In this paper, we introduce 3DBiCar, the\nfirst large-scale dataset of 3D biped cartoon characters, and RaBit, the\ncorresponding parametric model. Our dataset contains 1,500 topologically\nconsistent high-quality 3D textured models which are manually crafted by\nprofessional artists. Built upon the data, RaBit is thus designed with a\nSMPL-like linear blend shape model and a StyleGAN-based neural UV-texture\ngenerator, simultaneously expressing the shape, pose, and texture. To\ndemonstrate the practicality of 3DBiCar and RaBit, various applications are\nconducted, including single-view reconstruction, sketch-based modeling, and 3D\ncartoon animation. For the single-view reconstruction setting, we find a\nstraightforward global mapping from input images to the output UV-based texture\nmaps tends to lose detailed appearances of some local parts (e.g., nose, ears).\nThus, a part-sensitive texture reasoner is adopted to make all important local\nareas perceived. Experiments further demonstrate the effectiveness of our\nmethod both qualitatively and quantitatively. 3DBiCar and RaBit are available\nat gaplab.cuhk.edu.cn/projects/RaBit.\n","authors":["Zhongjin Luo","Shengcai Cai","Jinguo Dong","Ruibo Ming","Liangdong Qiu","Xiaohang Zhan","Xiaoguang Han"],"pdf_url":"https://arxiv.org/pdf/2303.12564v1.pdf","comment":"CVPR 2023, Project page: https://gaplab.cuhk.edu.cn/projects/RaBit/"},{"id":"http://arxiv.org/abs/2303.12557v1","updated":"2023-03-22T13:41:22Z","published":"2023-03-22T13:41:22Z","title":"Q-HyViT: Post-Training Quantization for Hybrid Vision Transformer with\n  Bridge Block Reconstruction","summary":"  Recently, vision transformers (ViT) have replaced convolutional neural\nnetwork models in numerous tasks, including classification, detection, and\nsegmentation. However, the high computational requirements of ViTs hinder their\nwidespread implementation. To address this issue, researchers have proposed\nefficient hybrid transformer architectures that combine convolutional and\ntransformer layers and optimize attention computation for linear complexity.\nAdditionally, post-training quantization has been proposed as a means of\nmitigating computational demands. Combining quantization techniques and\nefficient hybrid transformer structures is crucial to maximize the acceleration\nof vision transformers on mobile devices. However, no prior investigation has\napplied quantization to efficient hybrid transformers. In this paper, at first,\nwe discover that the straightforward manner to apply the existing PTQ methods\nfor ViT to efficient hybrid transformers results in a drastic accuracy drop due\nto the following challenges: (i) highly dynamic ranges, (ii) zero-point\noverflow, (iii) diverse normalization, and (iv) limited model parameters (<5M).\nTo overcome these challenges, we propose a new post-training quantization\nmethod, which is the first to quantize efficient hybrid vision transformers\n(MobileViTv1 and MobileViTv2) with a significant margin (an average improvement\nof 7.75%) compared to existing PTQ methods (EasyQuant, FQ-ViT, and PTQ4ViT). We\nplan to release our code at https://github.com/Q-HyViT.\n","authors":["Jemin Lee","Yongin Kwon","Jeman Park","Misun Yu","Hwanjun Song"],"pdf_url":"https://arxiv.org/pdf/2303.12557v1.pdf","comment":"12 pages, 9 figures"},{"id":"http://arxiv.org/abs/2108.02235v3","updated":"2023-03-22T13:26:59Z","published":"2021-08-04T18:29:42Z","title":"Dynamic Relevance Learning for Few-Shot Object Detection","summary":"  Expensive bounding-box annotations have limited the development of object\ndetection task. Thus, it is necessary to focus on more challenging task of\nfew-shot object detection. It requires the detector to recognize objects of\nnovel classes with only a few training samples. Nowadays, many existing popular\nmethods adopting training way similar to meta-learning have achieved promising\nperformance, such as Meta R-CNN series. However, support data is only used as\nthe class attention to guide the detecting of query images each time. Their\nrelevance to each other remains unexploited. Moreover, a lot of recent works\ntreat the support data and query images as independent branch without\nconsidering the relationship between them. To address this issue, we propose a\ndynamic relevance learning model, which utilizes the relationship between all\nsupport images and Region of Interest (RoI) on the query images to construct a\ndynamic graph convolutional network (GCN). By adjusting the prediction\ndistribution of the base detector using the output of this GCN, the proposed\nmodel serves as a hard auxiliary classification task, which guides the detector\nto improve the class representation implicitly. Comprehensive experiments have\nbeen conducted on Pascal VOC and MS-COCO dataset. The proposed model achieves\nthe best overall performance, which shows its effectiveness of learning more\ngeneralized features. Our code is available at\nhttps://github.com/liuweijie19980216/DRL-for-FSOD.\n","authors":["Weijie Liu","Chong Wang","Haohe Li","Shenghao Yu","Jiafei Wu"],"pdf_url":"https://arxiv.org/pdf/2108.02235v3.pdf","comment":"12 pages, 8 figures, 7 tables"},{"id":"http://arxiv.org/abs/2303.12540v1","updated":"2023-03-22T13:16:37Z","published":"2023-03-22T13:16:37Z","title":"Deployment of Image Analysis Algorithms under Prevalence Shifts","summary":"  Domain gaps are among the most relevant roadblocks in the clinical\ntranslation of machine learning (ML)-based solutions for medical image\nanalysis. While current research focuses on new training paradigms and network\narchitectures, little attention is given to the specific effect of prevalence\nshifts on an algorithm deployed in practice. Such discrepancies between class\nfrequencies in the data used for a method's development/validation and that in\nits deployment environment(s) are of great importance, for example in the\ncontext of artificial intelligence (AI) democratization, as disease prevalences\nmay vary widely across time and location. Our contribution is twofold. First,\nwe empirically demonstrate the potentially severe consequences of missing\nprevalence handling by analyzing (i) the extent of miscalibration, (ii) the\ndeviation of the decision threshold from the optimum, and (iii) the ability of\nvalidation metrics to reflect neural network performance on the deployment\npopulation as a function of the discrepancy between development and deployment\nprevalence. Second, we propose a workflow for prevalence-aware image\nclassification that uses estimated deployment prevalences to adjust a trained\nclassifier to a new environment, without requiring additional annotated\ndeployment data. Comprehensive experiments based on a diverse set of 30 medical\nclassification tasks showcase the benefit of the proposed workflow in\ngenerating better classifier decisions and more reliable performance estimates\ncompared to current practice.\n","authors":["Patrick Godau","Piotr Kalinowski","Evangelia Christodoulou","Annika Reinke","Minu Tizabi","Luciana Ferrer","Paul Jäger","Lena Maier-Hein"],"pdf_url":"https://arxiv.org/pdf/2303.12540v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12533v1","updated":"2023-03-22T13:06:39Z","published":"2023-03-22T13:06:39Z","title":"Pixel-wise Agricultural Image Time Series Classification: Comparisons\n  and a Deformable Prototype-based Approach","summary":"  Improvements in Earth observation by satellites allow for imagery of ever\nhigher temporal and spatial resolution. Leveraging this data for agricultural\nmonitoring is key for addressing environmental and economic challenges. Current\nmethods for crop segmentation using temporal data either rely on annotated data\nor are heavily engineered to compensate the lack of supervision. In this paper,\nwe present and compare datasets and methods for both supervised and\nunsupervised pixel-wise segmentation of satellite image time series (SITS). We\nalso introduce an approach to add invariance to spectral deformations and\ntemporal shifts to classical prototype-based methods such as K-means and\nNearest Centroid Classifier (NCC). We show this simple and highly interpretable\nmethod leads to meaningful results in both the supervised and unsupervised\nsettings and significantly improves the state of the art for unsupervised\nclassification of agricultural time series on four recent SITS datasets.\n","authors":["Elliot Vincent","Jean Ponce","Mathieu Aubry"],"pdf_url":"https://arxiv.org/pdf/2303.12533v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.12501v2","updated":"2023-03-22T13:03:02Z","published":"2022-11-22T18:59:52Z","title":"AeDet: Azimuth-invariant Multi-view 3D Object Detection","summary":"  Recent LSS-based multi-view 3D object detection has made tremendous progress,\nby processing the features in Brid-Eye-View (BEV) via the convolutional\ndetector. However, the typical convolution ignores the radial symmetry of the\nBEV features and increases the difficulty of the detector optimization. To\npreserve the inherent property of the BEV features and ease the optimization,\nwe propose an azimuth-equivariant convolution (AeConv) and an\nazimuth-equivariant anchor. The sampling grid of AeConv is always in the radial\ndirection, thus it can learn azimuth-invariant BEV features. The proposed\nanchor enables the detection head to learn predicting azimuth-irrelevant\ntargets. In addition, we introduce a camera-decoupled virtual depth to unify\nthe depth prediction for the images with different camera intrinsic parameters.\nThe resultant detector is dubbed Azimuth-equivariant Detector (AeDet).\nExtensive experiments are conducted on nuScenes, and AeDet achieves a 62.0%\nNDS, surpassing the recent multi-view 3D object detectors such as PETRv2 and\nBEVDepth by a large margin. Project page: https://fcjian.github.io/aedet.\n","authors":["Chengjian Feng","Zequn Jie","Yujie Zhong","Xiangxiang Chu","Lin Ma"],"pdf_url":"https://arxiv.org/pdf/2211.12501v2.pdf","comment":"CVPR2023"},{"id":"http://arxiv.org/abs/2203.09957v2","updated":"2023-03-22T13:01:43Z","published":"2022-03-18T13:49:25Z","title":"Enhancement of Novel View Synthesis Using Omnidirectional Image\n  Completion","summary":"  In this study, we present a method for synthesizing novel views from a single\n360-degree RGB-D image based on the neural radiance field (NeRF) . Prior\nstudies relied on the neighborhood interpolation capability of multi-layer\nperceptrons to complete missing regions caused by occlusion and zooming, which\nleads to artifacts. In the method proposed in this study, the input image is\nreprojected to 360-degree RGB images at other camera positions, the missing\nregions of the reprojected images are completed by a 2D image generative model,\nand the completed images are utilized to train the NeRF. Because multiple\ncompleted images contain inconsistencies in 3D, we introduce a method to learn\nthe NeRF model using a subset of completed images that cover the target scene\nwith less overlap of completed regions. The selection of such a subset of\nimages can be attributed to the maximum weight independent set problem, which\nis solved through simulated annealing. Experiments demonstrated that the\nproposed method can synthesize plausible novel views while preserving the\nfeatures of the scene for both artificial and real-world data.\n","authors":["Takayuki Hara","Tatsuya Harada"],"pdf_url":"https://arxiv.org/pdf/2203.09957v2.pdf","comment":"19 pages, 20 figures"},{"id":"http://arxiv.org/abs/2303.09556v2","updated":"2023-03-22T12:55:17Z","published":"2023-03-16T17:59:56Z","title":"Efficient Diffusion Training via Min-SNR Weighting Strategy","summary":"  Denoising diffusion models have been a mainstream approach for image\ngeneration, however, training these models often suffers from slow convergence.\nIn this paper, we discovered that the slow convergence is partly due to\nconflicting optimization directions between timesteps. To address this issue,\nwe treat the diffusion training as a multi-task learning problem, and introduce\na simple yet effective approach referred to as Min-SNR-$\\gamma$. This method\nadapts loss weights of timesteps based on clamped signal-to-noise ratios, which\neffectively balances the conflicts among timesteps. Our results demonstrate a\nsignificant improvement in converging speed, 3.4$\\times$ faster than previous\nweighting strategies. It is also more effective, achieving a new record FID\nscore of 2.06 on the ImageNet $256\\times256$ benchmark using smaller\narchitectures than that employed in previous state-of-the-art. The code is\navailable at https://github.com/TiankaiHang/Min-SNR-Diffusion-Training.\n","authors":["Tiankai Hang","Shuyang Gu","Chen Li","Jianmin Bao","Dong Chen","Han Hu","Xin Geng","Baining Guo"],"pdf_url":"https://arxiv.org/pdf/2303.09556v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12512v1","updated":"2023-03-22T12:46:15Z","published":"2023-03-22T12:46:15Z","title":"Sibling-Attack: Rethinking Transferable Adversarial Attacks against Face\n  Recognition","summary":"  A hard challenge in developing practical face recognition (FR) attacks is due\nto the black-box nature of the target FR model, i.e., inaccessible gradient and\nparameter information to attackers. While recent research took an important\nstep towards attacking black-box FR models through leveraging transferability,\ntheir performance is still limited, especially against online commercial FR\nsystems that can be pessimistic (e.g., a less than 50% ASR--attack success rate\non average). Motivated by this, we present Sibling-Attack, a new FR attack\ntechnique for the first time explores a novel multi-task perspective (i.e.,\nleveraging extra information from multi-correlated tasks to boost attacking\ntransferability). Intuitively, Sibling-Attack selects a set of tasks correlated\nwith FR and picks the Attribute Recognition (AR) task as the task used in\nSibling-Attack based on theoretical and quantitative analysis. Sibling-Attack\nthen develops an optimization framework that fuses adversarial gradient\ninformation through (1) constraining the cross-task features to be under the\nsame space, (2) a joint-task meta optimization framework that enhances the\ngradient compatibility among tasks, and (3) a cross-task gradient stabilization\nmethod which mitigates the oscillation effect during attacking. Extensive\nexperiments demonstrate that Sibling-Attack outperforms state-of-the-art FR\nattack techniques by a non-trivial margin, boosting ASR by 12.61% and 55.77% on\naverage on state-of-the-art pre-trained FR models and two well-known, widely\nused commercial FR systems.\n","authors":["Zexin Li","Bangjie Yin","Taiping Yao","Juefeng Guo","Shouhong Ding","Simin Chen","Cong Liu"],"pdf_url":"https://arxiv.org/pdf/2303.12512v1.pdf","comment":"8 pages, 5 fivures, accepted by CVPR 2023 as a poster paper"},{"id":"http://arxiv.org/abs/2206.10555v2","updated":"2023-03-22T12:43:10Z","published":"2022-06-21T17:35:57Z","title":"LargeKernel3D: Scaling up Kernels in 3D Sparse CNNs","summary":"  Recent advance in 2D CNNs has revealed that large kernels are important.\nHowever, when directly applying large convolutional kernels in 3D CNNs, severe\ndifficulties are met, where those successful module designs in 2D become\nsurprisingly ineffective on 3D networks, including the popular depth-wise\nconvolution. To address this vital challenge, we instead propose the\nspatial-wise partition convolution and its large-kernel module. As a result, it\navoids the optimization and efficiency issues of naive 3D large kernels. Our\nlarge-kernel 3D CNN network, LargeKernel3D, yields notable improvement in 3D\ntasks of semantic segmentation and object detection. It achieves 73.9% mIoU on\nthe ScanNetv2 semantic segmentation and 72.8% NDS nuScenes object detection\nbenchmarks, ranking 1st on the nuScenes LIDAR leaderboard. The performance\nfurther boosts to 74.2% NDS with a simple multi-modal fusion. In addition,\nLargeKernel3D can be scaled to 17x17x17 kernel size on Waymo 3D object\ndetection. For the first time, we show that large kernels are feasible and\nessential for 3D visual tasks.\n","authors":["Yukang Chen","Jianhui Liu","Xiangyu Zhang","Xiaojuan Qi","Jiaya Jia"],"pdf_url":"https://arxiv.org/pdf/2206.10555v2.pdf","comment":"In CVPR 2023. Code is at\n  https://github.com/dvlab-research/LargeKernel3D"},{"id":"http://arxiv.org/abs/2303.08345v2","updated":"2023-03-22T12:41:03Z","published":"2023-03-15T03:54:43Z","title":"Scanning Only Once: An End-to-end Framework for Fast Temporal Grounding\n  in Long Videos","summary":"  Video temporal grounding aims to pinpoint a video segment that matches the\nquery description. Despite the recent advance in short-form videos\n(\\textit{e.g.}, in minutes), temporal grounding in long videos (\\textit{e.g.},\nin hours) is still at its early stage. To address this challenge, a common\npractice is to employ a sliding window, yet can be inefficient and inflexible\ndue to the limited number of frames within the window. In this work, we propose\nan end-to-end framework for fast temporal grounding, which is able to model an\nhours-long video with \\textbf{one-time} network execution. Our pipeline is\nformulated in a coarse-to-fine manner, where we first extract context knowledge\nfrom non-overlapped video clips (\\textit{i.e.}, anchors), and then supplement\nthe anchors that highly response to the query with detailed content knowledge.\nBesides the remarkably high pipeline efficiency, another advantage of our\napproach is the capability of capturing long-range temporal correlation, thanks\nto modeling the entire video as a whole, and hence facilitates more accurate\ngrounding. Experimental results suggest that, on the long-form video datasets\nMAD and Ego4d, our method significantly outperforms state-of-the-arts, and\nachieves \\textbf{14.6$\\times$} / \\textbf{102.8$\\times$} higher efficiency\nrespectively. Project can be found at\n\\url{https://github.com/afcedf/SOONet.git}.\n","authors":["Yulin Pan","Xiangteng He","Biao Gong","Yiliang Lv","Yujun Shen","Yuxin Peng","Deli Zhao"],"pdf_url":"https://arxiv.org/pdf/2303.08345v2.pdf","comment":"11 pages, 8 figures"},{"id":"http://arxiv.org/abs/2303.12501v1","updated":"2023-03-22T12:11:59Z","published":"2023-03-22T12:11:59Z","title":"Cross-Modal Implicit Relation Reasoning and Aligning for Text-to-Image\n  Person Retrieval","summary":"  Text-to-image person retrieval aims to identify the target person based on a\ngiven textual description query. The primary challenge is to learn the mapping\nof visual and textual modalities into a common latent space. Prior works have\nattempted to address this challenge by leveraging separately pre-trained\nunimodal models to extract visual and textual features. However, these\napproaches lack the necessary underlying alignment capabilities required to\nmatch multimodal data effectively. Besides, these works use prior information\nto explore explicit part alignments, which may lead to the distortion of\nintra-modality information. To alleviate these issues, we present IRRA: a\ncross-modal Implicit Relation Reasoning and Aligning framework that learns\nrelations between local visual-textual tokens and enhances global image-text\nmatching without requiring additional prior supervision. Specifically, we first\ndesign an Implicit Relation Reasoning module in a masked language modeling\nparadigm. This achieves cross-modal interaction by integrating the visual cues\ninto the textual tokens with a cross-modal multimodal interaction encoder.\nSecondly, to globally align the visual and textual embeddings, Similarity\nDistribution Matching is proposed to minimize the KL divergence between\nimage-text similarity distributions and the normalized label matching\ndistributions. The proposed method achieves new state-of-the-art results on all\nthree public datasets, with a notable margin of about 3%-9% for Rank-1 accuracy\ncompared to prior methods.\n","authors":["Ding Jiang","Mang Ye"],"pdf_url":"https://arxiv.org/pdf/2303.12501v1.pdf","comment":"Accepted by CVPR 2023. Codes are available at this\n  https://github.com/anosorae/IRRA"},{"id":"http://arxiv.org/abs/2303.12499v1","updated":"2023-03-22T12:10:44Z","published":"2023-03-22T12:10:44Z","title":"On Domain-Specific Pre-Training for Effective Semantic Perception in\n  Agricultural Robotics","summary":"  Agricultural robots have the prospect to enable more efficient and\nsustainable agricultural production of food, feed, and fiber. Perception of\ncrops and weeds is a central component of agricultural robots that aim to\nmonitor fields and assess the plants as well as their growth stage in an\nautomatic manner. Semantic perception mostly relies on deep learning using\nsupervised approaches, which require time and qualified workers to label fairly\nlarge amounts of data. In this paper, we look into the problem of reducing the\namount of labels without compromising the final segmentation performance. For\nrobots operating in the field, pre-training networks in a supervised way is\nalready a popular method to reduce the number of required labeled images. We\ninvestigate the possibility of pre-training in a self-supervised fashion using\ndata from the target domain. To better exploit this data, we propose a set of\ndomain-specific augmentation strategies. We evaluate our pre-training on\nsemantic segmentation and leaf instance segmentation, two important tasks in\nour domain. The experimental results suggest that pre-training with\ndomain-specific data paired with our data augmentation strategy leads to\nsuperior performance compared to commonly used pre-trainings. Furthermore, the\npre-trained networks obtain similar performance to the fully supervised with\nless labeled data.\n","authors":["Gianmarco Roggiolani","Federico Magistri","Tiziano Guadagnino","Jan Weyler","Giorgio Grisetti","Cyrill Stachniss","Jens Behley"],"pdf_url":"https://arxiv.org/pdf/2303.12499v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.06429v3","updated":"2023-03-22T12:01:42Z","published":"2023-01-16T13:38:22Z","title":"Linguistic Query-Guided Mask Generation for Referring Image Segmentation","summary":"  Referring image segmentation aims to segment the image region of interest\naccording to the given language expression, which is a typical multi-modal\ntask. Existing methods either adopt the pixel classification-based or the\nlearnable query-based framework for mask generation, both of which are\ninsufficient to deal with various text-image pairs with a fix number of\nparametric prototypes. In this work, we propose an end-to-end framework built\non transformer to perform Linguistic query-Guided mask generation, dubbed\nLGFormer. It views the linguistic features as query to generate a specialized\nprototype for arbitrary input image-text pair, thus generating more consistent\nsegmentation results. Moreover, we design several cross-modal interaction\nmodules (\\eg, vision-language bidirectional attention module, VLBA) in both\nencoder and decoder to achieve better cross-modal alignment.\n","authors":["Zhichao Wei","Xiaohao Chen","Mingqiang Chen","Siyu Zhu"],"pdf_url":"https://arxiv.org/pdf/2301.06429v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12484v1","updated":"2023-03-22T11:51:49Z","published":"2023-03-22T11:51:49Z","title":"Label-Efficient Deep Learning in Medical Image Analysis: Challenges and\n  Future Directions","summary":"  Deep learning has seen rapid growth in recent years and achieved\nstate-of-the-art performance in a wide range of applications. However, training\nmodels typically requires expensive and time-consuming collection of large\nquantities of labeled data. This is particularly true within the scope of\nmedical imaging analysis (MIA), where data are limited and labels are expensive\nto be acquired. Thus, label-efficient deep learning methods are developed to\nmake comprehensive use of the labeled data as well as the abundance of\nunlabeled and weak-labeled data. In this survey, we extensively investigated\nover 300 recent papers to provide a comprehensive overview of recent progress\non label-efficient learning strategies in MIA. We first present the background\nof label-efficient learning and categorize the approaches into different\nschemes. Next, we examine the current state-of-the-art methods in detail\nthrough each scheme. Specifically, we provide an in-depth investigation,\ncovering not only canonical semi-supervised, self-supervised, and\nmulti-instance learning schemes, but also recently emerged active and\nannotation-efficient learning strategies. Moreover, as a comprehensive\ncontribution to the field, this survey not only elucidates the commonalities\nand unique features of the surveyed methods but also presents a detailed\nanalysis of the current challenges in the field and suggests potential avenues\nfor future research.\n","authors":["Cheng Jin","Zhengrui Guo","Yi Lin","Luyang Luo","Hao Chen"],"pdf_url":"https://arxiv.org/pdf/2303.12484v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08320v3","updated":"2023-03-22T11:24:34Z","published":"2023-03-15T02:16:39Z","title":"VideoFusion: Decomposed Diffusion Models for High-Quality Video\n  Generation","summary":"  A diffusion probabilistic model (DPM), which constructs a forward diffusion\nprocess by gradually adding noise to data points and learns the reverse\ndenoising process to generate new samples, has been shown to handle complex\ndata distribution. Despite its recent success in image synthesis, applying DPMs\nto video generation is still challenging due to high-dimensional data spaces.\nPrevious methods usually adopt a standard diffusion process, where frames in\nthe same video clip are destroyed with independent noises, ignoring the content\nredundancy and temporal correlation. This work presents a decomposed diffusion\nprocess via resolving the per-frame noise into a base noise that is shared\namong all frames and a residual noise that varies along the time axis. The\ndenoising pipeline employs two jointly-learned networks to match the noise\ndecomposition accordingly. Experiments on various datasets confirm that our\napproach, termed as VideoFusion, surpasses both GAN-based and diffusion-based\nalternatives in high-quality video generation. We further show that our\ndecomposed formulation can benefit from pre-trained image diffusion models and\nwell-support text-conditioned video creation.\n","authors":["Zhengxiong Luo","Dayou Chen","Yingya Zhang","Yan Huang","Liang Wang","Yujun Shen","Deli Zhao","Jingren Zhou","Tieniu Tan"],"pdf_url":"https://arxiv.org/pdf/2303.08320v3.pdf","comment":"Accepted to CVPR2023"},{"id":"http://arxiv.org/abs/2203.16244v3","updated":"2023-03-22T11:19:19Z","published":"2022-03-30T12:22:26Z","title":"CycDA: Unsupervised Cycle Domain Adaptation from Image to Video","summary":"  Although action recognition has achieved impressive results over recent\nyears, both collection and annotation of video training data are still\ntime-consuming and cost intensive. Therefore, image-to-video adaptation has\nbeen proposed to exploit labeling-free web image source for adapting on\nunlabeled target videos. This poses two major challenges: (1) spatial domain\nshift between web images and video frames; (2) modality gap between image and\nvideo data. To address these challenges, we propose Cycle Domain Adaptation\n(CycDA), a cycle-based approach for unsupervised image-to-video domain\nadaptation by leveraging the joint spatial information in images and videos on\nthe one hand and, on the other hand, training an independent spatio-temporal\nmodel to bridge the modality gap. We alternate between the spatial and\nspatio-temporal learning with knowledge transfer between the two in each cycle.\nWe evaluate our approach on benchmark datasets for image-to-video as well as\nfor mixed-source domain adaptation achieving state-of-the-art results and\ndemonstrating the benefits of our cyclic adaptation. Code is available at\n\\url{https://github.com/wlin-at/CycDA}.\n","authors":["Wei Lin","Anna Kukleva","Kunyang Sun","Horst Possegger","Hilde Kuehne","Horst Bischof"],"pdf_url":"https://arxiv.org/pdf/2203.16244v3.pdf","comment":"Accepted at ECCV2022. Supplementary included"},{"id":"http://arxiv.org/abs/2212.06726v2","updated":"2023-03-22T11:17:29Z","published":"2022-12-13T16:54:08Z","title":"Semantic Brain Decoding: from fMRI to conceptually similar image\n  reconstruction of visual stimuli","summary":"  Brain decoding is a field of computational neuroscience that uses measurable\nbrain activity to infer mental states or internal representations of perceptual\ninputs. Therefore, we propose a novel approach to brain decoding that also\nrelies on semantic and contextual similarity. We employ an fMRI dataset of\nnatural image vision and create a deep learning decoding pipeline inspired by\nthe existence of both bottom-up and top-down processes in human vision. We\ntrain a linear brain-to-feature model to map fMRI activity features to visual\nstimuli features, assuming that the brain projects visual information onto a\nspace that is homeomorphic to the latent space represented by the last\nconvolutional layer of a pretrained convolutional neural network, which\ntypically collects a variety of semantic features that summarize and highlight\nsimilarities and differences between concepts. These features are then\ncategorized in the latent space using a nearest-neighbor strategy, and the\nresults are used to condition a generative latent diffusion model to create\nnovel images. From fMRI data only, we produce reconstructions of visual stimuli\nthat match the original content very well on a semantic level, surpassing the\nstate of the art in previous literature. We evaluate our work and obtain good\nresults using a quantitative semantic metric (the Wu-Palmer similarity metric\nover the WordNet lexicon, which had an average value of 0.57) and perform a\nhuman evaluation experiment that resulted in correct evaluation, according to\nthe multiplicity of human criteria in evaluating image similarity, in over 80%\nof the test set.\n","authors":["Matteo Ferrante","Tommaso Boccato","Nicola Toschi"],"pdf_url":"https://arxiv.org/pdf/2212.06726v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09975v2","updated":"2023-03-22T11:14:08Z","published":"2023-03-17T13:48:17Z","title":"MedNeXt: Transformer-driven Scaling of ConvNets for Medical Image\n  Segmentation","summary":"  There has been exploding interest in embracing Transformer-based\narchitectures for medical image segmentation. However, the lack of large-scale\nannotated medical datasets make achieving performances equivalent to those in\nnatural images challenging. Convolutional networks, in contrast, have higher\ninductive biases and consequently, are easily trainable to high performance.\nRecently, the ConvNeXt architecture attempted to modernize the standard ConvNet\nby mirroring Transformer blocks. In this work, we improve upon this to design a\nmodernized and scalable convolutional architecture customized to challenges of\ndata-scarce medical settings. We introduce MedNeXt, a Transformer-inspired\nlarge kernel segmentation network which introduces - 1) A fully ConvNeXt 3D\nEncoder-Decoder Network for medical image segmentation, 2) Residual ConvNeXt up\nand downsampling blocks to preserve semantic richness across scales, 3) A novel\ntechnique to iteratively increase kernel sizes by upsampling small kernel\nnetworks, to prevent performance saturation on limited medical data, 4)\nCompound scaling at multiple levels (depth, width, kernel size) of MedNeXt.\nThis leads to state-of-the-art performance on 4 tasks on CT and MRI modalities\nand varying dataset sizes, representing a modernized deep architecture for\nmedical image segmentation.\n","authors":["Saikat Roy","Gregor Koehler","Constantin Ulrich","Michael Baumgartner","Jens Petersen","Fabian Isensee","Paul F. Jaeger","Klaus Maier-Hein"],"pdf_url":"https://arxiv.org/pdf/2303.09975v2.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2303.12445v1","updated":"2023-03-22T10:30:43Z","published":"2023-03-22T10:30:43Z","title":"MEDIMP: Medical Images and Prompts for renal transplant representation\n  learning","summary":"  Renal transplantation emerges as the most effective solution for end-stage\nrenal disease. Occurring from complex causes, a substantial risk of transplant\nchronic dysfunction persists and may lead to graft loss. Medical imaging plays\na substantial role in renal transplant monitoring in clinical practice.\nHowever, graft supervision is multi-disciplinary, notably joining nephrology,\nurology, and radiology, while identifying robust biomarkers from such\nhigh-dimensional and complex data for prognosis is challenging. In this work,\ntaking inspiration from the recent success of Large Language Models (LLMs), we\npropose MEDIMP -- Medical Images and Prompts -- a model to learn meaningful\nmulti-modal representations of renal transplant Dynamic Contrast-Enhanced\nMagnetic Resonance Imaging (DCE MRI) by incorporating structural\nclinicobiological data after translating them into text prompts. MEDIMP is\nbased on contrastive learning from joint text-image paired embeddings to\nperform this challenging task. Moreover, we propose a framework that generates\nmedical prompts using automatic textual data augmentations from LLMs. Our goal\nis to learn meaningful manifolds of renal transplant DCE MRI, interesting for\nthe prognosis of the transplant or patient status (2, 3, and 4 years after the\ntransplant), fully exploiting the available multi-modal data in the most\nefficient way. Extensive experiments and comparisons with other renal\ntransplant representation learning methods with limited data prove the\neffectiveness of MEDIMP in a relevant clinical setting, giving new directions\ntoward medical prompts. Our code is available at\nhttps://github.com/leomlck/MEDIMP.\n","authors":["Leo Milecki","Vicky Kalogeiton","Sylvain Bodard","Dany Anglicheau","Jean-Michel Correas","Marc-Olivier Timsit","Maria Vakalopoulou"],"pdf_url":"https://arxiv.org/pdf/2303.12445v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.12872v3","updated":"2023-03-22T10:05:15Z","published":"2022-11-23T11:26:24Z","title":"μSplit: efficient image decomposition for microscopy data","summary":"  We present uSplit, a dedicated approach for trained image decomposition in\nthe context of fluorescence microscopy images. We find that best results using\nregular deep architectures are achieved when large image patches are used\nduring training, making memory consumption the limiting factor to further\nimproving performance. We therefore introduce lateral contextualization (LC), a\nmemory efficient way to train powerful networks and show that LC leads to\nconsistent and significant improvements on the task at hand. We integrate LC\nwith U-Nets, Hierarchical AEs, and Hierarchical VAEs, for which we formulate a\nmodified ELBO loss. Additionally, LC enables training deeper hierarchical\nmodels than otherwise possible and, interestingly, helps to reduce tiling\nartefacts that are inherently impossible to avoid when using tiled VAE\npredictions. We apply uSplit to five decomposition tasks, one on a synthetic\ndataset, four others derived from real microscopy data. LC achieves SOTA\nresults (average improvements to the best baseline of 2.36 dB PSNR), while\nsimultaneously requiring considerably less GPU memory.\n","authors":[" Ashesh","Alexander Krull","Moises Di Sante","Francesco Silvio Pasqualini","Florian Jug"],"pdf_url":"https://arxiv.org/pdf/2211.12872v3.pdf","comment":"10 pages, 7 figures, 9 pages supplement, 8 supplementary figures"},{"id":"http://arxiv.org/abs/2303.12424v1","updated":"2023-03-22T09:51:08Z","published":"2023-03-22T09:51:08Z","title":"Unsupervised Domain Adaptation for Training Event-Based Networks Using\n  Contrastive Learning and Uncorrelated Conditioning","summary":"  Event-based cameras offer reliable measurements for preforming computer\nvision tasks in high-dynamic range environments and during fast motion\nmaneuvers. However, adopting deep learning in event-based vision faces the\nchallenge of annotated data scarcity due to recency of event cameras.\nTransferring the knowledge that can be obtained from conventional camera\nannotated data offers a practical solution to this challenge. We develop an\nunsupervised domain adaptation algorithm for training a deep network for\nevent-based data image classification using contrastive learning and\nuncorrelated conditioning of data. Our solution outperforms the existing\nalgorithms for this purpose.\n","authors":["Dayuan Jian","Mohammad Rostami"],"pdf_url":"https://arxiv.org/pdf/2303.12424v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12423v1","updated":"2023-03-22T09:49:53Z","published":"2023-03-22T09:49:53Z","title":"Text with Knowledge Graph Augmented Transformer for Video Captioning","summary":"  Video captioning aims to describe the content of videos using natural\nlanguage. Although significant progress has been made, there is still much room\nto improve the performance for real-world applications, mainly due to the\nlong-tail words challenge. In this paper, we propose a text with knowledge\ngraph augmented transformer (TextKG) for video captioning. Notably, TextKG is a\ntwo-stream transformer, formed by the external stream and internal stream. The\nexternal stream is designed to absorb additional knowledge, which models the\ninteractions between the additional knowledge, e.g., pre-built knowledge graph,\nand the built-in information of videos, e.g., the salient object regions,\nspeech transcripts, and video captions, to mitigate the long-tail words\nchallenge. Meanwhile, the internal stream is designed to exploit the\nmulti-modality information in videos (e.g., the appearance of video frames,\nspeech transcripts, and video captions) to ensure the quality of caption\nresults. In addition, the cross attention mechanism is also used in between the\ntwo streams for sharing information. In this way, the two streams can help each\nother for more accurate results. Extensive experiments conducted on four\nchallenging video captioning datasets, i.e., YouCookII, ActivityNet Captions,\nMSRVTT, and MSVD, demonstrate that the proposed method performs favorably\nagainst the state-of-the-art methods. Specifically, the proposed TextKG method\noutperforms the best published results by improving 18.7% absolute CIDEr scores\non the YouCookII dataset.\n","authors":["Xin Gu","Guang Chen","Yufei Wang","Libo Zhang","Tiejian Luo","Longyin Wen"],"pdf_url":"https://arxiv.org/pdf/2303.12423v1.pdf","comment":"Accepted by CVPR2023"},{"id":"http://arxiv.org/abs/2303.06919v2","updated":"2023-03-22T09:45:51Z","published":"2023-03-13T08:36:30Z","title":"NeRFLiX: High-Quality Neural View Synthesis by Learning a\n  Degradation-Driven Inter-viewpoint MiXer","summary":"  Neural radiance fields (NeRF) show great success in novel view synthesis.\nHowever, in real-world scenes, recovering high-quality details from the source\nimages is still challenging for the existing NeRF-based approaches, due to the\npotential imperfect calibration information and scene representation\ninaccuracy. Even with high-quality training frames, the synthetic novel views\nproduced by NeRF models still suffer from notable rendering artifacts, such as\nnoise, blur, etc. Towards to improve the synthesis quality of NeRF-based\napproaches, we propose NeRFLiX, a general NeRF-agnostic restorer paradigm by\nlearning a degradation-driven inter-viewpoint mixer. Specially, we design a\nNeRF-style degradation modeling approach and construct large-scale training\ndata, enabling the possibility of effectively removing NeRF-native rendering\nartifacts for existing deep neural networks. Moreover, beyond the degradation\nremoval, we propose an inter-viewpoint aggregation framework that is able to\nfuse highly related high-quality training images, pushing the performance of\ncutting-edge NeRF models to entirely new levels and producing highly\nphoto-realistic synthetic views.\n","authors":["Kun Zhou","Wenbo Li","Yi Wang","Tao Hu","Nianjuan Jiang","Xiaoguang Han","Jiangbo Lu"],"pdf_url":"https://arxiv.org/pdf/2303.06919v2.pdf","comment":"Accepted to CVPR 2023; Project Page: see\n  https://redrock303.github.io/nerflix/"},{"id":"http://arxiv.org/abs/2302.12242v2","updated":"2023-03-22T09:45:35Z","published":"2023-02-23T18:58:28Z","title":"Side Adapter Network for Open-Vocabulary Semantic Segmentation","summary":"  This paper presents a new framework for open-vocabulary semantic segmentation\nwith the pre-trained vision-language model, named Side Adapter Network (SAN).\nOur approach models the semantic segmentation task as a region recognition\nproblem. A side network is attached to a frozen CLIP model with two branches:\none for predicting mask proposals, and the other for predicting attention bias\nwhich is applied in the CLIP model to recognize the class of masks. This\ndecoupled design has the benefit CLIP in recognizing the class of mask\nproposals. Since the attached side network can reuse CLIP features, it can be\nvery light. In addition, the entire network can be trained end-to-end, allowing\nthe side network to be adapted to the frozen CLIP model, which makes the\npredicted mask proposals CLIP-aware. Our approach is fast, accurate, and only\nadds a few additional trainable parameters. We evaluate our approach on\nmultiple semantic segmentation benchmarks. Our method significantly outperforms\nother counterparts, with up to 18 times fewer trainable parameters and 19 times\nfaster inference speed. We hope our approach will serve as a solid baseline and\nhelp ease future research in open-vocabulary semantic segmentation. The code\nwill be available at https://github.com/MendelXu/SAN.\n","authors":["Mengde Xu","Zheng Zhang","Fangyun Wei","Han Hu","Xiang Bai"],"pdf_url":"https://arxiv.org/pdf/2302.12242v2.pdf","comment":"CVPR2023 Highlight"},{"id":"http://arxiv.org/abs/2303.12421v1","updated":"2023-03-22T09:38:34Z","published":"2023-03-22T09:38:34Z","title":"Region-wise matching for image inpainting based on adaptive weighted\n  low-rank decomposition","summary":"  Digital image inpainting is an interpolation problem, inferring the content\nin the missing (unknown) region to agree with the known region data such that\nthe interpolated result fulfills some prior knowledge. Low-rank and nonlocal\nself-similarity are two important priors for image inpainting. Based on the\nnonlocal self-similarity assumption, an image is divided into overlapped square\ntarget patches (submatrices) and the similar patches of any target patch are\nreshaped as vectors and stacked into a patch matrix. Such a patch matrix\nusually enjoys a property of low rank or approximately low rank, and its\nmissing entries are recoveried by low-rank matrix approximation (LRMA)\nalgorithms. Traditionally, $n$ nearest neighbor similar patches are searched\nwithin a local window centered at a target patch. However, for an image with\nmissing lines, the generated patch matrix is prone to having entirely-missing\nrows such that the downstream low-rank model fails to reconstruct it well. To\naddress this problem, we propose a region-wise matching (RwM) algorithm by\ndividing the neighborhood of a target patch into multiple subregions and then\nsearch the most similar one within each subregion. A non-convex weighted\nlow-rank decomposition (NC-WLRD) model for LRMA is also proposed to reconstruct\nall degraded patch matrices grouped by the proposed RwM algorithm. We solve the\nproposed NC-WLRD model by the alternating direction method of multipliers\n(ADMM) and analyze the convergence in detail. Numerous experiments on line\ninpainting (entire-row/column missing) demonstrate the superiority of our\nmethod over other competitive inpainting algorithms. Unlike other\nlow-rank-based matrix completion methods and inpainting algorithms, the\nproposed model NC-WLRD is also effective for removing random-valued impulse\nnoise and structural noise (stripes).\n","authors":["Shenghai Liao","Xuya Liu","Ruyi Han","Shujun Fu","Yuanfeng Zhou","Yuliang Li"],"pdf_url":"https://arxiv.org/pdf/2303.12421v1.pdf","comment":"region-wise matching algorithm, image inpainting, 20 pages, 18\n  figures"},{"id":"http://arxiv.org/abs/2303.12419v1","updated":"2023-03-22T09:33:50Z","published":"2023-03-22T09:33:50Z","title":"BiCro: Noisy Correspondence Rectification for Multi-modality Data via\n  Bi-directional Cross-modal Similarity Consistency","summary":"  As one of the most fundamental techniques in multimodal learning, cross-modal\nmatching aims to project various sensory modalities into a shared feature\nspace. To achieve this, massive and correctly aligned data pairs are required\nfor model training. However, unlike unimodal datasets, multimodal datasets are\nextremely harder to collect and annotate precisely. As an alternative, the\nco-occurred data pairs (e.g., image-text pairs) collected from the Internet\nhave been widely exploited in the area. Unfortunately, the cheaply collected\ndataset unavoidably contains many mismatched data pairs, which have been proven\nto be harmful to the model's performance. To address this, we propose a general\nframework called BiCro (Bidirectional Cross-modal similarity consistency),\nwhich can be easily integrated into existing cross-modal matching models and\nimprove their robustness against noisy data. Specifically, BiCro aims to\nestimate soft labels for noisy data pairs to reflect their true correspondence\ndegree. The basic idea of BiCro is motivated by that -- taking image-text\nmatching as an example -- similar images should have similar textual\ndescriptions and vice versa. Then the consistency of these two similarities can\nbe recast as the estimated soft labels to train the matching model. The\nexperiments on three popular cross-modal matching datasets demonstrate that our\nmethod significantly improves the noise-robustness of various matching models,\nand surpass the state-of-the-art by a clear margin.\n","authors":["Shuo Yang","Zhaopan Xu","Kai Wang","Yang You","Hongxun Yao","Tongliang Liu","Min Xu"],"pdf_url":"https://arxiv.org/pdf/2303.12419v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.12417v1","updated":"2023-03-22T09:32:45Z","published":"2023-03-22T09:32:45Z","title":"CLIP^2: Contrastive Language-Image-Point Pretraining from Real-World\n  Point Cloud Data","summary":"  Contrastive Language-Image Pre-training, benefiting from large-scale\nunlabeled text-image pairs, has demonstrated great performance in open-world\nvision understanding tasks. However, due to the limited Text-3D data pairs,\nadapting the success of 2D Vision-Language Models (VLM) to the 3D space remains\nan open problem. Existing works that leverage VLM for 3D understanding\ngenerally resort to constructing intermediate 2D representations for the 3D\ndata, but at the cost of losing 3D geometry information. To take a step toward\nopen-world 3D vision understanding, we propose Contrastive Language-Image-Point\nCloud Pretraining (CLIP^2) to directly learn the transferable 3D point cloud\nrepresentation in realistic scenarios with a novel proxy alignment mechanism.\nSpecifically, we exploit naturally-existed correspondences in 2D and 3D\nscenarios, and build well-aligned and instance-based text-image-point proxies\nfrom those complex scenarios. On top of that, we propose a cross-modal\ncontrastive objective to learn semantic and instance-level aligned point cloud\nrepresentation. Experimental results on both indoor and outdoor scenarios show\nthat our learned 3D representation has great transfer ability in downstream\ntasks, including zero-shot and few-shot 3D recognition, which boosts the\nstate-of-the-art methods by large margins. Furthermore, we provide analyses of\nthe capability of different representations in real scenarios and present the\noptional ensemble scheme.\n","authors":["Yihan Zeng","Chenhan Jiang","Jiageng Mao","Jianhua Han","Chaoqiang Ye","Qingqiu Huang","Dit-Yan Yeung","Zhen Yang","Xiaodan Liang","Hang Xu"],"pdf_url":"https://arxiv.org/pdf/2303.12417v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.06627v3","updated":"2023-03-22T09:32:26Z","published":"2022-11-12T10:29:05Z","title":"MARLIN: Masked Autoencoder for facial video Representation LearnINg","summary":"  This paper proposes a self-supervised approach to learn universal facial\nrepresentations from videos, that can transfer across a variety of facial\nanalysis tasks such as Facial Attribute Recognition (FAR), Facial Expression\nRecognition (FER), DeepFake Detection (DFD), and Lip Synchronization (LS). Our\nproposed framework, named MARLIN, is a facial video masked autoencoder, that\nlearns highly robust and generic facial embeddings from abundantly available\nnon-annotated web crawled facial videos. As a challenging auxiliary task,\nMARLIN reconstructs the spatio-temporal details of the face from the densely\nmasked facial regions which mainly include eyes, nose, mouth, lips, and skin to\ncapture local and global aspects that in turn help in encoding generic and\ntransferable features. Through a variety of experiments on diverse downstream\ntasks, we demonstrate MARLIN to be an excellent facial video encoder as well as\nfeature extractor, that performs consistently well across a variety of\ndownstream tasks including FAR (1.13% gain over supervised benchmark), FER\n(2.64% gain over unsupervised benchmark), DFD (1.86% gain over unsupervised\nbenchmark), LS (29.36% gain for Frechet Inception Distance), and even in low\ndata regime. Our code and models are available at\nhttps://github.com/ControlNet/MARLIN .\n","authors":["Zhixi Cai","Shreya Ghosh","Kalin Stefanov","Abhinav Dhall","Jianfei Cai","Hamid Rezatofighi","Reza Haffari","Munawar Hayat"],"pdf_url":"https://arxiv.org/pdf/2211.06627v3.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.12408v1","updated":"2023-03-22T09:17:01Z","published":"2023-03-22T09:17:01Z","title":"Balanced Spherical Grid for Egocentric View Synthesis","summary":"  We present EgoNeRF, a practical solution to reconstruct large-scale\nreal-world environments for VR assets. Given a few seconds of casually captured\n360 video, EgoNeRF can efficiently build neural radiance fields which enable\nhigh-quality rendering from novel viewpoints. Motivated by the recent\nacceleration of NeRF using feature grids, we adopt spherical coordinate instead\nof conventional Cartesian coordinate. Cartesian feature grid is inefficient to\nrepresent large-scale unbounded scenes because it has a spatially uniform\nresolution, regardless of distance from viewers. The spherical parameterization\nbetter aligns with the rays of egocentric images, and yet enables factorization\nfor performance enhancement. However, the na\\\"ive spherical grid suffers from\nirregularities at two poles, and also cannot represent unbounded scenes. To\navoid singularities near poles, we combine two balanced grids, which results in\na quasi-uniform angular grid. We also partition the radial grid exponentially\nand place an environment map at infinity to represent unbounded scenes.\nFurthermore, with our resampling technique for grid-based methods, we can\nincrease the number of valid samples to train NeRF volume. We extensively\nevaluate our method in our newly introduced synthetic and real-world egocentric\n360 video datasets, and it consistently achieves state-of-the-art performance.\n","authors":["Changwoon Choi","Sang Min Kim","Young Min Kim"],"pdf_url":"https://arxiv.org/pdf/2303.12408v1.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2108.11726v3","updated":"2023-03-22T09:13:00Z","published":"2021-08-26T12:04:32Z","title":"Learning to Diversify for Single Domain Generalization","summary":"  Domain generalization (DG) aims to generalize a model trained on multiple\nsource (i.e., training) domains to a distributionally different target (i.e.,\ntest) domain. In contrast to the conventional DG that strictly requires the\navailability of multiple source domains, this paper considers a more realistic\nyet challenging scenario, namely Single Domain Generalization (Single-DG),\nwhere only one source domain is available for training. In this scenario, the\nlimited diversity may jeopardize the model generalization on unseen target\ndomains. To tackle this problem, we propose a style-complement module to\nenhance the generalization power of the model by synthesizing images from\ndiverse distributions that are complementary to the source ones. More\nspecifically, we adopt a tractable upper bound of mutual information (MI)\nbetween the generated and source samples and perform a two-step optimization\niteratively: (1) by minimizing the MI upper bound approximation for each sample\npair, the generated images are forced to be diversified from the source\nsamples; (2) subsequently, we maximize the MI between the samples from the same\nsemantic category, which assists the network to learn discriminative features\nfrom diverse-styled images. Extensive experiments on three benchmark datasets\ndemonstrate the superiority of our approach, which surpasses the\nstate-of-the-art single-DG methods by up to 25.14%.\n","authors":["Zijian Wang","Yadan Luo","Ruihong Qiu","Zi Huang","Mahsa Baktashmotlagh"],"pdf_url":"https://arxiv.org/pdf/2108.11726v3.pdf","comment":"ICCV 2021"},{"id":"http://arxiv.org/abs/2106.10836v2","updated":"2023-03-22T09:12:09Z","published":"2021-06-21T03:55:33Z","title":"Active Learning for Deep Neural Networks on Edge Devices","summary":"  When dealing with deep neural network (DNN) applications on edge devices,\ncontinuously updating the model is important. Although updating a model with\nreal incoming data is ideal, using all of them is not always feasible due to\nlimits, such as labeling and communication costs. Thus, it is necessary to\nfilter and select the data to use for training (i.e., active learning) on the\ndevice. In this paper, we formalize a practical active learning problem for\nDNNs on edge devices and propose a general task-agnostic framework to tackle\nthis problem, which reduces it to a stream submodular maximization. This\nframework is light enough to be run with low computational resources, yet\nprovides solutions whose quality is theoretically guaranteed thanks to the\nsubmodular property. Through this framework, we can configure data selection\ncriteria flexibly, including using methods proposed in previous active learning\nstudies. We evaluate our approach on both classification and object detection\ntasks in a practical setting to simulate a real-life scenario. The results of\nour study show that the proposed framework outperforms all other methods in\nboth tasks, while running at a practical speed on real devices.\n","authors":["Yuya Senzaki","Christian Hamelain"],"pdf_url":"https://arxiv.org/pdf/2106.10836v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12400v1","updated":"2023-03-22T09:09:02Z","published":"2023-03-22T09:09:02Z","title":"UMC: A Unified Bandwidth-efficient and Multi-resolution based\n  Collaborative Perception Framework","summary":"  Multi-agent collaborative perception (MCP) has recently attracted much\nattention. It includes three key processes: communication for sharing,\ncollaboration for integration, and reconstruction for different downstream\ntasks. Existing methods pursue designing the collaboration process alone,\nignoring their intrinsic interactions and resulting in suboptimal performance.\nIn contrast, we aim to propose a Unified Collaborative perception framework\nnamed UMC, optimizing the communication, collaboration, and reconstruction\nprocesses with the Multi-resolution technique. The communication introduces a\nnovel trainable multi-resolution and selective-region (MRSR) mechanism,\nachieving higher quality and lower bandwidth. Then, a graph-based collaboration\nis proposed, conducting on each resolution to adapt the MRSR. Finally, the\nreconstruction integrates the multi-resolution collaborative features for\ndownstream tasks. Since the general metric can not reflect the performance\nenhancement brought by MCP systematically, we introduce a brand-new evaluation\nmetric that evaluates the MCP from different perspectives. To verify our\nalgorithm, we conducted experiments on the V2X-Sim and OPV2V datasets. Our\nquantitative and qualitative experiments prove that the proposed UMC greatly\noutperforms the state-of-the-art collaborative perception approaches.\n","authors":["Tianhang Wang","Guang Chen","Kai Chen","Zhengfa Liu","Bo Zhang","Alois Knoll","Changjun Jiang"],"pdf_url":"https://arxiv.org/pdf/2303.12400v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.12782v2","updated":"2023-03-22T09:08:09Z","published":"2022-11-23T08:50:03Z","title":"Hand Avatar: Free-Pose Hand Animation and Rendering from Monocular Video","summary":"  We present HandAvatar, a novel representation for hand animation and\nrendering, which can generate smoothly compositional geometry and\nself-occlusion-aware texture. Specifically, we first develop a MANO-HD model as\na high-resolution mesh topology to fit personalized hand shapes. Sequentially,\nwe decompose hand geometry into per-bone rigid parts, and then re-compose\npaired geometry encodings to derive an across-part consistent occupancy field.\nAs for texture modeling, we propose a self-occlusion-aware shading field\n(SelF). In SelF, drivable anchors are paved on the MANO-HD surface to record\nalbedo information under a wide variety of hand poses. Moreover, directed soft\noccupancy is designed to describe the ray-to-surface relation, which is\nleveraged to generate an illumination field for the disentanglement of\npose-independent albedo and pose-dependent illumination. Trained from monocular\nvideo data, our HandAvatar can perform free-pose hand animation and rendering\nwhile at the same time achieving superior appearance fidelity. We also\ndemonstrate that HandAvatar provides a route for hand appearance editing.\nProject website: https://seanchenxy.github.io/HandAvatarWeb.\n","authors":["Xingyu Chen","Baoyuan Wang","Heung-Yeung Shum"],"pdf_url":"https://arxiv.org/pdf/2211.12782v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12398v1","updated":"2023-03-22T09:06:07Z","published":"2023-03-22T09:06:07Z","title":"Multiscale Attention via Wavelet Neural Operators for Vision\n  Transformers","summary":"  Transformers have achieved widespread success in computer vision. At their\nheart, there is a Self-Attention (SA) mechanism, an inductive bias that\nassociates each token in the input with every other token through a weighted\nbasis. The standard SA mechanism has quadratic complexity with the sequence\nlength, which impedes its utility to long sequences appearing in high\nresolution vision. Recently, inspired by operator learning for PDEs, Adaptive\nFourier Neural Operators (AFNO) were introduced for high resolution attention\nbased on global convolution that is efficiently implemented via FFT. However,\nthe AFNO global filtering cannot well represent small and moderate scale\nstructures that commonly appear in natural images. To leverage the\ncoarse-to-fine scale structures we introduce a Multiscale Wavelet Attention\n(MWA) by leveraging wavelet neural operators which incurs linear complexity in\nthe sequence size. We replace the attention in ViT with MWA and our experiments\nwith CIFAR and ImageNet classification demonstrate significant improvement over\nalternative Fourier-based attentions such as AFNO and Global Filter Network\n(GFN).\n","authors":["Anahita Nekoozadeh","Mohammad Reza Ahmadzadeh","Zahra Mardani","Morteza Mardani"],"pdf_url":"https://arxiv.org/pdf/2303.12398v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12396v1","updated":"2023-03-22T09:02:54Z","published":"2023-03-22T09:02:54Z","title":"Rigidity-Aware Detection for 6D Object Pose Estimation","summary":"  Most recent 6D object pose estimation methods first use object detection to\nobtain 2D bounding boxes before actually regressing the pose. However, the\ngeneral object detection methods they use are ill-suited to handle cluttered\nscenes, thus producing poor initialization to the subsequent pose network. To\naddress this, we propose a rigidity-aware detection method exploiting the fact\nthat, in 6D pose estimation, the target objects are rigid. This lets us\nintroduce an approach to sampling positive object regions from the entire\nvisible object area during training, instead of naively drawing samples from\nthe bounding box center where the object might be occluded. As such, every\nvisible object part can contribute to the final bounding box prediction,\nyielding better detection robustness. Key to the success of our approach is a\nvisibility map, which we propose to build using a minimum barrier distance\nbetween every pixel in the bounding box and the box boundary. Our results on\nseven challenging 6D pose estimation datasets evidence that our method\noutperforms general detection frameworks by a large margin. Furthermore,\ncombined with a pose regression network, we obtain state-of-the-art pose\nestimation results on the challenging BOP benchmark.\n","authors":["Yang Hai","Rui Song","Jiaojiao Li","Mathieu Salzmann","Yinlin Hu"],"pdf_url":"https://arxiv.org/pdf/2303.12396v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.12394v1","updated":"2023-03-22T08:59:42Z","published":"2023-03-22T08:59:42Z","title":"Road Extraction with Satellite Images and Partial Road Maps","summary":"  Road extraction is a process of automatically generating road maps mainly\nfrom satellite images. Existing models all target to generate roads from the\nscratch despite that a large quantity of road maps, though incomplete, are\npublicly available (e.g. those from OpenStreetMap) and can help with road\nextraction. In this paper, we propose to conduct road extraction based on\nsatellite images and partial road maps, which is new. We then propose a\ntwo-branch Partial to Complete Network (P2CNet) for the task, which has two\nprominent components: Gated Self-Attention Module (GSAM) and Missing Part (MP)\nloss. GSAM leverages a channel-wise self-attention module and a gate module to\ncapture long-range semantics, filter out useless information, and better fuse\nthe features from two branches. MP loss is derived from the partial road maps,\ntrying to give more attention to the road pixels that do not exist in partial\nroad maps. Extensive experiments are conducted to demonstrate the effectiveness\nof our model, e.g. P2CNet achieves state-of-the-art performance with the IoU\nscores of 70.71% and 75.52%, respectively, on the SpaceNet and OSM datasets.\n","authors":["Qianxiong Xu","Cheng Long","Liang Yu","Chen Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.12394v1.pdf","comment":"This paper has been accepted by IEEE Transactions on Geoscience and\n  Remote Sensing"},{"id":"http://arxiv.org/abs/2212.00210v2","updated":"2023-03-22T08:58:15Z","published":"2022-12-01T01:39:28Z","title":"Shape-Guided Diffusion with Inside-Outside Attention","summary":"  When manipulating an object, existing text-to-image diffusion models often\nignore the shape of the object and generate content that is incorrectly scaled,\ncut off, or replaced with background content. We propose a training-free\nmethod, Shape-Guided Diffusion, that modifies pretrained diffusion models to be\nsensitive to shape input specified by a user or automatically inferred from\ntext. We use a novel Inside-Outside Attention mechanism during the inversion\nand generation process to apply this shape constraint to the cross- and\nself-attention maps. Our mechanism designates which spatial region is the\nobject (inside) vs. background (outside) then associates edits specified by\ntext prompts to the correct region. We demonstrate the efficacy of our method\non the shape-guided editing task, where the model must replace an object\naccording to a text prompt and object mask. We curate a new ShapePrompts\nbenchmark derived from MS-COCO and achieve SOTA results in shape faithfulness\nwithout a degradation in text alignment or image realism according to both\nautomatic metrics and annotator ratings. Our data and code will be made\navailable at https://shape-guided-diffusion.github.io.\n","authors":["Dong Huk Park","Grace Luo","Clayton Toste","Samaneh Azadi","Xihui Liu","Maka Karalashvili","Anna Rohrbach","Trevor Darrell"],"pdf_url":"https://arxiv.org/pdf/2212.00210v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11330v2","updated":"2023-03-22T08:48:15Z","published":"2023-03-20T17:59:58Z","title":"Legs as Manipulator: Pushing Quadrupedal Agility Beyond Locomotion","summary":"  Locomotion has seen dramatic progress for walking or running across\nchallenging terrains. However, robotic quadrupeds are still far behind their\nbiological counterparts, such as dogs, which display a variety of agile skills\nand can use the legs beyond locomotion to perform several basic manipulation\ntasks like interacting with objects and climbing. In this paper, we take a step\ntowards bridging this gap by training quadruped robots not only to walk but\nalso to use the front legs to climb walls, press buttons, and perform object\ninteraction in the real world. To handle this challenging optimization, we\ndecouple the skill learning broadly into locomotion, which involves anything\nthat involves movement whether via walking or climbing a wall, and\nmanipulation, which involves using one leg to interact while balancing on the\nother three legs. These skills are trained in simulation using curriculum and\ntransferred to the real world using our proposed sim2real variant that builds\nupon recent locomotion success. Finally, we combine these skills into a robust\nlong-term plan by learning a behavior tree that encodes a high-level task\nhierarchy from one clean expert demonstration. We evaluate our method in both\nsimulation and real-world showing successful executions of both short as well\nas long-range tasks and how robustness helps confront external perturbations.\nVideos at https://robot-skills.github.io\n","authors":["Xuxin Cheng","Ashish Kumar","Deepak Pathak"],"pdf_url":"https://arxiv.org/pdf/2303.11330v2.pdf","comment":"Accepted at ICRA 2023. Videos at https://robot-skills.github.io"},{"id":"http://arxiv.org/abs/2303.12384v1","updated":"2023-03-22T08:47:37Z","published":"2023-03-22T08:47:37Z","title":"RegFormer: An Efficient Projection-Aware Transformer Network for\n  Large-Scale Point Cloud Registration","summary":"  Although point cloud registration has achieved remarkable advances in\nobject-level and indoor scenes, large-scale registration methods are rarely\nexplored. Challenges mainly arise from the huge point number, complex\ndistribution, and outliers of outdoor LiDAR scans. In addition, most existing\nregistration works generally adopt a two-stage paradigm: They first find\ncorrespondences by extracting discriminative local features, and then leverage\nestimators (eg. RANSAC) to filter outliers, which are highly dependent on\nwell-designed descriptors and post-processing choices. To address these\nproblems, we propose an end-to-end transformer network (RegFormer) for\nlarge-scale point cloud alignment without any further post-processing.\nSpecifically, a projection-aware hierarchical transformer is proposed to\ncapture long-range dependencies and filter outliers by extracting point\nfeatures globally. Our transformer has linear complexity, which guarantees high\nefficiency even for large-scale scenes. Furthermore, to effectively reduce\nmismatches, a bijective association transformer is designed for regressing the\ninitial transformation. Extensive experiments on KITTI and NuScenes datasets\ndemonstrate that our RegFormer achieves state-of-the-art performance in terms\nof both accuracy and efficiency.\n","authors":["Jiuming Liu","Guangming Wang","Zhe Liu","Chaokang Jiang","Marc Pollefeys","Hesheng Wang"],"pdf_url":"https://arxiv.org/pdf/2303.12384v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.08175v5","updated":"2023-03-22T08:32:12Z","published":"2023-02-16T09:44:55Z","title":"A numerical approximation method for the Fisher-Rao distance between\n  multivariate normal distributions","summary":"  We present a simple method to approximate Rao's distance between multivariate\nnormal distributions based on discretizing curves joining normal distributions\nand approximating Rao's distances between successive nearby normal\ndistributions on the curves by the square root of Jeffreys divergence, the\nsymmetrized Kullback-Leibler divergence. We consider experimentally the linear\ninterpolation curves in the ordinary, natural and expectation parameterizations\nof the normal distributions, and compare these curves with a curve derived from\nthe Calvo and Oller's isometric embedding of the Fisher-Rao $d$-variate normal\nmanifold into the cone of $(d+1)\\times (d+1)$ symmetric positive-definite\nmatrices [Journal of multivariate analysis 35.2 (1990): 223-242]. We report on\nour experiments and assess the quality of our approximation technique by\ncomparing the numerical approximations with both lower and upper bounds.\nFinally, we present several information-geometric properties of the Calvo and\nOller's isometric embedding.\n","authors":["Frank Nielsen"],"pdf_url":"https://arxiv.org/pdf/2302.08175v5.pdf","comment":"42 pages, 17 figures, 3 tables"},{"id":"http://arxiv.org/abs/2303.12379v1","updated":"2023-03-22T08:28:23Z","published":"2023-03-22T08:28:23Z","title":"VMCML: Video and Music Matching via Cross-Modality Lifting","summary":"  We propose a content-based system for matching video and background music.\nThe system aims to address the challenges in music recommendation for new users\nor new music give short-form videos. To this end, we propose a cross-modal\nframework VMCML that finds a shared embedding space between video and music\nrepresentations. To ensure the embedding space can be effectively shared by\nboth representations, we leverage CosFace loss based on margin-based cosine\nsimilarity loss. Furthermore, we establish a large-scale dataset called MSVD,\nin which we provide 390 individual music and the corresponding matched 150,000\nvideos. We conduct extensive experiments on Youtube-8M and our MSVD datasets.\nOur quantitative and qualitative results demonstrate the effectiveness of our\nproposed framework and achieve state-of-the-art video and music matching\nperformance.\n","authors":["Yi-Shan Lee","Wei-Cheng Tseng","Fu-En Wang","Min Sun"],"pdf_url":"https://arxiv.org/pdf/2303.12379v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11910v2","updated":"2023-03-22T08:23:28Z","published":"2023-03-21T15:01:02Z","title":"360BEV: Panoramic Semantic Mapping for Indoor Bird's-Eye View","summary":"  Seeing only a tiny part of the whole is not knowing the full circumstance.\nBird's-eye-view (BEV) perception, a process of obtaining allocentric maps from\negocentric views, is restricted when using a narrow Field of View (FoV) alone.\nIn this work, mapping from 360{\\deg} panoramas to BEV semantics, the 360BEV\ntask, is established for the first time to achieve holistic representations of\nindoor scenes in a top-down view. Instead of relying on narrow-FoV image\nsequences, a panoramic image with depth information is sufficient to generate a\nholistic BEV semantic map. To benchmark 360BEV, we present two indoor datasets,\n360BEV-Matterport and 360BEV-Stanford, both of which include egocentric\npanoramic images and semantic segmentation labels, as well as allocentric\nsemantic maps. Besides delving deep into different mapping paradigms, we\npropose a dedicated solution for panoramic semantic mapping, namely 360Mapper.\nThrough extensive experiments, our methods achieve 44.32% and 45.78% in mIoU on\nboth datasets respectively, surpassing previous counterparts with gains of\n+7.60% and +9.70% in mIoU. Code and datasets will be available at:\nhttps://jamycheung.github.io/360BEV.html.\n","authors":["Zhifeng Teng","Jiaming Zhang","Kailun Yang","Kunyu Peng","Hao Shi","Simon Reiß","Ke Cao","Rainer Stiefelhagen"],"pdf_url":"https://arxiv.org/pdf/2303.11910v2.pdf","comment":"Code and datasets will be available at:\n  https://jamycheung.github.io/360BEV.html"},{"id":"http://arxiv.org/abs/2303.12371v1","updated":"2023-03-22T08:14:23Z","published":"2023-03-22T08:14:23Z","title":"$P^{3}O$: Transferring Visual Representations for Reinforcement Learning\n  via Prompting","summary":"  It is important for deep reinforcement learning (DRL) algorithms to transfer\ntheir learned policies to new environments that have different visual inputs.\nIn this paper, we introduce Prompt based Proximal Policy Optimization\n($P^{3}O$), a three-stage DRL algorithm that transfers visual representations\nfrom a target to a source environment by applying prompting. The process of\n$P^{3}O$ consists of three stages: pre-training, prompting, and predicting. In\nparticular, we specify a prompt-transformer for representation conversion and\npropose a two-step training process to train the prompt-transformer for the\ntarget environment, while the rest of the DRL pipeline remains unchanged. We\nimplement $P^{3}O$ and evaluate it on the OpenAI CarRacing video game. The\nexperimental results show that $P^{3}O$ outperforms the state-of-the-art visual\ntransferring schemes. In particular, $P^{3}O$ allows the learned policies to\nperform well in environments with different visual inputs, which is much more\neffective than retraining the policies in these environments.\n","authors":["Guoliang You","Xiaomeng Chu","Yifan Duan","Jie Peng","Jianmin Ji","Yu Zhang","Yanyong Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.12371v1.pdf","comment":"This paper has been accepted to be presented at the upcoming IEEE\n  International Conference on Multimedia & Expo (ICME) in 2023"},{"id":"http://arxiv.org/abs/2303.12370v1","updated":"2023-03-22T08:13:25Z","published":"2023-03-22T08:13:25Z","title":"Weakly Supervised Video Representation Learning with Unaligned Text for\n  Sequential Videos","summary":"  Sequential video understanding, as an emerging video understanding task, has\ndriven lots of researchers' attention because of its goal-oriented nature. This\npaper studies weakly supervised sequential video understanding where the\naccurate time-stamp level text-video alignment is not provided. We solve this\ntask by borrowing ideas from CLIP. Specifically, we use a transformer to\naggregate frame-level features for video representation and use a pre-trained\ntext encoder to encode the texts corresponding to each action and the whole\nvideo, respectively. To model the correspondence between text and video, we\npropose a multiple granularity loss, where the video-paragraph contrastive loss\nenforces matching between the whole video and the complete script, and a\nfine-grained frame-sentence contrastive loss enforces the matching between each\naction and its description. As the frame-sentence correspondence is not\navailable, we propose to use the fact that video actions happen sequentially in\nthe temporal domain to generate pseudo frame-sentence correspondence and\nsupervise the network training with the pseudo labels. Extensive experiments on\nvideo sequence verification and text-to-video matching show that our method\noutperforms baselines by a large margin, which validates the effectiveness of\nour proposed approach. Code is available at https://github.com/svip-lab/WeakSVR\n","authors":["Sixun Dong","Huazhang Hu","Dongze Lian","Weixin Luo","Yicheng Qian","Shenghua Gao"],"pdf_url":"https://arxiv.org/pdf/2303.12370v1.pdf","comment":"CVPR 2023. Code: https://github.com/svip-lab/WeakSVR"},{"id":"http://arxiv.org/abs/2303.12369v1","updated":"2023-03-22T08:11:22Z","published":"2023-03-22T08:11:22Z","title":"Unbiased Multiple Instance Learning for Weakly Supervised Video Anomaly\n  Detection","summary":"  Weakly Supervised Video Anomaly Detection (WSVAD) is challenging because the\nbinary anomaly label is only given on the video level, but the output requires\nsnippet-level predictions. So, Multiple Instance Learning (MIL) is prevailing\nin WSVAD. However, MIL is notoriously known to suffer from many false alarms\nbecause the snippet-level detector is easily biased towards the abnormal\nsnippets with simple context, confused by the normality with the same bias, and\nmissing the anomaly with a different pattern. To this end, we propose a new MIL\nframework: Unbiased MIL (UMIL), to learn unbiased anomaly features that improve\nWSVAD. At each MIL training iteration, we use the current detector to divide\nthe samples into two groups with different context biases: the most confident\nabnormal/normal snippets and the rest ambiguous ones. Then, by seeking the\ninvariant features across the two sample groups, we can remove the variant\ncontext biases. Extensive experiments on benchmarks UCF-Crime and TAD\ndemonstrate the effectiveness of our UMIL. Our code is provided at\nhttps://github.com/ktr-hubrt/UMIL.\n","authors":["Hui Lv","Zhongqi Yue","Qianru Sun","Bin Luo","Zhen Cui","Hanwang Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.12369v1.pdf","comment":"11 pages,10 figures"},{"id":"http://arxiv.org/abs/2303.12368v1","updated":"2023-03-22T08:07:28Z","published":"2023-03-22T08:07:28Z","title":"MAIR: Multi-view Attention Inverse Rendering with 3D Spatially-Varying\n  Lighting Estimation","summary":"  We propose a scene-level inverse rendering framework that uses multi-view\nimages to decompose the scene into geometry, a SVBRDF, and 3D spatially-varying\nlighting. Because multi-view images provide a variety of information about the\nscene, multi-view images in object-level inverse rendering have been taken for\ngranted. However, owing to the absence of multi-view HDR synthetic dataset,\nscene-level inverse rendering has mainly been studied using single-view image.\nWe were able to successfully perform scene-level inverse rendering using\nmulti-view images by expanding OpenRooms dataset and designing efficient\npipelines to handle multi-view images, and splitting spatially-varying\nlighting. Our experiments show that the proposed method not only achieves\nbetter performance than single-view-based methods, but also achieves robust\nperformance on unseen real-world scene. Also, our sophisticated 3D\nspatially-varying lighting volume allows for photorealistic object insertion in\nany 3D location.\n","authors":["JunYong Choi","SeokYeong Lee","Haesol Park","Seung-Won Jung","Ig-Jae Kim","Junghyun Cho"],"pdf_url":"https://arxiv.org/pdf/2303.12368v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12360v1","updated":"2023-03-22T07:51:32Z","published":"2023-03-22T07:51:32Z","title":"Automatically Predict Material Properties with Microscopic Image Example\n  Polymer Compatibility","summary":"  Many material properties are manifested in the morphological appearance and\ncharacterized with microscopic image, such as scanning electron microscopy\n(SEM). Polymer compatibility is a key physical quantity of polymer material and\ncommonly and intuitively judged by SEM images. However, human observation and\njudgement for the images is time-consuming, labor-intensive and hard to be\nquantified. Computer image recognition with machine learning method can make up\nthe defects of artificial judging, giving accurate and quantitative judgement.\nWe achieve automatic compatibility recognition utilizing convolution neural\nnetwork and transfer learning method, and the model obtains up to 94% accuracy.\nWe also put forward a quantitative criterion for polymer compatibility with\nthis model. The proposed method can be widely applied to the quantitative\ncharacterization of the microstructure and properties of various materials.\n","authors":["Zhilong Liang","Zhenzhi Tan","Ruixin Hong","Wanli Ouyang","Jinying Yuan","Changshui Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.12360v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2201.10981v3","updated":"2023-03-22T07:35:10Z","published":"2022-01-26T14:52:23Z","title":"Joint Liver and Hepatic Lesion Segmentation in MRI using a Hybrid CNN\n  with Transformer Layers","summary":"  Deep learning-based segmentation of the liver and hepatic lesions therein\nsteadily gains relevance in clinical practice due to the increasing incidence\nof liver cancer each year. Whereas various network variants with overall\npromising results in the field of medical image segmentation have been\nsuccessfully developed over the last years, almost all of them struggle with\nthe challenge of accurately segmenting hepatic lesions in magnetic resonance\nimaging (MRI). This led to the idea of combining elements of convolutional and\ntransformer-based architectures to overcome the existing limitations. This work\npresents a hybrid network called SWTR-Unet, consisting of a pretrained ResNet,\ntransformer blocks as well as a common Unet-style decoder path. This network\nwas primarily applied to single-modality non-contrast-enhanced liver MRI and\nadditionally to the publicly available computed tomography (CT) data of the\nliver tumor segmentation (LiTS) challenge to verify the applicability on other\nmodalities. For a broader evaluation, multiple state-of-the-art networks were\nimplemented and applied, ensuring a direct comparability. Furthermore,\ncorrelation analysis and an ablation study were carried out, to investigate\nvarious influencing factors on the segmentation accuracy of the presented\nmethod. With Dice scores of averaged 98+-2% for liver and 81+-28% lesion\nsegmentation on the MRI dataset and 97+-2% and 79+-25%, respectively on the CT\ndataset, the proposed SWTR-Unet proved to be a precise approach for liver and\nhepatic lesion segmentation with state-of-the-art results for MRI and competing\naccuracy in CT imaging. The achieved segmentation accuracy was found to be on\npar with manually performed expert segmentations as indicated by inter-observer\nvariabilities for liver lesion segmentation. In conclusion, the presented\nmethod could save valuable time and resources in clinical practice.\n","authors":["Georg Hille","Shubham Agrawal","Pavan Tummala","Christian Wybranski","Maciej Pech","Alexey Surov","Sylvia Saalfeld"],"pdf_url":"https://arxiv.org/pdf/2201.10981v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.00690v3","updated":"2023-03-22T07:20:37Z","published":"2022-08-01T08:58:02Z","title":"Generative Bias for Robust Visual Question Answering","summary":"  The task of Visual Question Answering (VQA) is known to be plagued by the\nissue of VQA models exploiting biases within the dataset to make its final\nprediction. Various previous ensemble based debiasing methods have been\nproposed where an additional model is purposefully trained to be biased in\norder to train a robust target model. However, these methods compute the bias\nfor a model simply from the label statistics of the training data or from\nsingle modal branches. In this work, in order to better learn the bias a target\nVQA model suffers from, we propose a generative method to train the bias model\ndirectly from the target model, called GenB. In particular, GenB employs a\ngenerative network to learn the bias in the target model through a combination\nof the adversarial objective and knowledge distillation. We then debias our\ntarget model with GenB as a bias model, and show through extensive experiments\nthe effects of our method on various VQA bias datasets including VQA-CP2,\nVQA-CP1, GQA-OOD, and VQA-CE, and show state-of-the-art results with the LXMERT\narchitecture on VQA-CP2.\n","authors":["Jae Won Cho","Dong-jin Kim","Hyeonggon Ryu","In So Kweon"],"pdf_url":"https://arxiv.org/pdf/2208.00690v3.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2211.12824v2","updated":"2023-03-22T07:20:06Z","published":"2022-11-23T10:14:12Z","title":"Tell Me What Happened: Unifying Text-guided Video Completion via\n  Multimodal Masked Video Generation","summary":"  Generating a video given the first several static frames is challenging as it\nanticipates reasonable future frames with temporal coherence. Besides video\nprediction, the ability to rewind from the last frame or infilling between the\nhead and tail is also crucial, but they have rarely been explored for video\ncompletion. Since there could be different outcomes from the hints of just a\nfew frames, a system that can follow natural language to perform video\ncompletion may significantly improve controllability. Inspired by this, we\nintroduce a novel task, text-guided video completion (TVC), which requests the\nmodel to generate a video from partial frames guided by an instruction. We then\npropose Multimodal Masked Video Generation (MMVG) to address this TVC task.\nDuring training, MMVG discretizes the video frames into visual tokens and masks\nmost of them to perform video completion from any time point. At inference\ntime, a single MMVG model can address all 3 cases of TVC, including video\nprediction, rewind, and infilling, by applying corresponding masking\nconditions. We evaluate MMVG in various video scenarios, including egocentric,\nanimation, and gaming. Extensive experimental results indicate that MMVG is\neffective in generating high-quality visual appearances with text guidance for\nTVC.\n","authors":["Tsu-Jui Fu","Licheng Yu","Ning Zhang","Cheng-Yang Fu","Jong-Chyi Su","William Yang Wang","Sean Bell"],"pdf_url":"https://arxiv.org/pdf/2211.12824v2.pdf","comment":"CVPR'23"},{"id":"http://arxiv.org/abs/2303.12346v1","updated":"2023-03-22T07:10:09Z","published":"2023-03-22T07:10:09Z","title":"NUWA-XL: Diffusion over Diffusion for eXtremely Long Video Generation","summary":"  In this paper, we propose NUWA-XL, a novel Diffusion over Diffusion\narchitecture for eXtremely Long video generation. Most current work generates\nlong videos segment by segment sequentially, which normally leads to the gap\nbetween training on short videos and inferring long videos, and the sequential\ngeneration is inefficient. Instead, our approach adopts a ``coarse-to-fine''\nprocess, in which the video can be generated in parallel at the same\ngranularity. A global diffusion model is applied to generate the keyframes\nacross the entire time range, and then local diffusion models recursively fill\nin the content between nearby frames. This simple yet effective strategy allows\nus to directly train on long videos (3376 frames) to reduce the\ntraining-inference gap, and makes it possible to generate all segments in\nparallel. To evaluate our model, we build FlintstonesHD dataset, a new\nbenchmark for long video generation. Experiments show that our model not only\ngenerates high-quality long videos with both global and local coherence, but\nalso decreases the average inference time from 7.55min to 26s (by 94.26\\%) at\nthe same hardware setting when generating 1024 frames. The homepage link is\n\\url{https://msra-nuwa.azurewebsites.net/}\n","authors":["Shengming Yin","Chenfei Wu","Huan Yang","Jianfeng Wang","Xiaodong Wang","Minheng Ni","Zhengyuan Yang","Linjie Li","Shuguang Liu","Fan Yang","Jianlong Fu","Gong Ming","Lijuan Wang","Zicheng Liu","Houqiang Li","Nan Duan"],"pdf_url":"https://arxiv.org/pdf/2303.12346v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.00288v3","updated":"2023-03-22T07:03:38Z","published":"2022-11-01T05:48:18Z","title":"Self-supervised Character-to-Character Distillation for Text Recognition","summary":"  When handling complicated text images (e.g., irregular structures, low\nresolution, heavy occlusion, and uneven illumination), existing supervised text\nrecognition methods are data-hungry. Although these methods employ large-scale\nsynthetic text images to reduce the dependence on annotated real images, the\ndomain gap still limits the recognition performance. Therefore, exploring the\nrobust text feature representations on unlabeled real images by self-supervised\nlearning is a good solution. However, existing self-supervised text recognition\nmethods conduct sequence-to-sequence representation learning by roughly\nsplitting the visual features along the horizontal axis, which limits the\nflexibility of the augmentations, as large geometric-based augmentations may\nlead to sequence-to-sequence feature inconsistency. Motivated by this, we\npropose a novel self-supervised Character-to-Character Distillation method,\nCCD, which enables versatile augmentations to facilitate general text\nrepresentation learning. Specifically, we delineate the character structures of\nunlabeled real images by designing a self-supervised character segmentation\nmodule. Following this, CCD easily enriches the diversity of local characters\nwhile keeping their pairwise alignment under flexible augmentations, using the\ntransformation matrix between two augmented views from images. Experiments\ndemonstrate that CCD achieves state-of-the-art results, with average\nperformance gains of 1.38% in text recognition, 1.7% in text segmentation, 0.24\ndB (PSNR) and 0.0321 (SSIM) in text super-resolution. Code will be released\nsoon.\n","authors":["Tongkun Guan","Wei Shen","Xue Yang","Qi Feng","Zekun Jiang"],"pdf_url":"https://arxiv.org/pdf/2211.00288v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.06597v3","updated":"2023-03-22T07:02:46Z","published":"2022-11-12T07:24:29Z","title":"OpenGait: Revisiting Gait Recognition Toward Better Practicality","summary":"  Gait recognition is one of the most critical long-distance identification\ntechnologies and increasingly gains popularity in both research and industry\ncommunities. Despite the significant progress made in indoor datasets, much\nevidence shows that gait recognition techniques perform poorly in the wild.\nMore importantly, we also find that some conclusions drawn from indoor datasets\ncannot be generalized to real applications. Therefore, the primary goal of this\npaper is to present a comprehensive benchmark study for better practicality\nrather than only a particular model for better performance. To this end, we\nfirst develop a flexible and efficient gait recognition codebase named\nOpenGait. Based on OpenGait, we deeply revisit the recent development of gait\nrecognition by re-conducting the ablative experiments. Encouragingly,we detect\nsome unperfect parts of certain prior woks, as well as new insights. Inspired\nby these discoveries, we develop a structurally simple, empirically powerful,\nand practically robust baseline model, GaitBase. Experimentally, we\ncomprehensively compare GaitBase with many current gait recognition methods on\nmultiple public datasets, and the results reflect that GaitBase achieves\nsignificantly strong performance in most cases regardless of indoor or outdoor\nsituations. Code is available at https://github.com/ShiqiYu/OpenGait.\n","authors":["Chao Fan","Junhao Liang","Chuanfu Shen","Saihui Hou","Yongzhen Huang","Shiqi Yu"],"pdf_url":"https://arxiv.org/pdf/2211.06597v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12343v1","updated":"2023-03-22T06:55:01Z","published":"2023-03-22T06:55:01Z","title":"LD-ZNet: A Latent Diffusion Approach for Text-Based Image Segmentation","summary":"  We present a technique for segmenting real and AI-generated images using\nlatent diffusion models (LDMs) trained on internet-scale datasets. First, we\nshow that the latent space of LDMs (z-space) is a better input representation\ncompared to other feature representations like RGB images or CLIP encodings for\ntext-based image segmentation. By training the segmentation models on the\nlatent z-space, which creates a compressed representation across several\ndomains like different forms of art, cartoons, illustrations, and photographs,\nwe are also able to bridge the domain gap between real and AI-generated images.\nWe show that the internal features of LDMs contain rich semantic information\nand present a technique in the form of LD-ZNet to further boost the performance\nof text-based segmentation. Overall, we show up to 6% improvement over standard\nbaselines for text-to-image segmentation on natural images. For AI-generated\nimagery, we show close to 20% improvement compared to state-of-the-art\ntechniques.\n","authors":["Koutilya Pnvr","Bharat Singh","Pallabi Ghosh","Behjat Siddiquie","David Jacobs"],"pdf_url":"https://arxiv.org/pdf/2303.12343v1.pdf","comment":"Supplementary material is included in the paper following the\n  references section"},{"id":"http://arxiv.org/abs/2301.09120v2","updated":"2023-03-22T06:41:10Z","published":"2023-01-22T13:07:24Z","title":"Causality-based Dual-Contrastive Learning Framework for Domain\n  Generalization","summary":"  Domain Generalization (DG) is essentially a sub-branch of out-of-distribution\ngeneralization, which trains models from multiple source domains and\ngeneralizes to unseen target domains. Recently, some domain generalization\nalgorithms have emerged, but most of them were designed with non-transferable\ncomplex architecture. Additionally, contrastive learning has become a promising\nsolution for simplicity and efficiency in DG. However, existing contrastive\nlearning neglected domain shifts that caused severe model confusions. In this\npaper, we propose a Dual-Contrastive Learning (DCL) module on feature and\nprototype contrast. Moreover, we design a novel Causal Fusion Attention (CFA)\nmodule to fuse diverse views of a single image to attain prototype.\nFurthermore, we introduce a Similarity-based Hard-pair Mining (SHM) strategy to\nleverage information on diversity shift. Extensive experiments show that our\nmethod outperforms state-of-the-art algorithms on three DG datasets. The\nproposed algorithm can also serve as a plug-and-play module without usage of\ndomain labels.\n","authors":["Zining Chen","Weiqiu Wang","Zhicheng Zhao","Aidong Men"],"pdf_url":"https://arxiv.org/pdf/2301.09120v2.pdf","comment":"Inadequate proof of the effectiveness of the method"},{"id":"http://arxiv.org/abs/2303.12342v1","updated":"2023-03-22T06:41:09Z","published":"2023-03-22T06:41:09Z","title":"One-Step Detection Paradigm for Hyperspectral Anomaly Detection via\n  Spectral Deviation Relationship Learning","summary":"  Hyperspectral anomaly detection (HAD) involves identifying the targets that\ndeviate spectrally from their surroundings, without prior knowledge. Recently,\ndeep learning based methods have become the mainstream HAD methods, due to\ntheir powerful spatial-spectral feature extraction ability. However, the\ncurrent deep detection models are optimized to complete a proxy task (two-step\nparadigm), such as background reconstruction or generation, rather than\nachieving anomaly detection directly. This leads to suboptimal results and poor\ntransferability, which means that the deep model is trained and tested on the\nsame image. In this paper, an unsupervised transferred direct detection (TDD)\nmodel is proposed, which is optimized directly for the anomaly detection task\n(one-step paradigm) and has transferability. Specially, the TDD model is\noptimized to identify the spectral deviation relationship according to the\nanomaly definition. Compared to learning the specific background distribution\nas most models do, the spectral deviation relationship is universal for\ndifferent images and guarantees the model transferability. To train the TDD\nmodel in an unsupervised manner, an anomaly sample simulation strategy is\nproposed to generate numerous pairs of anomaly samples. Furthermore, a global\nself-attention module and a local self-attention module are designed to help\nthe model focus on the \"spectrally deviating\" relationship. The TDD model was\nvalidated on four public HAD datasets. The results show that the proposed TDD\nmodel can successfully overcome the limitation of traditional model training\nand testing on a single image, and the model has a powerful detection ability\nand excellent transferability.\n","authors":["Jingtao Li","Xinyu Wang","Shaoyu Wang","Hengwei Zhao","Liangpei Zhang","Yanfei Zhong"],"pdf_url":"https://arxiv.org/pdf/2303.12342v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.06551v2","updated":"2023-03-22T06:34:14Z","published":"2022-10-12T19:46:25Z","title":"Learning Human Motion Representations: A Unified Perspective","summary":"  We present a unified perspective on tackling various human-centric video\ntasks by learning human motion representations from large-scale and\nheterogeneous data resources. Specifically, we propose a pretraining stage in\nwhich a motion encoder is trained to recover the underlying 3D motion from\nnoisy partial 2D observations. The motion representations acquired in this way\nincorporate geometric, kinematic, and physical knowledge about human motion,\nwhich can be easily transferred to multiple downstream tasks. We implement the\nmotion encoder with a Dual-stream Spatio-temporal Transformer (DSTformer)\nneural network. It could capture long-range spatio-temporal relationships among\nthe skeletal joints comprehensively and adaptively, exemplified by the lowest\n3D pose estimation error so far when trained from scratch. Furthermore, our\nproposed framework achieves state-of-the-art performance on all three\ndownstream tasks by simply finetuning the pretrained motion encoder with a\nsimple regression head (1-2 layers), which demonstrates the versatility of the\nlearned motion representations.\n","authors":["Wentao Zhu","Xiaoxuan Ma","Zhaoyang Liu","Libin Liu","Wayne Wu","Yizhou Wang"],"pdf_url":"https://arxiv.org/pdf/2210.06551v2.pdf","comment":"Project page: https://motionbert.github.io/"},{"id":"http://arxiv.org/abs/2303.12337v1","updated":"2023-03-22T06:26:56Z","published":"2023-03-22T06:26:56Z","title":"Music-Driven Group Choreography","summary":"  Music-driven choreography is a challenging problem with a wide variety of\nindustrial applications. Recently, many methods have been proposed to\nsynthesize dance motions from music for a single dancer. However, generating\ndance motion for a group remains an open problem. In this paper, we present\n$\\rm AIOZ-GDANCE$, a new large-scale dataset for music-driven group dance\ngeneration. Unlike existing datasets that only support single dance, our new\ndataset contains group dance videos, hence supporting the study of group\nchoreography. We propose a semi-autonomous labeling method with humans in the\nloop to obtain the 3D ground truth for our dataset. The proposed dataset\nconsists of $16.7$ hours of paired music and 3D motion from in-the-wild videos,\ncovering $7$ dance styles and $16$ music genres. We show that naively applying\nsingle dance generation technique to creating group dance motion may lead to\nunsatisfactory results, such as inconsistent movements and collisions between\ndancers. Based on our new dataset, we propose a new method that takes an input\nmusic sequence and a set of 3D positions of dancers to efficiently produce\nmultiple group-coherent choreographies. We propose new evaluation metrics for\nmeasuring group dance quality and perform intensive experiments to demonstrate\nthe effectiveness of our method.\n","authors":["Nhat Le","Thang Pham","Tuong Do","Erman Tjiputra","Quang D. Tran","Anh Nguyen"],"pdf_url":"https://arxiv.org/pdf/2303.12337v1.pdf","comment":"accepted in cvpr 2023"},{"id":"http://arxiv.org/abs/2303.12332v1","updated":"2023-03-22T06:08:34Z","published":"2023-03-22T06:08:34Z","title":"Weakly-Supervised Temporal Action Localization by Inferring\n  Snippet-Feature Affinity","summary":"  Weakly-supervised temporal action localization aims to locate action regions\nand identify action categories in untrimmed videos, only taking video-level\nlabels as the supervised information. Pseudo label generation is a promising\nstrategy to solve the challenging problem, but most existing methods are\nlimited to employing snippet-wise classification results to guide the\ngeneration, and they ignore that the natural temporal structure of the video\ncan also provide rich information to assist such a generation process. In this\npaper, we propose a novel weakly-supervised temporal action localization method\nby inferring snippet-feature affinity. First, we design an affinity inference\nmodule that exploits the affinity relationship between temporal neighbor\nsnippets to generate initial coarse pseudo labels. Then, we introduce an\ninformation interaction module that refines the coarse labels by enhancing the\ndiscriminative nature of snippet-features through exploring intra- and\ninter-video relationships. Finally, the high-fidelity pseudo labels generated\nfrom the information interaction module are used to supervise the training of\nthe action localization network. Extensive experiments on two publicly\navailable datasets, i.e., THUMOS14 and ActivityNet v1.3, demonstrate our\nproposed method achieves significant improvements compared to the\nstate-of-the-art methods.\n","authors":["Wulian Yun","Mengshi Qi","Chuanming Wang","Huadong Ma"],"pdf_url":"https://arxiv.org/pdf/2303.12332v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10752v2","updated":"2023-03-22T06:02:06Z","published":"2023-03-19T19:59:48Z","title":"Fully Self-Supervised Depth Estimation from Defocus Clue","summary":"  Depth-from-defocus (DFD), modeling the relationship between depth and defocus\npattern in images, has demonstrated promising performance in depth estimation.\nRecently, several self-supervised works try to overcome the difficulties in\nacquiring accurate depth ground-truth. However, they depend on the all-in-focus\n(AIF) images, which cannot be captured in real-world scenarios. Such limitation\ndiscourages the applications of DFD methods. To tackle this issue, we propose a\ncompletely self-supervised framework that estimates depth purely from a sparse\nfocal stack. We show that our framework circumvents the needs for the depth and\nAIF image ground-truth, and receives superior predictions, thus closing the gap\nbetween the theoretical success of DFD works and their applications in the real\nworld. In particular, we propose (i) a more realistic setting for DFD tasks,\nwhere no depth or AIF image ground-truth is available; (ii) a novel\nself-supervision framework that provides reliable predictions of depth and AIF\nimage under the challenging setting. The proposed framework uses a neural model\nto predict the depth and AIF image, and utilizes an optical model to validate\nand refine the prediction. We verify our framework on three benchmark datasets\nwith rendered focal stacks and real focal stacks. Qualitative and quantitative\nevaluations show that our method provides a strong baseline for self-supervised\nDFD tasks.\n","authors":["Haozhe Si","Bin Zhao","Dong Wang","Yupeng Gao","Mulin Chen","Zhigang Wang","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2303.10752v2.pdf","comment":"CVPR 2023 camera-ready version. The code is released at\n  https://github.com/Ehzoahis/DEReD"},{"id":"http://arxiv.org/abs/2206.15083v2","updated":"2023-03-22T05:52:23Z","published":"2022-06-30T07:32:23Z","title":"UniDAformer: Unified Domain Adaptive Panoptic Segmentation Transformer\n  via Hierarchical Mask Calibration","summary":"  Domain adaptive panoptic segmentation aims to mitigate data annotation\nchallenge by leveraging off-the-shelf annotated data in one or multiple related\nsource domains. However, existing studies employ two separate networks for\ninstance segmentation and semantic segmentation which lead to excessive network\nparameters as well as complicated and computationally intensive training and\ninference processes. We design UniDAformer, a unified domain adaptive panoptic\nsegmentation transformer that is simple but can achieve domain adaptive\ninstance segmentation and semantic segmentation simultaneously within a single\nnetwork. UniDAformer introduces Hierarchical Mask Calibration (HMC) that\nrectifies inaccurate predictions at the level of regions, superpixels and\npixels via online self-training on the fly. It has three unique features: 1) it\nenables unified domain adaptive panoptic adaptation; 2) it mitigates false\npredictions and improves domain adaptive panoptic segmentation effectively; 3)\nit is end-to-end trainable with a much simpler training and inference pipeline.\nExtensive experiments over multiple public benchmarks show that UniDAformer\nachieves superior domain adaptive panoptic segmentation as compared with the\nstate-of-the-art.\n","authors":["Jingyi Zhang","Jiaxing Huang","Xiaoqin Zhang","Shijian Lu"],"pdf_url":"https://arxiv.org/pdf/2206.15083v2.pdf","comment":"Accepted to CVPR2023"},{"id":"http://arxiv.org/abs/2303.12326v1","updated":"2023-03-22T05:51:53Z","published":"2023-03-22T05:51:53Z","title":"Make Encoder Great Again in 3D GAN Inversion through Geometry and\n  Occlusion-Aware Encoding","summary":"  3D GAN inversion aims to achieve high reconstruction fidelity and reasonable\n3D geometry simultaneously from a single image input. However, existing 3D GAN\ninversion methods rely on time-consuming optimization for each individual case.\nIn this work, we introduce a novel encoder-based inversion framework based on\nEG3D, one of the most widely-used 3D GAN models. We leverage the inherent\nproperties of EG3D's latent space to design a discriminator and a background\ndepth regularization. This enables us to train a geometry-aware encoder capable\nof converting the input image into corresponding latent code. Additionally, we\nexplore the feature space of EG3D and develop an adaptive refinement stage that\nimproves the representation ability of features in EG3D to enhance the recovery\nof fine-grained textural details. Finally, we propose an occlusion-aware fusion\noperation to prevent distortion in unobserved regions. Our method achieves\nimpressive results comparable to optimization-based methods while operating up\nto 500 times faster. Our framework is well-suited for applications such as\nsemantic editing.\n","authors":["Ziyang Yuan","Yiming Zhu","Yu Li","Hongyu Liu","Chun Yuan"],"pdf_url":"https://arxiv.org/pdf/2303.12326v1.pdf","comment":"Project page: https://eg3d-goae.github.io/"},{"id":"http://arxiv.org/abs/2303.12317v1","updated":"2023-03-22T05:21:21Z","published":"2023-03-22T05:21:21Z","title":"Re-thinking Federated Active Learning based on Inter-class Diversity","summary":"  Although federated learning has made awe-inspiring advances, most studies\nhave assumed that the client's data are fully labeled. However, in a real-world\nscenario, every client may have a significant amount of unlabeled instances.\nAmong the various approaches to utilizing unlabeled data, a federated active\nlearning framework has emerged as a promising solution. In the decentralized\nsetting, there are two types of available query selector models, namely\n'global' and 'local-only' models, but little literature discusses their\nperformance dominance and its causes. In this work, we first demonstrate that\nthe superiority of two selector models depends on the global and local\ninter-class diversity. Furthermore, we observe that the global and local-only\nmodels are the keys to resolving the imbalance of each side. Based on our\nfindings, we propose LoGo, a FAL sampling strategy robust to varying local\nheterogeneity levels and global imbalance ratio, that integrates both models by\ntwo steps of active selection scheme. LoGo consistently outperforms six active\nlearning strategies in the total number of 38 experimental settings.\n","authors":["SangMook Kim","Sangmin Bae","Hwanjun Song","Se-Young Yun"],"pdf_url":"https://arxiv.org/pdf/2303.12317v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.10598v2","updated":"2023-03-22T05:21:14Z","published":"2023-03-19T08:26:06Z","title":"StyleRF: Zero-shot 3D Style Transfer of Neural Radiance Fields","summary":"  3D style transfer aims to render stylized novel views of a 3D scene with\nmulti-view consistency. However, most existing work suffers from a three-way\ndilemma over accurate geometry reconstruction, high-quality stylization, and\nbeing generalizable to arbitrary new styles. We propose StyleRF (Style Radiance\nFields), an innovative 3D style transfer technique that resolves the three-way\ndilemma by performing style transformation within the feature space of a\nradiance field. StyleRF employs an explicit grid of high-level features to\nrepresent 3D scenes, with which high-fidelity geometry can be reliably restored\nvia volume rendering. In addition, it transforms the grid features according to\nthe reference style which directly leads to high-quality zero-shot style\ntransfer. StyleRF consists of two innovative designs. The first is\nsampling-invariant content transformation that makes the transformation\ninvariant to the holistic statistics of the sampled 3D points and accordingly\nensures multi-view consistency. The second is deferred style transformation of\n2D feature maps which is equivalent to the transformation of 3D points but\ngreatly reduces memory footprint without degrading multi-view consistency.\nExtensive experiments show that StyleRF achieves superior 3D stylization\nquality with precise geometry reconstruction and it can generalize to various\nnew styles in a zero-shot manner.\n","authors":["Kunhao Liu","Fangneng Zhan","Yiwen Chen","Jiahui Zhang","Yingchen Yu","Abdulmotaleb El Saddik","Shijian Lu","Eric Xing"],"pdf_url":"https://arxiv.org/pdf/2303.10598v2.pdf","comment":"Accepted to CVPR 2023. Project website:\n  https://kunhao-liu.github.io/StyleRF/"},{"id":"http://arxiv.org/abs/2211.16312v2","updated":"2023-03-22T05:17:01Z","published":"2022-11-29T15:52:22Z","title":"PLA: Language-Driven Open-Vocabulary 3D Scene Understanding","summary":"  Open-vocabulary scene understanding aims to localize and recognize unseen\ncategories beyond the annotated label space. The recent breakthrough of 2D\nopen-vocabulary perception is largely driven by Internet-scale paired\nimage-text data with rich vocabulary concepts. However, this success cannot be\ndirectly transferred to 3D scenarios due to the inaccessibility of large-scale\n3D-text pairs. To this end, we propose to distill knowledge encoded in\npre-trained vision-language (VL) foundation models through captioning\nmulti-view images from 3D, which allows explicitly associating 3D and\nsemantic-rich captions. Further, to foster coarse-to-fine visual-semantic\nrepresentation learning from captions, we design hierarchical 3D-caption pairs,\nleveraging geometric constraints between 3D scenes and multi-view images.\nFinally, by employing contrastive learning, the model learns language-aware\nembeddings that connect 3D and text for open-vocabulary tasks. Our method not\nonly remarkably outperforms baseline methods by 25.8% $\\sim$ 44.7% hIoU and\n14.5% $\\sim$ 50.4% hAP$_{50}$ in open-vocabulary semantic and instance\nsegmentation, but also shows robust transferability on challenging zero-shot\ndomain transfer tasks. See the project website at\nhttps://dingry.github.io/projects/PLA.\n","authors":["Runyu Ding","Jihan Yang","Chuhui Xue","Wenqing Zhang","Song Bai","Xiaojuan Qi"],"pdf_url":"https://arxiv.org/pdf/2211.16312v2.pdf","comment":"CVPR2023"},{"id":"http://arxiv.org/abs/2103.17084v2","updated":"2023-03-22T05:15:36Z","published":"2021-03-31T13:55:56Z","title":"DA-DETR: Domain Adaptive Detection Transformer with Information Fusion","summary":"  The recent detection transformer (DETR) simplifies the object detection\npipeline by removing hand-crafted designs and hyperparameters as employed in\nconventional two-stage object detectors. However, how to leverage the simple\nyet effective DETR architecture in domain adaptive object detection is largely\nneglected. Inspired by the unique DETR attention mechanisms, we design DA-DETR,\na domain adaptive object detection transformer that introduces information\nfusion for effective transfer from a labeled source domain to an unlabeled\ntarget domain. DA-DETR introduces a novel CNN-Transformer Blender (CTBlender)\nthat fuses the CNN features and Transformer features ingeniously for effective\nfeature alignment and knowledge transfer across domains. Specifically,\nCTBlender employs the Transformer features to modulate the CNN features across\nmultiple scales where the high-level semantic information and the low-level\nspatial information are fused for accurate object identification and\nlocalization. Extensive experiments show that DA-DETR achieves superior\ndetection performance consistently across multiple widely adopted domain\nadaptation benchmarks.\n","authors":["Jingyi Zhang","Jiaxing Huang","Zhipeng Luo","Gongjie Zhang","Xiaoqin Zhang","Shijian Lu"],"pdf_url":"https://arxiv.org/pdf/2103.17084v2.pdf","comment":"Accepted to CVPR2023"},{"id":"http://arxiv.org/abs/2303.12313v1","updated":"2023-03-22T05:03:14Z","published":"2023-03-22T05:03:14Z","title":"Distribution Aligned Diffusion and Prototype-guided network for\n  Unsupervised Domain Adaptive Segmentation","summary":"  The Diffusion Probabilistic Model (DPM) has emerged as a highly effective\ngenerative model in the field of computer vision. Its intermediate latent\nvectors offer rich semantic information, making it an attractive option for\nvarious downstream tasks such as segmentation and detection. In order to\nexplore its potential further, we have taken a step forward and considered a\nmore complex scenario in the medical image domain, specifically, under an\nunsupervised adaptation condition. To this end, we propose a Diffusion-based\nand Prototype-guided network (DP-Net) for unsupervised domain adaptive\nsegmentation. Concretely, our DP-Net consists of two stages: 1) Distribution\nAligned Diffusion (DADiff), which involves training a domain discriminator to\nminimize the difference between the intermediate features generated by the DPM,\nthereby aligning the inter-domain distribution; and 2) Prototype-guided\nConsistency Learning (PCL), which utilizes feature centroids as prototypes and\napplies a prototype-guided loss to ensure that the segmentor learns consistent\ncontent from both source and target domains. Our approach is evaluated on\nfundus datasets through a series of experiments, which demonstrate that the\nperformance of the proposed method is reliable and outperforms state-of-the-art\nmethods. Our work presents a promising direction for using DPM in complex\nmedical image scenarios, opening up new possibilities for further research in\nmedical imaging.\n","authors":["Haipeng Zhou","Lei Zhu","Yuyin Zhou"],"pdf_url":"https://arxiv.org/pdf/2303.12313v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12307v1","updated":"2023-03-22T04:49:23Z","published":"2023-03-22T04:49:23Z","title":"Curvature-Balanced Feature Manifold Learning for Long-Tailed\n  Classification","summary":"  To address the challenges of long-tailed classification, researchers have\nproposed several approaches to reduce model bias, most of which assume that\nclasses with few samples are weak classes. However, recent studies have shown\nthat tail classes are not always hard to learn, and model bias has been\nobserved on sample-balanced datasets, suggesting the existence of other factors\nthat affect model bias. In this work, we systematically propose a series of\ngeometric measurements for perceptual manifolds in deep neural networks, and\nthen explore the effect of the geometric characteristics of perceptual\nmanifolds on classification difficulty and how learning shapes the geometric\ncharacteristics of perceptual manifolds. An unanticipated finding is that the\ncorrelation between the class accuracy and the separation degree of perceptual\nmanifolds gradually decreases during training, while the negative correlation\nwith the curvature gradually increases, implying that curvature imbalance leads\nto model bias. Therefore, we propose curvature regularization to facilitate the\nmodel to learn curvature-balanced and flatter perceptual manifolds. Evaluations\non multiple long-tailed and non-long-tailed datasets show the excellent\nperformance and exciting generality of our approach, especially in achieving\nsignificant performance improvements based on current state-of-the-art\ntechniques. Our work opens up a geometric analysis perspective on model bias\nand reminds researchers to pay attention to model bias on non-long-tailed and\neven sample-balanced datasets. The code and model will be made public.\n","authors":["Yanbiao Ma","Licheng Jiao","Fang Liu","Shuyuan Yang","Xu Liu","Lingling Li"],"pdf_url":"https://arxiv.org/pdf/2303.12307v1.pdf","comment":"20pages, Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2303.11616v2","updated":"2023-03-22T04:36:11Z","published":"2023-03-21T06:26:18Z","title":"HRDFuse: Monocular 360°Depth Estimation by Collaboratively Learning\n  Holistic-with-Regional Depth Distributions","summary":"  Depth estimation from a monocular 360{\\deg} image is a burgeoning problem\nowing to its holistic sensing of a scene. Recently, some methods, \\eg,\nOmniFusion, have applied the tangent projection (TP) to represent a\n360{\\deg}image and predicted depth values via patch-wise regressions, which are\nmerged to get a depth map with equirectangular projection (ERP) format.\nHowever, these methods suffer from 1) non-trivial process of merging plenty of\npatches; 2) capturing less holistic-with-regional contextual information by\ndirectly regressing the depth value of each pixel. In this paper, we propose a\nnovel framework, \\textbf{HRDFuse}, that subtly combines the potential of\nconvolutional neural networks (CNNs) and transformers by collaboratively\nlearning the \\textit{holistic} contextual information from the ERP and the\n\\textit{regional} structural information from the TP. Firstly, we propose a\nspatial feature alignment (\\textbf{SFA}) module that learns feature\nsimilarities between the TP and ERP to aggregate the TP features into a\ncomplete ERP feature map in a pixel-wise manner. Secondly, we propose a\ncollaborative depth distribution classification (\\textbf{CDDC}) module that\nlearns the \\textbf{holistic-with-regional} histograms capturing the ERP and TP\ndepth distributions. As such, the final depth values can be predicted as a\nlinear combination of histogram bin centers. Lastly, we adaptively combine the\ndepth predictions from ERP and TP to obtain the final depth map. Extensive\nexperiments show that our method predicts\\textbf{ more smooth and accurate\ndepth} results while achieving \\textbf{favorably better} results than the SOTA\nmethods.\n","authors":["Hao Ai","Zidong cao","Yan-pei Cao","Ying Shan","Lin Wang"],"pdf_url":"https://arxiv.org/pdf/2303.11616v2.pdf","comment":"To appear at CVPR2023, 20 pages"},{"id":"http://arxiv.org/abs/2303.12304v1","updated":"2023-03-22T04:33:02Z","published":"2023-03-22T04:33:02Z","title":"SiamTHN: Siamese Target Highlight Network for Visual Tracking","summary":"  Siamese network based trackers develop rapidly in the field of visual object\ntracking in recent years. The majority of siamese network based trackers now in\nuse treat each channel in the feature maps generated by the backbone network\nequally, making the similarity response map sensitive to background influence\nand hence challenging to focus on the target region. Additionally, there are no\nstructural links between the classification and regression branches in these\ntrackers, and the two branches are optimized separately during training.\nTherefore, there is a misalignment between the classification and regression\nbranches, which results in less accurate tracking results. In this paper, a\nTarget Highlight Module is proposed to help the generated similarity response\nmaps to be more focused on the target region. To reduce the misalignment and\nproduce more precise tracking results, we propose a corrective loss to train\nthe model. The two branches of the model are jointly tuned with the use of\ncorrective loss to produce more reliable prediction results. Experiments on 5\nchallenging benchmark datasets reveal that the method outperforms current\nmodels in terms of performance, and runs at 38 fps, proving its effectiveness\nand efficiency.\n","authors":["Jiahao Bao","Kaiqiang Chen","Xian Sun","Liangjin Zhao","Wenhui Diao","Menglong Yan"],"pdf_url":"https://arxiv.org/pdf/2303.12304v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.00954v3","updated":"2023-03-22T04:27:59Z","published":"2023-01-03T05:30:56Z","title":"PanopticPartFormer++: A Unified and Decoupled View for Panoptic Part\n  Segmentation","summary":"  Panoptic Part Segmentation (PPS) unifies panoptic and part segmentation into\none task. Previous works utilize separate approaches to handle things, stuff,\nand part predictions without shared computation and task association. We aim to\nunify these tasks at the architectural level, designing the first end-to-end\nunified framework, Panoptic-PartFormer. Moreover, we find the previous metric\nPartPQ biases to PQ. To handle both issues, we first design a meta-architecture\nthat decouples part features and things/stuff features, respectively. We model\nthings, stuff, and parts as object queries and directly learn to optimize all\nthree forms of prediction as a unified mask prediction and classification\nproblem. We term our model as Panoptic-PartFormer. Second, we propose a new\nmetric Part-Whole Quality (PWQ), better to measure this task from pixel-region\nand part-whole perspectives. It also decouples the errors for part segmentation\nand panoptic segmentation. Third, inspired by Mask2Former, based on our\nmeta-architecture, we propose Panoptic-PartFormer++ and design a new part-whole\ncross-attention scheme to boost part segmentation qualities further. We design\na new part-whole interaction method using masked cross attention. Finally,\nextensive ablation studies and analysis demonstrate the effectiveness of both\nPanoptic-PartFormer and Panoptic-PartFormer++. Compared with previous\nPanoptic-PartFormer, our Panoptic-PartFormer++ achieves 2% PartPQ and 3% PWQ\nimprovements on the Cityscapes PPS dataset and 5% PartPQ on the Pascal Context\nPPS dataset. On both datasets, Panoptic-PartFormer++ achieves new\nstate-of-the-art results. Our models can serve as a strong baseline and aid\nfuture research in PPS. The source code and trained models will be available\nat~\\url{https://github.com/lxtGH/Panoptic-PartFormer}.\n","authors":["Xiangtai Li","Shilin Xu","Yibo Yang","Haobo Yuan","Guangliang Cheng","Yunhai Tong","Zhouchen Lin","Ming-Hsuan Yang","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2301.00954v3.pdf","comment":"Extension of PanopticPartFormer (ECCV 2022). Code:\n  https://github.com/lxtGH/Panoptic-PartFormer. Update Results"},{"id":"http://arxiv.org/abs/2303.11305v2","updated":"2023-03-22T04:23:56Z","published":"2023-03-20T17:45:02Z","title":"SVDiff: Compact Parameter Space for Diffusion Fine-Tuning","summary":"  Diffusion models have achieved remarkable success in text-to-image\ngeneration, enabling the creation of high-quality images from text prompts or\nother modalities. However, existing methods for customizing these models are\nlimited by handling multiple personalized subjects and the risk of overfitting.\nMoreover, their large number of parameters is inefficient for model storage. In\nthis paper, we propose a novel approach to address these limitations in\nexisting text-to-image diffusion models for personalization. Our method\ninvolves fine-tuning the singular values of the weight matrices, leading to a\ncompact and efficient parameter space that reduces the risk of overfitting and\nlanguage-drifting. We also propose a Cut-Mix-Unmix data-augmentation technique\nto enhance the quality of multi-subject image generation and a simple\ntext-based image editing framework. Our proposed SVDiff method has a\nsignificantly smaller model size (1.7MB for StableDiffusion) compared to\nexisting methods (vanilla DreamBooth 3.66GB, Custom Diffusion 73MB), making it\nmore practical for real-world applications.\n","authors":["Ligong Han","Yinxiao Li","Han Zhang","Peyman Milanfar","Dimitris Metaxas","Feng Yang"],"pdf_url":"https://arxiv.org/pdf/2303.11305v2.pdf","comment":"20 pages, 21 figures"},{"id":"http://arxiv.org/abs/2212.02809v3","updated":"2023-03-22T04:08:48Z","published":"2022-12-06T07:58:21Z","title":"An advanced YOLOv3 method for small object detection","summary":"  Small object detection has important application value in the fields of\nautonomous driving and drone scene analysis. As one of the most advanced object\ndetection algorithms, YOLOv3 suffers some challenges when detecting small\nobjects, such as the problem of detection failure of small objects and occluded\nobjects. To solve these problems, an improved YOLOv3 algorithm for small object\ndetection is proposed. In the proposed method, the dilated convolutions mish\n(DCM) module is introduced into the backbone network of YOLOv3 to improve the\nfeature expression ability by fusing the feature maps of different receptive\nfields. In the neck network of YOLOv3, the convolutional block attention module\n(CBAM) and multi-level fusion module are introduced to select the important\ninformation for small object detection in the shallow network, suppress the\nuncritical information, and use the fusion module to fuse the feature maps of\ndifferent scales, so as to improve the detection accuracy of the algorithm. In\naddition, the Soft-NMS and Complete-IoU (CloU) strategies are applied to\ncandidate frame screening, which improves the accuracy of the algorithm for the\ndetection of occluded objects. The ablation experiment of the MS COCO2017\nobject detection task proves the effectiveness of several modules introduced in\nthis paper for small object detection. The experimental results on the MS\nCOCO2017, VOC2007, and VOC2012 datasets show that the Average Precision (AP) of\nthis method is 16.5%, 8.71%, and 9.68% higher than that of YOLOv3,\nrespectively.\n","authors":["Baokai Liu","Fengjie He","Shiqiang Du","Jiacheng Li","Wenjie Liu"],"pdf_url":"https://arxiv.org/pdf/2212.02809v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.05149v3","updated":"2023-03-22T03:47:41Z","published":"2022-06-10T14:44:43Z","title":"Referring Image Matting","summary":"  Different from conventional image matting, which either requires user-defined\nscribbles/trimap to extract a specific foreground object or directly extracts\nall the foreground objects in the image indiscriminately, we introduce a new\ntask named Referring Image Matting (RIM) in this paper, which aims to extract\nthe meticulous alpha matte of the specific object that best matches the given\nnatural language description, thus enabling a more natural and simpler\ninstruction for image matting. First, we establish a large-scale challenging\ndataset RefMatte by designing a comprehensive image composition and expression\ngeneration engine to automatically produce high-quality images along with\ndiverse text attributes based on public datasets. RefMatte consists of 230\nobject categories, 47,500 images, 118,749 expression-region entities, and\n474,996 expressions. Additionally, we construct a real-world test set with 100\nhigh-resolution natural images and manually annotate complex phrases to\nevaluate the out-of-domain generalization abilities of RIM methods.\nFurthermore, we present a novel baseline method CLIPMat for RIM, including a\ncontext-embedded prompt, a text-driven semantic pop-up, and a multi-level\ndetails extractor. Extensive experiments on RefMatte in both keyword and\nexpression settings validate the superiority of CLIPMat over representative\nmethods. We hope this work could provide novel insights into image matting and\nencourage more follow-up studies. The dataset, code and models are available at\nhttps://github.com/JizhiziLi/RIM.\n","authors":["Jizhizi Li","Jing Zhang","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2206.05149v3.pdf","comment":"Accepted to CVPR2023. The dataset, code and models are available at\n  https://github.com/JizhiziLi/RIM"},{"id":"http://arxiv.org/abs/2303.11728v2","updated":"2023-03-22T03:30:47Z","published":"2023-03-21T10:32:27Z","title":"ExtremeNeRF: Few-shot Neural Radiance Fields Under Unconstrained\n  Illumination","summary":"  In this paper, we propose a new challenge that synthesizes a novel view in a\nmore practical environment, where the number of input multi-view images is\nlimited and illumination variations are significant. Despite recent success,\nneural radiance fields (NeRF) require a massive amount of input multi-view\nimages taken under constrained illuminations. To address the problem, we\nsuggest ExtremeNeRF, which utilizes occlusion-aware multiview albedo\nconsistency, supported by geometric alignment and depth consistency. We extract\nintrinsic image components that should be illumination-invariant across\ndifferent views, enabling direct appearance comparison between the input and\nnovel view under unconstrained illumination. We provide extensive experimental\nresults for an evaluation of the task, using the newly built NeRF Extreme\nbenchmark, which is the first in-the-wild novel view synthesis benchmark taken\nunder multiple viewing directions and varying illuminations. The project page\nis at https://seokyeong94.github.io/ExtremeNeRF/\n","authors":["SeokYeong Lee","JunYong Choi","Seungryong Kim","Ig-Jae Kim","Junghyun Cho"],"pdf_url":"https://arxiv.org/pdf/2303.11728v2.pdf","comment":"Project Page: https://seokyeong94.github.io/ExtremeNeRF/"},{"id":"http://arxiv.org/abs/2211.11629v2","updated":"2023-03-22T03:28:46Z","published":"2022-11-21T16:43:33Z","title":"PVT++: A Simple End-to-End Latency-Aware Visual Tracking Framework","summary":"  Visual object tracking is essential to intelligent robots. Most existing\napproaches have ignored the online latency that can cause severe performance\ndegradation during real-world processing. Especially for unmanned aerial\nvehicles (UAVs), where robust tracking is more challenging and onboard\ncomputation is limited, the latency issue can be fatal. In this work, we\npresent a simple framework for end-to-end latency-aware tracking, i.e.,\nend-to-end predictive visual tracking (PVT++). Unlike existing solutions that\nnaively append Kalman Filters after trackers, PVT++ can be jointly optimized,\nso that it takes not only motion information but can also leverage the rich\nvisual knowledge in most pre-trained tracker models for robust prediction.\nBesides, to bridge the training-evaluation domain gap, we propose a relative\nmotion factor, empowering PVT++ to generalize to the challenging and complex\nUAV tracking scenes. These careful designs have made the small-capacity\nlightweight PVT++ a widely effective solution. Additionally, this work presents\nan extended latency-aware evaluation benchmark for assessing an any-speed\ntracker in the online setting. Empirical results on a robotic platform from the\naerial perspective show that PVT++ can achieve significant performance gain on\nvarious trackers and exhibit higher accuracy than prior solutions, largely\nmitigating the degradation brought by latency. Our code will be made public.\n","authors":["Bowen Li","Ziyuan Huang","Junjie Ye","Yiming Li","Sebastian Scherer","Hang Zhao","Changhong Fu"],"pdf_url":"https://arxiv.org/pdf/2211.11629v2.pdf","comment":"18 pages, 10 figures"},{"id":"http://arxiv.org/abs/2303.12280v1","updated":"2023-03-22T03:13:55Z","published":"2023-03-22T03:13:55Z","title":"NLOS-NeuS: Non-line-of-sight Neural Implicit Surface","summary":"  Non-line-of-sight (NLOS) imaging is conducted to infer invisible scenes from\nindirect light on visible objects. The neural transient field (NeTF) was\nproposed for representing scenes as neural radiance fields in NLOS scenes. We\npropose NLOS neural implicit surface (NLOS-NeuS), which extends the NeTF to\nneural implicit surfaces with a signed distance function (SDF) for\nreconstructing three-dimensional surfaces in NLOS scenes. We introduce two\nconstraints as loss functions for correctly learning an SDF to avoid non-zero\nlevel-set surfaces. We also introduce a lower bound constraint of an SDF based\non the geometry of the first-returning photons. The experimental results\nindicate that these constraints are essential for learning a correct SDF in\nNLOS scenes. Compared with previous methods with discretized representation,\nNLOS-NeuS with the neural continuous representation enables us to reconstruct\nsmooth surfaces while preserving fine details in NLOS scenes. To the best of\nour knowledge, this is the first study on neural implicit surfaces with volume\nrendering in NLOS scenes.\n","authors":["Yuki Fujimura","Takahiro Kushida","Takuya Funatomi","Yasuhiro Mukaigawa"],"pdf_url":"https://arxiv.org/pdf/2303.12280v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2303.07543v3","updated":"2023-03-22T03:11:28Z","published":"2023-03-14T00:13:57Z","title":"WDiscOOD: Out-of-Distribution Detection via Whitened Linear Discriminant\n  Analysis","summary":"  Deep neural networks are susceptible to generating overconfident yet\nerroneous predictions when presented with data beyond known concepts. This\nchallenge underscores the importance of detecting out-of-distribution (OOD)\nsamples in the open world. In this work, we propose a novel feature-space OOD\ndetection score that jointly reasons with both class-specific and\nclass-agnostic information. Specifically, our approach utilizes Whitened Linear\nDiscriminant Analysis to project features into two subspaces - the\ndiscriminative and residual subspaces - in which the ID classes are maximally\nseparated and closely clustered, respectively. The OOD score is then determined\nby combining the deviation from the input data to the ID distribution in both\nsubspaces. The efficacy of our method, named WDiscOOD, is verified on the\nlarge-scale ImageNet-1k benchmark, with six OOD datasets that covers a variety\nof distribution shifts. WDiscOOD demonstrates superior performance on deep\nclassifiers with diverse backbone architectures, including CNN and vision\ntransformer. Furthermore, we also show that our method can more effectively\ndetect novel concepts in representation space trained with contrastive\nobjectives, including supervised contrastive loss and multi-modality\ncontrastive loss.\n","authors":["Yiye Chen","Yunzhi Lin","Ruinian Xu","Patricio A. Vela"],"pdf_url":"https://arxiv.org/pdf/2303.07543v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12274v1","updated":"2023-03-22T02:47:42Z","published":"2023-03-22T02:47:42Z","title":"A Hierarchical Hybrid Learning Framework for Multi-agent Trajectory\n  Prediction","summary":"  Accurate and robust trajectory prediction of neighboring agents is critical\nfor autonomous vehicles traversing in complex scenes. Most methods proposed in\nrecent years are deep learning-based due to their strength in encoding complex\ninteractions. However, unplausible predictions are often generated since they\nrely heavily on past observations and cannot effectively capture the transient\nand contingency interactions from sparse samples. In this paper, we propose a\nhierarchical hybrid framework of deep learning (DL) and reinforcement learning\n(RL) for multi-agent trajectory prediction, to cope with the challenge of\npredicting motions shaped by multi-scale interactions. In the DL stage, the\ntraffic scene is divided into multiple intermediate-scale heterogenous graphs\nbased on which Transformer-style GNNs are adopted to encode heterogenous\ninteractions at intermediate and global levels. In the RL stage, we divide the\ntraffic scene into local sub-scenes utilizing the key future points predicted\nin the DL stage. To emulate the motion planning procedure so as to produce\ntrajectory predictions, a Transformer-based Proximal Policy Optimization (PPO)\nincorporated with a vehicle kinematics model is devised to plan motions under\nthe dominant influence of microscopic interactions. A multi-objective reward is\ndesigned to balance between agent-centric accuracy and scene-wise\ncompatibility. Experimental results show that our proposal matches the\nstate-of-the-arts on the Argoverse forecasting benchmark. It's also revealed by\nthe visualized results that the hierarchical learning framework captures the\nmulti-scale interactions and improves the feasibility and compliance of the\npredicted trajectories.\n","authors":["Yujun Jiao","Mingze Miao","Zhishuai Yin","Chunyuan Lei","Xu Zhu","Linzhen Nie","Bo Tao"],"pdf_url":"https://arxiv.org/pdf/2303.12274v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07060v2","updated":"2023-03-22T02:44:57Z","published":"2022-12-14T07:03:23Z","title":"VINet: Lightweight, Scalable, and Heterogeneous Cooperative Perception\n  for 3D Object Detection","summary":"  Utilizing the latest advances in Artificial Intelligence (AI), the computer\nvision community is now witnessing an unprecedented evolution in all kinds of\nperception tasks, particularly in object detection. Based on multiple spatially\nseparated perception nodes, Cooperative Perception (CP) has emerged to\nsignificantly advance the perception of automated driving. However, current\ncooperative object detection methods mainly focus on ego-vehicle efficiency\nwithout considering the practical issues of system-wide costs. In this paper,\nwe introduce VINet, a unified deep learning-based CP network for scalable,\nlightweight, and heterogeneous cooperative 3D object detection. VINet is the\nfirst CP method designed from the standpoint of large-scale system-level\nimplementation and can be divided into three main phases: 1) Global\nPre-Processing and Lightweight Feature Extraction which prepare the data into\nglobal style and extract features for cooperation in a lightweight manner; 2)\nTwo-Stream Fusion which fuses the features from scalable and heterogeneous\nperception nodes; and 3) Central Feature Backbone and 3D Detection Head which\nfurther process the fused features and generate cooperative detection results.\nAn open-source data experimental platform is designed and developed for CP\ndataset acquisition and model evaluation. The experimental analysis shows that\nVINet can reduce 84% system-level computational cost and 94% system-level\ncommunication cost while improving the 3D detection accuracy.\n","authors":["Zhengwei Bai","Guoyuan Wu","Matthew J. Barth","Yongkang Liu","Emrah Akin Sisbot","Kentaro Oguchi"],"pdf_url":"https://arxiv.org/pdf/2212.07060v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.12037v8","updated":"2023-03-22T02:41:58Z","published":"2022-04-26T02:22:28Z","title":"Causal Reasoning Meets Visual Representation Learning: A Prospective\n  Study","summary":"  Visual representation learning is ubiquitous in various real-world\napplications, including visual comprehension, video understanding, multi-modal\nanalysis, human-computer interaction, and urban computing. Due to the emergence\nof huge amounts of multi-modal heterogeneous spatial/temporal/spatial-temporal\ndata in big data era, the lack of interpretability, robustness, and\nout-of-distribution generalization are becoming the challenges of the existing\nvisual models. The majority of the existing methods tend to fit the original\ndata/variable distributions and ignore the essential causal relations behind\nthe multi-modal knowledge, which lacks unified guidance and analysis about why\nmodern visual representation learning methods easily collapse into data bias\nand have limited generalization and cognitive abilities. Inspired by the strong\ninference ability of human-level agents, recent years have therefore witnessed\ngreat effort in developing causal reasoning paradigms to realize robust\nrepresentation and model learning with good cognitive ability. In this paper,\nwe conduct a comprehensive review of existing causal reasoning methods for\nvisual representation learning, covering fundamental theories, models, and\ndatasets. The limitations of current methods and datasets are also discussed.\nMoreover, we propose some prospective challenges, opportunities, and future\nresearch directions for benchmarking causal reasoning algorithms in visual\nrepresentation learning. This paper aims to provide a comprehensive overview of\nthis emerging field, attract attention, encourage discussions, bring to the\nforefront the urgency of developing novel causal reasoning methods, publicly\navailable benchmarks, and consensus-building standards for reliable visual\nrepresentation learning and related real-world applications more efficiently.\n","authors":["Yang Liu","Yushen Wei","Hong Yan","Guanbin Li","Liang Lin"],"pdf_url":"https://arxiv.org/pdf/2204.12037v8.pdf","comment":"35 pages, 14 figures. This work has been accepted by Machine\n  Intelligence Research. The arxiv version is kept updating by adding more\n  novel methods, datasets and insights. The official video interpretation of\n  this paper can be referred at https://youtu.be/2lfNaTkcTHI"},{"id":"http://arxiv.org/abs/2211.12764v3","updated":"2023-03-22T02:36:52Z","published":"2022-11-23T08:20:29Z","title":"VoP: Text-Video Co-operative Prompt Tuning for Cross-Modal Retrieval","summary":"  Many recent studies leverage the pre-trained CLIP for text-video cross-modal\nretrieval by tuning the backbone with additional heavy modules, which not only\nbrings huge computational burdens with much more parameters, but also leads to\nthe knowledge forgetting from upstream models. In this work, we propose the\nVoP: Text-Video Co-operative Prompt Tuning for efficient tuning on the\ntext-video retrieval task. The proposed VoP is an end-to-end framework with\nboth video & text prompts introducing, which can be regarded as a powerful\nbaseline with only 0.1% trainable parameters. Further, based on the\nspatio-temporal characteristics of videos, we develop three novel video prompt\nmechanisms to improve the performance with different scales of trainable\nparameters. The basic idea of the VoP enhancement is to model the frame\nposition, frame context, and layer function with specific trainable prompts,\nrespectively. Extensive experiments show that compared to full fine-tuning, the\nenhanced VoP achieves a 1.4% average R@1 gain across five text-video retrieval\nbenchmarks with 6x less parameter overhead. The code will be available at\nhttps://github.com/bighuang624/VoP.\n","authors":["Siteng Huang","Biao Gong","Yulin Pan","Jianwen Jiang","Yiliang Lv","Yuyuan Li","Donglin Wang"],"pdf_url":"https://arxiv.org/pdf/2211.12764v3.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2303.12270v1","updated":"2023-03-22T02:36:13Z","published":"2023-03-22T02:36:13Z","title":"EBSR: Enhanced Binary Neural Network for Image Super-Resolution","summary":"  While the performance of deep convolutional neural networks for image\nsuper-resolution (SR) has improved significantly, the rapid increase of memory\nand computation requirements hinders their deployment on resource-constrained\ndevices. Quantized networks, especially binary neural networks (BNN) for SR\nhave been proposed to significantly improve the model inference efficiency but\nsuffer from large performance degradation. We observe the activation\ndistribution of SR networks demonstrates very large pixel-to-pixel,\nchannel-to-channel, and image-to-image variation, which is important for high\nperformance SR but gets lost during binarization. To address the problem, we\npropose two effective methods, including the spatial re-scaling as well as\nchannel-wise shifting and re-scaling, which augments binary convolutions by\nretaining more spatial and channel-wise information. Our proposed models,\ndubbed EBSR, demonstrate superior performance over prior art methods both\nquantitatively and qualitatively across different datasets and different model\nsizes. Specifically, for x4 SR on Set5 and Urban100, EBSRlight improves the\nPSNR by 0.31 dB and 0.28 dB compared to SRResNet-E2FIF, respectively, while\nEBSR outperforms EDSR-E2FIF by 0.29 dB and 0.32 dB PSNR, respectively.\n","authors":["Renjie Wei","Shuwen Zhang","Zechun Liu","Meng Li","Yuchen Fan","Runsheng Wang","Ru Huang"],"pdf_url":"https://arxiv.org/pdf/2303.12270v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12267v1","updated":"2023-03-22T02:28:54Z","published":"2023-03-22T02:28:54Z","title":"AUTO: Adaptive Outlier Optimization for Online Test-Time OOD Detection","summary":"  Out-of-distribution (OOD) detection is a crucial aspect of deploying machine\nlearning models in open-world applications. Empirical evidence suggests that\ntraining with auxiliary outliers substantially improves OOD detection. However,\nsuch outliers typically exhibit a distribution gap compared to the test OOD\ndata and do not cover all possible test OOD scenarios. Additionally,\nincorporating these outliers introduces additional training burdens. In this\npaper, we introduce a novel paradigm called test-time OOD detection, which\nutilizes unlabeled online data directly at test time to improve OOD detection\nperformance. While this paradigm is efficient, it also presents challenges such\nas catastrophic forgetting. To address these challenges, we propose adaptive\noutlier optimization (AUTO), which consists of an in-out-aware filter, an ID\nmemory bank, and a semantically-consistent objective. AUTO adaptively mines\npseudo-ID and pseudo-OOD samples from test data, utilizing them to optimize\nnetworks in real time during inference. Extensive results on CIFAR-10,\nCIFAR-100, and ImageNet benchmarks demonstrate that AUTO significantly enhances\nOOD detection performance.\n","authors":["Puning Yang","Jian Liang","Jie Cao","Ran He"],"pdf_url":"https://arxiv.org/pdf/2303.12267v1.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2303.04995v2","updated":"2023-03-22T02:21:50Z","published":"2023-03-09T02:38:32Z","title":"Text-Visual Prompting for Efficient 2D Temporal Video Grounding","summary":"  In this paper, we study the problem of temporal video grounding (TVG), which\naims to predict the starting/ending time points of moments described by a text\nsentence within a long untrimmed video. Benefiting from fine-grained 3D visual\nfeatures, the TVG techniques have achieved remarkable progress in recent years.\nHowever, the high complexity of 3D convolutional neural networks (CNNs) makes\nextracting dense 3D visual features time-consuming, which calls for intensive\nmemory and computing resources. Towards efficient TVG, we propose a novel\ntext-visual prompting (TVP) framework, which incorporates optimized\nperturbation patterns (that we call 'prompts') into both visual inputs and\ntextual features of a TVG model. In sharp contrast to 3D CNNs, we show that TVP\nallows us to effectively co-train vision encoder and language encoder in a 2D\nTVG model and improves the performance of crossmodal feature fusion using only\nlow-complexity sparse 2D visual features. Further, we propose a\nTemporal-Distance IoU (TDIoU) loss for efficient learning of TVG. Experiments\non two benchmark datasets, Charades-STA and ActivityNet Captions datasets,\nempirically show that the proposed TVP significantly boosts the performance of\n2D TVG (e.g., 9.79% improvement on Charades-STA and 30.77% improvement on\nActivityNet Captions) and achieves 5x inference acceleration over TVG using 3D\nvisual features. Codes are available at Open.Intel.\n","authors":["Yimeng Zhang","Xin Chen","Jinghan Jia","Sijia Liu","Ke Ding"],"pdf_url":"https://arxiv.org/pdf/2303.04995v2.pdf","comment":"Accepted to the CVPR 2023"},{"id":"http://arxiv.org/abs/2303.12255v1","updated":"2023-03-22T01:45:35Z","published":"2023-03-22T01:45:35Z","title":"Encoding Binary Concepts in the Latent Space of Generative Models for\n  Enhancing Data Representation","summary":"  Binary concepts are empirically used by humans to generalize efficiently. And\nthey are based on Bernoulli distribution which is the building block of\ninformation. These concepts span both low-level and high-level features such as\n\"large vs small\" and \"a neuron is active or inactive\". Binary concepts are\nubiquitous features and can be used to transfer knowledge to improve model\ngeneralization. We propose a novel binarized regularization to facilitate\nlearning of binary concepts to improve the quality of data generation in\nautoencoders. We introduce a binarizing hyperparameter $r$ in data generation\nprocess to disentangle the latent space symmetrically. We demonstrate that this\nmethod can be applied easily to existing variational autoencoder (VAE) variants\nto encourage symmetric disentanglement, improve reconstruction quality, and\nprevent posterior collapse without computation overhead. We also demonstrate\nthat this method can boost existing models to learn more transferable\nrepresentations and generate more representative samples for the input\ndistribution which can alleviate catastrophic forgetting using generative\nreplay under continual learning settings.\n","authors":["Zizhao Hu","Mohammad Rostami"],"pdf_url":"https://arxiv.org/pdf/2303.12255v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12249v1","updated":"2023-03-22T01:14:52Z","published":"2023-03-22T01:14:52Z","title":"State-of-the-art optical-based physical adversarial attacks for deep\n  learning computer vision systems","summary":"  Adversarial attacks can mislead deep learning models to make false\npredictions by implanting small perturbations to the original input that are\nimperceptible to the human eye, which poses a huge security threat to the\ncomputer vision systems based on deep learning. Physical adversarial attacks,\nwhich is more realistic, as the perturbation is introduced to the input before\nit is being captured and converted to a binary image inside the vision system,\nwhen compared to digital adversarial attacks. In this paper, we focus on\nphysical adversarial attacks and further classify them into invasive and\nnon-invasive. Optical-based physical adversarial attack techniques (e.g. using\nlight irradiation) belong to the non-invasive category. As the perturbations\ncan be easily ignored by humans as the perturbations are very similar to the\neffects generated by a natural environment in the real world. They are highly\ninvisibility and executable and can pose a significant or even lethal threats\nto real systems. This paper focuses on optical-based physical adversarial\nattack techniques for computer vision systems, with emphasis on the\nintroduction and discussion of optical-based physical adversarial attack\ntechniques.\n","authors":["Junbin Fang","You Jiang","Canjian Jiang","Zoe L. Jiang","Siu-Ming Yiu","Chuanyi Liu"],"pdf_url":"https://arxiv.org/pdf/2303.12249v1.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2203.11437v3","updated":"2023-03-22T01:10:08Z","published":"2022-03-22T03:17:15Z","title":"Representation Uncertainty in Self-Supervised Learning as Variational\n  Inference","summary":"  In this paper, a novel self-supervised learning (SSL) method is proposed,\nwhich learns not only representations but also representations uncertainties by\nconsidering SSL in terms of variational inference. SSL is a method of learning\nrepresentation without labels by maximizing the similarity between image\nrepresentations of different augmented views of the same image. Variational\nautoencoder (VAE) is an unsupervised representation learning method that trains\na probabilistic generative model with variational inference. VAE and SSL can\nlearn representations without labels, but the relationship between VAE and SSL\nhas not been revealed. In this paper, the theoretical relationship between SSL\nand variational inference is clarified. In addition, variational inference\nSimSiam (VI-SimSiam) is proposed, which can predict the representation\nuncertainty by interpreting SimSiam with variational inference and defining the\nlatent space distribution. The experiment qualitatively showed that VISimSiam\ncould learn uncertainty by comparing input images and predicted uncertainties.\nWe also revealed a relationship between estimated uncertainty and\nclassification accuracy.\n","authors":["Hiroki Nakamura","Masashi Okada","Tadahiro Taniguchi"],"pdf_url":"https://arxiv.org/pdf/2203.11437v3.pdf","comment":"15 pages, 12 figures, work in progress"},{"id":"http://arxiv.org/abs/2303.12247v1","updated":"2023-03-22T01:01:14Z","published":"2023-03-22T01:01:14Z","title":"Exploring the Benefits of Visual Prompting in Differential Privacy","summary":"  Visual Prompting (VP) is an emerging and powerful technique that allows\nsample-efficient adaptation to downstream tasks by engineering a well-trained\nfrozen source model. In this work, we explore the benefits of VP in\nconstructing compelling neural network classifiers with differential privacy\n(DP). We explore and integrate VP into canonical DP training methods and\ndemonstrate its simplicity and efficiency. In particular, we discover that VP\nin tandem with PATE, a state-of-the-art DP training method that leverages the\nknowledge transfer from an ensemble of teachers, achieves the state-of-the-art\nprivacy-utility trade-off with minimum expenditure of privacy budget. Moreover,\nwe conduct additional experiments on cross-domain image classification with a\nsufficient domain gap to further unveil the advantage of VP in DP. Lastly, we\nalso conduct extensive ablation studies to validate the effectiveness and\ncontribution of VP under DP consideration.\n","authors":["Yizhe Li","Yu-Lin Tsai","Xuebin Ren","Chia-Mu Yu","Pin-Yu Chen"],"pdf_url":"https://arxiv.org/pdf/2303.12247v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12246v1","updated":"2023-03-22T00:55:53Z","published":"2023-03-22T00:55:53Z","title":"Object Pose Estimation with Statistical Guarantees: Conformal Keypoint\n  Detection and Geometric Uncertainty Propagation","summary":"  The two-stage object pose estimation paradigm first detects semantic\nkeypoints on the image and then estimates the 6D pose by minimizing\nreprojection errors. Despite performing well on standard benchmarks, existing\ntechniques offer no provable guarantees on the quality and uncertainty of the\nestimation. In this paper, we inject two fundamental changes, namely conformal\nkeypoint detection and geometric uncertainty propagation, into the two-stage\nparadigm and propose the first pose estimator that endows an estimation with\nprovable and computable worst-case error bounds. On one hand, conformal\nkeypoint detection applies the statistical machinery of inductive conformal\nprediction to convert heuristic keypoint detections into circular or elliptical\nprediction sets that cover the groundtruth keypoints with a user-specified\nmarginal probability (e.g., 90%). Geometric uncertainty propagation, on the\nother, propagates the geometric constraints on the keypoints to the 6D object\npose, leading to a Pose UnceRtainty SEt (PURSE) that guarantees coverage of the\ngroundtruth pose with the same probability. The PURSE, however, is a nonconvex\nset that does not directly lead to estimated poses and uncertainties.\nTherefore, we develop RANdom SAmple averaGing (RANSAG) to compute an average\npose and apply semidefinite relaxation to upper bound the worst-case errors\nbetween the average pose and the groundtruth. On the LineMOD Occlusion dataset\nwe demonstrate: (i) the PURSE covers the groundtruth with valid probabilities;\n(ii) the worst-case error bounds provide correct uncertainty quantification;\nand (iii) the average pose achieves better or similar accuracy as\nrepresentative methods based on sparse keypoints.\n","authors":["Heng Yang","Marco Pavone"],"pdf_url":"https://arxiv.org/pdf/2303.12246v1.pdf","comment":"Accepted at CVPR 2023 as a highlight paper"},{"id":"http://arxiv.org/abs/2205.14320v3","updated":"2023-03-22T00:55:32Z","published":"2022-05-28T03:32:56Z","title":"RIAV-MVS: Recurrent-Indexing an Asymmetric Volume for Multi-View Stereo","summary":"  This paper presents a learning-based method for multi-view depth estimation\nfrom posed images. Our core idea is a \"learning-to-optimize\" paradigm that\niteratively indexes a plane-sweeping cost volume and regresses the depth map\nvia a convolutional Gated Recurrent Unit (GRU). Since the cost volume plays a\nparamount role in encoding the multi-view geometry, we aim to improve its\nconstruction both at pixel- and frame- levels. At the pixel level, we propose\nto break the symmetry of the Siamese network (which is typically used in MVS to\nextract image features) by introducing a transformer block to the reference\nimage (but not to the source images). Such an asymmetric volume allows the\nnetwork to extract global features from the reference image to predict its\ndepth map. Given potential inaccuracies in the poses between reference and\nsource images, we propose to incorporate a residual pose network to correct the\nrelative poses. This essentially rectifies the cost volume at the frame level.\nWe conduct extensive experiments on real-world MVS datasets and show that our\nmethod achieves state-of-the-art performance in terms of both within-dataset\nevaluation and cross-dataset generalization.\n","authors":["Changjiang Cai","Pan Ji","Qingan Yan","Yi Xu"],"pdf_url":"https://arxiv.org/pdf/2205.14320v3.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2204.14079v3","updated":"2023-03-22T00:49:31Z","published":"2022-04-29T13:21:14Z","title":"Fix the Noise: Disentangling Source Feature for Transfer Learning of\n  StyleGAN","summary":"  Transfer learning of StyleGAN has recently shown great potential to solve\ndiverse tasks, especially in domain translation. Previous methods utilized a\nsource model by swapping or freezing weights during transfer learning, however,\nthey have limitations on visual quality and controlling source features. In\nother words, they require additional models that are computationally demanding\nand have restricted control steps that prevent a smooth transition. In this\npaper, we propose a new approach to overcome these limitations. Instead of\nswapping or freezing, we introduce a simple feature matching loss to improve\ngeneration quality. In addition, to control the degree of source features, we\ntrain a target model with the proposed strategy, FixNoise, to preserve the\nsource features only in a disentangled subspace of a target feature space.\nOwing to the disentangled feature space, our method can smoothly control the\ndegree of the source features in a single model. Extensive experiments\ndemonstrate that the proposed method can generate more consistent and realistic\nimages than previous works.\n","authors":["Dongyeun Lee","Jae Young Lee","Doyeon Kim","Jaehyun Choi","Junmo Kim"],"pdf_url":"https://arxiv.org/pdf/2204.14079v3.pdf","comment":"Full CVPR 2023 paper is available at arXiv:2303.11545. Best paper of\n  CVPRW AICC 2022 (CVPR 2022 Workshop on AI for Content Creation). The code is\n  available at https://github.com/LeeDongYeun/FixNoise"},{"id":"http://arxiv.org/abs/2212.04493v2","updated":"2023-03-22T00:30:56Z","published":"2022-12-08T18:59:05Z","title":"SDFusion: Multimodal 3D Shape Completion, Reconstruction, and Generation","summary":"  In this work, we present a novel framework built to simplify 3D asset\ngeneration for amateur users. To enable interactive generation, our method\nsupports a variety of input modalities that can be easily provided by a human,\nincluding images, text, partially observed shapes and combinations of these,\nfurther allowing to adjust the strength of each input. At the core of our\napproach is an encoder-decoder, compressing 3D shapes into a compact latent\nrepresentation, upon which a diffusion model is learned. To enable a variety of\nmulti-modal inputs, we employ task-specific encoders with dropout followed by a\ncross-attention mechanism. Due to its flexibility, our model naturally supports\na variety of tasks, outperforming prior works on shape completion, image-based\n3D reconstruction, and text-to-3D. Most interestingly, our model can combine\nall these tasks into one swiss-army-knife tool, enabling the user to perform\nshape generation using incomplete shapes, images, and textual descriptions at\nthe same time, providing the relative weights for each input and facilitating\ninteractivity. Despite our approach being shape-only, we further show an\nefficient method to texture the generated shape using large-scale text-to-image\nmodels.\n","authors":["Yen-Chi Cheng","Hsin-Ying Lee","Sergey Tulyakov","Alexander Schwing","Liangyan Gui"],"pdf_url":"https://arxiv.org/pdf/2212.04493v2.pdf","comment":"In CVPR 2023. Project page and code is available at:\n  https://yccyenchicheng.github.io/SDFusion/. Fix some typos"},{"id":"http://arxiv.org/abs/2212.00937v4","updated":"2023-03-22T00:30:39Z","published":"2022-12-02T02:52:01Z","title":"StructVPR: Distill Structural Knowledge with Weighting Samples for\n  Visual Place Recognition","summary":"  Visual place recognition (VPR) is usually considered as a specific image\nretrieval problem. Limited by existing training frameworks, most deep\nlearning-based works cannot extract sufficiently stable global features from\nRGB images and rely on a time-consuming re-ranking step to exploit spatial\nstructural information for better performance. In this paper, we propose\nStructVPR, a novel training architecture for VPR, to enhance structural\nknowledge in RGB global features and thus improve feature stability in a\nconstantly changing environment. Specifically, StructVPR uses segmentation\nimages as a more definitive source of structural knowledge input into a CNN\nnetwork and applies knowledge distillation to avoid online segmentation and\ninference of seg-branch in testing. Considering that not all samples contain\nhigh-quality and helpful knowledge, and some even hurt the performance of\ndistillation, we partition samples and weigh each sample's distillation loss to\nenhance the expected knowledge precisely. Finally, StructVPR achieves\nimpressive performance on several benchmarks using only global retrieval and\neven outperforms many two-stage approaches by a large margin. After adding\nadditional re-ranking, ours achieves state-of-the-art performance while\nmaintaining a low computational cost.\n","authors":["Yanqing Shen","Sanping Zhou","Jingwen Fu","Ruotong Wang","Shitao Chen","Nanning Zheng"],"pdf_url":"https://arxiv.org/pdf/2212.00937v4.pdf","comment":"accepted by CVPR2023"},{"id":"http://arxiv.org/abs/2303.09915v3","updated":"2023-03-22T00:29:34Z","published":"2023-03-17T12:08:47Z","title":"Privacy-preserving Pedestrian Tracking using Distributed 3D LiDARs","summary":"  The growing demand for intelligent environments unleashes an extraordinary\ncycle of privacy-aware applications that makes individuals' life more\ncomfortable and safe. Examples of these applications include pedestrian\ntracking systems in large areas. Although the ubiquity of camera-based systems,\nthey are not a preferable solution due to the vulnerability of leaking the\nprivacy of pedestrians. In this paper, we introduce a novel privacy-preserving\nsystem for pedestrian tracking in smart environments using multiple distributed\nLiDARs of non-overlapping views. The system is designed to leverage LiDAR\ndevices to track pedestrians in partially covered areas due to practical\nconstraints, e.g., occlusion or cost. Therefore, the system uses the point\ncloud captured by different LiDARs to extract discriminative features that are\nused to train a metric learning model for pedestrian matching purposes. To\nboost the system's robustness, we leverage a probabilistic approach to model\nand adapt the dynamic mobility patterns of individuals and thus connect their\nsub-trajectories. We deployed the system in a large-scale testbed with 70\ncolorless LiDARs and conducted three different experiments. The evaluation\nresult at the entrance hall confirms the system's ability to accurately track\nthe pedestrians with a 0.98 F-measure even with zero-covered areas. This result\nhighlights the promise of the proposed system as the next generation of\nprivacy-preserving tracking means in smart environments.\n","authors":["Masakazu Ohno","Riki Ukyo","Tatsuya Amano","Hamada Rizk","Hirozumi Yamaguchi"],"pdf_url":"https://arxiv.org/pdf/2303.09915v3.pdf","comment":"Accepted to the 21st International Conference on Pervasive Computing\n  and Communications (PerCom 2023)"},{"id":"http://arxiv.org/abs/2212.07593v3","updated":"2023-03-22T00:23:39Z","published":"2022-12-15T02:45:57Z","title":"Enhanced Training of Query-Based Object Detection via Selective Query\n  Recollection","summary":"  This paper investigates a phenomenon where query-based object detectors\nmispredict at the last decoding stage while predicting correctly at an\nintermediate stage. We review the training process and attribute the overlooked\nphenomenon to two limitations: lack of training emphasis and cascading errors\nfrom decoding sequence. We design and present Selective Query Recollection\n(SQR), a simple and effective training strategy for query-based object\ndetectors. It cumulatively collects intermediate queries as decoding stages go\ndeeper and selectively forwards the queries to the downstream stages aside from\nthe sequential structure. Such-wise, SQR places training emphasis on later\nstages and allows later stages to work with intermediate queries from earlier\nstages directly. SQR can be easily plugged into various query-based object\ndetectors and significantly enhances their performance while leaving the\ninference pipeline unchanged. As a result, we apply SQR on Adamixer, DAB-DETR,\nand Deformable-DETR across various settings (backbone, number of queries,\nschedule) and consistently brings 1.4-2.8 AP improvement.\n","authors":["Fangyi Chen","Han Zhang","Kai Hu","Yu-kai Huang","Chenchen Zhu","Marios Savvides"],"pdf_url":"https://arxiv.org/pdf/2212.07593v3.pdf","comment":"CVPR2023"},{"id":"http://arxiv.org/abs/2303.12241v1","updated":"2023-03-22T00:21:50Z","published":"2023-03-22T00:21:50Z","title":"Preventing Dimensional Collapse of Incomplete Multi-View Clustering via\n  Direct Contrastive Learning","summary":"  Incomplete multi-view clustering (IMVC) is an unsupervised approach, among\nwhich IMVC via contrastive learning has received attention due to its excellent\nperformance. The previous methods have the following problems: 1) Over-reliance\non additional projection heads when solving the dimensional collapse problem in\nwhich latent features are only valid in lower-dimensional subspaces during\nclustering. However, many parameters in the projection heads are unnecessary.\n2) The recovered view contain inconsistent private information and useless\nprivate information will mislead the learning of common semantics due to\nconsistent learning and reconstruction learning on the same feature. To address\nthe above issues, we propose a novel incomplete multi-view contrastive\nclustering framework. This framework directly optimizes the latent feature\nsubspace, utilizes the learned feature vectors and their sub-vectors for\nreconstruction learning and consistency learning, thereby effectively avoiding\ndimensional collapse without relying on projection heads. Since reconstruction\nloss and contrastive loss are performed on different features, the adverse\neffect of useless private information is reduced. For the incomplete data, the\nmissing information is recovered by the cross-view prediction mechanism and the\ninconsistent information from different views is discarded by the minimum\nconditional entropy to further avoid the influence of private information.\nExtensive experimental results of the method on 5 public datasets show that the\nmethod achieves state-of-the-art clustering results.\n","authors":["Kaiwu Zhang","Shiqiang Du","Baokai Liu","Shengxia Gao"],"pdf_url":"https://arxiv.org/pdf/2303.12241v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2303.12507v1","updated":"2023-03-22T12:29:18Z","published":"2023-03-22T12:29:18Z","title":"End-to-End Personalized Next Location Recommendation via Contrastive\n  User Preference Modeling","summary":"  Predicting the next location is a highly valuable and common need in many\nlocation-based services such as destination prediction and route planning. The\ngoal of next location recommendation is to predict the next point-of-interest a\nuser might go to based on the user's historical trajectory. Most existing\nmodels learn mobility patterns merely from users' historical check-in sequences\nwhile overlooking the significance of user preference modeling. In this work, a\nnovel Point-of-Interest Transformer (POIFormer) with contrastive user\npreference modeling is developed for end-to-end next location recommendation.\nThis model consists of three major modules: history encoder, query generator,\nand preference decoder. History encoder is designed to model mobility patterns\nfrom historical check-in sequences, while query generator explicitly learns\nuser preferences to generate user-specific intention queries. Finally,\npreference decoder combines the intention queries and historical information to\npredict the user's next location. Extensive comparisons with representative\nschemes and ablation studies on four real-world datasets demonstrate the\neffectiveness and superiority of the proposed scheme under various settings.\n","authors":["Yan Luo","Ye Liu","Fu-lai Chung","Yu Liu","Chang Wen Chen"],"pdf_url":"https://arxiv.org/pdf/2303.12507v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.01918v4","updated":"2023-03-22T11:15:58Z","published":"2022-08-03T08:54:13Z","title":"DeepProphet2 -- A Deep Learning Gene Recommendation Engine","summary":"  New powerful tools for tackling life science problems have been created by\nrecent advances in machine learning. The purpose of the paper is to discuss the\npotential advantages of gene recommendation performed by artificial\nintelligence (AI). Indeed, gene recommendation engines try to solve this\nproblem: if the user is interested in a set of genes, which other genes are\nlikely to be related to the starting set and should be investigated? This task\nwas solved with a custom deep learning recommendation engine, DeepProphet2\n(DP2), which is freely available to researchers worldwide via\nhttps://www.generecommender.com?utm_source=DeepProphet2_paper&utm_medium=pdf.\nHereafter, insights behind the algorithm and its practical applications are\nillustrated.\n  The gene recommendation problem can be addressed by mapping the genes to a\nmetric space where a distance can be defined to represent the real semantic\ndistance between them. To achieve this objective a transformer-based model has\nbeen trained on a well-curated freely available paper corpus, PubMed. The paper\ndescribes multiple optimization procedures that were employed to obtain the\nbest bias-variance trade-off, focusing on embedding size and network depth. In\nthis context, the model's ability to discover sets of genes implicated in\ndiseases and pathways was assessed through cross-validation. A simple\nassumption guided the procedure: the network had no direct knowledge of\npathways and diseases but learned genes' similarities and the interactions\namong them. Moreover, to further investigate the space where the neural network\nrepresents genes, the dimensionality of the embedding was reduced, and the\nresults were projected onto a human-comprehensible space. In conclusion, a set\nof use cases illustrates the algorithm's potential applications in a real word\nsetting.\n","authors":["Daniele Brambilla","Davide Maria Giacomini","Luca Muscarnera","Andrea Mazzoleni"],"pdf_url":"https://arxiv.org/pdf/2208.01918v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2111.03837v2","updated":"2023-03-22T22:18:01Z","published":"2021-11-06T09:04:16Z","title":"Focusing on Potential Named Entities During Active Label Acquisition","summary":"  Named entity recognition (NER) aims to identify mentions of named entities in\nan unstructured text and classify them into predefined named entity classes.\nWhile deep learning-based pre-trained language models help to achieve good\npredictive performances in NER, many domain-specific NER applications still\ncall for a substantial amount of labeled data. Active learning (AL), a general\nframework for the label acquisition problem, has been used for NER tasks to\nminimize the annotation cost without sacrificing model performance. However,\nthe heavily imbalanced class distribution of tokens introduces challenges in\ndesigning effective AL querying methods for NER. We propose several AL sentence\nquery evaluation functions that pay more attention to potential positive\ntokens, and evaluate these proposed functions with both sentence-based and\ntoken-based cost evaluation strategies. We also propose a better data-driven\nnormalization approach to penalize sentences that are too long or too short.\nOur experiments on three datasets from different domains reveal that the\nproposed approach reduces the number of annotated tokens while achieving better\nor comparable prediction performance with conventional methods.\n","authors":["Ali Osman Berk Sapci","Oznur Tastan","Reyyan Yeniterzi"],"pdf_url":"https://arxiv.org/pdf/2111.03837v2.pdf","comment":"20 pages, 8 figures"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2211.09703v2","updated":"2023-03-22T17:58:09Z","published":"2022-11-17T17:38:55Z","title":"EfficientTrain: Exploring Generalized Curriculum Learning for Training\n  Visual Backbones","summary":"  The superior performance of modern deep networks usually comes with a costly\ntraining procedure. This paper presents a new curriculum learning approach for\nthe efficient training of visual backbones (e.g., vision Transformers). Our\nwork is inspired by the inherent learning dynamics of deep networks: we\nexperimentally show that at an earlier training stage, the model mainly learns\nto recognize some 'easier-to-learn' discriminative patterns within each\nexample, e.g., the lower-frequency components of images and the original\ninformation before data augmentation. Driven by this phenomenon, we propose a\ncurriculum where the model always leverages all the training data at each\nepoch, while the curriculum starts with only exposing the 'easier-to-learn'\npatterns of each example, and introduces gradually more difficult patterns. To\nimplement this idea, we 1) introduce a cropping operation in the Fourier\nspectrum of the inputs, which enables the model to learn from only the\nlower-frequency components efficiently, 2) demonstrate that exposing the\nfeatures of original images amounts to adopting weaker data augmentation, and\n3) integrate 1) and 2) and design a curriculum learning schedule with a\ngreedy-search algorithm. The resulting approach, EfficientTrain, is simple,\ngeneral, yet surprisingly effective. In the absence of hyper-parameter tuning,\nit reduces the training wall-time of a wide variety of popular models (e.g.,\nResNet, ConvNeXt, DeiT, PVT, Swin, and CSWin) by >1.5x on ImageNet-1K/22K\nwithout sacrificing the accuracy. It is also effective for self-supervised\nlearning (e.g., MAE). Code is available at\nhttps://github.com/LeapLabTHU/EfficientTrain.\n","authors":["Yulin Wang","Yang Yue","Rui Lu","Tianjiao Liu","Zhao Zhong","Shiji Song","Gao Huang"],"pdf_url":"https://arxiv.org/pdf/2211.09703v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12788v1","updated":"2023-03-22T17:57:47Z","published":"2023-03-22T17:57:47Z","title":"Open-source Frame Semantic Parsing","summary":"  While the state-of-the-art for frame semantic parsing has progressed\ndramatically in recent years, it is still difficult for end-users to apply\nstate-of-the-art models in practice. To address this, we present Frame Semantic\nTransformer, an open-source Python library which achieves near state-of-the-art\nperformance on FrameNet 1.7, while focusing on ease-of-use. We use a T5 model\nfine-tuned on Propbank and FrameNet exemplars as a base, and improve\nperformance by using FrameNet lexical units to provide hints to T5 at inference\ntime. We enhance robustness to real-world data by using textual data\naugmentations during training.\n","authors":["David Chanin"],"pdf_url":"https://arxiv.org/pdf/2303.12788v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12785v1","updated":"2023-03-22T17:56:18Z","published":"2023-03-22T17:56:18Z","title":"Matryoshka Policy Gradient for Entropy-Regularized RL: Convergence and\n  Global Optimality","summary":"  A novel Policy Gradient (PG) algorithm, called Matryoshka Policy Gradient\n(MPG), is introduced and studied, in the context of max-entropy reinforcement\nlearning, where an agent aims at maximising entropy bonuses additional to its\ncumulative rewards. MPG differs from standard PG in that it trains a sequence\nof policies to learn finite horizon tasks simultaneously, instead of a single\npolicy for the single standard objective. For softmax policies, we prove\nconvergence of MPG and global optimality of the limit by showing that the only\ncritical point of the MPG objective is the optimal policy; these results hold\ntrue even in the case of continuous compact state space. MPG is intuitive,\ntheoretically sound and we furthermore show that the optimal policy of the\nstandard max-entropy objective can be approximated arbitrarily well by the\noptimal policy of the MPG framework. Finally, we justify that MPG is well\nsuited when the policies are parametrized with neural networks and we provide\nan simple criterion to verify the global optimality of the policy at\nconvergence. As a proof of concept, we evaluate numerically MPG on standard\ntest benchmarks.\n","authors":["François Ged","Maria Han Veiga"],"pdf_url":"https://arxiv.org/pdf/2303.12785v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12783v1","updated":"2023-03-22T17:52:54Z","published":"2023-03-22T17:52:54Z","title":"Conformal Prediction for Time Series with Modern Hopfield Networks","summary":"  To quantify uncertainty, conformal prediction methods are gaining\ncontinuously more interest and have already been successfully applied to\nvarious domains. However, they are difficult to apply to time series as the\nautocorrelative structure of time series violates basic assumptions required by\nconformal prediction. We propose HopCPT, a novel conformal prediction approach\nfor time series that not only copes with temporal structures but leverages\nthem. We show that our approach is theoretically well justified for time series\nwhere temporal dependencies are present. In experiments, we demonstrate that\nour new approach outperforms state-of-the-art conformal prediction methods on\nmultiple real-world time series datasets from four different domains.\n","authors":["Andreas Auer","Martin Gauch","Daniel Klotz","Sepp Hochreiter"],"pdf_url":"https://arxiv.org/pdf/2303.12783v1.pdf","comment":"under Review"},{"id":"http://arxiv.org/abs/2303.12767v1","updated":"2023-03-22T17:32:56Z","published":"2023-03-22T17:32:56Z","title":"Can we trust the evaluation on ChatGPT?","summary":"  ChatGPT, the first large language model (LLM) with mass adoption, has\ndemonstrated remarkable performance in numerous natural language tasks. Despite\nits evident usefulness, evaluating ChatGPT's performance in diverse problem\ndomains remains challenging due to the closed nature of the model and its\ncontinuous updates via Reinforcement Learning from Human Feedback (RLHF). We\nhighlight the issue of data contamination in ChatGPT evaluations, with a case\nstudy of the task of stance detection. We discuss the challenge of preventing\ndata contamination and ensuring fair model evaluation in the age of closed and\ncontinuously trained models.\n","authors":["Rachith Aiyappa","Jisun An","Haewoon Kwak","Yong-Yeol Ahn"],"pdf_url":"https://arxiv.org/pdf/2303.12767v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09540v3","updated":"2023-03-22T17:22:35Z","published":"2023-03-16T17:53:24Z","title":"SemDeDup: Data-efficient learning at web-scale through semantic\n  deduplication","summary":"  Progress in machine learning has been driven in large part by massive\nincreases in data. However, large web-scale datasets such as LAION are largely\nuncurated beyond searches for exact duplicates, potentially leaving much\nredundancy. Here, we introduce SemDeDup, a method which leverages embeddings\nfrom pre-trained models to identify and remove semantic duplicates: data pairs\nwhich are semantically similar, but not exactly identical. Removing semantic\nduplicates preserves performance and speeds up learning. Analyzing a subset of\nLAION, we show that SemDeDup can remove 50% of the data with minimal\nperformance loss, effectively halving training time. Moreover, performance\nincreases out of distribution. Also, analyzing language models trained on C4, a\npartially curated dataset, we show that SemDeDup improves over prior approaches\nwhile providing efficiency gains. SemDeDup provides an example of how simple\nways of leveraging quality embeddings can be used to make models learn faster\nwith less data.\n","authors":["Amro Abbas","Kushal Tirumala","Dániel Simig","Surya Ganguli","Ari S. Morcos"],"pdf_url":"https://arxiv.org/pdf/2303.09540v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12761v1","updated":"2023-03-22T17:14:38Z","published":"2023-03-22T17:14:38Z","title":"LSTM-based Video Quality Prediction Accounting for Temporal Distortions\n  in Videoconferencing Calls","summary":"  Current state-of-the-art video quality models, such as VMAF, give excellent\nprediction results by comparing the degraded video with its reference video.\nHowever, they do not consider temporal distortions (e.g., frame freezes or\nskips) that occur during videoconferencing calls. In this paper, we present a\ndata-driven approach for modeling such distortions automatically by training an\nLSTM with subjective quality ratings labeled via crowdsourcing. The videos were\ncollected from live videoconferencing calls in 83 different network conditions.\nWe applied QR codes as markers on the source videos to create aligned\nreferences and compute temporal features based on the alignment vectors. Using\nthese features together with VMAF core features, our proposed model achieves a\nPCC of 0.99 on the validation set. Furthermore, our model outputs per-frame\nquality that gives detailed insight into the cause of video quality\nimpairments. The VCM model and dataset are open-sourced at\nhttps://github.com/microsoft/Video_Call_MOS.\n","authors":["Gabriel Mittag","Babak Naderi","Vishak Gopal","Ross Cutler"],"pdf_url":"https://arxiv.org/pdf/2303.12761v1.pdf","comment":"Accepted at ICASSP 2023"},{"id":"http://arxiv.org/abs/2205.09900v3","updated":"2023-03-22T17:13:37Z","published":"2022-05-19T23:43:15Z","title":"Estimating the randomness of quantum circuit ensembles up to 50 qubits","summary":"  Random quantum circuits have been utilized in the contexts of quantum\nsupremacy demonstrations, variational quantum algorithms for chemistry and\nmachine learning, and blackhole information. The ability of random circuits to\napproximate any random unitaries has consequences on their complexity,\nexpressibility, and trainability. To study this property of random circuits, we\ndevelop numerical protocols for estimating the frame potential, the distance\nbetween a given ensemble and the exact randomness. Our tensor-network-based\nalgorithm has polynomial complexity for shallow circuits and is high-performing\nusing CPU and GPU parallelism. We study 1. local and parallel random circuits\nto verify the linear growth in complexity as stated by the Brown-Susskind\nconjecture, and; 2. hardware-efficient ans\\\"atze to shed light on its\nexpressibility and the barren plateau problem in the context of variational\nalgorithms. Our work shows that large-scale tensor network simulations could\nprovide important hints toward open problems in quantum information science.\n","authors":["Minzhao Liu","Junyu Liu","Yuri Alexeev","Liang Jiang"],"pdf_url":"https://arxiv.org/pdf/2205.09900v3.pdf","comment":"11 pages, many figures"},{"id":"http://arxiv.org/abs/2303.12718v1","updated":"2023-03-22T16:58:44Z","published":"2023-03-22T16:58:44Z","title":"Strategy Synthesis in Markov Decision Processes Under Limited Sampling\n  Access","summary":"  A central task in control theory, artificial intelligence, and formal methods\nis to synthesize reward-maximizing strategies for agents that operate in\npartially unknown environments. In environments modeled by gray-box Markov\ndecision processes (MDPs), the impact of the agents' actions are known in terms\nof successor states but not the stochastics involved. In this paper, we devise\na strategy synthesis algorithm for gray-box MDPs via reinforcement learning\nthat utilizes interval MDPs as internal model. To compete with limited sampling\naccess in reinforcement learning, we incorporate two novel concepts into our\nalgorithm, focusing on rapid and successful learning rather than on stochastic\nguarantees and optimality: lower confidence bound exploration reinforces\nvariants of already learned practical strategies and action scoping reduces the\nlearning action space to promising actions. We illustrate benefits of our\nalgorithms by means of a prototypical implementation applied on examples from\nthe AI and formal methods communities.\n","authors":["Christel Baier","Clemens Dubslaff","Patrick Wienhöft","Stefan J. Kiebel"],"pdf_url":"https://arxiv.org/pdf/2303.12718v1.pdf","comment":"Accepted for publication at NASA Formal Methods (NFM) 2023. This is\n  an extended version with the full appendix containing proofs, further\n  pseudocode with explanations and additional experiment figures"},{"id":"http://arxiv.org/abs/2303.12703v1","updated":"2023-03-22T16:45:54Z","published":"2023-03-22T16:45:54Z","title":"Causal Reasoning in the Presence of Latent Confounders via Neural ADMG\n  Learning","summary":"  Latent confounding has been a long-standing obstacle for causal reasoning\nfrom observational data. One popular approach is to model the data using\nacyclic directed mixed graphs (ADMGs), which describe ancestral relations\nbetween variables using directed and bidirected edges. However, existing\nmethods using ADMGs are based on either linear functional assumptions or a\ndiscrete search that is complicated to use and lacks computational tractability\nfor large datasets. In this work, we further extend the existing body of work\nand develop a novel gradient-based approach to learning an ADMG with non-linear\nfunctional relations from observational data. We first show that the presence\nof latent confounding is identifiable under the assumptions of bow-free ADMGs\nwith non-linear additive noise models. With this insight, we propose a novel\nneural causal model based on autoregressive flows for ADMG learning. This not\nonly enables us to determine complex causal structural relationships behind the\ndata in the presence of latent confounding, but also estimate their functional\nrelationships (hence treatment effects) simultaneously. We further validate our\napproach via experiments on both synthetic and real-world datasets, and\ndemonstrate the competitive performance against relevant baselines.\n","authors":["Matthew Ashman","Chao Ma","Agrin Hilmkil","Joel Jennings","Cheng Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.12703v1.pdf","comment":"Camera ready version for ICLR 2023"},{"id":"http://arxiv.org/abs/2303.12695v1","updated":"2023-03-22T16:42:19Z","published":"2023-03-22T16:42:19Z","title":"Adaptive Conformal Prediction by Reweighting Nonconformity Score","summary":"  Despite attractive theoretical guarantees and practical successes, Predictive\nInterval (PI) given by Conformal Prediction (CP) may not reflect the\nuncertainty of a given model. This limitation arises from CP methods using a\nconstant correction for all test points, disregarding their individual\nuncertainties, to ensure coverage properties. To address this issue, we propose\nusing a Quantile Regression Forest (QRF) to learn the distribution of\nnonconformity scores and utilizing the QRF's weights to assign more importance\nto samples with residuals similar to the test point. This approach results in\nPI lengths that are more aligned with the model's uncertainty. In addition, the\nweights learnt by the QRF provide a partition of the features space, allowing\nfor more efficient computations and improved adaptiveness of the PI through\ngroupwise conformalization. Our approach enjoys an assumption-free finite\nsample marginal and training-conditional coverage, and under suitable\nassumptions, it also ensures conditional coverage. Our methods work for any\nnonconformity score and are available as a Python package. We conduct\nexperiments on simulated and real-world data that demonstrate significant\nimprovements compared to existing methods.\n","authors":["Salim I. Amoukou","Nicolas J. B Brunel"],"pdf_url":"https://arxiv.org/pdf/2303.12695v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12669v1","updated":"2023-03-22T15:47:16Z","published":"2023-03-22T15:47:16Z","title":"An Extended Study of Human-like Behavior under Adversarial Training","summary":"  Neural networks have a number of shortcomings. Amongst the severest ones is\nthe sensitivity to distribution shifts which allows models to be easily fooled\ninto wrong predictions by small perturbations to inputs that are often\nimperceivable to humans and do not have to carry semantic meaning. Adversarial\ntraining poses a partial solution to address this issue by training models on\nworst-case perturbations. Yet, recent work has also pointed out that the\nreasoning in neural networks is different from humans. Humans identify objects\nby shape, while neural nets mainly employ texture cues. Exemplarily, a model\ntrained on photographs will likely fail to generalize to datasets containing\nsketches. Interestingly, it was also shown that adversarial training seems to\nfavorably increase the shift toward shape bias. In this work, we revisit this\nobservation and provide an extensive analysis of this effect on various\narchitectures, the common $\\ell_2$- and $\\ell_\\infty$-training, and\nTransformer-based models. Further, we provide a possible explanation for this\nphenomenon from a frequency perspective.\n","authors":["Paul Gavrikov","Janis Keuper","Margret Keuper"],"pdf_url":"https://arxiv.org/pdf/2303.12669v1.pdf","comment":"6 pages, accepted at the CVPR 2023 Workshop \"The 3rd Workshop of\n  Adversarial Machine Learning on Computer Vision: Art of Robustness\""},{"id":"http://arxiv.org/abs/2303.12659v1","updated":"2023-03-22T15:37:43Z","published":"2023-03-22T15:37:43Z","title":"Posthoc Interpretation via Quantization","summary":"  In this paper, we introduce a new approach, called \"Posthoc Interpretation\nvia Quantization (PIQ)\", for interpreting decisions made by trained\nclassifiers. Our method utilizes vector quantization to transform the\nrepresentations of a classifier into a discrete, class-specific latent space.\nThe class-specific codebooks act as a bottleneck that forces the interpreter to\nfocus on the parts of the input data deemed relevant by the classifier for\nmaking a prediction. We evaluated our method through quantitative and\nqualitative studies and found that PIQ generates interpretations that are more\neasily understood by participants to our user studies when compared to several\nother interpretation methods in the literature.\n","authors":["Cem Subakan","Francesco Paissan","Mirco Ravanelli"],"pdf_url":"https://arxiv.org/pdf/2303.12659v1.pdf","comment":"* Equal contribution"},{"id":"http://arxiv.org/abs/2303.12658v1","updated":"2023-03-22T15:36:19Z","published":"2023-03-22T15:36:19Z","title":"Reliable and Efficient Evaluation of Adversarial Robustness for Deep\n  Hashing-Based Retrieval","summary":"  Deep hashing has been extensively applied to massive image retrieval due to\nits efficiency and effectiveness. Recently, several adversarial attacks have\nbeen presented to reveal the vulnerability of deep hashing models against\nadversarial examples. However, existing attack methods suffer from degraded\nperformance or inefficiency because they underutilize the semantic relations\nbetween original samples or spend a lot of time learning these relations with a\ndeep neural network. In this paper, we propose a novel Pharos-guided Attack,\ndubbed PgA, to evaluate the adversarial robustness of deep hashing networks\nreliably and efficiently. Specifically, we design pharos code to represent the\nsemantics of the benign image, which preserves the similarity to semantically\nrelevant samples and dissimilarity to irrelevant ones. It is proven that we can\nquickly calculate the pharos code via a simple math formula. Accordingly, PgA\ncan directly conduct a reliable and efficient attack on deep hashing-based\nretrieval by maximizing the similarity between the hash code of the adversarial\nexample and the pharos code. Extensive experiments on the benchmark datasets\nverify that the proposed algorithm outperforms the prior state-of-the-arts in\nboth attack strength and speed.\n","authors":["Xunguang Wang","Jiawang Bai","Xinyue Xu","Xiaomeng Li"],"pdf_url":"https://arxiv.org/pdf/2303.12658v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2204.10779"},{"id":"http://arxiv.org/abs/2303.12643v1","updated":"2023-03-22T15:25:07Z","published":"2023-03-22T15:25:07Z","title":"Traffic Volume Prediction using Memory-Based Recurrent Neural Networks:\n  A comparative analysis of LSTM and GRU","summary":"  Predicting traffic volume in real-time can improve both traffic flow and road\nsafety. A precise traffic volume forecast helps alert drivers to the flow of\ntraffic along their preferred routes, preventing potential deadlock situations.\nExisting parametric models cannot reliably forecast traffic volume in dynamic\nand complex traffic conditions. Therefore, in order to evaluate and forecast\nthe traffic volume for every given time step in a real-time manner, we develop\nnon-linear memory-based deep neural network models. Our extensive experiments\nrun on the Metro Interstate Traffic Volume dataset demonstrate the\neffectiveness of the proposed models in predicting traffic volume in highly\ndynamic and heterogeneous traffic environments.\n","authors":["Lokesh Chandra Das"],"pdf_url":"https://arxiv.org/pdf/2303.12643v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12642v1","updated":"2023-03-22T15:23:22Z","published":"2023-03-22T15:23:22Z","title":"Democratising AI: Multiple Meanings, Goals, and Methods","summary":"  Numerous parties are calling for the democratisation of AI, but the phrase is\nused to refer to a variety of goals, the pursuit of which sometimes conflict.\nThis paper identifies four kinds of AI democratisation that are commonly\ndiscussed: (1) the democratisation of AI use, (2) the democratisation of AI\ndevelopment, (3) the democratisation of AI profits, and (4) the democratisation\nof AI governance. Numerous goals and methods of achieving each form of\ndemocratisation are discussed. The main takeaway from this paper is that AI\ndemocratisation is a multifarious and sometimes conflicting concept that should\nnot be conflated with improving AI accessibility. If we want to move beyond\nambiguous commitments to democratising AI, to productive discussions of\nconcrete policies and trade-offs, then we need to recognise the principal role\nof the democratisation of AI governance in navigating tradeoffs and risks\nacross decisions around use, development, and profits.\n","authors":["Elizabeth Seger","Aviv Ovadya","Ben Garfinkel","Divya Siddarth","Allan Dafoe"],"pdf_url":"https://arxiv.org/pdf/2303.12642v1.pdf","comment":"Submitted to conference"},{"id":"http://arxiv.org/abs/2102.05368v2","updated":"2023-03-22T15:21:10Z","published":"2021-02-10T10:13:39Z","title":"RoBIC: A benchmark suite for assessing classifiers robustness","summary":"  Many defenses have emerged with the development of adversarial attacks.\nModels must be objectively evaluated accordingly. This paper systematically\ntackles this concern by proposing a new parameter-free benchmark we coin RoBIC.\nRoBIC fairly evaluates the robustness of image classifiers using a new\nhalf-distortion measure. It gauges the robustness of the network against white\nand black box attacks, independently of its accuracy. RoBIC is faster than the\nother available benchmarks. We present the significant differences in the\nrobustness of 16 recent models as assessed by RoBIC.\n","authors":["Thibault Maho","Benoît Bonnet","Teddy Furon","Erwan Le Merrer"],"pdf_url":"https://arxiv.org/pdf/2102.05368v2.pdf","comment":"4 pages, accepted to ICIP 2021"},{"id":"http://arxiv.org/abs/2209.04187v2","updated":"2023-03-22T15:19:17Z","published":"2022-09-09T08:51:01Z","title":"Efficient Multi-view Clustering via Unified and Discrete Bipartite Graph\n  Learning","summary":"  Although previous graph-based multi-view clustering algorithms have gained\nsignificant progress, most of them are still faced with three limitations.\nFirst, they often suffer from high computational complexity, which restricts\ntheir applications in large-scale scenarios. Second, they usually perform graph\nlearning either at the single-view level or at the view-consensus level, but\noften neglect the possibility of the joint learning of single-view and\nconsensus graphs. Third, many of them rely on the k-means for discretization of\nthe spectral embeddings, which lack the ability to directly learn the graph\nwith discrete cluster structure. In light of this, this paper presents an\nefficient multi-view clustering approach via unified and discrete bipartite\ngraph learning (UDBGL). Specifically, the anchor-based subspace learning is\nincorporated to learn the view-specific bipartite graphs from multiple views,\nupon which the bipartite graph fusion is leveraged to learn a view-consensus\nbipartite graph with adaptive weight learning. Further, the Laplacian rank\nconstraint is imposed to ensure that the fused bipartite graph has discrete\ncluster structures (with a specific number of connected components). By\nsimultaneously formulating the view-specific bipartite graph learning, the\nview-consensus bipartite graph learning, and the discrete cluster structure\nlearning into a unified objective function, an efficient minimization algorithm\nis then designed to tackle this optimization problem and directly achieve a\ndiscrete clustering solution without requiring additional partitioning, which\nnotably has linear time complexity in data size. Experiments on a variety of\nmulti-view datasets demonstrate the robustness and efficiency of our UDBGL\napproach. The code is available at https://github.com/huangdonghere/UDBGL.\n","authors":["Si-Guo Fang","Dong Huang","Xiao-Sha Cai","Chang-Dong Wang","Chaobo He","Yong Tang"],"pdf_url":"https://arxiv.org/pdf/2209.04187v2.pdf","comment":"Accepted by IEEE Transactions on Neural Networks and Learning Systems"},{"id":"http://arxiv.org/abs/2303.12634v1","updated":"2023-03-22T15:17:16Z","published":"2023-03-22T15:17:16Z","title":"Semi-supervised counterfactual explanations","summary":"  Counterfactual explanations for machine learning models are used to find\nminimal interventions to the feature values such that the model changes the\nprediction to a different output or a target output. A valid counterfactual\nexplanation should have likely feature values. Here, we address the challenge\nof generating counterfactual explanations that lie in the same data\ndistribution as that of the training data and more importantly, they belong to\nthe target class distribution. This requirement has been addressed through the\nincorporation of auto-encoder reconstruction loss in the counterfactual search\nprocess. Connecting the output behavior of the classifier to the latent space\nof the auto-encoder has further improved the speed of the counterfactual search\nprocess and the interpretability of the resulting counterfactual explanations.\nContinuing this line of research, we show further improvement in the\ninterpretability of counterfactual explanations when the auto-encoder is\ntrained in a semi-supervised fashion with class tagged input data. We\nempirically evaluate our approach on several datasets and show considerable\nimprovement in-terms of several metrics.\n","authors":["Shravan Kumar Sajja","Sumanta Mukherjee","Satyam Dwivedi"],"pdf_url":"https://arxiv.org/pdf/2303.12634v1.pdf","comment":"12 pages, 3 figures, 4 tables"},{"id":"http://arxiv.org/abs/2204.10779v5","updated":"2023-03-22T14:57:41Z","published":"2022-04-18T04:51:08Z","title":"CgAT: Center-Guided Adversarial Training for Deep Hashing-Based\n  Retrieval","summary":"  Deep hashing has been extensively utilized in massive image retrieval because\nof its efficiency and effectiveness. However, deep hashing models are\nvulnerable to adversarial examples, making it essential to develop adversarial\ndefense methods for image retrieval. Existing solutions achieved limited\ndefense performance because of using weak adversarial samples for training and\nlacking discriminative optimization objectives to learn robust features. In\nthis paper, we present a min-max based Center-guided Adversarial Training,\nnamely CgAT, to improve the robustness of deep hashing networks through worst\nadversarial examples. Specifically, we first formulate the center code as a\nsemantically-discriminative representative of the input image content, which\npreserves the semantic similarity with positive samples and dissimilarity with\nnegative examples. We prove that a mathematical formula can calculate the\ncenter code immediately. After obtaining the center codes in each optimization\niteration of the deep hashing network, they are adopted to guide the\nadversarial training process. On the one hand, CgAT generates the worst\nadversarial examples as augmented data by maximizing the Hamming distance\nbetween the hash codes of the adversarial examples and the center codes. On the\nother hand, CgAT learns to mitigate the effects of adversarial samples by\nminimizing the Hamming distance to the center codes. Extensive experiments on\nthe benchmark datasets demonstrate the effectiveness of our adversarial\ntraining algorithm in defending against adversarial attacks for deep\nhashing-based retrieval. Compared with the current state-of-the-art defense\nmethod, we significantly improve the defense performance by an average of\n18.61\\%, 12.35\\%, and 11.56\\% on FLICKR-25K, NUS-WIDE, and MS-COCO,\nrespectively. The code is available at https://github.com/xunguangwang/CgAT.\n","authors":["Xunguang Wang","Yiqun Lin","Xiaomeng Li"],"pdf_url":"https://arxiv.org/pdf/2204.10779v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.03262v2","updated":"2023-03-22T14:31:42Z","published":"2023-02-07T05:20:20Z","title":"Membership Inference Attacks against Diffusion Models","summary":"  Diffusion models have attracted attention in recent years as innovative\ngenerative models. In this paper, we investigate whether a diffusion model is\nresistant to a membership inference attack, which evaluates the privacy leakage\nof a machine learning model. We primarily discuss the diffusion model from the\nstandpoints of comparison with a generative adversarial network (GAN) as\nconventional models and hyperparameters unique to the diffusion model, i.e.,\ntime steps, sampling steps, and sampling variances. We conduct extensive\nexperiments with DDIM as a diffusion model and DCGAN as a GAN on the CelebA and\nCIFAR-10 datasets in both white-box and black-box settings and then confirm if\nthe diffusion model is comparably resistant to a membership inference attack as\nGAN. Next, we demonstrate that the impact of time steps is significant and\nintermediate steps in a noise schedule are the most vulnerable to the attack.\nWe also found two key insights through further analysis. First, we identify\nthat DDIM is vulnerable to the attack for small sample sizes instead of\nachieving a lower FID. Second, sampling steps in hyperparameters are important\nfor resistance to the attack, whereas the impact of sampling variances is quite\nlimited.\n","authors":["Tomoya Matsumoto","Takayuki Miura","Naoto Yanai"],"pdf_url":"https://arxiv.org/pdf/2302.03262v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10880v3","updated":"2023-03-22T14:25:54Z","published":"2023-03-20T05:38:30Z","title":"Rotating without Seeing: Towards In-hand Dexterity through Touch","summary":"  Tactile information plays a critical role in human dexterity. It reveals\nuseful contact information that may not be inferred directly from vision. In\nfact, humans can even perform in-hand dexterous manipulation without using\nvision. Can we enable the same ability for the multi-finger robot hand? In this\npaper, we present Touch Dexterity, a new system that can perform in-hand object\nrotation using only touching without seeing the object. Instead of relying on\nprecise tactile sensing in a small region, we introduce a new system design\nusing dense binary force sensors (touch or no touch) overlaying one side of the\nwhole robot hand (palm, finger links, fingertips). Such a design is low-cost,\ngiving a larger coverage of the object, and minimizing the Sim2Real gap at the\nsame time. We train an in-hand rotation policy using Reinforcement Learning on\ndiverse objects in simulation. Relying on touch-only sensing, we can directly\ndeploy the policy in a real robot hand and rotate novel objects that are not\npresented in training. Extensive ablations are performed on how tactile\ninformation help in-hand manipulation.Our project is available at\nhttps://touchdexterity.github.io.\n","authors":["Zhao-Heng Yin","Binghao Huang","Yuzhe Qin","Qifeng Chen","Xiaolong Wang"],"pdf_url":"https://arxiv.org/pdf/2303.10880v3.pdf","comment":"Project page: https://touchdexterity.github.io"},{"id":"http://arxiv.org/abs/2303.10382v2","updated":"2023-03-22T14:19:24Z","published":"2023-03-18T10:13:32Z","title":"Interpretable Reinforcement Learning via Neural Additive Models for\n  Inventory Management","summary":"  The COVID-19 pandemic has highlighted the importance of supply chains and the\nrole of digital management to react to dynamic changes in the environment. In\nthis work, we focus on developing dynamic inventory ordering policies for a\nmulti-echelon, i.e. multi-stage, supply chain. Traditional inventory\noptimization methods aim to determine a static reordering policy. Thus, these\npolicies are not able to adjust to dynamic changes such as those observed\nduring the COVID-19 crisis. On the other hand, conventional strategies offer\nthe advantage of being interpretable, which is a crucial feature for supply\nchain managers in order to communicate decisions to their stakeholders. To\naddress this limitation, we propose an interpretable reinforcement learning\napproach that aims to be as interpretable as the traditional static policies\nwhile being as flexible and environment-agnostic as other deep learning-based\nreinforcement learning solutions. We propose to use Neural Additive Models as\nan interpretable dynamic policy of a reinforcement learning agent, showing that\nthis approach is competitive with a standard full connected policy. Finally, we\nuse the interpretability property to gain insights into a complex ordering\nstrategy for a simple, linear three-echelon inventory supply chain.\n","authors":["Julien Siems","Maximilian Schambach","Sebastian Schulze","Johannes S. Otterbach"],"pdf_url":"https://arxiv.org/pdf/2303.10382v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12589v1","updated":"2023-03-22T14:19:06Z","published":"2023-03-22T14:19:06Z","title":"Do Backdoors Assist Membership Inference Attacks?","summary":"  When an adversary provides poison samples to a machine learning model,\nprivacy leakage, such as membership inference attacks that infer whether a\nsample was included in the training of the model, becomes effective by moving\nthe sample to an outlier. However, the attacks can be detected because\ninference accuracy deteriorates due to poison samples. In this paper, we\ndiscuss a \\textit{backdoor-assisted membership inference attack}, a novel\nmembership inference attack based on backdoors that return the adversary's\nexpected output for a triggered sample. We found three crucial insights through\nexperiments with an academic benchmark dataset. We first demonstrate that the\nbackdoor-assisted membership inference attack is unsuccessful. Second, when we\nanalyzed loss distributions to understand the reason for the unsuccessful\nresults, we found that backdoors cannot separate loss distributions of training\nand non-training samples. In other words, backdoors cannot affect the\ndistribution of clean samples. Third, we also show that poison and triggered\nsamples activate neurons of different distributions. Specifically, backdoors\nmake any clean sample an inlier, contrary to poisoning samples. As a result, we\nconfirm that backdoors cannot assist membership inference.\n","authors":["Yumeki Goto","Nami Ashizawa","Toshiki Shibahara","Naoto Yanai"],"pdf_url":"https://arxiv.org/pdf/2303.12589v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.05568v3","updated":"2023-03-22T14:08:17Z","published":"2022-11-10T13:44:57Z","title":"Unbiased Supervised Contrastive Learning","summary":"  Many datasets are biased, namely they contain easy-to-learn features that are\nhighly correlated with the target class only in the dataset but not in the true\nunderlying distribution of the data. For this reason, learning unbiased models\nfrom biased data has become a very relevant research topic in the last years.\nIn this work, we tackle the problem of learning representations that are robust\nto biases. We first present a margin-based theoretical framework that allows us\nto clarify why recent contrastive losses (InfoNCE, SupCon, etc.) can fail when\ndealing with biased data. Based on that, we derive a novel formulation of the\nsupervised contrastive loss (epsilon-SupInfoNCE), providing more accurate\ncontrol of the minimal distance between positive and negative samples.\nFurthermore, thanks to our theoretical framework, we also propose FairKL, a new\ndebiasing regularization loss, that works well even with extremely biased data.\nWe validate the proposed losses on standard vision datasets including CIFAR10,\nCIFAR100, and ImageNet, and we assess the debiasing capability of FairKL with\nepsilon-SupInfoNCE, reaching state-of-the-art performance on a number of biased\ndatasets, including real instances of biases in the wild.\n","authors":["Carlo Alberto Barbano","Benoit Dufumier","Enzo Tartaglione","Marco Grangetto","Pietro Gori"],"pdf_url":"https://arxiv.org/pdf/2211.05568v3.pdf","comment":"Accepted at ICLR 2023"},{"id":"http://arxiv.org/abs/2303.12578v1","updated":"2023-03-22T14:03:23Z","published":"2023-03-22T14:03:23Z","title":"Neuro-Symbolic Reasoning Shortcuts: Mitigation Strategies and their\n  Limitations","summary":"  Neuro-symbolic predictors learn a mapping from sub-symbolic inputs to\nhigher-level concepts and then carry out (probabilistic) logical inference on\nthis intermediate representation. This setup offers clear advantages in terms\nof consistency to symbolic prior knowledge, and is often believed to provide\ninterpretability benefits in that - by virtue of complying with the knowledge -\nthe learned concepts can be better understood by human stakeholders. However,\nit was recently shown that this setup is affected by reasoning shortcuts\nwhereby predictions attain high accuracy by leveraging concepts with unintended\nsemantics, yielding poor out-of-distribution performance and compromising\ninterpretability. In this short paper, we establish a formal link between\nreasoning shortcuts and the optima of the loss function, and identify\nsituations in which reasoning shortcuts can arise. Based on this, we discuss\nlimitations of natural mitigation strategies such as reconstruction and concept\nsupervision.\n","authors":["Emanuele Marconato","Stefano Teso","Andrea Passerini"],"pdf_url":"https://arxiv.org/pdf/2303.12578v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12558v1","updated":"2023-03-22T13:41:42Z","published":"2023-03-22T13:41:42Z","title":"Wasserstein Auto-encoded MDPs: Formal Verification of Efficiently\n  Distilled RL Policies with Many-sided Guarantees","summary":"  Although deep reinforcement learning (DRL) has many success stories, the\nlarge-scale deployment of policies learned through these advanced techniques in\nsafety-critical scenarios is hindered by their lack of formal guarantees.\nVariational Markov Decision Processes (VAE-MDPs) are discrete latent space\nmodels that provide a reliable framework for distilling formally verifiable\ncontrollers from any RL policy. While the related guarantees address relevant\npractical aspects such as the satisfaction of performance and safety\nproperties, the VAE approach suffers from several learning flaws (posterior\ncollapse, slow learning speed, poor dynamics estimates), primarily due to the\nabsence of abstraction and representation guarantees to support latent\noptimization. We introduce the Wasserstein auto-encoded MDP (WAE-MDP), a latent\nspace model that fixes those issues by minimizing a penalized form of the\noptimal transport between the behaviors of the agent executing the original\npolicy and the distilled policy, for which the formal guarantees apply. Our\napproach yields bisimulation guarantees while learning the distilled policy,\nallowing concrete optimization of the abstraction and representation model\nquality. Our experiments show that, besides distilling policies up to 10 times\nfaster, the latent model quality is indeed better in general. Moreover, we\npresent experiments from a simple time-to-failure verification algorithm on the\nlatent space. The fact that our approach enables such simple verification\ntechniques highlights its applicability.\n","authors":["Florent Delgrange","Ann Nowé","Guillermo A. Pérez"],"pdf_url":"https://arxiv.org/pdf/2303.12558v1.pdf","comment":"ICLR 2023, 9 pages main text, 14 pages appendix (excluding\n  references)"},{"id":"http://arxiv.org/abs/2108.02235v3","updated":"2023-03-22T13:26:59Z","published":"2021-08-04T18:29:42Z","title":"Dynamic Relevance Learning for Few-Shot Object Detection","summary":"  Expensive bounding-box annotations have limited the development of object\ndetection task. Thus, it is necessary to focus on more challenging task of\nfew-shot object detection. It requires the detector to recognize objects of\nnovel classes with only a few training samples. Nowadays, many existing popular\nmethods adopting training way similar to meta-learning have achieved promising\nperformance, such as Meta R-CNN series. However, support data is only used as\nthe class attention to guide the detecting of query images each time. Their\nrelevance to each other remains unexploited. Moreover, a lot of recent works\ntreat the support data and query images as independent branch without\nconsidering the relationship between them. To address this issue, we propose a\ndynamic relevance learning model, which utilizes the relationship between all\nsupport images and Region of Interest (RoI) on the query images to construct a\ndynamic graph convolutional network (GCN). By adjusting the prediction\ndistribution of the base detector using the output of this GCN, the proposed\nmodel serves as a hard auxiliary classification task, which guides the detector\nto improve the class representation implicitly. Comprehensive experiments have\nbeen conducted on Pascal VOC and MS-COCO dataset. The proposed model achieves\nthe best overall performance, which shows its effectiveness of learning more\ngeneralized features. Our code is available at\nhttps://github.com/liuweijie19980216/DRL-for-FSOD.\n","authors":["Weijie Liu","Chong Wang","Haohe Li","Shenghao Yu","Jiafei Wu"],"pdf_url":"https://arxiv.org/pdf/2108.02235v3.pdf","comment":"12 pages, 8 figures, 7 tables"},{"id":"http://arxiv.org/abs/2303.09986v2","updated":"2023-03-22T13:24:45Z","published":"2023-03-17T14:02:35Z","title":"Towards AI-controlled FES-restoration of movements: Learning cycling\n  stimulation pattern with reinforcement learning","summary":"  Functional electrical stimulation (FES) has been increasingly integrated with\nother rehabilitation devices, including robots. FES cycling is one of the\ncommon FES applications in rehabilitation, which is performed by stimulating\nleg muscles in a certain pattern. The appropriate pattern varies across\nindividuals and requires manual tuning which can be time-consuming and\nchallenging for the individual user. Here, we present an AI-based method for\nfinding the patterns, which requires no extra hardware or sensors. Our method\nhas two phases, starting with finding model-based patterns using reinforcement\nlearning and detailed musculoskeletal models. The models, built using\nopen-source software, can be customised through our automated script and can be\ntherefore used by non-technical individuals without extra cost. Next, our\nmethod fine-tunes the pattern using real cycling data. We test our both in\nsimulation and experimentally on a stationary tricycle. In the simulation test,\nour method can robustly deliver model-based patterns for different cycling\nconfigurations. The experimental evaluation shows that our method can find a\nmodel-based pattern that induces higher cycling speed than an EMG-based\npattern. By using just 100 seconds of cycling data, our method can deliver a\nfine-tuned pattern that gives better cycling performance. Beyond FES cycling,\nthis work is a showcase, displaying the feasibility and potential of\nhuman-in-the-loop AI in real-world rehabilitation.\n","authors":["Nat Wannawas","A. Aldo Faisal"],"pdf_url":"https://arxiv.org/pdf/2303.09986v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12540v1","updated":"2023-03-22T13:16:37Z","published":"2023-03-22T13:16:37Z","title":"Deployment of Image Analysis Algorithms under Prevalence Shifts","summary":"  Domain gaps are among the most relevant roadblocks in the clinical\ntranslation of machine learning (ML)-based solutions for medical image\nanalysis. While current research focuses on new training paradigms and network\narchitectures, little attention is given to the specific effect of prevalence\nshifts on an algorithm deployed in practice. Such discrepancies between class\nfrequencies in the data used for a method's development/validation and that in\nits deployment environment(s) are of great importance, for example in the\ncontext of artificial intelligence (AI) democratization, as disease prevalences\nmay vary widely across time and location. Our contribution is twofold. First,\nwe empirically demonstrate the potentially severe consequences of missing\nprevalence handling by analyzing (i) the extent of miscalibration, (ii) the\ndeviation of the decision threshold from the optimum, and (iii) the ability of\nvalidation metrics to reflect neural network performance on the deployment\npopulation as a function of the discrepancy between development and deployment\nprevalence. Second, we propose a workflow for prevalence-aware image\nclassification that uses estimated deployment prevalences to adjust a trained\nclassifier to a new environment, without requiring additional annotated\ndeployment data. Comprehensive experiments based on a diverse set of 30 medical\nclassification tasks showcase the benefit of the proposed workflow in\ngenerating better classifier decisions and more reliable performance estimates\ncompared to current practice.\n","authors":["Patrick Godau","Piotr Kalinowski","Evangelia Christodoulou","Annika Reinke","Minu Tizabi","Luciana Ferrer","Paul Jäger","Lena Maier-Hein"],"pdf_url":"https://arxiv.org/pdf/2303.12540v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.17323v2","updated":"2023-03-22T13:10:47Z","published":"2022-10-31T13:42:40Z","title":"GPTQ: Accurate Post-Training Quantization for Generative Pre-trained\n  Transformers","summary":"  Generative Pre-trained Transformer models, known as GPT or OPT, set\nthemselves apart through breakthrough performance across complex language\nmodelling tasks, but also by their extremely high computational and storage\ncosts. Specifically, due to their massive size, even inference for large,\nhighly-accurate GPT models may require multiple performant GPUs, which limits\nthe usability of such models. While there is emerging work on relieving this\npressure via model compression, the applicability and performance of existing\ncompression techniques is limited by the scale and complexity of GPT models. In\nthis paper, we address this challenge, and propose GPTQ, a new one-shot weight\nquantization method based on approximate second-order information, that is both\nhighly-accurate and highly-efficient. Specifically, GPTQ can quantize GPT\nmodels with 175 billion parameters in approximately four GPU hours, reducing\nthe bitwidth down to 3 or 4 bits per weight, with negligible accuracy\ndegradation relative to the uncompressed baseline. Our method more than doubles\nthe compression gains relative to previously-proposed one-shot quantization\nmethods, preserving accuracy, allowing us for the first time to execute an 175\nbillion-parameter model inside a single GPU for generative inference. Moreover,\nwe also show that our method can still provide reasonable accuracy in the\nextreme quantization regime, in which weights are quantized to 2-bit or even\nternary quantization levels. We show experimentally that these improvements can\nbe leveraged for end-to-end inference speedups over FP16, of around 3.25x when\nusing high-end GPUs (NVIDIA A100) and 4.5x when using more cost-effective ones\n(NVIDIA A6000). The implementation is available at\nhttps://github.com/IST-DASLab/gptq.\n","authors":["Elias Frantar","Saleh Ashkboos","Torsten Hoefler","Dan Alistarh"],"pdf_url":"https://arxiv.org/pdf/2210.17323v2.pdf","comment":"ICLR 2023"},{"id":"http://arxiv.org/abs/2303.12524v1","updated":"2023-03-22T13:00:00Z","published":"2023-03-22T13:00:00Z","title":"Split-Et-Impera: A Framework for the Design of Distributed Deep Learning\n  Applications","summary":"  Many recent pattern recognition applications rely on complex distributed\narchitectures in which sensing and computational nodes interact together\nthrough a communication network. Deep neural networks (DNNs) play an important\nrole in this scenario, furnishing powerful decision mechanisms, at the price of\na high computational effort. Consequently, powerful state-of-the-art DNNs are\nfrequently split over various computational nodes, e.g., a first part stays on\nan embedded device and the rest on a server. Deciding where to split a DNN is a\nchallenge in itself, making the design of deep learning applications even more\ncomplicated. Therefore, we propose Split-Et-Impera, a novel and practical\nframework that i) determines the set of the best-split points of a neural\nnetwork based on deep network interpretability principles without performing a\ntedious try-and-test approach, ii) performs a communication-aware simulation\nfor the rapid evaluation of different neural network rearrangements, and iii)\nsuggests the best match between the quality of service requirements of the\napplication and the performance in terms of accuracy and latency time.\n","authors":["Luigi Capogrosso","Federico Cunico","Michele Lora","Marco Cristani","Franco Fummi","Davide Quaglia"],"pdf_url":"https://arxiv.org/pdf/2303.12524v1.pdf","comment":"26th International Symposium on Design and Diagnostics of Electronic\n  Circuits and Systems (DDECS)"},{"id":"http://arxiv.org/abs/2210.01081v2","updated":"2023-03-22T12:53:18Z","published":"2022-10-03T16:51:12Z","title":"On The Effects Of Data Normalisation For Domain Adaptation On EEG Data","summary":"  In the Machine Learning (ML) literature, a well-known problem is the Dataset\nShift problem where, differently from the ML standard hypothesis, the data in\nthe training and test sets can follow different probability distributions,\nleading ML systems toward poor generalisation performances. This problem is\nintensely felt in the Brain-Computer Interface (BCI) context, where bio-signals\nas Electroencephalographic (EEG) are often used. In fact, EEG signals are\nhighly non-stationary both over time and between different subjects. To\novercome this problem, several proposed solutions are based on recent transfer\nlearning approaches such as Domain Adaption (DA). In several cases, however,\nthe actual causes of the improvements remain ambiguous. This paper focuses on\nthe impact of data normalisation, or standardisation strategies applied\ntogether with DA methods. In particular, using \\textit{SEED}, \\textit{DEAP},\nand \\textit{BCI Competition IV 2a} EEG datasets, we experimentally evaluated\nthe impact of different normalization strategies applied with and without\nseveral well-known DA methods, comparing the obtained performances. It results\nthat the choice of the normalisation strategy plays a key role on the\nclassifier performances in DA scenarios, and interestingly, in several cases,\nthe use of only an appropriate normalisation schema outperforms the DA\ntechnique.\n","authors":["Andrea Apicella","Francesco Isgrò","Andrea Pollastro","Roberto Prevete"],"pdf_url":"https://arxiv.org/pdf/2210.01081v2.pdf","comment":"Accepted to be published in its final version on Engineering\n  Applications of Artificial Intelligence (EAAI)"},{"id":"http://arxiv.org/abs/2206.10555v2","updated":"2023-03-22T12:43:10Z","published":"2022-06-21T17:35:57Z","title":"LargeKernel3D: Scaling up Kernels in 3D Sparse CNNs","summary":"  Recent advance in 2D CNNs has revealed that large kernels are important.\nHowever, when directly applying large convolutional kernels in 3D CNNs, severe\ndifficulties are met, where those successful module designs in 2D become\nsurprisingly ineffective on 3D networks, including the popular depth-wise\nconvolution. To address this vital challenge, we instead propose the\nspatial-wise partition convolution and its large-kernel module. As a result, it\navoids the optimization and efficiency issues of naive 3D large kernels. Our\nlarge-kernel 3D CNN network, LargeKernel3D, yields notable improvement in 3D\ntasks of semantic segmentation and object detection. It achieves 73.9% mIoU on\nthe ScanNetv2 semantic segmentation and 72.8% NDS nuScenes object detection\nbenchmarks, ranking 1st on the nuScenes LIDAR leaderboard. The performance\nfurther boosts to 74.2% NDS with a simple multi-modal fusion. In addition,\nLargeKernel3D can be scaled to 17x17x17 kernel size on Waymo 3D object\ndetection. For the first time, we show that large kernels are feasible and\nessential for 3D visual tasks.\n","authors":["Yukang Chen","Jianhui Liu","Xiangyu Zhang","Xiaojuan Qi","Jiaya Jia"],"pdf_url":"https://arxiv.org/pdf/2206.10555v2.pdf","comment":"In CVPR 2023. Code is at\n  https://github.com/dvlab-research/LargeKernel3D"},{"id":"http://arxiv.org/abs/2301.00774v3","updated":"2023-03-22T12:33:46Z","published":"2023-01-02T17:48:56Z","title":"SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot","summary":"  We show for the first time that large-scale generative pretrained transformer\n(GPT) family models can be pruned to at least 50% sparsity in one-shot, without\nany retraining, at minimal loss of accuracy. This is achieved via a new pruning\nmethod called SparseGPT, specifically designed to work efficiently and\naccurately on massive GPT-family models. We can execute SparseGPT on the\nlargest available open-source models, OPT-175B and BLOOM-176B, in under 4.5\nhours, and can reach 60% unstructured sparsity with negligible increase in\nperplexity: remarkably, more than 100 billion weights from these models can be\nignored at inference time. SparseGPT generalizes to semi-structured (2:4 and\n4:8) patterns, and is compatible with weight quantization approaches. The code\nis available at: https://github.com/IST-DASLab/sparsegpt.\n","authors":["Elias Frantar","Dan Alistarh"],"pdf_url":"https://arxiv.org/pdf/2301.00774v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12499v1","updated":"2023-03-22T12:10:44Z","published":"2023-03-22T12:10:44Z","title":"On Domain-Specific Pre-Training for Effective Semantic Perception in\n  Agricultural Robotics","summary":"  Agricultural robots have the prospect to enable more efficient and\nsustainable agricultural production of food, feed, and fiber. Perception of\ncrops and weeds is a central component of agricultural robots that aim to\nmonitor fields and assess the plants as well as their growth stage in an\nautomatic manner. Semantic perception mostly relies on deep learning using\nsupervised approaches, which require time and qualified workers to label fairly\nlarge amounts of data. In this paper, we look into the problem of reducing the\namount of labels without compromising the final segmentation performance. For\nrobots operating in the field, pre-training networks in a supervised way is\nalready a popular method to reduce the number of required labeled images. We\ninvestigate the possibility of pre-training in a self-supervised fashion using\ndata from the target domain. To better exploit this data, we propose a set of\ndomain-specific augmentation strategies. We evaluate our pre-training on\nsemantic segmentation and leaf instance segmentation, two important tasks in\nour domain. The experimental results suggest that pre-training with\ndomain-specific data paired with our data augmentation strategy leads to\nsuperior performance compared to commonly used pre-trainings. Furthermore, the\npre-trained networks obtain similar performance to the fully supervised with\nless labeled data.\n","authors":["Gianmarco Roggiolani","Federico Magistri","Tiziano Guadagnino","Jan Weyler","Giorgio Grisetti","Cyrill Stachniss","Jens Behley"],"pdf_url":"https://arxiv.org/pdf/2303.12499v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12497v1","updated":"2023-03-22T12:09:12Z","published":"2023-03-22T12:09:12Z","title":"Lower Bound on the Bayesian Risk via Information Measure","summary":"  This paper focuses on parameter estimation and introduces a new method for\nlower bounding the Bayesian risk. The method allows for the use of virtually\n\\emph{any} information measure, including R\\'enyi's $\\alpha$,\n$\\varphi$-Divergences, and Sibson's $\\alpha$-Mutual Information. The approach\nconsiders divergences as functionals of measures and exploits the duality\nbetween spaces of measures and spaces of functions. In particular, we show that\none can lower bound the risk with any information measure by upper bounding its\ndual via Markov's inequality. We are thus able to provide estimator-independent\nimpossibility results thanks to the Data-Processing Inequalities that\ndivergences satisfy. The results are then applied to settings of interest\ninvolving both discrete and continuous parameters, including the\n``Hide-and-Seek'' problem, and compared to the state-of-the-art techniques. An\nimportant observation is that the behaviour of the lower bound in the number of\nsamples is influenced by the choice of the information measure. We leverage\nthis by introducing a new divergence inspired by the ``Hockey-Stick''\nDivergence, which is demonstrated empirically to provide the largest\nlower-bound across all considered settings. If the observations are subject to\nprivatisation, stronger impossibility results can be obtained via Strong\nData-Processing Inequalities. The paper also discusses some generalisations and\nalternative directions.\n","authors":["Amedeo Roberto Esposito","Adrien Vandenbroucque","Michael Gastpar"],"pdf_url":"https://arxiv.org/pdf/2303.12497v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12481v1","updated":"2023-03-22T11:49:35Z","published":"2023-03-22T11:49:35Z","title":"Revisiting DeepFool: generalization and improvement","summary":"  Deep neural networks have been known to be vulnerable to adversarial\nexamples, which are inputs that are modified slightly to fool the network into\nmaking incorrect predictions. This has led to a significant amount of research\non evaluating the robustness of these networks against such perturbations. One\nparticularly important robustness metric is the robustness to minimal l2\nadversarial perturbations. However, existing methods for evaluating this\nrobustness metric are either computationally expensive or not very accurate. In\nthis paper, we introduce a new family of adversarial attacks that strike a\nbalance between effectiveness and computational efficiency. Our proposed\nattacks are generalizations of the well-known DeepFool (DF) attack, while they\nremain simple to understand and implement. We demonstrate that our attacks\noutperform existing methods in terms of both effectiveness and computational\nefficiency. Our proposed attacks are also suitable for evaluating the\nrobustness of large models and can be used to perform adversarial training (AT)\nto achieve state-of-the-art robustness to minimal l2 adversarial perturbations.\n","authors":["Alireza Abdollahpourrostam","Mahed Abroshan","Seyed-Mohsen Moosavi-Dezfooli"],"pdf_url":"https://arxiv.org/pdf/2303.12481v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.04192v2","updated":"2023-03-22T11:20:01Z","published":"2022-06-08T23:34:39Z","title":"ExpressivE: A Spatio-Functional Embedding For Knowledge Graph Completion","summary":"  Knowledge graphs are inherently incomplete. Therefore substantial research\nhas been directed toward knowledge graph completion (KGC), i.e., predicting\nmissing triples from the information represented in the knowledge graph (KG).\nKG embedding models (KGEs) have yielded promising results for KGC, yet any\ncurrent KGE is incapable of: (1) fully capturing vital inference patterns\n(e.g., composition), (2) capturing prominent patterns jointly (e.g., hierarchy\nand composition), and (3) providing an intuitive interpretation of captured\npatterns. In this work, we propose ExpressivE, a fully expressive\nspatio-functional KGE that solves all these challenges simultaneously.\nExpressivE embeds pairs of entities as points and relations as\nhyper-parallelograms in the virtual triple space $\\mathbb{R}^{2d}$. This model\ndesign allows ExpressivE not only to capture a rich set of inference patterns\njointly but additionally to display any supported inference pattern through the\nspatial relation of hyper-parallelograms, offering an intuitive and consistent\ngeometric interpretation of ExpressivE embeddings and their captured patterns.\nExperimental results on standard KGC benchmarks reveal that ExpressivE is\ncompetitive with state-of-the-art KGEs and even significantly outperforms them\non WN18RR.\n","authors":["Aleksandar Pavlović","Emanuel Sallinger"],"pdf_url":"https://arxiv.org/pdf/2206.04192v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.01918v4","updated":"2023-03-22T11:15:58Z","published":"2022-08-03T08:54:13Z","title":"DeepProphet2 -- A Deep Learning Gene Recommendation Engine","summary":"  New powerful tools for tackling life science problems have been created by\nrecent advances in machine learning. The purpose of the paper is to discuss the\npotential advantages of gene recommendation performed by artificial\nintelligence (AI). Indeed, gene recommendation engines try to solve this\nproblem: if the user is interested in a set of genes, which other genes are\nlikely to be related to the starting set and should be investigated? This task\nwas solved with a custom deep learning recommendation engine, DeepProphet2\n(DP2), which is freely available to researchers worldwide via\nhttps://www.generecommender.com?utm_source=DeepProphet2_paper&utm_medium=pdf.\nHereafter, insights behind the algorithm and its practical applications are\nillustrated.\n  The gene recommendation problem can be addressed by mapping the genes to a\nmetric space where a distance can be defined to represent the real semantic\ndistance between them. To achieve this objective a transformer-based model has\nbeen trained on a well-curated freely available paper corpus, PubMed. The paper\ndescribes multiple optimization procedures that were employed to obtain the\nbest bias-variance trade-off, focusing on embedding size and network depth. In\nthis context, the model's ability to discover sets of genes implicated in\ndiseases and pathways was assessed through cross-validation. A simple\nassumption guided the procedure: the network had no direct knowledge of\npathways and diseases but learned genes' similarities and the interactions\namong them. Moreover, to further investigate the space where the neural network\nrepresents genes, the dimensionality of the embedding was reduced, and the\nresults were projected onto a human-comprehensible space. In conclusion, a set\nof use cases illustrates the algorithm's potential applications in a real word\nsetting.\n","authors":["Daniele Brambilla","Davide Maria Giacomini","Luca Muscarnera","Andrea Mazzoleni"],"pdf_url":"https://arxiv.org/pdf/2208.01918v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09975v2","updated":"2023-03-22T11:14:08Z","published":"2023-03-17T13:48:17Z","title":"MedNeXt: Transformer-driven Scaling of ConvNets for Medical Image\n  Segmentation","summary":"  There has been exploding interest in embracing Transformer-based\narchitectures for medical image segmentation. However, the lack of large-scale\nannotated medical datasets make achieving performances equivalent to those in\nnatural images challenging. Convolutional networks, in contrast, have higher\ninductive biases and consequently, are easily trainable to high performance.\nRecently, the ConvNeXt architecture attempted to modernize the standard ConvNet\nby mirroring Transformer blocks. In this work, we improve upon this to design a\nmodernized and scalable convolutional architecture customized to challenges of\ndata-scarce medical settings. We introduce MedNeXt, a Transformer-inspired\nlarge kernel segmentation network which introduces - 1) A fully ConvNeXt 3D\nEncoder-Decoder Network for medical image segmentation, 2) Residual ConvNeXt up\nand downsampling blocks to preserve semantic richness across scales, 3) A novel\ntechnique to iteratively increase kernel sizes by upsampling small kernel\nnetworks, to prevent performance saturation on limited medical data, 4)\nCompound scaling at multiple levels (depth, width, kernel size) of MedNeXt.\nThis leads to state-of-the-art performance on 4 tasks on CT and MRI modalities\nand varying dataset sizes, representing a modernized deep architecture for\nmedical image segmentation.\n","authors":["Saikat Roy","Gregor Koehler","Constantin Ulrich","Michael Baumgartner","Jens Petersen","Fabian Isensee","Paul F. Jaeger","Klaus Maier-Hein"],"pdf_url":"https://arxiv.org/pdf/2303.09975v2.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2303.12454v1","updated":"2023-03-22T10:52:21Z","published":"2023-03-22T10:52:21Z","title":"$\\mathcal{C}^k$-continuous Spline Approximation with TensorFlow Gradient\n  Descent Optimizers","summary":"  In this work we present an \"out-of-the-box\" application of Machine Learning\n(ML) optimizers for an industrial optimization problem. We introduce a\npiecewise polynomial model (spline) for fitting of $\\mathcal{C}^k$-continuos\nfunctions, which can be deployed in a cam approximation setting. We then use\nthe gradient descent optimization context provided by the machine learning\nframework TensorFlow to optimize the model parameters with respect to\napproximation quality and $\\mathcal{C}^k$-continuity and evaluate available\noptimizers. Our experiments show that the problem solution is feasible using\nTensorFlow gradient tapes and that AMSGrad and SGD show the best results among\navailable TensorFlow optimizers. Furthermore, we introduce a novel\nregularization approach to improve SGD convergence. Although experiments show\nthat remaining discontinuities after optimization are small, we can eliminate\nthese errors using a presented algorithm which has impact only on affected\nderivatives in the local spline segment.\n","authors":["Stefan Huber","Hannes Waclawek"],"pdf_url":"https://arxiv.org/pdf/2303.12454v1.pdf","comment":"This preprint has not undergone peer review or any post-submission\n  improvements or corrections. The Version of Record of this contribution is\n  published in Computer Aided Systems Theory - EUROCAST 2022 and is available\n  online at https://doi.org/10.1007/978-3-031-25312-6_68"},{"id":"http://arxiv.org/abs/2303.12440v1","updated":"2023-03-22T10:20:56Z","published":"2023-03-22T10:20:56Z","title":"Learning Human-Inspired Force Strategies for Robotic Assembly","summary":"  The programming of robotic assembly tasks is a key component in manufacturing\nand automation. Force-sensitive assembly, however, often requires reactive\nstrategies to handle slight changes in positioning and unforeseen part jamming.\nLearning such strategies from human performance is a promising approach, but\nfaces two common challenges: the handling of low part clearances which is\ndifficult to capture from demonstrations and learning intuitive strategies\noffline without access to the real hardware. We address these two challenges by\nlearning probabilistic force strategies from data that are easily acquired\noffline in a robot-less simulation from human demonstrations with a joystick.\nWe combine a Long Short Term Memory (LSTM) and a Mixture Density Network (MDN)\nto model human-inspired behavior in such a way that the learned strategies\ntransfer easily onto real hardware. The experiments show a UR10e robot that\ncompletes a plastic assembly with clearances of less than 100 micrometers whose\nstrategies were solely demonstrated in simulation.\n","authors":["Stefan Scherzinger","Arne Roennau","Rüdiger Dillmann"],"pdf_url":"https://arxiv.org/pdf/2303.12440v1.pdf","comment":"8 pages, 8 figures. Submitted to the IEEE International Conference on\n  Automation Science and Engineering (CASE) 2023"},{"id":"http://arxiv.org/abs/2211.12872v3","updated":"2023-03-22T10:05:15Z","published":"2022-11-23T11:26:24Z","title":"μSplit: efficient image decomposition for microscopy data","summary":"  We present uSplit, a dedicated approach for trained image decomposition in\nthe context of fluorescence microscopy images. We find that best results using\nregular deep architectures are achieved when large image patches are used\nduring training, making memory consumption the limiting factor to further\nimproving performance. We therefore introduce lateral contextualization (LC), a\nmemory efficient way to train powerful networks and show that LC leads to\nconsistent and significant improvements on the task at hand. We integrate LC\nwith U-Nets, Hierarchical AEs, and Hierarchical VAEs, for which we formulate a\nmodified ELBO loss. Additionally, LC enables training deeper hierarchical\nmodels than otherwise possible and, interestingly, helps to reduce tiling\nartefacts that are inherently impossible to avoid when using tiled VAE\npredictions. We apply uSplit to five decomposition tasks, one on a synthetic\ndataset, four others derived from real microscopy data. LC achieves SOTA\nresults (average improvements to the best baseline of 2.36 dB PSNR), while\nsimultaneously requiring considerably less GPU memory.\n","authors":[" Ashesh","Alexander Krull","Moises Di Sante","Francesco Silvio Pasqualini","Florian Jug"],"pdf_url":"https://arxiv.org/pdf/2211.12872v3.pdf","comment":"10 pages, 7 figures, 9 pages supplement, 8 supplementary figures"},{"id":"http://arxiv.org/abs/2302.08757v2","updated":"2023-03-22T09:54:23Z","published":"2023-02-17T08:41:31Z","title":"Enhanced Sampling of Configuration and Path Space in a Generalized\n  Ensemble by Shooting Point Exchange","summary":"  The computer simulation of many molecular processes is complicated by long\ntime scales caused by rare transitions between long-lived states. Here, we\npropose a new approach to simulate such rare events, which combines transition\npath sampling with enhanced exploration of configuration space. The method\nrelies on exchange moves between configuration and trajectory space, carried\nout based on a generalized ensemble. This scheme substantially enhances the\nefficiency of the transition path sampling simulations, particularly for\nsystems with multiple transition channels, and yields information on\nthermodynamics, kinetics and reaction coordinates of molecular processes\nwithout distorting their dynamics. The method is illustrated using the\nisomerization of proline in the KPTP tetrapeptide.\n","authors":["Sebastian Falkner","Alessandro Coretti","Christoph Dellago"],"pdf_url":"https://arxiv.org/pdf/2302.08757v2.pdf","comment":"Added Supplementary Information for simulation details and network\n  parameters"},{"id":"http://arxiv.org/abs/2301.12876v2","updated":"2023-03-22T09:52:25Z","published":"2023-01-30T13:30:56Z","title":"Guiding Online Reinforcement Learning with Action-Free Offline\n  Pretraining","summary":"  Offline RL methods have been shown to reduce the need for environment\ninteraction by training agents using offline collected episodes. However, these\nmethods typically require action information to be logged during data\ncollection, which can be difficult or even impossible in some practical cases.\nIn this paper, we investigate the potential of using action-free offline\ndatasets to improve online reinforcement learning, name this problem\nReinforcement Learning with Action-Free Offline Pretraining (AFP-RL). We\nintroduce Action-Free Guide (AF-Guide), a method that guides online training by\nextracting knowledge from action-free offline datasets. AF-Guide consists of an\nAction-Free Decision Transformer (AFDT) implementing a variant of Upside-Down\nReinforcement Learning. It learns to plan the next states from the offline\ndataset, and a Guided Soft Actor-Critic (Guided SAC) that learns online with\nguidance from AFDT. Experimental results show that AF-Guide can improve sample\nefficiency and performance in online training thanks to the knowledge from the\naction-free offline dataset. Code is available at\nhttps://github.com/Vision-CAIR/AF-Guide.\n","authors":["Deyao Zhu","Yuhui Wang","Jürgen Schmidhuber","Mohamed Elhoseiny"],"pdf_url":"https://arxiv.org/pdf/2301.12876v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12424v1","updated":"2023-03-22T09:51:08Z","published":"2023-03-22T09:51:08Z","title":"Unsupervised Domain Adaptation for Training Event-Based Networks Using\n  Contrastive Learning and Uncorrelated Conditioning","summary":"  Event-based cameras offer reliable measurements for preforming computer\nvision tasks in high-dynamic range environments and during fast motion\nmaneuvers. However, adopting deep learning in event-based vision faces the\nchallenge of annotated data scarcity due to recency of event cameras.\nTransferring the knowledge that can be obtained from conventional camera\nannotated data offers a practical solution to this challenge. We develop an\nunsupervised domain adaptation algorithm for training a deep network for\nevent-based data image classification using contrastive learning and\nuncorrelated conditioning of data. Our solution outperforms the existing\nalgorithms for this purpose.\n","authors":["Dayuan Jian","Mohammad Rostami"],"pdf_url":"https://arxiv.org/pdf/2303.12424v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12414v1","updated":"2023-03-22T09:23:29Z","published":"2023-03-22T09:23:29Z","title":"Delay-Aware Hierarchical Federated Learning","summary":"  Federated learning has gained popularity as a means of training models\ndistributed across the wireless edge. The paper introduces delay-aware\nfederated learning (DFL) to improve the efficiency of distributed machine\nlearning (ML) model training by addressing communication delays between edge\nand cloud. DFL employs multiple stochastic gradient descent iterations on\ndevice datasets during each global aggregation interval and intermittently\naggregates model parameters through edge servers in local subnetworks. The\ncloud server synchronizes the local models with the global deployed model\ncomputed via a local-global combiner at global synchronization. The convergence\nbehavior of DFL is theoretically investigated under a generalized data\nheterogeneity metric. A set of conditions is obtained to achieve the sub-linear\nconvergence rate of O(1/k). Based on these findings, an adaptive control\nalgorithm is developed for DFL, implementing policies to mitigate energy\nconsumption and edge-to-cloud communication latency while aiming for a\nsublinear convergence rate. Numerical evaluations show DFL's superior\nperformance in terms of faster global model convergence, reduced resource\nconsumption, and robustness against communication delays compared to existing\nFL algorithms. In summary, this proposed method offers improved efficiency and\nsatisfactory results when dealing with both convex and non-convex loss\nfunctions.\n","authors":["Frank Po-Chen Lin","Seyyedali Hosseinalipour","Christopher Brinton","Nicolò Michelusi"],"pdf_url":"https://arxiv.org/pdf/2303.12414v1.pdf","comment":"A condensed version of this paper was presented at IEEE Globecom 2020"},{"id":"http://arxiv.org/abs/2303.12410v1","updated":"2023-03-22T09:19:39Z","published":"2023-03-22T09:19:39Z","title":"EDGI: Equivariant Diffusion for Planning with Embodied Agents","summary":"  Embodied agents operate in a structured world, often solving tasks with\nspatial, temporal, and permutation symmetries. Most algorithms for planning and\nmodel-based reinforcement learning (MBRL) do not take this rich geometric\nstructure into account, leading to sample inefficiency and poor generalization.\nWe introduce the Equivariant Diffuser for Generating Interactions (EDGI), an\nalgorithm for MBRL and planning that is equivariant with respect to the product\nof the spatial symmetry group $\\mathrm{SE(3)}$, the discrete-time translation\ngroup $\\mathbb{Z}$, and the object permutation group $\\mathrm{S}_n$. EDGI\nfollows the Diffuser framework (Janner et al. 2022) in treating both learning a\nworld model and planning in it as a conditional generative modeling problem,\ntraining a diffusion model on an offline trajectory dataset. We introduce a new\n$\\mathrm{SE(3)} \\times \\mathbb{Z} \\times \\mathrm{S}_n$-equivariant diffusion\nmodel that supports multiple representations. We integrate this model in a\nplanning loop, where conditioning and classifier-based guidance allow us to\nsoftly break the symmetry for specific tasks as needed. On navigation and\nobject manipulation tasks, EDGI improves sample efficiency and generalization.\n","authors":["Johann Brehmer","Joey Bose","Pim de Haan","Taco Cohen"],"pdf_url":"https://arxiv.org/pdf/2303.12410v1.pdf","comment":"Reincarnating RL workshop at ICLR 2023"},{"id":"http://arxiv.org/abs/2102.01977v5","updated":"2023-03-22T09:17:12Z","published":"2021-02-03T09:51:03Z","title":"Instance-Dependent Bounds for Zeroth-order Lipschitz Optimization with\n  Error Certificates","summary":"  We study the problem of zeroth-order (black-box) optimization of a Lipschitz\nfunction $f$ defined on a compact subset $\\mathcal X$ of $\\mathbb R^d$, with\nthe additional constraint that algorithms must certify the accuracy of their\nrecommendations. We characterize the optimal number of evaluations of any\nLipschitz function $f$ to find and certify an approximate maximizer of $f$ at\naccuracy $\\varepsilon$. Under a weak assumption on $\\mathcal X$, this optimal\nsample complexity is shown to be nearly proportional to the integral\n$\\int_{\\mathcal X} \\mathrm{d}\\boldsymbol x/( \\max(f) - f(\\boldsymbol x) +\n\\varepsilon )^d$. This result, which was only (and partially) known in\ndimension $d=1$, solves an open problem dating back to 1991. In terms of\ntechniques, our upper bound relies on a packing bound by Bouttier al. (2020)\nfor the Piyavskii-Shubert algorithm that we link to the above integral. We also\nshow that a certified version of the computationally tractable DOO algorithm\nmatches these packing and integral bounds. Our instance-dependent lower bound\ndiffers from traditional worst-case lower bounds in the Lipschitz setting and\nrelies on a local worst-case analysis that could likely prove useful for other\nlearning tasks.\n","authors":["François Bachoc","Tommaso R Cesari","Sébastien Gerchinovitz"],"pdf_url":"https://arxiv.org/pdf/2102.01977v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2106.10836v2","updated":"2023-03-22T09:12:09Z","published":"2021-06-21T03:55:33Z","title":"Active Learning for Deep Neural Networks on Edge Devices","summary":"  When dealing with deep neural network (DNN) applications on edge devices,\ncontinuously updating the model is important. Although updating a model with\nreal incoming data is ideal, using all of them is not always feasible due to\nlimits, such as labeling and communication costs. Thus, it is necessary to\nfilter and select the data to use for training (i.e., active learning) on the\ndevice. In this paper, we formalize a practical active learning problem for\nDNNs on edge devices and propose a general task-agnostic framework to tackle\nthis problem, which reduces it to a stream submodular maximization. This\nframework is light enough to be run with low computational resources, yet\nprovides solutions whose quality is theoretically guaranteed thanks to the\nsubmodular property. Through this framework, we can configure data selection\ncriteria flexibly, including using methods proposed in previous active learning\nstudies. We evaluate our approach on both classification and object detection\ntasks in a practical setting to simulate a real-life scenario. The results of\nour study show that the proposed framework outperforms all other methods in\nboth tasks, while running at a practical speed on real devices.\n","authors":["Yuya Senzaki","Christian Hamelain"],"pdf_url":"https://arxiv.org/pdf/2106.10836v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12398v1","updated":"2023-03-22T09:06:07Z","published":"2023-03-22T09:06:07Z","title":"Multiscale Attention via Wavelet Neural Operators for Vision\n  Transformers","summary":"  Transformers have achieved widespread success in computer vision. At their\nheart, there is a Self-Attention (SA) mechanism, an inductive bias that\nassociates each token in the input with every other token through a weighted\nbasis. The standard SA mechanism has quadratic complexity with the sequence\nlength, which impedes its utility to long sequences appearing in high\nresolution vision. Recently, inspired by operator learning for PDEs, Adaptive\nFourier Neural Operators (AFNO) were introduced for high resolution attention\nbased on global convolution that is efficiently implemented via FFT. However,\nthe AFNO global filtering cannot well represent small and moderate scale\nstructures that commonly appear in natural images. To leverage the\ncoarse-to-fine scale structures we introduce a Multiscale Wavelet Attention\n(MWA) by leveraging wavelet neural operators which incurs linear complexity in\nthe sequence size. We replace the attention in ViT with MWA and our experiments\nwith CIFAR and ImageNet classification demonstrate significant improvement over\nalternative Fourier-based attentions such as AFNO and Global Filter Network\n(GFN).\n","authors":["Anahita Nekoozadeh","Mohammad Reza Ahmadzadeh","Zahra Mardani","Morteza Mardani"],"pdf_url":"https://arxiv.org/pdf/2303.12398v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.00210v2","updated":"2023-03-22T08:58:15Z","published":"2022-12-01T01:39:28Z","title":"Shape-Guided Diffusion with Inside-Outside Attention","summary":"  When manipulating an object, existing text-to-image diffusion models often\nignore the shape of the object and generate content that is incorrectly scaled,\ncut off, or replaced with background content. We propose a training-free\nmethod, Shape-Guided Diffusion, that modifies pretrained diffusion models to be\nsensitive to shape input specified by a user or automatically inferred from\ntext. We use a novel Inside-Outside Attention mechanism during the inversion\nand generation process to apply this shape constraint to the cross- and\nself-attention maps. Our mechanism designates which spatial region is the\nobject (inside) vs. background (outside) then associates edits specified by\ntext prompts to the correct region. We demonstrate the efficacy of our method\non the shape-guided editing task, where the model must replace an object\naccording to a text prompt and object mask. We curate a new ShapePrompts\nbenchmark derived from MS-COCO and achieve SOTA results in shape faithfulness\nwithout a degradation in text alignment or image realism according to both\nautomatic metrics and annotator ratings. Our data and code will be made\navailable at https://shape-guided-diffusion.github.io.\n","authors":["Dong Huk Park","Grace Luo","Clayton Toste","Samaneh Azadi","Xihui Liu","Maka Karalashvili","Anna Rohrbach","Trevor Darrell"],"pdf_url":"https://arxiv.org/pdf/2212.00210v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11330v2","updated":"2023-03-22T08:48:15Z","published":"2023-03-20T17:59:58Z","title":"Legs as Manipulator: Pushing Quadrupedal Agility Beyond Locomotion","summary":"  Locomotion has seen dramatic progress for walking or running across\nchallenging terrains. However, robotic quadrupeds are still far behind their\nbiological counterparts, such as dogs, which display a variety of agile skills\nand can use the legs beyond locomotion to perform several basic manipulation\ntasks like interacting with objects and climbing. In this paper, we take a step\ntowards bridging this gap by training quadruped robots not only to walk but\nalso to use the front legs to climb walls, press buttons, and perform object\ninteraction in the real world. To handle this challenging optimization, we\ndecouple the skill learning broadly into locomotion, which involves anything\nthat involves movement whether via walking or climbing a wall, and\nmanipulation, which involves using one leg to interact while balancing on the\nother three legs. These skills are trained in simulation using curriculum and\ntransferred to the real world using our proposed sim2real variant that builds\nupon recent locomotion success. Finally, we combine these skills into a robust\nlong-term plan by learning a behavior tree that encodes a high-level task\nhierarchy from one clean expert demonstration. We evaluate our method in both\nsimulation and real-world showing successful executions of both short as well\nas long-range tasks and how robustness helps confront external perturbations.\nVideos at https://robot-skills.github.io\n","authors":["Xuxin Cheng","Ashish Kumar","Deepak Pathak"],"pdf_url":"https://arxiv.org/pdf/2303.11330v2.pdf","comment":"Accepted at ICRA 2023. Videos at https://robot-skills.github.io"},{"id":"http://arxiv.org/abs/2302.08175v5","updated":"2023-03-22T08:32:12Z","published":"2023-02-16T09:44:55Z","title":"A numerical approximation method for the Fisher-Rao distance between\n  multivariate normal distributions","summary":"  We present a simple method to approximate Rao's distance between multivariate\nnormal distributions based on discretizing curves joining normal distributions\nand approximating Rao's distances between successive nearby normal\ndistributions on the curves by the square root of Jeffreys divergence, the\nsymmetrized Kullback-Leibler divergence. We consider experimentally the linear\ninterpolation curves in the ordinary, natural and expectation parameterizations\nof the normal distributions, and compare these curves with a curve derived from\nthe Calvo and Oller's isometric embedding of the Fisher-Rao $d$-variate normal\nmanifold into the cone of $(d+1)\\times (d+1)$ symmetric positive-definite\nmatrices [Journal of multivariate analysis 35.2 (1990): 223-242]. We report on\nour experiments and assess the quality of our approximation technique by\ncomparing the numerical approximations with both lower and upper bounds.\nFinally, we present several information-geometric properties of the Calvo and\nOller's isometric embedding.\n","authors":["Frank Nielsen"],"pdf_url":"https://arxiv.org/pdf/2302.08175v5.pdf","comment":"42 pages, 17 figures, 3 tables"},{"id":"http://arxiv.org/abs/2303.12375v1","updated":"2023-03-22T08:22:12Z","published":"2023-03-22T08:22:12Z","title":"Disturbance Injection under Partial Automation: Robust Imitation\n  Learning for Long-horizon Tasks","summary":"  Partial Automation (PA) with intelligent support systems has been introduced\nin industrial machinery and advanced automobiles to reduce the burden of long\nhours of human operation. Under PA, operators perform manual operations\n(providing actions) and operations that switch to automatic/manual mode\n(mode-switching). Since PA reduces the total duration of manual operation,\nthese two action and mode-switching operations can be replicated by imitation\nlearning with high sample efficiency. To this end, this paper proposes\nDisturbance Injection under Partial Automation (DIPA) as a novel imitation\nlearning framework. In DIPA, mode and actions (in the manual mode) are assumed\nto be observables in each state and are used to learn both action and\nmode-switching policies. The above learning is robustified by injecting\ndisturbances into the operator's actions to optimize the disturbance's level\nfor minimizing the covariate shift under PA. We experimentally validated the\neffectiveness of our method for long-horizon tasks in two simulations and a\nreal robot environment and confirmed that our method outperformed the previous\nmethods and reduced the demonstration burden.\n","authors":["Hirotaka Tahara","Hikaru Sasaki","Hanbit Oh","Edgar Anarossi","Takamitsu Matsubara"],"pdf_url":"https://arxiv.org/pdf/2303.12375v1.pdf","comment":"8 pages, Accepted by Robotics and Automation Letters (RA-L) 2023"},{"id":"http://arxiv.org/abs/2303.12367v1","updated":"2023-03-22T08:06:41Z","published":"2023-03-22T08:06:41Z","title":"AIIPot: Adaptive Intelligent-Interaction Honeypot for IoT Devices","summary":"  The proliferation of the Internet of Things (IoT) has raised concerns about\nthe security of connected devices. There is a need to develop suitable and\ncost-efficient methods to identify vulnerabilities in IoT devices in order to\naddress them before attackers seize opportunities to compromise them. The\ndeception technique is a prominent approach to improving the security posture\nof IoT systems. Honeypot is a popular deception technique that mimics\ninteraction in real fashion and encourages unauthorised users (attackers) to\nlaunch attacks. Due to the large number and the heterogeneity of IoT devices,\nmanually crafting the low and high-interaction honeypots is not affordable.\nThis has forced researchers to seek innovative ways to build honeypots for IoT\ndevices. In this paper, we propose a honeypot for IoT devices that uses machine\nlearning techniques to learn and interact with attackers automatically. The\nevaluation of the proposed model indicates that our system can improve the\nsession length with attackers and capture more attacks on the IoT network.\n","authors":["Volviane Saphir Mfogo","Alain Zemkoho","Laurent Njilla","Marcellin Nkenlifack","Charles Kamhoua"],"pdf_url":"https://arxiv.org/pdf/2303.12367v1.pdf","comment":"7 pages, 7 figures"},{"id":"http://arxiv.org/abs/2303.12364v1","updated":"2023-03-22T08:03:27Z","published":"2023-03-22T08:03:27Z","title":"ExBEHRT: Extended Transformer for Electronic Health Records to Predict\n  Disease Subtypes & Progressions","summary":"  In this study, we introduce ExBEHRT, an extended version of BEHRT (BERT\napplied to electronic health records), and apply different algorithms to\ninterpret its results. While BEHRT considers only diagnoses and patient age, we\nextend the feature space to several multimodal records, namely demographics,\nclinical characteristics, vital signs, smoking status, diagnoses, procedures,\nmedications, and laboratory tests, by applying a novel method to unify the\nfrequencies and temporal dimensions of the different features. We show that\nadditional features significantly improve model performance for various\ndownstream tasks in different diseases. To ensure robustness, we interpret\nmodel predictions using an adaptation of expected gradients, which has not been\npreviously applied to transformers with EHR data and provides more granular\ninterpretations than previous approaches such as feature and token importances.\nFurthermore, by clustering the model representations of oncology patients, we\nshow that the model has an implicit understanding of the disease and is able to\nclassify patients with the same cancer type into different risk groups. Given\nthe additional features and interpretability, ExBEHRT can help make informed\ndecisions about disease trajectories, diagnoses, and risk factors of various\ndiseases.\n","authors":["Maurice Rupp","Oriane Peter","Thirupathi Pattipaka"],"pdf_url":"https://arxiv.org/pdf/2303.12364v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12363v1","updated":"2023-03-22T07:56:59Z","published":"2023-03-22T07:56:59Z","title":"Distribution-restrained Softmax Loss for the Model Robustness","summary":"  Recently, the robustness of deep learning models has received widespread\nattention, and various methods for improving model robustness have been\nproposed, including adversarial training, model architecture modification,\ndesign of loss functions, certified defenses, and so on. However, the principle\nof the robustness to attacks is still not fully understood, also the related\nresearch is still not sufficient. Here, we have identified a significant factor\nthat affects the robustness of models: the distribution characteristics of\nsoftmax values for non-real label samples. We found that the results after an\nattack are highly correlated with the distribution characteristics, and thus we\nproposed a loss function to suppress the distribution diversity of softmax. A\nlarge number of experiments have shown that our method can improve robustness\nwithout significant time consumption.\n","authors":["Hao Wang","Chen Li","Jinzhe Jiang","Xin Zhang","Yaqian Zhao","Weifeng Gong"],"pdf_url":"https://arxiv.org/pdf/2303.12363v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12357v1","updated":"2023-03-22T07:50:15Z","published":"2023-03-22T07:50:15Z","title":"Wasserstein Adversarial Examples on Univariant Time Series Data","summary":"  Adversarial examples are crafted by adding indistinguishable perturbations to\nnormal examples in order to fool a well-trained deep learning model to\nmisclassify. In the context of computer vision, this notion of\nindistinguishability is typically bounded by $L_{\\infty}$ or other norms.\nHowever, these norms are not appropriate for measuring indistinguishiability\nfor time series data. In this work, we propose adversarial examples in the\nWasserstein space for time series data for the first time and utilize\nWasserstein distance to bound the perturbation between normal examples and\nadversarial examples. We introduce Wasserstein projected gradient descent\n(WPGD), an adversarial attack method for perturbing univariant time series\ndata. We leverage the closed-form solution of Wasserstein distance in the 1D\nspace to calculate the projection step of WPGD efficiently with the gradient\ndescent method. We further propose a two-step projection so that the search of\nadversarial examples in the Wasserstein space is guided and constrained by\nEuclidean norms to yield more effective and imperceptible perturbations. We\nempirically evaluate the proposed attack on several time series datasets in the\nhealthcare domain. Extensive results demonstrate that the Wasserstein attack is\npowerful and can successfully attack most of the target classifiers with a high\nattack success rate. To better study the nature of Wasserstein adversarial\nexample, we evaluate a strong defense mechanism named Wasserstein smoothing for\npotential certified robustness defense. Although the defense can achieve some\naccuracy gain, it still has limitations in many cases and leaves space for\ndeveloping a stronger certified robustness method to Wasserstein adversarial\nexamples on univariant time series data.\n","authors":["Wenjie Wang","Li Xiong","Jian Lou"],"pdf_url":"https://arxiv.org/pdf/2303.12357v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12354v1","updated":"2023-03-22T07:44:35Z","published":"2023-03-22T07:44:35Z","title":"Deep Reinforcement Learning for Localizability-Enhanced Navigation in\n  Dynamic Human Environments","summary":"  Reliable localization is crucial for autonomous robots to navigate\nefficiently and safely. Some navigation methods can plan paths with high\nlocalizability (which describes the capability of acquiring reliable\nlocalization). By following these paths, the robot can access the sensor\nstreams that facilitate more accurate location estimation results by the\nlocalization algorithms. However, most of these methods require prior knowledge\nand struggle to adapt to unseen scenarios or dynamic changes. To overcome these\nlimitations, we propose a novel approach for localizability-enhanced navigation\nvia deep reinforcement learning in dynamic human environments. Our proposed\nplanner automatically extracts geometric features from 2D laser data that are\nhelpful for localization. The planner learns to assign different importance to\nthe geometric features and encourages the robot to navigate through areas that\nare helpful for laser localization. To facilitate the learning of the planner,\nwe suggest two techniques: (1) an augmented state representation that considers\nthe dynamic changes and the confidence of the localization results, which\nprovides more information and allows the robot to make better decisions, (2) a\nreward metric that is capable to offer both sparse and dense feedback on\nbehaviors that affect localization accuracy. Our method exhibits significant\nimprovements in lost rate and arrival rate when tested in previously unseen\nenvironments.\n","authors":["Yuan Chen","Quecheng Qiu","Xiangyu Liu","Guangda Chen","Shunyi Yao","Jie Peng","Jianmin Ji","Yanyong Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.12354v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12352v1","updated":"2023-03-22T07:40:01Z","published":"2023-03-22T07:40:01Z","title":"Training Multilayer Perceptrons by Sampling with Quantum Annealers","summary":"  A successful application of quantum annealing to machine learning is training\nrestricted Boltzmann machines (RBM). However, many neural networks for vision\napplications are feedforward structures, such as multilayer perceptrons (MLP).\nBackpropagation is currently the most effective technique to train MLPs for\nsupervised learning. This paper aims to be forward-looking by exploring the\ntraining of MLPs using quantum annealers. We exploit an equivalence between\nMLPs and energy-based models (EBM), which are a variation of RBMs with a\nmaximum conditional likelihood objective. This leads to a strategy to train\nMLPs with quantum annealers as a sampling engine. We prove our setup for MLPs\nwith sigmoid activation functions and one hidden layer, and demonstrated\ntraining of binary image classifiers on small subsets of the MNIST and\nFashion-MNIST datasets using the D-Wave quantum annealer. Although problem\nsizes that are feasible on current annealers are limited, we obtained\ncomprehensive results on feasible instances that validate our ideas. Our work\nestablishes the potential of quantum computing for training MLPs.\n","authors":["Frances Fengyi Yang","Michele Sasdelli","Tat-Jun Chin"],"pdf_url":"https://arxiv.org/pdf/2303.12352v1.pdf","comment":"22 pages, 15 figures"},{"id":"http://arxiv.org/abs/2201.10981v3","updated":"2023-03-22T07:35:10Z","published":"2022-01-26T14:52:23Z","title":"Joint Liver and Hepatic Lesion Segmentation in MRI using a Hybrid CNN\n  with Transformer Layers","summary":"  Deep learning-based segmentation of the liver and hepatic lesions therein\nsteadily gains relevance in clinical practice due to the increasing incidence\nof liver cancer each year. Whereas various network variants with overall\npromising results in the field of medical image segmentation have been\nsuccessfully developed over the last years, almost all of them struggle with\nthe challenge of accurately segmenting hepatic lesions in magnetic resonance\nimaging (MRI). This led to the idea of combining elements of convolutional and\ntransformer-based architectures to overcome the existing limitations. This work\npresents a hybrid network called SWTR-Unet, consisting of a pretrained ResNet,\ntransformer blocks as well as a common Unet-style decoder path. This network\nwas primarily applied to single-modality non-contrast-enhanced liver MRI and\nadditionally to the publicly available computed tomography (CT) data of the\nliver tumor segmentation (LiTS) challenge to verify the applicability on other\nmodalities. For a broader evaluation, multiple state-of-the-art networks were\nimplemented and applied, ensuring a direct comparability. Furthermore,\ncorrelation analysis and an ablation study were carried out, to investigate\nvarious influencing factors on the segmentation accuracy of the presented\nmethod. With Dice scores of averaged 98+-2% for liver and 81+-28% lesion\nsegmentation on the MRI dataset and 97+-2% and 79+-25%, respectively on the CT\ndataset, the proposed SWTR-Unet proved to be a precise approach for liver and\nhepatic lesion segmentation with state-of-the-art results for MRI and competing\naccuracy in CT imaging. The achieved segmentation accuracy was found to be on\npar with manually performed expert segmentations as indicated by inter-observer\nvariabilities for liver lesion segmentation. In conclusion, the presented\nmethod could save valuable time and resources in clinical practice.\n","authors":["Georg Hille","Shubham Agrawal","Pavan Tummala","Christian Wybranski","Maciej Pech","Alexey Surov","Sylvia Saalfeld"],"pdf_url":"https://arxiv.org/pdf/2201.10981v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.04589v2","updated":"2023-03-22T07:34:13Z","published":"2022-08-09T08:07:21Z","title":"Long-term Causal Effects Estimation via Latent Surrogates Representation\n  Learning","summary":"  Estimating long-term causal effects based on short-term surrogates is a\nsignificant but challenging problem in many real-world applications, e.g.,\nmarketing and medicine. Despite its success in certain domains, most existing\nmethods estimate causal effects in an idealistic and simplistic way - ignoring\nthe causal structure among short-term outcomes and treating all of them as\nsurrogates. However, such methods cannot be well applied to real-world\nscenarios, in which the partially observed surrogates are mixed with their\nproxies among short-term outcomes. To this end, we develop our flexible method,\nLaser, to estimate long-term causal effects in the more realistic situation\nthat the surrogates are observed or have observed proxies.Given the\nindistinguishability between the surrogates and proxies, we utilize\nidentifiable variational auto-encoder (iVAE) to recover the whole valid\nsurrogates on all the surrogates candidates without the need of distinguishing\nthe observed surrogates or the proxies of latent surrogates. With the help of\nthe recovered surrogates, we further devise an unbiased estimation of long-term\ncausal effects. Extensive experimental results on the real-world and\nsemi-synthetic datasets demonstrate the effectiveness of our proposed method.\n","authors":["Ruichu Cai","Weilin Chen","Zeqin Yang","Shu Wan","Chen Zheng","Xiaoqing Yang","Jiecheng Guo"],"pdf_url":"https://arxiv.org/pdf/2208.04589v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.00690v3","updated":"2023-03-22T07:20:37Z","published":"2022-08-01T08:58:02Z","title":"Generative Bias for Robust Visual Question Answering","summary":"  The task of Visual Question Answering (VQA) is known to be plagued by the\nissue of VQA models exploiting biases within the dataset to make its final\nprediction. Various previous ensemble based debiasing methods have been\nproposed where an additional model is purposefully trained to be biased in\norder to train a robust target model. However, these methods compute the bias\nfor a model simply from the label statistics of the training data or from\nsingle modal branches. In this work, in order to better learn the bias a target\nVQA model suffers from, we propose a generative method to train the bias model\ndirectly from the target model, called GenB. In particular, GenB employs a\ngenerative network to learn the bias in the target model through a combination\nof the adversarial objective and knowledge distillation. We then debias our\ntarget model with GenB as a bias model, and show through extensive experiments\nthe effects of our method on various VQA bias datasets including VQA-CP2,\nVQA-CP1, GQA-OOD, and VQA-CE, and show state-of-the-art results with the LXMERT\narchitecture on VQA-CP2.\n","authors":["Jae Won Cho","Dong-jin Kim","Hyeonggon Ryu","In So Kweon"],"pdf_url":"https://arxiv.org/pdf/2208.00690v3.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.12341v1","updated":"2023-03-22T06:35:08Z","published":"2023-03-22T06:35:08Z","title":"EasyDGL: Encode, Train and Interpret for Continuous-time Dynamic Graph\n  Learning","summary":"  Dynamic graphs arise in various real-world applications, and it is often\nwelcomed to model the dynamics directly in continuous time domain for its\nflexibility. This paper aims to design an easy-to-use pipeline (termed as\nEasyDGL which is also due to its implementation by DGL toolkit) composed of\nthree key modules with both strong fitting ability and interpretability.\nSpecifically the proposed pipeline which involves encoding, training and\ninterpreting: i) a temporal point process (TPP) modulated attention\narchitecture to endow the continuous-time resolution with the coupled\nspatiotemporal dynamics of the observed graph with edge-addition events; ii) a\nprincipled loss composed of task-agnostic TPP posterior maximization based on\nobserved events on the graph, and a task-aware loss with a masking strategy\nover dynamic graph, where the covered tasks include dynamic link prediction,\ndynamic node classification and node traffic forecasting; iii) interpretation\nof the model outputs (e.g., representations and predictions) with scalable\nperturbation-based quantitative analysis in the graph Fourier domain, which\ncould more comprehensively reflect the behavior of the learned model. Extensive\nexperimental results on public benchmarks show the superior performance of our\nEasyDGL for time-conditioned predictive tasks, and in particular demonstrate\nthat EasyDGL can effectively quantify the predictive power of frequency content\nthat a model learn from the evolving graph data.\n","authors":["Chao Chen","Haoyu Geng","Nianzu Yang","Xiaokang Yang","Junchi Yan"],"pdf_url":"https://arxiv.org/pdf/2303.12341v1.pdf","comment":"9 figures, 7 tables"},{"id":"http://arxiv.org/abs/2303.11577v2","updated":"2023-03-22T06:07:28Z","published":"2023-03-21T03:51:15Z","title":"Feature-adjacent multi-fidelity physics-informed machine learning for\n  partial differential equations","summary":"  Physics-informed neural networks have emerged as an alternative method for\nsolving partial differential equations. However, for complex problems, the\ntraining of such networks can still require high-fidelity data which can be\nexpensive to generate. To reduce or even eliminate the dependency on\nhigh-fidelity data, we propose a novel multi-fidelity architecture which is\nbased on a feature space shared by the low- and high-fidelity solutions. In the\nfeature space, the projections of the low-fidelity and high-fidelity solutions\nare adjacent by constraining their relative distance. The feature space is\nrepresented with an encoder and its mapping to the original solution space is\neffected through a decoder. The proposed multi-fidelity approach is validated\non forward and inverse problems for steady and unsteady problems described by\npartial differential equations.\n","authors":["Wenqian Chen","Panos Stinis"],"pdf_url":"https://arxiv.org/pdf/2303.11577v2.pdf","comment":"12 figures"},{"id":"http://arxiv.org/abs/2202.02892v4","updated":"2023-03-22T05:23:53Z","published":"2022-02-07T00:15:03Z","title":"Lossy Compression of Noisy Data for Private and Data-Efficient Learning","summary":"  Storage-efficient privacy-preserving learning is crucial due to increasing\namounts of sensitive user data required for modern learning tasks. We propose a\nframework for reducing the storage cost of user data while at the same time\nproviding privacy guarantees, without essential loss in the utility of the data\nfor learning. Our method comprises noise injection followed by lossy\ncompression. We show that, when appropriately matching the lossy compression to\nthe distribution of the added noise, the compressed examples converge, in\ndistribution, to that of the noise-free training data as the sample size of the\ntraining data (or the dimension of the training data) increases. In this sense,\nthe utility of the data for learning is essentially maintained, while reducing\nstorage and privacy leakage by quantifiable amounts. We present experimental\nresults on the CelebA dataset for gender classification and find that our\nsuggested pipeline delivers in practice on the promise of the theory: the\nindividuals in the images are unrecognizable (or less recognizable, depending\non the noise level), overall storage of the data is substantially reduced, with\nno essential loss (and in some cases a slight boost) to the classification\naccuracy. As an added bonus, our experiments suggest that our method yields a\nsubstantial boost to robustness in the face of adversarial test data.\n","authors":["Berivan Isik","Tsachy Weissman"],"pdf_url":"https://arxiv.org/pdf/2202.02892v4.pdf","comment":"Published at the IEEE Journal on Selected Areas in Information Theory\n  (JSAIT). Preliminary version was presented at the IEEE International\n  Symposium on Information Theory (ISIT), 2022, with a slightly different\n  title, \"Learning under Storage and Privacy Constraints.\""},{"id":"http://arxiv.org/abs/2303.12317v1","updated":"2023-03-22T05:21:21Z","published":"2023-03-22T05:21:21Z","title":"Re-thinking Federated Active Learning based on Inter-class Diversity","summary":"  Although federated learning has made awe-inspiring advances, most studies\nhave assumed that the client's data are fully labeled. However, in a real-world\nscenario, every client may have a significant amount of unlabeled instances.\nAmong the various approaches to utilizing unlabeled data, a federated active\nlearning framework has emerged as a promising solution. In the decentralized\nsetting, there are two types of available query selector models, namely\n'global' and 'local-only' models, but little literature discusses their\nperformance dominance and its causes. In this work, we first demonstrate that\nthe superiority of two selector models depends on the global and local\ninter-class diversity. Furthermore, we observe that the global and local-only\nmodels are the keys to resolving the imbalance of each side. Based on our\nfindings, we propose LoGo, a FAL sampling strategy robust to varying local\nheterogeneity levels and global imbalance ratio, that integrates both models by\ntwo steps of active selection scheme. LoGo consistently outperforms six active\nlearning strategies in the total number of 38 experimental settings.\n","authors":["SangMook Kim","Sangmin Bae","Hwanjun Song","Se-Young Yun"],"pdf_url":"https://arxiv.org/pdf/2303.12317v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.12316v1","updated":"2023-03-22T05:14:36Z","published":"2023-03-22T05:14:36Z","title":"TsSHAP: Robust model agnostic feature-based explainability for time\n  series forecasting","summary":"  A trustworthy machine learning model should be accurate as well as\nexplainable. Understanding why a model makes a certain decision defines the\nnotion of explainability. While various flavors of explainability have been\nwell-studied in supervised learning paradigms like classification and\nregression, literature on explainability for time series forecasting is\nrelatively scarce.\n  In this paper, we propose a feature-based explainability algorithm, TsSHAP,\nthat can explain the forecast of any black-box forecasting model. The method is\nagnostic of the forecasting model and can provide explanations for a forecast\nin terms of interpretable features defined by the user a prior.\n  The explanations are in terms of the SHAP values obtained by applying the\nTreeSHAP algorithm on a surrogate model that learns a mapping between the\ninterpretable feature space and the forecast of the black-box model.\n  Moreover, we formalize the notion of local, semi-local, and global\nexplanations in the context of time series forecasting, which can be useful in\nseveral scenarios. We validate the efficacy and robustness of TsSHAP through\nextensive experiments on multiple datasets.\n","authors":["Vikas C. Raykar","Arindam Jati","Sumanta Mukherjee","Nupur Aggarwal","Kanthi Sarpatwar","Giridhar Ganapavarapu","Roman Vaculin"],"pdf_url":"https://arxiv.org/pdf/2303.12316v1.pdf","comment":"11 pages, 8 figures"},{"id":"http://arxiv.org/abs/2303.12314v1","updated":"2023-03-22T05:04:21Z","published":"2023-03-22T05:04:21Z","title":"Self-supervised Meta-Prompt Learning with Meta-Gradient Regularization\n  for Few-shot Generalization","summary":"  Prompt tuning is a parameter-efficient method, which learns soft prompts and\nconditions frozen language models to perform specific downstream tasks. Though\neffective, prompt tuning under few-shot settings on the one hand heavily relies\non a good initialization of soft prompts. On the other hand, it can easily\nresult in overfitting. Existing works leverage pre-training or supervised\nmeta-learning to initialize soft prompts but they cannot data-efficiently\ngeneralize to unseen downstream tasks. To address the above problems, this\npaper proposes a novel Self-sUpervised meta-Prompt learning framework with\nmeta-gradient Regularization for few-shot generalization (SUPMER). We first\ndesign a set of self-supervised anchor meta-training tasks with different task\nformats and further enrich the task distribution with curriculum-based task\naugmentation. Then a novel meta-gradient regularization method is integrated\ninto meta-prompt learning. It meta-learns to transform the raw gradients during\nfew-shot learning into a domain-generalizable direction, thus alleviating the\nproblem of overfitting. Extensive experiments show that SUPMER achieves better\nperformance for different few-shot downstream tasks, and also exhibits a\nstronger domain generalization ability.\n","authors":["Kaihang Pan","Juncheng Li","Hongye Song","Jun Lin","Xiaozhong Liu","Siliang Tang"],"pdf_url":"https://arxiv.org/pdf/2303.12314v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12311v1","updated":"2023-03-22T05:01:14Z","published":"2023-03-22T05:01:14Z","title":"Frozen Language Model Helps ECG Zero-Shot Learning","summary":"  The electrocardiogram (ECG) is one of the most commonly used non-invasive,\nconvenient medical monitoring tools that assist in the clinical diagnosis of\nheart diseases. Recently, deep learning (DL) techniques, particularly\nself-supervised learning (SSL), have demonstrated great potential in the\nclassification of ECG. SSL pre-training has achieved competitive performance\nwith only a small amount of annotated data after fine-tuning. However, current\nSSL methods rely on the availability of annotated data and are unable to\npredict labels not existing in fine-tuning datasets. To address this challenge,\nwe propose Multimodal ECG-Text Self-supervised pre-training (METS), the first\nwork to utilize the auto-generated clinical reports to guide ECG SSL\npre-training. We use a trainable ECG encoder and a frozen language model to\nembed paired ECG and automatically machine-generated clinical reports\nseparately. The SSL aims to maximize the similarity between paired ECG and\nauto-generated report while minimize the similarity between ECG and other\nreports. In downstream classification tasks, METS achieves around 10%\nimprovement in performance without using any annotated data via zero-shot\nclassification, compared to other supervised and SSL baselines that rely on\nannotated data. Furthermore, METS achieves the highest recall and F1 scores on\nthe MIT-BIH dataset, despite MIT-BIH containing different classes of ECG\ncompared to the pre-trained dataset. The extensive experiments have\ndemonstrated the advantages of using ECG-Text multimodal self-supervised\nlearning in terms of generalizability, effectiveness, and efficiency.\n","authors":["Jun Li","Che Liu","Sibo Cheng","Rossella Arcucci","Shenda Hong"],"pdf_url":"https://arxiv.org/pdf/2303.12311v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.02672v2","updated":"2023-03-22T04:59:05Z","published":"2022-10-06T04:30:59Z","title":"Orthogonal Non-negative Matrix Factorization: a\n  Maximum-Entropy-Principle Approach","summary":"  In this paper, we introduce a new methodology to solve the orthogonal\nnonnegative matrix factorization (ONMF) problem, where the objective is to\napproximate an input data matrix by a product of two nonnegative matrices, the\nfeatures matrix and the mixing matrix, where one of them is orthogonal. We show\nhow the ONMF can be interpreted as a specific facility-location problem (FLP),\nand adapt a maximum-entropy-principle based solution for FLP to the ONMF\nproblem. The proposed approach guarantees orthogonality and sparsity of the\nfeatures or the mixing matrix, while ensuring nonnegativity of both.\nAdditionally, our methodology develops a quantitative characterization of\n``true\" number of underlying features - a hyperparameter required for the ONMF.\nAn evaluation of the proposed method conducted on synthetic datasets, as well\nas a standard genetic microarray dataset indicates significantly better\nsparsity, orthogonality, and performance speed compared to similar methods in\nthe literature, with comparable or improved reconstruction errors.\n","authors":["Salar Basiri","Mustafa Kapadia","Srinivasa Salapaka"],"pdf_url":"https://arxiv.org/pdf/2210.02672v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.02900v2","updated":"2023-03-22T04:57:56Z","published":"2022-05-05T19:39:32Z","title":"New-Onset Diabetes Assessment Using Artificial Intelligence-Enhanced\n  Electrocardiography","summary":"  Undiagnosed diabetes is present in 21.4% of adults with diabetes. Diabetes\ncan remain asymptomatic and undetected due to limitations in screening rates.\nTo address this issue, questionnaires, such as the American Diabetes\nAssociation (ADA) Risk test, have been recommended for use by physicians and\nthe public. Based on evidence that blood glucose concentration can affect\ncardiac electrophysiology, we hypothesized that an artificial intelligence\n(AI)-enhanced electrocardiogram (ECG) could identify adults with new-onset\ndiabetes. We trained a neural network to estimate HbA1c using a 12-lead ECG and\nreadily available demographics. We retrospectively assembled a dataset\ncomprised of patients with paired ECG and HbA1c data. The population of\npatients who receive both an ECG and HbA1c may a biased sample of the complete\noutpatient population, so we adjusted the importance placed on each patient to\ngenerate a more representative pseudo-population. We found ECG-based assessment\noutperforms the ADA Risk test, achieving a higher area under the curve (0.80\nvs. 0.68) and positive predictive value (13% vs. 9%) -- 2.6 times the\nprevalence of diabetes in the cohort. The AI-enhanced ECG significantly\noutperforms electrophysiologist interpretation of the ECG, suggesting that the\ntask is beyond current clinical capabilities. Given the prevalence of ECGs in\nclinics and via wearable devices, such a tool would make precise, automated\ndiabetes assessment widely accessible.\n","authors":["Neil Jethani","Aahlad Puli","Hao Zhang","Leonid Garber","Lior Jankelson","Yindalon Aphinyanaphongs","Rajesh Ranganath"],"pdf_url":"https://arxiv.org/pdf/2205.02900v2.pdf","comment":"21 pages, 8 figures"},{"id":"http://arxiv.org/abs/2303.12306v1","updated":"2023-03-22T04:49:00Z","published":"2023-03-22T04:49:00Z","title":"Logical Expressiveness of Graph Neural Network for Knowledge Graph\n  Reasoning","summary":"  Graph Neural Networks (GNNs) have been recently introduced to learn from\nknowledge graph (KG) and achieved state-of-the-art performance in KG reasoning.\nHowever, a theoretical certification for their good empirical performance is\nstill absent. Besides, while logic in KG is important for inductive and\ninterpretable inference, existing GNN-based methods are just designed to fit\ndata distributions with limited knowledge of their logical expressiveness. We\npropose to fill the above gap in this paper. Specifically, we theoretically\nanalyze GNN from logical expressiveness and find out what kind of logical rules\ncan be captured from KG. Our results first show that GNN can capture logical\nrules from graded modal logic, providing a new theoretical tool for analyzing\nthe expressiveness of GNN for KG reasoning; and a query labeling trick makes it\neasier for GNN to capture logical rules, explaining why SOTA methods are mainly\nbased on labeling trick. Finally, insights from our theory motivate the\ndevelopment of an entity labeling method for capturing difficult logical rules.\nExperimental results are consistent with our theoretical results and verify the\neffectiveness of our proposed method.\n","authors":["Haiquan Qiu","Yongqi Zhang","Yong Li","Quanming Yao"],"pdf_url":"https://arxiv.org/pdf/2303.12306v1.pdf","comment":"15 pages, 4 figures"},{"id":"http://arxiv.org/abs/2303.12302v1","updated":"2023-03-22T04:19:18Z","published":"2023-03-22T04:19:18Z","title":"Anomaly Detection in Aeronautics Data with Quantum-compatible Discrete\n  Deep Generative Model","summary":"  Deep generative learning cannot only be used for generating new data with\nstatistical characteristics derived from input data but also for anomaly\ndetection, by separating nominal and anomalous instances based on their\nreconstruction quality. In this paper, we explore the performance of three\nunsupervised deep generative models -- variational autoencoders (VAEs) with\nGaussian, Bernoulli, and Boltzmann priors -- in detecting anomalies in\nflight-operations data of commercial flights consisting of multivariate time\nseries. We devised two VAE models with discrete latent variables (DVAEs), one\nwith a factorized Bernoulli prior and one with a restricted Boltzmann machine\n(RBM) as prior, because of the demand for discrete-variable models in\nmachine-learning applications and because the integration of quantum devices\nbased on two-level quantum systems requires such models. The DVAE with RBM\nprior, using a relatively simple -- and classically or quantum-mechanically\nenhanceable -- sampling technique for the evolution of the RBM's negative\nphase, performed better than the Bernoulli DVAE and on par with the Gaussian\nmodel, which has a continuous latent space. Our studies demonstrate the\ncompetitiveness of a discrete deep generative model with its Gaussian\ncounterpart on anomaly-detection tasks. Moreover, the DVAE model with RBM prior\ncan be easily integrated with quantum sampling by outsourcing its generative\nprocess to measurements of quantum states obtained from a quantum annealer or\ngate-model device.\n","authors":["Thomas Templin","Milad Memarzadeh","Walter Vinci","P. Aaron Lott","Ata Akbari Asanjan","Anthony Alexiades Armenakas","Eleanor Rieffel"],"pdf_url":"https://arxiv.org/pdf/2303.12302v1.pdf","comment":"25 pages, 7 figures, 3 tables, appendix, supplementary material"},{"id":"http://arxiv.org/abs/2303.12298v1","updated":"2023-03-22T04:07:26Z","published":"2023-03-22T04:07:26Z","title":"A General Algorithm for Solving Rank-one Matrix Sensing","summary":"  Matrix sensing has many real-world applications in science and engineering,\nsuch as system control, distance embedding, and computer vision. The goal of\nmatrix sensing is to recover a matrix $A_\\star \\in \\mathbb{R}^{n \\times n}$,\nbased on a sequence of measurements $(u_i,b_i) \\in \\mathbb{R}^{n} \\times\n\\mathbb{R}$ such that $u_i^\\top A_\\star u_i = b_i$. Previous work [ZJD15]\nfocused on the scenario where matrix $A_{\\star}$ has a small rank, e.g.\nrank-$k$. Their analysis heavily relies on the RIP assumption, making it\nunclear how to generalize to high-rank matrices. In this paper, we relax that\nrank-$k$ assumption and solve a much more general matrix sensing problem. Given\nan accuracy parameter $\\delta \\in (0,1)$, we can compute $A \\in \\mathbb{R}^{n\n\\times n}$ in $\\widetilde{O}(m^{3/2} n^2 \\delta^{-1} )$, such that $ |u_i^\\top\nA u_i - b_i| \\leq \\delta$ for all $i \\in [m]$. We design an efficient algorithm\nwith provable convergence guarantees using stochastic gradient descent for this\nproblem.\n","authors":["Lianke Qin","Zhao Song","Ruizhe Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.12298v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12296v1","updated":"2023-03-22T04:06:29Z","published":"2023-03-22T04:06:29Z","title":"Prototype Helps Federated Learning: Towards Faster Convergence","summary":"  Federated learning (FL) is a distributed machine learning technique in which\nmultiple clients cooperate to train a shared model without exchanging their raw\ndata. However, heterogeneity of data distribution among clients usually leads\nto poor model inference. In this paper, a prototype-based federated learning\nframework is proposed, which can achieve better inference performance with only\na few changes to the last global iteration of the typical federated learning\nprocess. In the last iteration, the server aggregates the prototypes\ntransmitted from distributed clients and then sends them back to local clients\nfor their respective model inferences. Experiments on two baseline datasets\nshow that our proposal can achieve higher accuracy (at least 1%) and relatively\nefficient communication than two popular baselines under different\nheterogeneous settings.\n","authors":["Yu Qiao","Seong-Bae Park","Sun Moo Kang","Choong Seon Hong"],"pdf_url":"https://arxiv.org/pdf/2303.12296v1.pdf","comment":"3 pages, 3 figures"},{"id":"http://arxiv.org/abs/2211.13436v2","updated":"2023-03-22T04:05:32Z","published":"2022-11-24T06:36:45Z","title":"Solving Bilevel Knapsack Problem using Graph Neural Networks","summary":"  The Bilevel Optimization Problem is a hierarchical optimization problem with\ntwo agents, a leader and a follower. The leader make their own decisions first,\nand the followers make the best choices accordingly. The leader knows the\ninformation of the followers, and the goal of the problem is to find the\noptimal solution by considering the reactions of the followers from the\nleader's point of view. For the Bilevel Optimization Problem, there are no\ngeneral and efficient algorithms or commercial solvers to get an optimal\nsolution, and it is very difficult to get a good solution even for a simple\nproblem. In this paper, we propose a deep learning approach using Graph Neural\nNetworks to solve the bilevel knapsack problem. We train the model to predict\nthe leader's solution and use it to transform the hierarchical optimization\nproblem into a single-level optimization problem to get the solution. Our model\nfound the feasible solution that was about 500 times faster than the exact\nalgorithm with $1.7\\%$ optimal gap. Also, our model performed well on problems\nof different size from the size it was trained on.\n","authors":["Sunhyeon Kwon","Hwayong Choi","Sungsoo Park"],"pdf_url":"https://arxiv.org/pdf/2211.13436v2.pdf","comment":"27 pages, 2 figures"},{"id":"http://arxiv.org/abs/2303.12291v1","updated":"2023-03-22T03:46:51Z","published":"2023-03-22T03:46:51Z","title":"Fairness Improves Learning from Noisily Labeled Long-Tailed Data","summary":"  Both long-tailed and noisily labeled data frequently appear in real-world\napplications and impose significant challenges for learning. Most prior works\ntreat either problem in an isolated way and do not explicitly consider the\ncoupling effects of the two. Our empirical observation reveals that such\nsolutions fail to consistently improve the learning when the dataset is\nlong-tailed with label noise. Moreover, with the presence of label noise,\nexisting methods do not observe universal improvements across different\nsub-populations; in other words, some sub-populations enjoyed the benefits of\nimproved accuracy at the cost of hurting others. Based on these observations,\nwe introduce the Fairness Regularizer (FR), inspired by regularizing the\nperformance gap between any two sub-populations. We show that the introduced\nfairness regularizer improves the performances of sub-populations on the tail\nand the overall learning performance. Extensive experiments demonstrate the\neffectiveness of the proposed solution when complemented with certain existing\npopular robust or class-balanced methods.\n","authors":["Jiaheng Wei","Zhaowei Zhu","Gang Niu","Tongliang Liu","Sijia Liu","Masashi Sugiyama","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2303.12291v1.pdf","comment":"Paper under review"},{"id":"http://arxiv.org/abs/2303.12289v1","updated":"2023-03-22T03:42:39Z","published":"2023-03-22T03:42:39Z","title":"Adaptive Road Configurations for Improved Autonomous Vehicle-Pedestrian\n  Interactions using Reinforcement Learning","summary":"  The deployment of Autonomous Vehicles (AVs) poses considerable challenges and\nunique opportunities for the design and management of future urban road\ninfrastructure. In light of this disruptive transformation, the Right-Of-Way\n(ROW) composition of road space has the potential to be renewed. Design\napproaches and intelligent control models have been proposed to address this\nproblem, but we lack an operational framework that can dynamically generate ROW\nplans for AVs and pedestrians in response to real-time demand. Based on\nmicroscopic traffic simulation, this study explores Reinforcement Learning (RL)\nmethods for evolving ROW compositions. We implement a centralised paradigm and\na distributive learning paradigm to separately perform the dynamic control on\nseveral road network configurations. Experimental results indicate that the\nalgorithms have the potential to improve traffic flow efficiency and allocate\nmore space for pedestrians. Furthermore, the distributive learning algorithm\noutperforms its centralised counterpart regarding computational cost (49.55\\%),\nbenchmark rewards (25.35\\%), best cumulative rewards (24.58\\%), optimal actions\n(13.49\\%) and rate of convergence. This novel road management technique could\npotentially contribute to the flow-adaptive and active mobility-friendly\nstreets in the AVs era.\n","authors":["Qiming Ye","Yuxiang Feng","Jose Javier Escribano Macias","Marc Stettler","Panagiotis Angeloudis"],"pdf_url":"https://arxiv.org/pdf/2303.12289v1.pdf","comment":"11 pages, 7 figures, Copyright \\c{opyright} 2023, IEEE"},{"id":"http://arxiv.org/abs/2301.08403v2","updated":"2023-03-22T03:41:35Z","published":"2023-01-20T02:35:43Z","title":"Sequence Generation via Subsequence Similarity: Theory and Application\n  to UAV Identification","summary":"  The ability to generate synthetic sequences is crucial for a wide range of\napplications, and recent advances in deep learning architectures and generative\nframeworks have greatly facilitated this process. Particularly, unconditional\none-shot generative models constitute an attractive line of research that\nfocuses on capturing the internal information of a single image or video to\ngenerate samples with similar contents. Since many of those one-shot models are\nshifting toward efficient non-deep and non-adversarial approaches, we examine\nthe versatility of a one-shot generative model for augmenting whole datasets.\nIn this work, we focus on how similarity at the subsequence level affects\nsimilarity at the sequence level, and derive bounds on the optimal transport of\nreal and generated sequences based on that of corresponding subsequences. We\nuse a one-shot generative model to sample from the vicinity of individual\nsequences and generate subsequence-similar ones and demonstrate the improvement\nof this approach by applying it to the problem of Unmanned Aerial Vehicle (UAV)\nidentification using limited radio-frequency (RF) signals. In the context of\nUAV identification, RF fingerprinting is an effective method for distinguishing\nlegitimate devices from malicious ones, but heterogenous environments and\nchannel impairments can impose data scarcity and affect the performance of\nclassification models. By using subsequence similarity to augment sequences of\nRF data with a low ratio (5%-20%) of training dataset, we achieve significant\nimprovements in performance metrics such as accuracy, precision, recall, and F1\nscore.\n","authors":["Amir Kazemi","Salar Basiri","Volodymyr Kindratenko","Srinivasa Salapaka"],"pdf_url":"https://arxiv.org/pdf/2301.08403v2.pdf","comment":"9 pages, 8 figures, 2 tables"},{"id":"http://arxiv.org/abs/2210.13230v3","updated":"2023-03-22T03:33:32Z","published":"2022-10-19T22:07:13Z","title":"An Experimental Study of Dimension Reduction Methods on Machine Learning\n  Algorithms with Applications to Psychometrics","summary":"  Developing interpretable machine learning models has become an increasingly\nimportant issue. One way in which data scientists have been able to develop\ninterpretable models has been to use dimension reduction techniques. In this\npaper, we examine several dimension reduction techniques including two recent\napproaches developed in the network psychometrics literature called exploratory\ngraph analysis (EGA) and unique variable analysis (UVA). We compared EGA and\nUVA with two other dimension reduction techniques common in the machine\nlearning literature (principal component analysis and independent component\nanalysis) as well as no reduction to the variables real data. We show that EGA\nand UVA perform as well as the other reduction techniques or no reduction.\nConsistent with previous literature, we show that dimension reduction can\ndecrease, increase, or provide the same accuracy as no reduction of variables.\nOur tentative results find that dimension reduction tends to lead to better\nperformance when used for classification tasks.\n","authors":["Sean H. Merritt","Alexander P. Christensen"],"pdf_url":"https://arxiv.org/pdf/2210.13230v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12287v1","updated":"2023-03-22T03:28:12Z","published":"2023-03-22T03:28:12Z","title":"Hardness of Independent Learning and Sparse Equilibrium Computation in\n  Markov Games","summary":"  We consider the problem of decentralized multi-agent reinforcement learning\nin Markov games. A fundamental question is whether there exist algorithms that,\nwhen adopted by all agents and run independently in a decentralized fashion,\nlead to no-regret for each player, analogous to celebrated convergence results\nin normal-form games. While recent work has shown that such algorithms exist\nfor restricted settings (notably, when regret is defined with respect to\ndeviations to Markovian policies), the question of whether independent\nno-regret learning can be achieved in the standard Markov game framework was\nopen. We provide a decisive negative resolution this problem, both from a\ncomputational and statistical perspective. We show that:\n  - Under the widely-believed assumption that PPAD-hard problems cannot be\nsolved in polynomial time, there is no polynomial-time algorithm that attains\nno-regret in general-sum Markov games when executed independently by all\nplayers, even when the game is known to the algorithm designer and the number\nof players is a small constant.\n  - When the game is unknown, no algorithm, regardless of computational\nefficiency, can achieve no-regret without observing a number of episodes that\nis exponential in the number of players.\n  Perhaps surprisingly, our lower bounds hold even for seemingly easier setting\nin which all agents are controlled by a a centralized algorithm. They are\nproven via lower bounds for a simpler problem we refer to as SparseCCE, in\nwhich the goal is to compute a coarse correlated equilibrium that is sparse in\nthe sense that it can be represented as a mixture of a small number of product\npolicies. The crux of our approach is a novel application of aggregation\ntechniques from online learning, whereby we show that any algorithm for the\nSparseCCE problem can be used to compute approximate Nash equilibria for\nnon-zero sum normal-form games.\n","authors":["Dylan J. Foster","Noah Golowich","Sham M. Kakade"],"pdf_url":"https://arxiv.org/pdf/2303.12287v1.pdf","comment":"51 pages"},{"id":"http://arxiv.org/abs/2303.12285v1","updated":"2023-03-22T03:24:52Z","published":"2023-03-22T03:24:52Z","title":"Reducing Air Pollution through Machine Learning","summary":"  This paper presents a data-driven approach to mitigate the effects of air\npollution from industrial plants on nearby cities by linking operational\ndecisions with weather conditions. Our method combines predictive and\nprescriptive machine learning models to forecast short-term wind speed and\ndirection and recommend operational decisions to reduce or pause the industrial\nplant's production. We exhibit several trade-offs between reducing\nenvironmental impact and maintaining production activities. The predictive\ncomponent of our framework employs various machine learning models, such as\ngradient-boosted tree-based models and ensemble methods, for time series\nforecasting. The prescriptive component utilizes interpretable optimal policy\ntrees to propose multiple trade-offs, such as reducing dangerous emissions by\n33-47% and unnecessary costs by 40-63%. Our deployed models significantly\nreduced forecasting errors, with a range of 38-52% for less than 12-hour lead\ntime and 14-46% for 12 to 48-hour lead time compared to official weather\nforecasts. We have successfully implemented the predictive component at the OCP\nSafi site, which is Morocco's largest chemical industrial plant, and are\ncurrently in the process of deploying the prescriptive component. Our framework\nenables sustainable industrial development by eliminating the\npollution-industrial activity trade-off through data-driven weather-based\noperational decisions, significantly enhancing factory optimization and\nsustainability. This modernizes factory planning and resource allocation while\nmaintaining environmental compliance. The predictive component has boosted\nproduction efficiency, leading to cost savings and reduced environmental impact\nby minimizing air pollution.\n","authors":["Dimitris Bertsimas","Leonard Boussioux","Cynthia Zeng"],"pdf_url":"https://arxiv.org/pdf/2303.12285v1.pdf","comment":"Submitted to Manufacturing and Service Operations Management"},{"id":"http://arxiv.org/abs/2302.07260v2","updated":"2023-03-22T03:17:48Z","published":"2023-02-14T18:55:21Z","title":"Scalable Bayesian optimization with high-dimensional outputs using\n  randomized prior networks","summary":"  Several fundamental problems in science and engineering consist of global\noptimization tasks involving unknown high-dimensional (black-box) functions\nthat map a set of controllable variables to the outcomes of an expensive\nexperiment. Bayesian Optimization (BO) techniques are known to be effective in\ntackling global optimization problems using a relatively small number objective\nfunction evaluations, but their performance suffers when dealing with\nhigh-dimensional outputs. To overcome the major challenge of dimensionality,\nhere we propose a deep learning framework for BO and sequential decision making\nbased on bootstrapped ensembles of neural architectures with randomized priors.\nUsing appropriate architecture choices, we show that the proposed framework can\napproximate functional relationships between design variables and quantities of\ninterest, even in cases where the latter take values in high-dimensional vector\nspaces or even infinite-dimensional function spaces. In the context of BO, we\naugmented the proposed probabilistic surrogates with re-parameterized Monte\nCarlo approximations of multiple-point (parallel) acquisition functions, as\nwell as methodological extensions for accommodating black-box constraints and\nmulti-fidelity information sources. We test the proposed framework against\nstate-of-the-art methods for BO and demonstrate superior performance across\nseveral challenging tasks with high-dimensional outputs, including a\nconstrained optimization task involving shape optimization of rotor blades in\nturbo-machinery.\n","authors":["Mohamed Aziz Bhouri","Michael Joly","Robert Yu","Soumalya Sarkar","Paris Perdikaris"],"pdf_url":"https://arxiv.org/pdf/2302.07260v2.pdf","comment":"18 pages, 8 figures"},{"id":"http://arxiv.org/abs/2303.12281v1","updated":"2023-03-22T03:15:33Z","published":"2023-03-22T03:15:33Z","title":"Synthetic Health-related Longitudinal Data with Mixed-type Variables\n  Generated using Diffusion Models","summary":"  This paper presents a novel approach to simulating electronic health records\n(EHRs) using diffusion probabilistic models (DPMs). Specifically, we\ndemonstrate the effectiveness of DPMs in synthesising longitudinal EHRs that\ncapture mixed-type variables, including numeric, binary, and categorical\nvariables. To our knowledge, this represents the first use of DPMs for this\npurpose. We compared our DPM-simulated datasets to previous state-of-the-art\nresults based on generative adversarial networks (GANs) for two clinical\napplications: acute hypotension and human immunodeficiency virus (ART for HIV).\nGiven the lack of similar previous studies in DPMs, a core component of our\nwork involves exploring the advantages and caveats of employing DPMs across a\nwide range of aspects. In addition to assessing the realism of the synthetic\ndatasets, we also trained reinforcement learning (RL) agents on the synthetic\ndata to evaluate their utility for supporting the development of downstream\nmachine learning models. Finally, we estimated that our DPM-simulated datasets\nare secure and posed a low patient exposure risk for public access.\n","authors":["Nicholas I-Hsien Kuo","Louisa Jorm","Sebastiano Barbieri"],"pdf_url":"https://arxiv.org/pdf/2303.12281v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07543v3","updated":"2023-03-22T03:11:28Z","published":"2023-03-14T00:13:57Z","title":"WDiscOOD: Out-of-Distribution Detection via Whitened Linear Discriminant\n  Analysis","summary":"  Deep neural networks are susceptible to generating overconfident yet\nerroneous predictions when presented with data beyond known concepts. This\nchallenge underscores the importance of detecting out-of-distribution (OOD)\nsamples in the open world. In this work, we propose a novel feature-space OOD\ndetection score that jointly reasons with both class-specific and\nclass-agnostic information. Specifically, our approach utilizes Whitened Linear\nDiscriminant Analysis to project features into two subspaces - the\ndiscriminative and residual subspaces - in which the ID classes are maximally\nseparated and closely clustered, respectively. The OOD score is then determined\nby combining the deviation from the input data to the ID distribution in both\nsubspaces. The efficacy of our method, named WDiscOOD, is verified on the\nlarge-scale ImageNet-1k benchmark, with six OOD datasets that covers a variety\nof distribution shifts. WDiscOOD demonstrates superior performance on deep\nclassifiers with diverse backbone architectures, including CNN and vision\ntransformer. Furthermore, we also show that our method can more effectively\ndetect novel concepts in representation space trained with contrastive\nobjectives, including supervised contrastive loss and multi-modality\ncontrastive loss.\n","authors":["Yiye Chen","Yunzhi Lin","Ruinian Xu","Patricio A. Vela"],"pdf_url":"https://arxiv.org/pdf/2303.07543v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12277v1","updated":"2023-03-22T03:05:28Z","published":"2023-03-22T03:05:28Z","title":"Stochastic Nonsmooth Convex Optimization with Heavy-Tailed Noises","summary":"  Recently, several studies consider the stochastic optimization problem but in\na heavy-tailed noise regime, i.e., the difference between the stochastic\ngradient and the true gradient is assumed to have a finite $p$-th moment (say\nbeing upper bounded by $\\sigma^{p}$ for some $\\sigma\\geq0$) where $p\\in(1,2]$,\nwhich not only generalizes the traditional finite variance assumption ($p=2$)\nbut also has been observed in practice for several different tasks. Under this\nchallenging assumption, lots of new progress has been made for either convex or\nnonconvex problems, however, most of which only consider smooth objectives. In\ncontrast, people have not fully explored and well understood this problem when\nfunctions are nonsmooth. This paper aims to fill this crucial gap by providing\na comprehensive analysis of stochastic nonsmooth convex optimization with\nheavy-tailed noises. We revisit a simple clipping-based algorithm, whereas,\nwhich is only proved to converge in expectation but under the additional strong\nconvexity assumption. Under appropriate choices of parameters, for both convex\nand strongly convex functions, we not only establish the first high-probability\nrates but also give refined in-expectation bounds compared with existing works.\nRemarkably, all of our results are optimal (or nearly optimal up to logarithmic\nfactors) with respect to the time horizon $T$ even when $T$ is unknown in\nadvance. Additionally, we show how to make the algorithm parameter-free with\nrespect to $\\sigma$, in other words, the algorithm can still guarantee\nconvergence without any prior knowledge of $\\sigma$.\n","authors":["Zijian Liu","Zhengyuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2303.12277v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08996v2","updated":"2023-03-22T02:42:51Z","published":"2023-03-16T00:00:52Z","title":"Learning Spatio-Temporal Aggregations for Large-Scale Capacity Expansion\n  Problems","summary":"  Effective investment planning decisions are crucial to ensure cyber-physical\ninfrastructures satisfy performance requirements over an extended time horizon.\nComputing these decisions often requires solving Capacity Expansion Problems\n(CEPs). In the context of regional-scale energy systems, these problems are\nprohibitively expensive to solve due to large network sizes, heterogeneous node\ncharacteristics, and a large number of operational periods. To maintain\ntractability, traditional approaches aggregate network nodes and/or select a\nset of representative time periods. Often, these reductions do not capture\nsupply-demand variations that crucially impact CEP costs and constraints,\nleading to suboptimal decisions. Here, we propose a novel graph convolutional\nautoencoder approach for spatio-temporal aggregation of a generic CEP with\nheterogeneous nodes (CEPHN). Our architecture leverages graph pooling to\nidentify nodes with similar characteristics and minimizes a multi-objective\nloss function. This loss function is tailored to induce desirable spatial and\ntemporal aggregations with regard to tractability and optimality. In\nparticular, the output of the graph pooling provides a spatial aggregation\nwhile clustering the low-dimensional encoded representations yields a temporal\naggregation. We apply our approach to generation expansion planning of a\ncoupled 88-node power and natural gas system in New England. The resulting\naggregation leads to a simpler CEPHN with 6 nodes and a small set of\nrepresentative days selected from one year. We evaluate aggregation outcomes\nover a range of hyperparameters governing the loss function and compare\nresulting upper bounds on the original problem with those obtained using\nbenchmark methods. We show that our approach provides upper bounds that are 33%\n(resp. 10%) lower those than obtained from benchmark spatial (resp. temporal)\naggregation approaches.\n","authors":["Aron Brenner","Rahman Khorramfar","Saurabh Amin"],"pdf_url":"https://arxiv.org/pdf/2303.08996v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.12037v8","updated":"2023-03-22T02:41:58Z","published":"2022-04-26T02:22:28Z","title":"Causal Reasoning Meets Visual Representation Learning: A Prospective\n  Study","summary":"  Visual representation learning is ubiquitous in various real-world\napplications, including visual comprehension, video understanding, multi-modal\nanalysis, human-computer interaction, and urban computing. Due to the emergence\nof huge amounts of multi-modal heterogeneous spatial/temporal/spatial-temporal\ndata in big data era, the lack of interpretability, robustness, and\nout-of-distribution generalization are becoming the challenges of the existing\nvisual models. The majority of the existing methods tend to fit the original\ndata/variable distributions and ignore the essential causal relations behind\nthe multi-modal knowledge, which lacks unified guidance and analysis about why\nmodern visual representation learning methods easily collapse into data bias\nand have limited generalization and cognitive abilities. Inspired by the strong\ninference ability of human-level agents, recent years have therefore witnessed\ngreat effort in developing causal reasoning paradigms to realize robust\nrepresentation and model learning with good cognitive ability. In this paper,\nwe conduct a comprehensive review of existing causal reasoning methods for\nvisual representation learning, covering fundamental theories, models, and\ndatasets. The limitations of current methods and datasets are also discussed.\nMoreover, we propose some prospective challenges, opportunities, and future\nresearch directions for benchmarking causal reasoning algorithms in visual\nrepresentation learning. This paper aims to provide a comprehensive overview of\nthis emerging field, attract attention, encourage discussions, bring to the\nforefront the urgency of developing novel causal reasoning methods, publicly\navailable benchmarks, and consensus-building standards for reliable visual\nrepresentation learning and related real-world applications more efficiently.\n","authors":["Yang Liu","Yushen Wei","Hong Yan","Guanbin Li","Liang Lin"],"pdf_url":"https://arxiv.org/pdf/2204.12037v8.pdf","comment":"35 pages, 14 figures. This work has been accepted by Machine\n  Intelligence Research. The arxiv version is kept updating by adding more\n  novel methods, datasets and insights. The official video interpretation of\n  this paper can be referred at https://youtu.be/2lfNaTkcTHI"},{"id":"http://arxiv.org/abs/2201.12224v4","updated":"2023-03-22T02:33:47Z","published":"2022-01-28T16:27:21Z","title":"Learning Stationary Nash Equilibrium Policies in $n$-Player Stochastic\n  Games with Independent Chains","summary":"  We consider a subclass of $n$-player stochastic games, in which players have\ntheir own internal state/action spaces while they are coupled through their\npayoff functions. It is assumed that players' internal chains are driven by\nindependent transition probabilities. Moreover, players can receive only\nrealizations of their payoffs, not the actual functions, and cannot observe\neach other's states/actions. For this class of games, we first show that\nfinding a stationary Nash equilibrium (NE) policy without any assumption on the\nreward functions is interactable. However, for general reward functions, we\ndevelop polynomial-time learning algorithms based on dual averaging and dual\nmirror descent, which converge in terms of the averaged Nikaido-Isoda distance\nto the set of $\\epsilon$-NE policies almost surely or in expectation. In\nparticular, under extra assumptions on the reward functions such as social\nconcavity, we derive polynomial upper bounds on the number of iterates to\nachieve an $\\epsilon$-NE policy with high probability. Finally, we evaluate the\neffectiveness of the proposed algorithms in learning $\\epsilon$-NE policies\nusing numerical experiments for energy management in smart grids.\n","authors":["S. Rasoul Etesami"],"pdf_url":"https://arxiv.org/pdf/2201.12224v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12267v1","updated":"2023-03-22T02:28:54Z","published":"2023-03-22T02:28:54Z","title":"AUTO: Adaptive Outlier Optimization for Online Test-Time OOD Detection","summary":"  Out-of-distribution (OOD) detection is a crucial aspect of deploying machine\nlearning models in open-world applications. Empirical evidence suggests that\ntraining with auxiliary outliers substantially improves OOD detection. However,\nsuch outliers typically exhibit a distribution gap compared to the test OOD\ndata and do not cover all possible test OOD scenarios. Additionally,\nincorporating these outliers introduces additional training burdens. In this\npaper, we introduce a novel paradigm called test-time OOD detection, which\nutilizes unlabeled online data directly at test time to improve OOD detection\nperformance. While this paradigm is efficient, it also presents challenges such\nas catastrophic forgetting. To address these challenges, we propose adaptive\noutlier optimization (AUTO), which consists of an in-out-aware filter, an ID\nmemory bank, and a semantically-consistent objective. AUTO adaptively mines\npseudo-ID and pseudo-OOD samples from test data, utilizing them to optimize\nnetworks in real time during inference. Extensive results on CIFAR-10,\nCIFAR-100, and ImageNet benchmarks demonstrate that AUTO significantly enhances\nOOD detection performance.\n","authors":["Puning Yang","Jian Liang","Jie Cao","Ran He"],"pdf_url":"https://arxiv.org/pdf/2303.12267v1.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2303.12261v1","updated":"2023-03-22T02:04:39Z","published":"2023-03-22T02:04:39Z","title":"Challenges and opportunities for machine learning in multiscale\n  computational modeling","summary":"  Many mechanical engineering applications call for multiscale computational\nmodeling and simulation. However, solving for complex multiscale systems\nremains computationally onerous due to the high dimensionality of the solution\nspace. Recently, machine learning (ML) has emerged as a promising solution that\ncan either serve as a surrogate for, accelerate or augment traditional\nnumerical methods. Pioneering work has demonstrated that ML provides solutions\nto governing systems of equations with comparable accuracy to those obtained\nusing direct numerical methods, but with significantly faster computational\nspeed. These high-speed, high-fidelity estimations can facilitate the solving\nof complex multiscale systems by providing a better initial solution to\ntraditional solvers. This paper provides a perspective on the opportunities and\nchallenges of using ML for complex multiscale modeling and simulation. We first\noutline the current state-of-the-art ML approaches for simulating multiscale\nsystems and highlight some of the landmark developments. Next, we discuss\ncurrent challenges for ML in multiscale computational modeling, such as the\ndata and discretization dependence, interpretability, and data sharing and\ncollaborative platform development. Finally, we suggest several potential\nresearch directions for the future.\n","authors":["Phong C. H. Nguyen","Joseph B. Choi","H. S. Udaykumar","Stephen Baek"],"pdf_url":"https://arxiv.org/pdf/2303.12261v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12260v1","updated":"2023-03-22T02:00:51Z","published":"2023-03-22T02:00:51Z","title":"Information-Based Sensor Placement for Data-Driven Estimation of\n  Unsteady Flows","summary":"  Estimation of unsteady flow fields around flight vehicles may improve flow\ninteractions and lead to enhanced vehicle performance. Although flow-field\nrepresentations can be very high-dimensional, their dynamics can have low-order\nrepresentations and may be estimated using a few, appropriately placed\nmeasurements. This paper presents a sensor-selection framework for the intended\napplication of data-driven, flow-field estimation. This framework combines\ndata-driven modeling, steady-state Kalman Filter design, and a sparsification\ntechnique for sequential selection of sensors. This paper also uses the sensor\nselection framework to design sensor arrays that can perform well across a\nvariety of operating conditions. Flow estimation results on numerical data show\nthat the proposed framework produces arrays that are highly effective at\nflow-field estimation for the flow behind and an airfoil at a high angle of\nattack using embedded pressure sensors. Analysis of the flow fields reveals\nthat paths of impinging stagnation points along the airfoil's surface during a\nshedding period of the flow are highly informative locations for placement of\npressure sensors.\n","authors":["John Graff","Albert Medina","Francis Lagor"],"pdf_url":"https://arxiv.org/pdf/2303.12260v1.pdf","comment":"23 pages, 9 figures, submitted to AIAA Journal"},{"id":"http://arxiv.org/abs/2211.04561v2","updated":"2023-03-22T01:53:41Z","published":"2022-11-08T21:16:00Z","title":"A physics-aware deep learning model for energy localization in\n  multiscale shock-to-detonation simulations of heterogeneous energetic\n  materials","summary":"  Predictive simulations of the shock-to-detonation transition (SDT) in\nheterogeneous energetic materials (EM) are vital to the design and control of\ntheir energy release and sensitivity. Due to the complexity of the\nthermo-mechanics of EM during the SDT, both macro-scale response and sub-grid\nmesoscale energy localization must be captured accurately. This work proposes\nan efficient and accurate multiscale framework for SDT simulations of EM. We\nintroduce a new approach for SDT simulation by using deep learning to model the\nmesoscale energy localization of shock-initiated EM microstructures. The\nproposed multiscale modeling framework is divided into two stages. First, a\nphysics-aware recurrent convolutional neural network (PARC) is used to model\nthe mesoscale energy localization of shock-initiated heterogeneous EM\nmicrostructures. PARC is trained using direct numerical simulations (DNS) of\nhotspot ignition and growth within microstructures of pressed HMX material\nsubjected to different input shock strengths. After training, PARC is employed\nto supply hotspot ignition and growth rates for macroscale SDT simulations. We\nshow that PARC can play the role of a surrogate model in a multiscale\nsimulation framework, while drastically reducing the computation cost and\nproviding improved representations of the sub-grid physics. The proposed\nmultiscale modeling approach will provide a new tool for material scientists in\ndesigning high-performance and safer energetic materials.\n","authors":["Phong C. H. Nguyen","Yen-Thi Nguyen","Pradeep K. Seshadri","Joseph B. Choi","H. S. Udaykumar","Stephen Baek"],"pdf_url":"https://arxiv.org/pdf/2211.04561v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12255v1","updated":"2023-03-22T01:45:35Z","published":"2023-03-22T01:45:35Z","title":"Encoding Binary Concepts in the Latent Space of Generative Models for\n  Enhancing Data Representation","summary":"  Binary concepts are empirically used by humans to generalize efficiently. And\nthey are based on Bernoulli distribution which is the building block of\ninformation. These concepts span both low-level and high-level features such as\n\"large vs small\" and \"a neuron is active or inactive\". Binary concepts are\nubiquitous features and can be used to transfer knowledge to improve model\ngeneralization. We propose a novel binarized regularization to facilitate\nlearning of binary concepts to improve the quality of data generation in\nautoencoders. We introduce a binarizing hyperparameter $r$ in data generation\nprocess to disentangle the latent space symmetrically. We demonstrate that this\nmethod can be applied easily to existing variational autoencoder (VAE) variants\nto encourage symmetric disentanglement, improve reconstruction quality, and\nprevent posterior collapse without computation overhead. We also demonstrate\nthat this method can boost existing models to learn more transferable\nrepresentations and generate more representative samples for the input\ndistribution which can alleviate catastrophic forgetting using generative\nreplay under continual learning settings.\n","authors":["Zizhao Hu","Mohammad Rostami"],"pdf_url":"https://arxiv.org/pdf/2303.12255v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11553v2","updated":"2023-03-22T01:13:23Z","published":"2023-03-21T02:44:15Z","title":"Dynamic Vertex Replacement Grammars","summary":"  Context-free graph grammars have shown a remarkable ability to model\nstructures in real-world relational data. However, graph grammars lack the\nability to capture time-changing phenomena since the left-to-right transitions\nof a production rule do not represent temporal change. In the present work, we\ndescribe dynamic vertex-replacement grammars (DyVeRG), which generalize vertex\nreplacement grammars in the time domain by providing a formal framework for\nupdating a learned graph grammar in accordance with modifications to its\nunderlying data. We show that DyVeRG grammars can be learned from, and used to\ngenerate, real-world dynamic graphs faithfully while remaining\nhuman-interpretable. We also demonstrate their ability to forecast by computing\ndyvergence scores, a novel graph similarity measurement exposed by this\nframework.\n","authors":["Daniel Gonzalez Cedre","Justus Isaiah Hibshman","Timothy La Fond","Grant Boquet","Tim Weninger"],"pdf_url":"https://arxiv.org/pdf/2303.11553v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12247v1","updated":"2023-03-22T01:01:14Z","published":"2023-03-22T01:01:14Z","title":"Exploring the Benefits of Visual Prompting in Differential Privacy","summary":"  Visual Prompting (VP) is an emerging and powerful technique that allows\nsample-efficient adaptation to downstream tasks by engineering a well-trained\nfrozen source model. In this work, we explore the benefits of VP in\nconstructing compelling neural network classifiers with differential privacy\n(DP). We explore and integrate VP into canonical DP training methods and\ndemonstrate its simplicity and efficiency. In particular, we discover that VP\nin tandem with PATE, a state-of-the-art DP training method that leverages the\nknowledge transfer from an ensemble of teachers, achieves the state-of-the-art\nprivacy-utility trade-off with minimum expenditure of privacy budget. Moreover,\nwe conduct additional experiments on cross-domain image classification with a\nsufficient domain gap to further unveil the advantage of VP in DP. Lastly, we\nalso conduct extensive ablation studies to validate the effectiveness and\ncontribution of VP under DP consideration.\n","authors":["Yizhe Li","Yu-Lin Tsai","Xuebin Ren","Chia-Mu Yu","Pin-Yu Chen"],"pdf_url":"https://arxiv.org/pdf/2303.12247v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12245v1","updated":"2023-03-22T00:51:11Z","published":"2023-03-22T00:51:11Z","title":"Error Analysis of Physics-Informed Neural Networks for Approximating\n  Dynamic PDEs of Second Order in Time","summary":"  We consider the approximation of a class of dynamic partial differential\nequations (PDE) of second order in time by the physics-informed neural network\n(PINN) approach, and provide an error analysis of PINN for the wave equation,\nthe Sine-Gordon equation and the linear elastodynamic equation. Our analyses\nshow that, with feed-forward neural networks having two hidden layers and the\n$\\tanh$ activation function, the PINN approximation errors for the solution\nfield, its time derivative and its gradient field can be effectively bounded by\nthe training loss and the number of training data points (quadrature points).\nOur analyses further suggest new forms for the training loss function, which\ncontain certain residuals that are crucial to the error estimate but would be\nabsent from the canonical PINN loss formulation. Adopting these new forms for\nthe loss function leads to a variant PINN algorithm. We present ample numerical\nexperiments with the new PINN algorithm for the wave equation, the Sine-Gordon\nequation and the linear elastodynamic equation, which show that the method can\ncapture the solution well.\n","authors":["Yanxia Qian","Yongchao Zhang","Yunqing Huang","Suchuan Dong"],"pdf_url":"https://arxiv.org/pdf/2303.12245v1.pdf","comment":"46 pages, 14 figures, 3 tables"},{"id":"http://arxiv.org/abs/2204.14079v3","updated":"2023-03-22T00:49:31Z","published":"2022-04-29T13:21:14Z","title":"Fix the Noise: Disentangling Source Feature for Transfer Learning of\n  StyleGAN","summary":"  Transfer learning of StyleGAN has recently shown great potential to solve\ndiverse tasks, especially in domain translation. Previous methods utilized a\nsource model by swapping or freezing weights during transfer learning, however,\nthey have limitations on visual quality and controlling source features. In\nother words, they require additional models that are computationally demanding\nand have restricted control steps that prevent a smooth transition. In this\npaper, we propose a new approach to overcome these limitations. Instead of\nswapping or freezing, we introduce a simple feature matching loss to improve\ngeneration quality. In addition, to control the degree of source features, we\ntrain a target model with the proposed strategy, FixNoise, to preserve the\nsource features only in a disentangled subspace of a target feature space.\nOwing to the disentangled feature space, our method can smoothly control the\ndegree of the source features in a single model. Extensive experiments\ndemonstrate that the proposed method can generate more consistent and realistic\nimages than previous works.\n","authors":["Dongyeun Lee","Jae Young Lee","Doyeon Kim","Jaehyun Choi","Junmo Kim"],"pdf_url":"https://arxiv.org/pdf/2204.14079v3.pdf","comment":"Full CVPR 2023 paper is available at arXiv:2303.11545. Best paper of\n  CVPRW AICC 2022 (CVPR 2022 Workshop on AI for Content Creation). The code is\n  available at https://github.com/LeeDongYeun/FixNoise"},{"id":"http://arxiv.org/abs/2212.04493v2","updated":"2023-03-22T00:30:56Z","published":"2022-12-08T18:59:05Z","title":"SDFusion: Multimodal 3D Shape Completion, Reconstruction, and Generation","summary":"  In this work, we present a novel framework built to simplify 3D asset\ngeneration for amateur users. To enable interactive generation, our method\nsupports a variety of input modalities that can be easily provided by a human,\nincluding images, text, partially observed shapes and combinations of these,\nfurther allowing to adjust the strength of each input. At the core of our\napproach is an encoder-decoder, compressing 3D shapes into a compact latent\nrepresentation, upon which a diffusion model is learned. To enable a variety of\nmulti-modal inputs, we employ task-specific encoders with dropout followed by a\ncross-attention mechanism. Due to its flexibility, our model naturally supports\na variety of tasks, outperforming prior works on shape completion, image-based\n3D reconstruction, and text-to-3D. Most interestingly, our model can combine\nall these tasks into one swiss-army-knife tool, enabling the user to perform\nshape generation using incomplete shapes, images, and textual descriptions at\nthe same time, providing the relative weights for each input and facilitating\ninteractivity. Despite our approach being shape-only, we further show an\nefficient method to texture the generated shape using large-scale text-to-image\nmodels.\n","authors":["Yen-Chi Cheng","Hsin-Ying Lee","Sergey Tulyakov","Alexander Schwing","Liangyan Gui"],"pdf_url":"https://arxiv.org/pdf/2212.04493v2.pdf","comment":"In CVPR 2023. Project page and code is available at:\n  https://yccyenchicheng.github.io/SDFusion/. Fix some typos"},{"id":"http://arxiv.org/abs/2208.09723v2","updated":"2023-03-22T00:20:32Z","published":"2022-08-20T17:13:43Z","title":"Matrix Completion with Cross-Concentrated Sampling: Bridging Uniform\n  Sampling and CUR Sampling","summary":"  While uniform sampling has been widely studied in the matrix completion\nliterature, CUR sampling approximates a low-rank matrix via row and column\nsamples. Unfortunately, both sampling models lack flexibility for various\ncircumstances in real-world applications. In this work, we propose a novel and\neasy-to-implement sampling strategy, coined Cross-Concentrated Sampling (CCS).\nBy bridging uniform sampling and CUR sampling, CCS provides extra flexibility\nthat can potentially save sampling costs in applications. In addition, we also\nprovide a sufficient condition for CCS-based matrix completion. Moreover, we\npropose a highly efficient non-convex algorithm, termed Iterative CUR\nCompletion (ICURC), for the proposed CCS model. Numerical experiments verify\nthe empirical advantages of CCS and ICURC against uniform sampling and its\nbaseline algorithms, on both synthetic and real-world datasets.\n","authors":["HanQin Cai","Longxiu Huang","Pengyu Li","Deanna Needell"],"pdf_url":"https://arxiv.org/pdf/2208.09723v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12961v1","updated":"2023-03-22T23:54:14Z","published":"2023-03-22T23:54:14Z","title":"The Shaky Foundations of Clinical Foundation Models: A Survey of Large\n  Language Models and Foundation Models for EMRs","summary":"  The successes of foundation models such as ChatGPT and AlphaFold have spurred\nsignificant interest in building similar models for electronic medical records\n(EMRs) to improve patient care and hospital operations. However, recent hype\nhas obscured critical gaps in our understanding of these models' capabilities.\nWe review over 80 foundation models trained on non-imaging EMR data (i.e.\nclinical text and/or structured data) and create a taxonomy delineating their\narchitectures, training data, and potential use cases. We find that most models\nare trained on small, narrowly-scoped clinical datasets (e.g. MIMIC-III) or\nbroad, public biomedical corpora (e.g. PubMed) and are evaluated on tasks that\ndo not provide meaningful insights on their usefulness to health systems. In\nlight of these findings, we propose an improved evaluation framework for\nmeasuring the benefits of clinical foundation models that is more closely\ngrounded to metrics that matter in healthcare.\n","authors":["Michael Wornow","Yizhe Xu","Rahul Thapa","Birju Patel","Ethan Steinberg","Scott Fleming","Michael A. Pfeffer","Jason Fries","Nigam H. Shah"],"pdf_url":"https://arxiv.org/pdf/2303.12961v1.pdf","comment":"16 pages, 4 figures, submitted to NPJ Digital Medicine"},{"id":"http://arxiv.org/abs/2303.12959v1","updated":"2023-03-22T23:38:10Z","published":"2023-03-22T23:38:10Z","title":"Variantional autoencoder with decremental information bottleneck for\n  disentanglement","summary":"  One major challenge of disentanglement learning with variational autoencoders\nis the trade-off between disentanglement and reconstruction fidelity. Previous\nincremental methods with only on latent space cannot optimize these two targets\nsimultaneously, so they expand the Information Bottleneck while training to\n{optimize from disentanglement to reconstruction. However, a large bottleneck\nwill lose the constraint of disentanglement, causing the information diffusion\nproblem. To tackle this issue, we present a novel decremental variational\nautoencoder with disentanglement-invariant transformations to optimize multiple\nobjectives in different layers, termed DeVAE, for balancing disentanglement and\nreconstruction fidelity by decreasing the information bottleneck of diverse\nlatent spaces gradually. Benefiting from the multiple latent spaces, DeVAE\nallows simultaneous optimization of multiple objectives to optimize\nreconstruction while keeping the constraint of disentanglement, avoiding\ninformation diffusion. DeVAE is also compatible with large models with\nhigh-dimension latent space. Experimental results on dSprites and Shapes3D that\nDeVAE achieves \\fix{R2q6}{a good balance between disentanglement and\nreconstruction.DeVAE shows high tolerant of hyperparameters and on\nhigh-dimensional latent spaces.\n","authors":["Jiantao Wu","Shentong Mo","Muhammad Awais","Sara Atito","Xingshen Zhang","Lin Wang","Xiang Yang"],"pdf_url":"https://arxiv.org/pdf/2303.12959v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12957v1","updated":"2023-03-22T23:37:28Z","published":"2023-03-22T23:37:28Z","title":"Reinforcement Learning with Exogenous States and Rewards","summary":"  Exogenous state variables and rewards can slow reinforcement learning by\ninjecting uncontrolled variation into the reward signal. This paper formalizes\nexogenous state variables and rewards and shows that if the reward function\ndecomposes additively into endogenous and exogenous components, the MDP can be\ndecomposed into an exogenous Markov Reward Process (based on the exogenous\nreward) and an endogenous Markov Decision Process (optimizing the endogenous\nreward). Any optimal policy for the endogenous MDP is also an optimal policy\nfor the original MDP, but because the endogenous reward typically has reduced\nvariance, the endogenous MDP is easier to solve. We study settings where the\ndecomposition of the state space into exogenous and endogenous state spaces is\nnot given but must be discovered. The paper introduces and proves correctness\nof algorithms for discovering the exogenous and endogenous subspaces of the\nstate space when they are mixed through linear combination. These algorithms\ncan be applied during reinforcement learning to discover the exogenous space,\nremove the exogenous reward, and focus reinforcement learning on the endogenous\nMDP. Experiments on a variety of challenging synthetic MDPs show that these\nmethods, applied online, discover large exogenous state spaces and produce\nsubstantial speedups in reinforcement learning.\n","authors":["George Trimponias","Thomas G. Dietterich"],"pdf_url":"https://arxiv.org/pdf/2303.12957v1.pdf","comment":"Greatly extends the initial work reported in 1806.01584"},{"id":"http://arxiv.org/abs/2303.12952v1","updated":"2023-03-22T23:24:47Z","published":"2023-03-22T23:24:47Z","title":"TSI-GAN: Unsupervised Time Series Anomaly Detection using Convolutional\n  Cycle-Consistent Generative Adversarial Networks","summary":"  Anomaly detection is widely used in network intrusion detection, autonomous\ndriving, medical diagnosis, credit card frauds, etc. However, several key\nchallenges remain open, such as lack of ground truth labels, presence of\ncomplex temporal patterns, and generalizing over different datasets. This paper\nproposes TSI-GAN, an unsupervised anomaly detection model for time-series that\ncan learn complex temporal patterns automatically and generalize well, i.e., no\nneed for choosing dataset-specific parameters, making statistical assumptions\nabout underlying data, or changing model architectures. To achieve these goals,\nwe convert each input time-series into a sequence of 2D images using two\nencoding techniques with the intent of capturing temporal patterns and various\ntypes of deviance. Moreover, we design a reconstructive GAN that uses\nconvolutional layers in an encoder-decoder network and employs\ncycle-consistency loss during training to ensure that inverse mappings are\naccurate as well. In addition, we also instrument a Hodrick-Prescott filter in\npost-processing to mitigate false positives. We evaluate TSI-GAN using 250\nwell-curated and harder-than-usual datasets and compare with 8 state-of-the-art\nbaseline methods. The results demonstrate the superiority of TSI-GAN to all the\nbaselines, offering an overall performance improvement of 13% and 31% over the\nsecond-best performer MERLIN and the third-best performer LSTM-AE,\nrespectively.\n","authors":["Shyam Sundar Saravanan","Tie Luo","Mao Van Ngo"],"pdf_url":"https://arxiv.org/pdf/2303.12952v1.pdf","comment":"To appear in the Proceedings of PAKDD 2023 (27th Pacific-Asia\n  Conference on Knowledge Discovery and Data Mining)"},{"id":"http://arxiv.org/abs/2303.02829v2","updated":"2023-03-22T22:51:36Z","published":"2023-03-06T01:46:51Z","title":"Attribution-Scores and Causal Counterfactuals as Explanations in\n  Artificial Intelligence","summary":"  In this expository article we highlight the relevance of explanations for\nartificial intelligence, in general, and for the newer developments in {\\em\nexplainable AI}, referring to origins and connections of and among different\napproaches. We describe in simple terms, explanations in data management and\nmachine learning that are based on attribution-scores, and counterfactuals as\nfound in the area of causality. We elaborate on the importance of logical\nreasoning when dealing with counterfactuals, and their use for score\ncomputation.\n","authors":["Leopoldo Bertossi"],"pdf_url":"https://arxiv.org/pdf/2303.02829v2.pdf","comment":"Submitted as chapter contribution. In this version some additional\n  comments were added, and some wrong equation references corrected"},{"id":"http://arxiv.org/abs/2110.15332v2","updated":"2023-03-22T22:24:18Z","published":"2021-10-28T17:46:14Z","title":"Proximal Reinforcement Learning: Efficient Off-Policy Evaluation in\n  Partially Observed Markov Decision Processes","summary":"  In applications of offline reinforcement learning to observational data, such\nas in healthcare or education, a general concern is that observed actions might\nbe affected by unobserved factors, inducing confounding and biasing estimates\nderived under the assumption of a perfect Markov decision process (MDP) model.\nHere we tackle this by considering off-policy evaluation in a partially\nobserved MDP (POMDP). Specifically, we consider estimating the value of a given\ntarget policy in a POMDP given trajectories with only partial state\nobservations generated by a different and unknown policy that may depend on the\nunobserved state. We tackle two questions: what conditions allow us to identify\nthe target policy value from the observed data and, given identification, how\nto best estimate it. To answer these, we extend the framework of proximal\ncausal inference to our POMDP setting, providing a variety of settings where\nidentification is made possible by the existence of so-called bridge functions.\nWe then show how to construct semiparametrically efficient estimators in these\nsettings. We term the resulting framework proximal reinforcement learning\n(PRL). We demonstrate the benefits of PRL in an extensive simulation study and\non the problem of sepsis management.\n","authors":["Andrew Bennett","Nathan Kallus"],"pdf_url":"https://arxiv.org/pdf/2110.15332v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2111.03837v2","updated":"2023-03-22T22:18:01Z","published":"2021-11-06T09:04:16Z","title":"Focusing on Potential Named Entities During Active Label Acquisition","summary":"  Named entity recognition (NER) aims to identify mentions of named entities in\nan unstructured text and classify them into predefined named entity classes.\nWhile deep learning-based pre-trained language models help to achieve good\npredictive performances in NER, many domain-specific NER applications still\ncall for a substantial amount of labeled data. Active learning (AL), a general\nframework for the label acquisition problem, has been used for NER tasks to\nminimize the annotation cost without sacrificing model performance. However,\nthe heavily imbalanced class distribution of tokens introduces challenges in\ndesigning effective AL querying methods for NER. We propose several AL sentence\nquery evaluation functions that pay more attention to potential positive\ntokens, and evaluate these proposed functions with both sentence-based and\ntoken-based cost evaluation strategies. We also propose a better data-driven\nnormalization approach to penalize sentences that are too long or too short.\nOur experiments on three datasets from different domains reveal that the\nproposed approach reduces the number of annotated tokens while achieving better\nor comparable prediction performance with conventional methods.\n","authors":["Ali Osman Berk Sapci","Oznur Tastan","Reyyan Yeniterzi"],"pdf_url":"https://arxiv.org/pdf/2111.03837v2.pdf","comment":"20 pages, 8 figures"},{"id":"http://arxiv.org/abs/2303.12928v1","updated":"2023-03-22T21:55:30Z","published":"2023-03-22T21:55:30Z","title":"Leveraging Multi-time Hamilton-Jacobi PDEs for Certain Scientific\n  Machine Learning Problems","summary":"  Hamilton-Jacobi partial differential equations (HJ PDEs) have deep\nconnections with a wide range of fields, including optimal control,\ndifferential games, and imaging sciences. By considering the time variable to\nbe a higher dimensional quantity, HJ PDEs can be extended to the multi-time\ncase. In this paper, we establish a novel theoretical connection between\nspecific optimization problems arising in machine learning and the multi-time\nHopf formula, which corresponds to a representation of the solution to certain\nmulti-time HJ PDEs. Through this connection, we increase the interpretability\nof the training process of certain machine learning applications by showing\nthat when we solve these learning problems, we also solve a multi-time HJ PDE\nand, by extension, its corresponding optimal control problem. As a first\nexploration of this connection, we develop the relation between the regularized\nlinear regression problem and the Linear Quadratic Regulator (LQR). We then\nleverage our theoretical connection to adapt standard LQR solvers (namely,\nthose based on the Riccati ordinary differential equations) to design new\ntraining approaches for machine learning. Finally, we provide some numerical\nexamples that demonstrate the versatility and possible computational advantages\nof our Riccati-based approach in the context of continual learning,\npost-training calibration, transfer learning, and sparse dynamics\nidentification.\n","authors":["Paula Chen","Tingwei Meng","Zongren Zou","Jérôme Darbon","George Em Karniadakis"],"pdf_url":"https://arxiv.org/pdf/2303.12928v1.pdf","comment":"26 pages in total, 23 pages for the main text, 9 figures"},{"id":"http://arxiv.org/abs/2301.02232v2","updated":"2023-03-22T21:53:02Z","published":"2023-01-05T18:57:12Z","title":"CA$^2$T-Net: Category-Agnostic 3D Articulation Transfer from Single\n  Image","summary":"  We present a neural network approach to transfer the motion from a single\nimage of an articulated object to a rest-state (i.e., unarticulated) 3D model.\nOur network learns to predict the object's pose, part segmentation, and\ncorresponding motion parameters to reproduce the articulation shown in the\ninput image. The network is composed of three distinct branches that take a\nshared joint image-shape embedding and is trained end-to-end. Unlike previous\nmethods, our approach is independent of the topology of the object and can work\nwith objects from arbitrary categories. Our method, trained with only synthetic\ndata, can be used to automatically animate a mesh, infer motion from real\nimages, and transfer articulation to functionally similar but geometrically\ndistinct 3D models at test time.\n","authors":["Jasmine Collins","Anqi Liang","Jitendra Malik","Hao Zhang","Frédéric Devernay"],"pdf_url":"https://arxiv.org/pdf/2301.02232v2.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2012.09422v4","updated":"2023-03-22T21:49:23Z","published":"2020-12-17T07:21:06Z","title":"The Variational Method of Moments","summary":"  The conditional moment problem is a powerful formulation for describing\nstructural causal parameters in terms of observables, a prominent example being\ninstrumental variable regression. A standard approach reduces the problem to a\nfinite set of marginal moment conditions and applies the optimally weighted\ngeneralized method of moments (OWGMM), but this requires we know a finite set\nof identifying moments, can still be inefficient even if identifying, or can be\ntheoretically efficient but practically unwieldy if we use a growing sieve of\nmoment conditions. Motivated by a variational minimax reformulation of OWGMM,\nwe define a very general class of estimators for the conditional moment\nproblem, which we term the variational method of moments (VMM) and which\nnaturally enables controlling infinitely-many moments. We provide a detailed\ntheoretical analysis of multiple VMM estimators, including ones based on kernel\nmethods and neural nets, and provide conditions under which these are\nconsistent, asymptotically normal, and semiparametrically efficient in the full\nconditional moment model. We additionally provide algorithms for valid\nstatistical inference based on the same kind of variational reformulations,\nboth for kernel- and neural-net-based varieties. Finally, we demonstrate the\nstrong performance of our proposed estimation and inference algorithms in a\ndetailed series of synthetic experiments.\n","authors":["Andrew Bennett","Nathan Kallus"],"pdf_url":"https://arxiv.org/pdf/2012.09422v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12922v1","updated":"2023-03-22T21:36:56Z","published":"2023-03-22T21:36:56Z","title":"Revisiting the Fragility of Influence Functions","summary":"  In the last few years, many works have tried to explain the predictions of\ndeep learning models. Few methods, however, have been proposed to verify the\naccuracy or faithfulness of these explanations. Recently, influence functions,\nwhich is a method that approximates the effect that leave-one-out training has\non the loss function, has been shown to be fragile. The proposed reason for\ntheir fragility remains unclear. Although previous work suggests the use of\nregularization to increase robustness, this does not hold in all cases. In this\nwork, we seek to investigate the experiments performed in the prior work in an\neffort to understand the underlying mechanisms of influence function fragility.\nFirst, we verify influence functions using procedures from the literature under\nconditions where the convexity assumptions of influence functions are met.\nThen, we relax these assumptions and study the effects of non-convexity by\nusing deeper models and more complex datasets. Here, we analyze the key metrics\nand procedures that are used to validate influence functions. Our results\nindicate that the validation procedures may cause the observed fragility.\n","authors":["Jacob R. Epifano","Ravi P. Ramachandran","Aaron J. Masino","Ghulam Rasool"],"pdf_url":"https://arxiv.org/pdf/2303.12922v1.pdf","comment":"11 pages, 5 figures, accepted to Neural Networks"},{"id":"http://arxiv.org/abs/2303.12921v1","updated":"2023-03-22T21:35:50Z","published":"2023-03-22T21:35:50Z","title":"Stability is Stable: Connections between Replicability, Privacy, and\n  Adaptive Generalization","summary":"  The notion of replicable algorithms was introduced in Impagliazzo et al.\n[STOC '22] to describe randomized algorithms that are stable under the\nresampling of their inputs. More precisely, a replicable algorithm gives the\nsame output with high probability when its randomness is fixed and it is run on\na new i.i.d. sample drawn from the same distribution. Using replicable\nalgorithms for data analysis can facilitate the verification of published\nresults by ensuring that the results of an analysis will be the same with high\nprobability, even when that analysis is performed on a new data set.\n  In this work, we establish new connections and separations between\nreplicability and standard notions of algorithmic stability. In particular, we\ngive sample-efficient algorithmic reductions between perfect generalization,\napproximate differential privacy, and replicability for a broad class of\nstatistical problems. Conversely, we show any such equivalence must break down\ncomputationally: there exist statistical problems that are easy under\ndifferential privacy, but that cannot be solved replicably without breaking\npublic-key cryptography. Furthermore, these results are tight: our reductions\nare statistically optimal, and we show that any computational separation\nbetween DP and replicability must imply the existence of one-way functions.\n  Our statistical reductions give a new algorithmic framework for translating\nbetween notions of stability, which we instantiate to answer several open\nquestions in replicability and privacy. This includes giving sample-efficient\nreplicable algorithms for various PAC learning, distribution estimation, and\ndistribution testing problems, algorithmic amplification of $\\delta$ in\napproximate DP, conversions from item-level to user-level privacy, and the\nexistence of private agnostic-to-realizable learning reductions under\nstructured distributions.\n","authors":["Mark Bun","Marco Gaboardi","Max Hopkins","Russell Impagliazzo","Rex Lei","Toniann Pitassi","Jessica Sorrell","Satchit Sivakumar"],"pdf_url":"https://arxiv.org/pdf/2303.12921v1.pdf","comment":"STOC 2023"},{"id":"http://arxiv.org/abs/2303.12915v1","updated":"2023-03-22T21:09:54Z","published":"2023-03-22T21:09:54Z","title":"Self-distillation for surgical action recognition","summary":"  Surgical scene understanding is a key prerequisite for contextaware decision\nsupport in the operating room. While deep learning-based approaches have\nalready reached or even surpassed human performance in various fields, the task\nof surgical action recognition remains a major challenge. With this\ncontribution, we are the first to investigate the concept of self-distillation\nas a means of addressing class imbalance and potential label ambiguity in\nsurgical video analysis. Our proposed method is a heterogeneous ensemble of\nthree models that use Swin Transfomers as backbone and the concepts of\nself-distillation and multi-task learning as core design choices. According to\nablation studies performed with the CholecT45 challenge data via\ncross-validation, the biggest performance boost is achieved by the usage of\nsoft labels obtained by self-distillation. External validation of our method on\nan independent test set was achieved by providing a Docker container of our\ninference model to the challenge organizers. According to their analysis, our\nmethod outperforms all other solutions submitted to the latest challenge in the\nfield. Our approach thus shows the potential of self-distillation for becoming\nan important tool in medical image analysis applications.\n","authors":["Amine Yamlahi","Thuy Nuong Tran","Patrick Godau","Melanie Schellenberg","Dominik Michael","Finn-Henri Smidt","Jan-Hinrich Noelke","Tim Adler","Minu Dietlinde Tizabi","Chinedu Nwoye","Nicolas Padoy","Lena Maier-Hein"],"pdf_url":"https://arxiv.org/pdf/2303.12915v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12914v1","updated":"2023-03-22T21:09:49Z","published":"2023-03-22T21:09:49Z","title":"TRON: Transformer Neural Network Acceleration with Non-Coherent Silicon\n  Photonics","summary":"  Transformer neural networks are rapidly being integrated into\nstate-of-the-art solutions for natural language processing (NLP) and computer\nvision. However, the complex structure of these models creates challenges for\naccelerating their execution on conventional electronic platforms. We propose\nthe first silicon photonic hardware neural network accelerator called TRON for\ntransformer-based models such as BERT, and Vision Transformers. Our analysis\ndemonstrates that TRON exhibits at least 14x better throughput and 8x better\nenergy efficiency, in comparison to state-of-the-art transformer accelerators.\n","authors":["Salma Afifi","Febin Sunny","Mahdi Nikdast","Sudeep Pasricha"],"pdf_url":"https://arxiv.org/pdf/2303.12914v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12910v1","updated":"2023-03-22T21:03:40Z","published":"2023-03-22T21:03:40Z","title":"Cross-Layer Design for AI Acceleration with Non-Coherent Optical\n  Computing","summary":"  Emerging AI applications such as ChatGPT, graph convolutional networks, and\nother deep neural networks require massive computational resources for training\nand inference. Contemporary computing platforms such as CPUs, GPUs, and TPUs\nare struggling to keep up with the demands of these AI applications.\nNon-coherent optical computing represents a promising approach for light-speed\nacceleration of AI workloads. In this paper, we show how cross-layer design can\novercome challenges in non-coherent optical computing platforms. We describe\napproaches for optical device engineering, tuning circuit enhancements, and\narchitectural innovations to adapt optical computing to a variety of AI\nworkloads. We also discuss techniques for hardware/software co-design that can\nintelligently map and adapt AI software to improve its performance on\nnon-coherent optical computing platforms.\n","authors":["Febin Sunny","Mahdi Nikdast","Sudeep Pasricha"],"pdf_url":"https://arxiv.org/pdf/2303.12910v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.00870v3","updated":"2023-03-22T20:56:42Z","published":"2022-03-02T04:44:52Z","title":"Faith-Shap: The Faithful Shapley Interaction Index","summary":"  Shapley values, which were originally designed to assign attributions to\nindividual players in coalition games, have become a commonly used approach in\nexplainable machine learning to provide attributions to input features for\nblack-box machine learning models. A key attraction of Shapley values is that\nthey uniquely satisfy a very natural set of axiomatic properties. However,\nextending the Shapley value to assigning attributions to interactions rather\nthan individual players, an interaction index, is non-trivial: as the natural\nset of axioms for the original Shapley values, extended to the context of\ninteractions, no longer specify a unique interaction index. Many proposals thus\nintroduce additional less ''natural'' axioms, while sacrificing the key axiom\nof efficiency, in order to obtain unique interaction indices. In this work,\nrather than introduce additional conflicting axioms, we adopt the viewpoint of\nShapley values as coefficients of the most faithful linear approximation to the\npseudo-Boolean coalition game value function. By extending linear to\n$\\ell$-order polynomial approximations, we can then define the general family\nof faithful interaction indices. We show that by additionally requiring the\nfaithful interaction indices to satisfy interaction-extensions of the standard\nindividual Shapley axioms (dummy, symmetry, linearity, and efficiency), we\nobtain a unique Faithful Shapley Interaction index, which we denote Faith-Shap,\nas a natural generalization of the Shapley value to interactions. We then\nprovide some illustrative contrasts of Faith-Shap with previously proposed\ninteraction indices, and further investigate some of its interesting algebraic\nproperties. We further show the computational efficiency of computing\nFaith-Shap, together with some additional qualitative insights, via some\nillustrative experiments.\n","authors":["Che-Ping Tsai","Chih-Kuan Yeh","Pradeep Ravikumar"],"pdf_url":"https://arxiv.org/pdf/2203.00870v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12891v1","updated":"2023-03-22T20:09:31Z","published":"2023-03-22T20:09:31Z","title":"Feature Reduction Method Comparison Towards Explainability and\n  Efficiency in Cybersecurity Intrusion Detection Systems","summary":"  In the realm of cybersecurity, intrusion detection systems (IDS) detect and\nprevent attacks based on collected computer and network data. In recent\nresearch, IDS models have been constructed using machine learning (ML) and deep\nlearning (DL) methods such as Random Forest (RF) and deep neural networks\n(DNN). Feature selection (FS) can be used to construct faster, more\ninterpretable, and more accurate models. We look at three different FS\ntechniques; RF information gain (RF-IG), correlation feature selection using\nthe Bat Algorithm (CFS-BA), and CFS using the Aquila Optimizer (CFS-AO). Our\nresults show CFS-BA to be the most efficient of the FS methods, building in 55%\nof the time of the best RF-IG model while achieving 99.99% of its accuracy.\nThis reinforces prior contributions attesting to CFS-BA's accuracy while\nbuilding upon the relationship between subset size, CFS score, and RF-IG score\nin final results.\n","authors":["Adam M. Lehavi","Seongtae Kim"],"pdf_url":"https://arxiv.org/pdf/2303.12891v1.pdf","comment":"Published in 2022 21st IEEE International Conference on Machine\n  Learning and Applications. 8 pages. 5 figures"},{"id":"http://arxiv.org/abs/2303.12888v1","updated":"2023-03-22T20:05:22Z","published":"2023-03-22T20:05:22Z","title":"A dynamic risk score for early prediction of cardiogenic shock using\n  machine learning","summary":"  Myocardial infarction and heart failure are major cardiovascular diseases\nthat affect millions of people in the US. The morbidity and mortality are\nhighest among patients who develop cardiogenic shock. Early recognition of\ncardiogenic shock is critical. Prompt implementation of treatment measures can\nprevent the deleterious spiral of ischemia, low blood pressure, and reduced\ncardiac output due to cardiogenic shock. However, early identification of\ncardiogenic shock has been challenging due to human providers' inability to\nprocess the enormous amount of data in the cardiac intensive care unit (ICU)\nand lack of an effective risk stratification tool. We developed a deep\nlearning-based risk stratification tool, called CShock, for patients admitted\ninto the cardiac ICU with acute decompensated heart failure and/or myocardial\ninfarction to predict onset of cardiogenic shock. To develop and validate\nCShock, we annotated cardiac ICU datasets with physician adjudicated outcomes.\nCShock achieved an area under the receiver operator characteristic curve\n(AUROC) of 0.820, which substantially outperformed CardShock (AUROC 0.519), a\nwell-established risk score for cardiogenic shock prognosis. CShock was\nexternally validated in an independent patient cohort and achieved an AUROC of\n0.800, demonstrating its generalizability in other cardiac ICUs.\n","authors":["Yuxuan Hu","Albert Lui","Mark Goldstein","Mukund Sudarshan","Andrea Tinsay","Cindy Tsui","Samuel Maidman","John Medamana","Neil Jethani","Aaalad Puli","Vuthy Nguy","Yindalon Aphinyanaphongs","Nicholas Kiefer","Nathaniel Smilowitz","James Horowitz","Tania Ahuja","Glenn Fishman","Judith Hochman","Stuart Katz","Samuel Bernard","Rajesh Ranganath"],"pdf_url":"https://arxiv.org/pdf/2303.12888v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12878v1","updated":"2023-03-22T19:36:56Z","published":"2023-03-22T19:36:56Z","title":"Robust Consensus in Ranking Data Analysis: Definitions, Properties and\n  Computational Issues","summary":"  As the issue of robustness in AI systems becomes vital, statistical learning\ntechniques that are reliable even in presence of partly contaminated data have\nto be developed. Preference data, in the form of (complete) rankings in the\nsimplest situations, are no exception and the demand for appropriate concepts\nand tools is all the more pressing given that technologies fed by or producing\nthis type of data (e.g. search engines, recommending systems) are now massively\ndeployed. However, the lack of vector space structure for the set of rankings\n(i.e. the symmetric group $\\mathfrak{S}_n$) and the complex nature of\nstatistics considered in ranking data analysis make the formulation of\nrobustness objectives in this domain challenging. In this paper, we introduce\nnotions of robustness, together with dedicated statistical methods, for\nConsensus Ranking the flagship problem in ranking data analysis, aiming at\nsummarizing a probability distribution on $\\mathfrak{S}_n$ by a median ranking.\nPrecisely, we propose specific extensions of the popular concept of breakdown\npoint, tailored to consensus ranking, and address the related computational\nissues. Beyond the theoretical contributions, the relevance of the approach\nproposed is supported by an experimental study.\n","authors":["Morgane Goibert","Clément Calauzènes","Ekhine Irurozki","Stéphan Clémençon"],"pdf_url":"https://arxiv.org/pdf/2303.12878v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12872v1","updated":"2023-03-22T19:17:57Z","published":"2023-03-22T19:17:57Z","title":"Human Uncertainty in Concept-Based AI Systems","summary":"  Placing a human in the loop may abate the risks of deploying AI systems in\nsafety-critical settings (e.g., a clinician working with a medical AI system).\nHowever, mitigating risks arising from human error and uncertainty within such\nhuman-AI interactions is an important and understudied issue. In this work, we\nstudy human uncertainty in the context of concept-based models, a family of AI\nsystems that enable human feedback via concept interventions where an expert\nintervenes on human-interpretable concepts relevant to the task. Prior work in\nthis space often assumes that humans are oracles who are always certain and\ncorrect. Yet, real-world decision-making by humans is prone to occasional\nmistakes and uncertainty. We study how existing concept-based models deal with\nuncertain interventions from humans using two novel datasets: UMNIST, a visual\ndataset with controlled simulated uncertainty based on the MNIST dataset, and\nCUB-S, a relabeling of the popular CUB concept dataset with rich,\ndensely-annotated soft labels from humans. We show that training with uncertain\nconcept labels may help mitigate weaknesses of concept-based systems when\nhandling uncertain interventions. These results allow us to identify several\nopen challenges, which we argue can be tackled through future multidisciplinary\nresearch on building interactive uncertainty-aware systems. To facilitate\nfurther research, we release a new elicitation platform, UElic, to collect\nuncertain feedback from humans in collaborative prediction tasks.\n","authors":["Katherine M. Collins","Matthew Barker","Mateo Espinosa Zarlenga","Naveen Raman","Umang Bhatt","Mateja Jamnik","Ilia Sucholutsky","Adrian Weller","Krishnamurthy Dvijotham"],"pdf_url":"https://arxiv.org/pdf/2303.12872v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12865v1","updated":"2023-03-22T18:59:48Z","published":"2023-03-22T18:59:48Z","title":"NeRF-GAN Distillation for Efficient 3D-Aware Generation with\n  Convolutions","summary":"  Pose-conditioned convolutional generative models struggle with high-quality\n3D-consistent image generation from single-view datasets, due to their lack of\nsufficient 3D priors. Recently, the integration of Neural Radiance Fields\n(NeRFs) and generative models, such as Generative Adversarial Networks (GANs),\nhas transformed 3D-aware generation from single-view images. NeRF-GANs exploit\nthe strong inductive bias of 3D neural representations and volumetric rendering\nat the cost of higher computational complexity. This study aims at revisiting\npose-conditioned 2D GANs for efficient 3D-aware generation at inference time by\ndistilling 3D knowledge from pretrained NeRF-GANS. We propose a simple and\neffective method, based on re-using the well-disentangled latent space of a\npre-trained NeRF-GAN in a pose-conditioned convolutional network to directly\ngenerate 3D-consistent images corresponding to the underlying 3D\nrepresentations. Experiments on several datasets demonstrate that the proposed\nmethod obtains results comparable with volumetric rendering in terms of quality\nand 3D consistency while benefiting from the superior computational advantage\nof convolutional networks. The code will be available at:\nhttps://github.com/mshahbazi72/NeRF-GAN-Distillation\n","authors":["Mohamad Shahbazi","Evangelos Ntavelis","Alessio Tonioni","Edo Collins","Danda Pani Paudel","Martin Danelljan","Luc Van Gool"],"pdf_url":"https://arxiv.org/pdf/2303.12865v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12861v1","updated":"2023-03-22T18:55:43Z","published":"2023-03-22T18:55:43Z","title":"Cube-Based 3D Denoising Diffusion Probabilistic Model for Cone Beam\n  Computed Tomography Reconstruction with Incomplete Data","summary":"  Deep learning (DL) has been extensively researched in the field of computed\ntomography (CT) reconstruction with incomplete data, particularly in\nsparse-view CT reconstruction. However, applying DL to sparse-view cone beam CT\n(CBCT) remains challenging. Many models learn the mapping from sparse-view CT\nimages to ground truth but struggle to achieve satisfactory performance in\nterms of global artifact removal. Incorporating sinogram data and utilizing\ndual-domain information can enhance anti-artifact performance, but this\nrequires storing the entire sinogram in memory. This presents a memory issue\nfor high-resolution CBCT sinograms, limiting further research and application.\nIn this paper, we propose a cube-based 3D denoising diffusion probabilistic\nmodel (DDPM) for CBCT reconstruction using down-sampled data. A DDPM network,\ntrained on cubes extracted from paired fully sampled sinograms and down-sampled\nsinograms, is employed to inpaint down-sampled sinograms. Our method divides\nthe entire sinogram into overlapping cubes and processes these cubes in\nparallel using multiple GPUs, overcoming memory limitations. Experimental\nresults demonstrate that our approach effectively suppresses few-view artifacts\nwhile preserving textural details faithfully.\n","authors":["Wenjun Xia","Chuang Niu","Wenxiang Cong","Ge Wang"],"pdf_url":"https://arxiv.org/pdf/2303.12861v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2010.01400v2","updated":"2023-03-22T18:49:54Z","published":"2020-10-03T17:48:57Z","title":"Joint Inference of Diffusion and Structure in Partially Observed Social\n  Networks Using Coupled Matrix Factorization","summary":"  Access to complete data in large-scale networks is often infeasible.\nTherefore, the problem of missing data is a crucial and unavoidable issue in\nthe analysis and modeling of real-world social networks. However, most of the\nresearch on different aspects of social networks does not consider this\nlimitation. One effective way to solve this problem is to recover the missing\ndata as a pre-processing step. In this paper, a model is learned from partially\nobserved data to infer unobserved diffusion and structure networks. To jointly\ndiscover omitted diffusion activities and hidden network structures, we develop\na probabilistic generative model called \"DiffStru.\" The interrelations among\nlinks of nodes and cascade processes are utilized in the proposed method via\nlearning coupled with low-dimensional latent factors. Besides inferring unseen\ndata, latent factors such as community detection may also aid in network\nclassification problems. We tested different missing data scenarios on\nsimulated independent cascades over LFR networks and real datasets, including\nTwitter and Memtracker. Experiments on these synthetic and real-world datasets\nshow that the proposed method successfully detects invisible social behaviors,\npredicts links, and identifies latent features.\n","authors":["Maryam Ramezani","Aryan Ahadinia","Amirmohammad Ziaei","Hamid R. Rabiee"],"pdf_url":"https://arxiv.org/pdf/2010.01400v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.14868v2","updated":"2023-03-22T18:37:20Z","published":"2022-10-26T17:17:06Z","title":"Multi-lingual Evaluation of Code Generation Models","summary":"  We present new benchmarks on evaluation code generation models: MBXP and\nMultilingual HumanEval, and MathQA-X. These datasets cover over 10 programming\nlanguages and are generated using a scalable conversion framework that\ntranspiles prompts and test cases from the original Python datasets into the\ncorresponding data in the target language. Using these benchmarks, we are able\nto assess the performance of code generation models in a multi-lingual fashion,\nand discovered generalization ability of language models on out-of-domain\nlanguages, advantages of multi-lingual models over mono-lingual, the ability of\nfew-shot prompting to teach the model new languages, and zero-shot translation\nabilities even on mono-lingual settings. Furthermore, we use our code\ngeneration model to perform large-scale bootstrapping to obtain synthetic\ncanonical solutions in several languages, which can be used for other\ncode-related evaluations such as code insertion, robustness, or summarization\ntasks. Overall, our benchmarks represents a significant step towards a deeper\nunderstanding of language models' code generation abilities. We publicly\nrelease our code and datasets at https://github.com/amazon-research/mxeval.\n","authors":["Ben Athiwaratkun","Sanjay Krishna Gouda","Zijian Wang","Xiaopeng Li","Yuchen Tian","Ming Tan","Wasi Uddin Ahmad","Shiqi Wang","Qing Sun","Mingyue Shang","Sujan Kumar Gonugondla","Hantian Ding","Varun Kumar","Nathan Fulton","Arash Farahani","Siddhartha Jain","Robert Giaquinto","Haifeng Qian","Murali Krishna Ramanathan","Ramesh Nallapati","Baishakhi Ray","Parminder Bhatia","Sudipta Sengupta","Dan Roth","Bing Xiang"],"pdf_url":"https://arxiv.org/pdf/2210.14868v2.pdf","comment":"Code and data release: https://github.com/amazon-research/mxeval"},{"id":"http://arxiv.org/abs/2303.12856v1","updated":"2023-03-22T18:31:15Z","published":"2023-03-22T18:31:15Z","title":"Anti-symmetric Barron functions and their approximation with sums of\n  determinants","summary":"  A fundamental problem in quantum physics is to encode functions that are\ncompletely anti-symmetric under permutations of identical particles. The Barron\nspace consists of high-dimensional functions that can be parameterized by\ninfinite neural networks with one hidden layer. By explicitly encoding the\nanti-symmetric structure, we prove that the anti-symmetric functions which\nbelong to the Barron space can be efficiently approximated with sums of\ndeterminants. This yields a factorial improvement in complexity compared to the\nstandard representation in the Barron space and provides a theoretical\nexplanation for the effectiveness of determinant-based architectures in\nab-initio quantum chemistry.\n","authors":["Nilin Abrahamsen","Lin Lin"],"pdf_url":"https://arxiv.org/pdf/2303.12856v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12853v1","updated":"2023-03-22T18:23:24Z","published":"2023-03-22T18:23:24Z","title":"Three iterations of $(1-d)$-WL test distinguish non isometric clouds of\n  $d$-dimensional points","summary":"  The Weisfeiler--Lehman (WL) test is a fundamental iterative algorithm for\nchecking isomorphism of graphs. It has also been observed that it underlies the\ndesign of several graph neural network architectures, whose capabilities and\nperformance can be understood in terms of the expressive power of this test.\nMotivated by recent developments in machine learning applications to datasets\ninvolving three-dimensional objects, we study when the WL test is {\\em\ncomplete} for clouds of euclidean points represented by complete distance\ngraphs, i.e., when it can distinguish, up to isometry, any arbitrary such\ncloud.\n  Our main result states that the $(d-1)$-dimensional WL test is complete for\npoint clouds in $d$-dimensional Euclidean space, for any $d\\ge 2$, and that\nonly three iterations of the test suffice. Our result is tight for $d = 2, 3$.\nWe also observe that the $d$-dimensional WL test only requires one iteration to\nachieve completeness.\n","authors":["Valentino Delle Rose","Alexander Kozachinskiy","Cristóbal Rojas","Mircea Petrache","Pablo Barceló"],"pdf_url":"https://arxiv.org/pdf/2303.12853v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2303.12848v1","updated":"2023-03-22T18:14:02Z","published":"2023-03-22T18:14:02Z","title":"Test-time Defense against Adversarial Attacks: Detection and\n  Reconstruction of Adversarial Examples via Masked Autoencoder","summary":"  Existing defense methods against adversarial attacks can be categorized into\ntraining time and test time defenses. Training time defense, i.e., adversarial\ntraining, requires a significant amount of extra time for training and is often\nnot able to be generalized to unseen attacks. On the other hand, test time\ndefense by test time weight adaptation requires access to perform gradient\ndescent on (part of) the model weights, which could be infeasible for models\nwith frozen weights. To address these challenges, we propose DRAM, a novel\ndefense method to Detect and Reconstruct multiple types of Adversarial attacks\nvia Masked autoencoder (MAE). We demonstrate how to use MAE losses to build a\nKS-test to detect adversarial attacks. Moreover, the MAE losses can be used to\nrepair adversarial samples from unseen attack types. In this sense, DRAM\nneither requires model weight updates in test time nor augments the training\nset with more adversarial samples. Evaluating DRAM on the large-scale ImageNet\ndata, we achieve the best detection rate of 82% on average on eight types of\nadversarial attacks compared with other detection baselines. For\nreconstruction, DRAM improves the robust accuracy by 6% ~ 41% for Standard\nResNet50 and 3% ~ 8% for Robust ResNet50 compared with other self-supervision\ntasks, such as rotation prediction and contrastive learning.\n","authors":["Yun-Yun Tsai","Ju-Chin Chao","Albert Wen","Zhaoyuan Yang","Chengzhi Mao","Tapan Shah","Junfeng Yang"],"pdf_url":"https://arxiv.org/pdf/2303.12848v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.00366v2","updated":"2023-03-22T18:11:14Z","published":"2023-01-01T07:42:50Z","title":"Self-Supervised Object Segmentation with a Cut-and-Pasting GAN","summary":"  This paper proposes a novel self-supervised based Cut-and-Paste GAN to\nperform foreground object segmentation and generate realistic composite images\nwithout manual annotations. We accomplish this goal by a simple yet effective\nself-supervised approach coupled with the U-Net based discriminator. The\nproposed method extends the ability of the standard discriminators to learn not\nonly the global data representations via classification (real/fake) but also\nlearn semantic and structural information through pseudo labels created using\nthe self-supervised task. The proposed method empowers the generator to create\nmeaningful masks by forcing it to learn informative per-pixel as well as global\nimage feedback from the discriminator. Our experiments demonstrate that our\nproposed method significantly outperforms the state-of-the-art methods on the\nstandard benchmark datasets.\n","authors":["Kunal Chaturvedi","Ali Braytee","Jun Li","Mukesh Prasad"],"pdf_url":"https://arxiv.org/pdf/2301.00366v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.11010v5","updated":"2023-03-22T18:03:42Z","published":"2022-08-23T14:46:54Z","title":"Convex mixed-integer optimization with Frank-Wolfe methods","summary":"  Mixed-integer nonlinear optimization encompasses a broad class of problems\nthat present both theoretical and computational challenges. We propose a new\ntype of method to solve these problems based on a branch-and-bound algorithm\nwith convex node relaxations. These relaxations are solved with a Frank-Wolfe\nalgorithm over the convex hull of mixed-integer feasible points instead of the\ncontinuous relaxation via calls to a mixed-integer linear solver as the linear\noracle. The proposed method computes feasible solutions while working on a\nsingle representation of the polyhedral constraints, leveraging the full extent\nof mixed-integer linear solvers without an outer approximation scheme and can\nexploit inexact solutions of node subproblems.\n","authors":["Deborah Hendrych","Hannah Troppens","Mathieu Besançon","Sebastian Pokutta"],"pdf_url":"https://arxiv.org/pdf/2208.11010v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12834v1","updated":"2023-03-22T18:00:02Z","published":"2023-03-22T18:00:02Z","title":"The power and limitations of learning quantum dynamics incoherently","summary":"  Quantum process learning is emerging as an important tool to study quantum\nsystems. While studied extensively in coherent frameworks, where the target and\nmodel system can share quantum information, less attention has been paid to\nwhether the dynamics of quantum systems can be learned without the system and\ntarget directly interacting. Such incoherent frameworks are practically\nappealing since they open up methods of transpiling quantum processes between\nthe different physical platforms without the need for technically challenging\nhybrid entanglement schemes. Here we provide bounds on the sample complexity of\nlearning unitary processes incoherently by analyzing the number of measurements\nthat are required to emulate well-established coherent learning strategies. We\nprove that if arbitrary measurements are allowed, then any efficiently\nrepresentable unitary can be efficiently learned within the incoherent\nframework; however, when restricted to shallow-depth measurements only\nlow-entangling unitaries can be learned. We demonstrate our incoherent learning\nalgorithm for low entangling unitaries by successfully learning a 16-qubit\nunitary on \\texttt{ibmq\\_kolkata}, and further demonstrate the scalabilty of\nour proposed algorithm through extensive numerical experiments.\n","authors":["Sofiene Jerbi","Joe Gibbs","Manuel S. Rudolph","Matthias C. Caro","Patrick J. Coles","Hsin-Yuan Huang","Zoë Holmes"],"pdf_url":"https://arxiv.org/pdf/2303.12834v1.pdf","comment":"6+9 pages, 7 figures"},{"id":"http://arxiv.org/abs/2303.12821v1","updated":"2023-03-22T16:47:48Z","published":"2023-03-22T16:47:48Z","title":"Towards A Visual Programming Tool to Create Deep Learning Models","summary":"  Deep Learning (DL) developers come from different backgrounds, e.g.,\nmedicine, genomics, finance, and computer science. To create a DL model, they\nmust learn and use high-level programming languages (e.g., Python), thus\nneeding to handle related setups and solve programming errors. This paper\npresents DeepBlocks, a visual programming tool that allows DL developers to\ndesign, train, and evaluate models without relying on specific programming\nlanguages. DeepBlocks works by building on the typical model structure: a\nsequence of learnable functions whose arrangement defines the specific\ncharacteristics of the model. We derived DeepBlocks' design goals from a\n5-participants formative interview, and we validated the first implementation\nof the tool through a typical use case. Results are promising and show that\ndevelopers could visually design complex DL architectures.\n","authors":["Tommaso Calò","Luigi De Russis"],"pdf_url":"https://arxiv.org/pdf/2303.12821v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2303.12658v1","updated":"2023-03-22T15:36:19Z","published":"2023-03-22T15:36:19Z","title":"Reliable and Efficient Evaluation of Adversarial Robustness for Deep\n  Hashing-Based Retrieval","summary":"  Deep hashing has been extensively applied to massive image retrieval due to\nits efficiency and effectiveness. Recently, several adversarial attacks have\nbeen presented to reveal the vulnerability of deep hashing models against\nadversarial examples. However, existing attack methods suffer from degraded\nperformance or inefficiency because they underutilize the semantic relations\nbetween original samples or spend a lot of time learning these relations with a\ndeep neural network. In this paper, we propose a novel Pharos-guided Attack,\ndubbed PgA, to evaluate the adversarial robustness of deep hashing networks\nreliably and efficiently. Specifically, we design pharos code to represent the\nsemantics of the benign image, which preserves the similarity to semantically\nrelevant samples and dissimilarity to irrelevant ones. It is proven that we can\nquickly calculate the pharos code via a simple math formula. Accordingly, PgA\ncan directly conduct a reliable and efficient attack on deep hashing-based\nretrieval by maximizing the similarity between the hash code of the adversarial\nexample and the pharos code. Extensive experiments on the benchmark datasets\nverify that the proposed algorithm outperforms the prior state-of-the-arts in\nboth attack strength and speed.\n","authors":["Xunguang Wang","Jiawang Bai","Xinyue Xu","Xiaomeng Li"],"pdf_url":"https://arxiv.org/pdf/2303.12658v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2204.10779"},{"id":"http://arxiv.org/abs/2303.12337v1","updated":"2023-03-22T06:26:56Z","published":"2023-03-22T06:26:56Z","title":"Music-Driven Group Choreography","summary":"  Music-driven choreography is a challenging problem with a wide variety of\nindustrial applications. Recently, many methods have been proposed to\nsynthesize dance motions from music for a single dancer. However, generating\ndance motion for a group remains an open problem. In this paper, we present\n$\\rm AIOZ-GDANCE$, a new large-scale dataset for music-driven group dance\ngeneration. Unlike existing datasets that only support single dance, our new\ndataset contains group dance videos, hence supporting the study of group\nchoreography. We propose a semi-autonomous labeling method with humans in the\nloop to obtain the 3D ground truth for our dataset. The proposed dataset\nconsists of $16.7$ hours of paired music and 3D motion from in-the-wild videos,\ncovering $7$ dance styles and $16$ music genres. We show that naively applying\nsingle dance generation technique to creating group dance motion may lead to\nunsatisfactory results, such as inconsistent movements and collisions between\ndancers. Based on our new dataset, we propose a new method that takes an input\nmusic sequence and a set of 3D positions of dancers to efficiently produce\nmultiple group-coherent choreographies. We propose new evaluation metrics for\nmeasuring group dance quality and perform intensive experiments to demonstrate\nthe effectiveness of our method.\n","authors":["Nhat Le","Thang Pham","Tuong Do","Erman Tjiputra","Quang D. Tran","Anh Nguyen"],"pdf_url":"https://arxiv.org/pdf/2303.12337v1.pdf","comment":"accepted in cvpr 2023"},{"id":"http://arxiv.org/abs/2303.12296v1","updated":"2023-03-22T04:06:29Z","published":"2023-03-22T04:06:29Z","title":"Prototype Helps Federated Learning: Towards Faster Convergence","summary":"  Federated learning (FL) is a distributed machine learning technique in which\nmultiple clients cooperate to train a shared model without exchanging their raw\ndata. However, heterogeneity of data distribution among clients usually leads\nto poor model inference. In this paper, a prototype-based federated learning\nframework is proposed, which can achieve better inference performance with only\na few changes to the last global iteration of the typical federated learning\nprocess. In the last iteration, the server aggregates the prototypes\ntransmitted from distributed clients and then sends them back to local clients\nfor their respective model inferences. Experiments on two baseline datasets\nshow that our proposal can achieve higher accuracy (at least 1%) and relatively\nefficient communication than two popular baselines under different\nheterogeneous settings.\n","authors":["Yu Qiao","Seong-Bae Park","Sun Moo Kang","Choong Seon Hong"],"pdf_url":"https://arxiv.org/pdf/2303.12296v1.pdf","comment":"3 pages, 3 figures"},{"id":"http://arxiv.org/abs/2207.11900v4","updated":"2023-03-22T02:51:19Z","published":"2022-07-25T04:22:41Z","title":"GA2MIF: Graph and Attention based Two-stage Multi-source Information\n  Fusion for Conversational Emotion Detection","summary":"  Multimodal Emotion Recognition in Conversation (ERC) plays an influential\nrole in the field of human-computer interaction and conversational robotics\nsince it can motivate machines to provide empathetic services. Multimodal data\nmodeling is an up-and-coming research area in recent years, which is inspired\nby human capability to integrate multiple senses. Several graph-based\napproaches claim to capture interactive information between modalities, but the\nheterogeneity of multimodal data makes these methods prohibit optimal\nsolutions. In this work, we introduce a multimodal fusion approach named Graph\nand Attention based Two-stage Multi-source Information Fusion (GA2MIF) for\nemotion detection in conversation. Our proposed method circumvents the problem\nof taking heterogeneous graph as input to the model while eliminating complex\nredundant connections in the construction of graph. GA2MIF focuses on\ncontextual modeling and cross-modal modeling through leveraging Multi-head\nDirected Graph ATtention networks (MDGATs) and Multi-head Pairwise Cross-modal\nATtention networks (MPCATs), respectively. Extensive experiments on two public\ndatasets (i.e., IEMOCAP and MELD) demonstrate that the proposed GA2MIF has the\ncapacity to validly capture intra-modal long-range contextual information and\ninter-modal complementary information, as well as outperforms the prevalent\nState-Of-The-Art (SOTA) models by a remarkable margin.\n","authors":["Jiang Li","Xiaoping Wang","Guoqing Lv","Zhigang Zeng"],"pdf_url":"https://arxiv.org/pdf/2207.11900v4.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2204.12037v8","updated":"2023-03-22T02:41:58Z","published":"2022-04-26T02:22:28Z","title":"Causal Reasoning Meets Visual Representation Learning: A Prospective\n  Study","summary":"  Visual representation learning is ubiquitous in various real-world\napplications, including visual comprehension, video understanding, multi-modal\nanalysis, human-computer interaction, and urban computing. Due to the emergence\nof huge amounts of multi-modal heterogeneous spatial/temporal/spatial-temporal\ndata in big data era, the lack of interpretability, robustness, and\nout-of-distribution generalization are becoming the challenges of the existing\nvisual models. The majority of the existing methods tend to fit the original\ndata/variable distributions and ignore the essential causal relations behind\nthe multi-modal knowledge, which lacks unified guidance and analysis about why\nmodern visual representation learning methods easily collapse into data bias\nand have limited generalization and cognitive abilities. Inspired by the strong\ninference ability of human-level agents, recent years have therefore witnessed\ngreat effort in developing causal reasoning paradigms to realize robust\nrepresentation and model learning with good cognitive ability. In this paper,\nwe conduct a comprehensive review of existing causal reasoning methods for\nvisual representation learning, covering fundamental theories, models, and\ndatasets. The limitations of current methods and datasets are also discussed.\nMoreover, we propose some prospective challenges, opportunities, and future\nresearch directions for benchmarking causal reasoning algorithms in visual\nrepresentation learning. This paper aims to provide a comprehensive overview of\nthis emerging field, attract attention, encourage discussions, bring to the\nforefront the urgency of developing novel causal reasoning methods, publicly\navailable benchmarks, and consensus-building standards for reliable visual\nrepresentation learning and related real-world applications more efficiently.\n","authors":["Yang Liu","Yushen Wei","Hong Yan","Guanbin Li","Liang Lin"],"pdf_url":"https://arxiv.org/pdf/2204.12037v8.pdf","comment":"35 pages, 14 figures. This work has been accepted by Machine\n  Intelligence Research. The arxiv version is kept updating by adding more\n  novel methods, datasets and insights. The official video interpretation of\n  this paper can be referred at https://youtu.be/2lfNaTkcTHI"},{"id":"http://arxiv.org/abs/2303.12930v1","updated":"2023-03-22T22:00:17Z","published":"2023-03-22T22:00:17Z","title":"Dense-Localizing Audio-Visual Events in Untrimmed Videos: A Large-Scale\n  Benchmark and Baseline","summary":"  Existing audio-visual event localization (AVE) handles manually trimmed\nvideos with only a single instance in each of them. However, this setting is\nunrealistic as natural videos often contain numerous audio-visual events with\ndifferent categories. To better adapt to real-life applications, in this paper\nwe focus on the task of dense-localizing audio-visual events, which aims to\njointly localize and recognize all audio-visual events occurring in an\nuntrimmed video. The problem is challenging as it requires fine-grained\naudio-visual scene and context understanding. To tackle this problem, we\nintroduce the first Untrimmed Audio-Visual (UnAV-100) dataset, which contains\n10K untrimmed videos with over 30K audio-visual events. Each video has 2.8\naudio-visual events on average, and the events are usually related to each\nother and might co-occur as in real-life scenes. Next, we formulate the task\nusing a new learning-based framework, which is capable of fully integrating\naudio and visual modalities to localize audio-visual events with various\nlengths and capture dependencies between them in a single pass. Extensive\nexperiments demonstrate the effectiveness of our method as well as the\nsignificance of multi-scale cross-modal perception and dependency modeling for\nthis task.\n","authors":["Tiantian Geng","Teng Wang","Jinming Duan","Runmin Cong","Feng Zheng"],"pdf_url":"https://arxiv.org/pdf/2303.12930v1.pdf","comment":"Accepted by CVPR2023"}]},"2023-03-23T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2303.13519v1","updated":"2023-03-23T17:59:54Z","published":"2023-03-23T17:59:54Z","title":"Learning and Verification of Task Structure in Instructional Videos","summary":"  Given the enormous number of instructional videos available online, learning\na diverse array of multi-step task models from videos is an appealing goal. We\nintroduce a new pre-trained video model, VideoTaskformer, focused on\nrepresenting the semantics and structure of instructional videos. We pre-train\nVideoTaskformer using a simple and effective objective: predicting weakly\nsupervised textual labels for steps that are randomly masked out from an\ninstructional video (masked step modeling). Compared to prior work which learns\nstep representations locally, our approach involves learning them globally,\nleveraging video of the entire surrounding task as context. From these learned\nrepresentations, we can verify if an unseen video correctly executes a given\ntask, as well as forecast which steps are likely to be taken after a given\nstep. We introduce two new benchmarks for detecting mistakes in instructional\nvideos, to verify if there is an anomalous step and if steps are executed in\nthe right order. We also introduce a long-term forecasting benchmark, where the\ngoal is to predict long-range future steps from a given step. Our method\noutperforms previous baselines on these tasks, and we believe the tasks will be\na valuable way for the community to measure the quality of step\nrepresentations. Additionally, we evaluate VideoTaskformer on 3 existing\nbenchmarks -- procedural activity recognition, step classification, and step\nforecasting -- and demonstrate on each that our method outperforms existing\nbaselines and achieves new state-of-the-art performance.\n","authors":["Medhini Narasimhan","Licheng Yu","Sean Bell","Ning Zhang","Trevor Darrell"],"pdf_url":"https://arxiv.org/pdf/2303.13519v1.pdf","comment":"Wesbite at https://medhini.github.io/task_structure"},{"id":"http://arxiv.org/abs/2303.13455v1","updated":"2023-03-23T17:24:31Z","published":"2023-03-23T17:24:31Z","title":"CoBIT: A Contrastive Bi-directional Image-Text Generation Model","summary":"  The field of vision and language has witnessed a proliferation of pre-trained\nfoundation models. Most existing methods are independently pre-trained with\ncontrastive objective like CLIP, image-to-text generative objective like PaLI,\nor text-to-image generative objective like Parti. However, the three objectives\ncan be pre-trained on the same data, image-text pairs, and intuitively they\ncomplement each other as contrasting provides global alignment capacity and\ngeneration grants fine-grained understanding. In this work, we present a\nContrastive Bi-directional Image-Text generation model (CoBIT), which attempts\nto unify the three pre-training objectives in one framework. Specifically,\nCoBIT employs a novel unicoder-decoder structure, consisting of an image\nunicoder, a text unicoder and a cross-modal decoder. The image/text unicoders\ncan switch between encoding and decoding in different tasks, enabling\nflexibility and shared knowledge that benefits both image-to-text and\ntext-to-image generations. CoBIT achieves superior performance in image\nunderstanding, image-text understanding (Retrieval, Captioning, VQA, SNLI-VE)\nand text-based content creation, particularly in zero-shot scenarios. For\ninstance, 82.7% in zero-shot ImageNet classification, 9.37 FID score in\nzero-shot text-to-image generation and 44.8 CIDEr in zero-shot captioning.\n","authors":["Haoxuan You","Mandy Guo","Zhecan Wang","Kai-Wei Chang","Jason Baldridge","Jiahui Yu"],"pdf_url":"https://arxiv.org/pdf/2303.13455v1.pdf","comment":"14 pages, 5 figures"},{"id":"http://arxiv.org/abs/2303.13451v1","updated":"2023-03-23T17:17:46Z","published":"2023-03-23T17:17:46Z","title":"Development and validation of a natural language processing algorithm to\n  pseudonymize documents in the context of a clinical data warehouse","summary":"  The objective of this study is to address the critical issue of\nde-identification of clinical reports in order to allow access to data for\nresearch purposes, while ensuring patient privacy. The study highlights the\ndifficulties faced in sharing tools and resources in this domain and presents\nthe experience of the Greater Paris University Hospitals (AP-HP) in\nimplementing a systematic pseudonymization of text documents from its Clinical\nData Warehouse. We annotated a corpus of clinical documents according to 12\ntypes of identifying entities, and built a hybrid system, merging the results\nof a deep learning model as well as manual rules. Our results show an overall\nperformance of 0.99 of F1-score. We discuss implementation choices and present\nexperiments to better understand the effort involved in such a task, including\ndataset size, document types, language models, or rule addition. We share\nguidelines and code under a 3-Clause BSD license.\n","authors":["Xavier Tannier","Perceval Wajsbürt","Alice Calliger","Basile Dura","Alexandre Mouchet","Martin Hilka","Romain Bey"],"pdf_url":"https://arxiv.org/pdf/2303.13451v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13408v1","updated":"2023-03-23T16:29:27Z","published":"2023-03-23T16:29:27Z","title":"Paraphrasing evades detectors of AI-generated text, but retrieval is an\n  effective defense","summary":"  To detect the deployment of large language models for malicious use cases\n(e.g., fake content creation or academic plagiarism), several approaches have\nrecently been proposed for identifying AI-generated text via watermarks or\nstatistical irregularities. How robust are these detection algorithms to\nparaphrases of AI-generated text? To stress test these detectors, we first\ntrain an 11B parameter paraphrase generation model (DIPPER) that can paraphrase\nparagraphs, optionally leveraging surrounding text (e.g., user-written prompts)\nas context. DIPPER also uses scalar knobs to control the amount of lexical\ndiversity and reordering in the paraphrases. Paraphrasing text generated by\nthree large language models (including GPT3.5-davinci-003) with DIPPER\nsuccessfully evades several detectors, including watermarking, GPTZero,\nDetectGPT, and OpenAI's text classifier. For example, DIPPER drops the\ndetection accuracy of DetectGPT from 70.3% to 4.6% (at a constant false\npositive rate of 1%), without appreciably modifying the input semantics. To\nincrease the robustness of AI-generated text detection to paraphrase attacks,\nwe introduce a simple defense that relies on retrieving semantically-similar\ngenerations and must be maintained by a language model API provider. Given a\ncandidate text, our algorithm searches a database of sequences previously\ngenerated by the API, looking for sequences that match the candidate text\nwithin a certain threshold. We empirically verify our defense using a database\nof 15M generations from a fine-tuned T5-XXL model and find that it can detect\n80% to 97% of paraphrased generations across different settings, while only\nclassifying 1% of human-written sequences as AI-generated. We will open source\nour code, model and data for future research.\n","authors":["Kalpesh Krishna","Yixiao Song","Marzena Karpinska","John Wieting","Mohit Iyyer"],"pdf_url":"https://arxiv.org/pdf/2303.13408v1.pdf","comment":"Preprint (27 pages). Code, models, data will be added to\n  https://github.com/martiansideofthemoon/ai-detection-paraphrases"},{"id":"http://arxiv.org/abs/2303.13386v1","updated":"2023-03-23T15:58:41Z","published":"2023-03-23T15:58:41Z","title":"Compositional Zero-Shot Domain Transfer with Text-to-Text Models","summary":"  Label scarcity is a bottleneck for improving task performance in specialised\ndomains. We propose a novel compositional transfer learning framework (DoT5 -\ndomain compositional zero-shot T5) for zero-shot domain transfer. Without\naccess to in-domain labels, DoT5 jointly learns domain knowledge (from MLM of\nunlabelled in-domain free text) and task knowledge (from task training on more\nreadily available general-domain data) in a multi-task manner. To improve the\ntransferability of task training, we design a strategy named NLGU: we\nsimultaneously train NLG for in-domain label-to-data generation which enables\ndata augmentation for self-finetuning and NLU for label prediction. We evaluate\nDoT5 on the biomedical domain and the resource-lean subdomain of radiology,\nfocusing on NLI, text summarisation and embedding learning. DoT5 demonstrates\nthe effectiveness of compositional transfer learning through multi-task\nlearning. In particular, DoT5 outperforms the current SOTA in zero-shot\ntransfer by over 7 absolute points in accuracy on RadNLI. We validate DoT5 with\nablations and a case study demonstrating its ability to solve challenging NLI\nexamples requiring in-domain expertise.\n","authors":["Fangyu Liu","Qianchu Liu","Shruthi Bannur","Fernando Pérez-García","Naoto Usuyama","Sheng Zhang","Tristan Naumann","Aditya Nori","Hoifung Poon","Javier Alvarez-Valle","Ozan Oktay","Stephanie L. Hyland"],"pdf_url":"https://arxiv.org/pdf/2303.13386v1.pdf","comment":"Accepted at TACL, pre-MIT Press publication version. 16 pages, 4\n  figures"},{"id":"http://arxiv.org/abs/2303.13351v1","updated":"2023-03-23T15:29:21Z","published":"2023-03-23T15:29:21Z","title":"DBLP-QuAD: A Question Answering Dataset over the DBLP Scholarly\n  Knowledge Graph","summary":"  In this work we create a question answering dataset over the DBLP scholarly\nknowledge graph (KG). DBLP is an on-line reference for bibliographic\ninformation on major computer science publications that indexes over 4.4\nmillion publications published by more than 2.2 million authors. Our dataset\nconsists of 10,000 question answer pairs with the corresponding SPARQL queries\nwhich can be executed over the DBLP KG to fetch the correct answer. DBLP-QuAD\nis the largest scholarly question answering dataset.\n","authors":["Debayan Banerjee","Sushil Awale","Ricardo Usbeck","Chris Biemann"],"pdf_url":"https://arxiv.org/pdf/2303.13351v1.pdf","comment":"12 pages ceur-ws 1 column accepted at International Bibliometric\n  Information Retrieval Workshp @ ECIR 2023"},{"id":"http://arxiv.org/abs/2303.13310v1","updated":"2023-03-23T14:44:47Z","published":"2023-03-23T14:44:47Z","title":"SwissBERT: The Multilingual Language Model for Switzerland","summary":"  We present SwissBERT, a masked language model created specifically for\nprocessing Switzerland-related text. SwissBERT is a pre-trained model that we\nadapted to news articles written in the national languages of Switzerland --\nGerman, French, Italian, and Romansh. We evaluate SwissBERT on natural language\nunderstanding tasks related to Switzerland and find that it tends to outperform\nprevious models on these tasks, especially when processing contemporary news\nand/or Romansh Grischun. Since SwissBERT uses language adapters, it may be\nextended to Swiss German dialects in future work. The model and our open-source\ncode are publicly released at https://github.com/ZurichNLP/swissbert.\n","authors":["Jannis Vamvas","Johannes Graën","Rico Sennrich"],"pdf_url":"https://arxiv.org/pdf/2303.13310v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.16848v2","updated":"2023-03-23T14:35:30Z","published":"2022-10-30T14:15:43Z","title":"Using Context-to-Vector with Graph Retrofitting to Improve Word\n  Embeddings","summary":"  Although contextualized embeddings generated from large-scale pre-trained\nmodels perform well in many tasks, traditional static embeddings (e.g.,\nSkip-gram, Word2Vec) still play an important role in low-resource and\nlightweight settings due to their low computational cost, ease of deployment,\nand stability. In this paper, we aim to improve word embeddings by 1)\nincorporating more contextual information from existing pre-trained models into\nthe Skip-gram framework, which we call Context-to-Vec; 2) proposing a\npost-processing retrofitting method for static embeddings independent of\ntraining by employing priori synonym knowledge and weighted vector\ndistribution. Through extrinsic and intrinsic tasks, our methods are well\nproven to outperform the baselines by a large margin.\n","authors":["Jiangbin Zheng","Yile Wang","Ge Wang","Jun Xia","Yufei Huang","Guojiang Zhao","Yue Zhang","Stan Z. Li"],"pdf_url":"https://arxiv.org/pdf/2210.16848v2.pdf","comment":"Accepted to ACL 2022"},{"id":"http://arxiv.org/abs/2303.13284v1","updated":"2023-03-23T14:06:26Z","published":"2023-03-23T14:06:26Z","title":"GETT-QA: Graph Embedding based T2T Transformer for Knowledge Graph\n  Question Answering","summary":"  In this work, we present an end-to-end Knowledge Graph Question Answering\n(KGQA) system named GETT-QA. GETT-QA uses T5, a popular text-to-text\npre-trained language model. The model takes a question in natural language as\ninput and produces a simpler form of the intended SPARQL query. In the simpler\nform, the model does not directly produce entity and relation IDs. Instead, it\nproduces corresponding entity and relation labels. The labels are grounded to\nKG entity and relation IDs in a subsequent step. To further improve the\nresults, we instruct the model to produce a truncated version of the KG\nembedding for each entity. The truncated KG embedding enables a finer search\nfor disambiguation purposes. We find that T5 is able to learn the truncated KG\nembeddings without any change of loss function, improving KGQA performance. As\na result, we report strong results for LC-QuAD 2.0 and SimpleQuestions-Wikidata\ndatasets on end-to-end KGQA over Wikidata.\n","authors":["Debayan Banerjee","Pranav Ajit Nair","Ricardo Usbeck","Chris Biemann"],"pdf_url":"https://arxiv.org/pdf/2303.13284v1.pdf","comment":"16 pages single column format accepted at ESWC 2023 research track"},{"id":"http://arxiv.org/abs/2303.13283v1","updated":"2023-03-23T14:04:23Z","published":"2023-03-23T14:04:23Z","title":"Visual-Language Prompt Tuning with Knowledge-guided Context Optimization","summary":"  Prompt tuning is an effective way to adapt the pre-trained visual-language\nmodel (VLM) to the downstream task using task-related textual tokens.\nRepresentative CoOp-based work combines the learnable textual tokens with the\nclass tokens to obtain specific textual knowledge. However, the specific\ntextual knowledge is the worse generalization to the unseen classes because it\nforgets the essential general textual knowledge having a strong generalization\nability. To tackle this issue, we introduce a novel Knowledge-guided Context\nOptimization (KgCoOp) to enhance the generalization ability of the learnable\nprompt for unseen classes. The key insight of KgCoOp is that forgetting about\nessential knowledge can be alleviated by reducing the discrepancy between the\nlearnable prompt and the hand-crafted prompt. Especially, KgCoOp minimizes the\ndiscrepancy between the textual embeddings generated by learned prompts and the\nhand-crafted prompts. Finally, adding the KgCoOp upon the contrastive loss can\nmake a discriminative prompt for both seen and unseen tasks. Extensive\nevaluation of several benchmarks demonstrates that the proposed\nKnowledge-guided Context Optimization is an efficient method for prompt tuning,\n\\emph{i.e.,} achieves better performance with less training time.\n","authors":["Hantao Yao","Rui Zhang","Changsheng Xu"],"pdf_url":"https://arxiv.org/pdf/2303.13283v1.pdf","comment":"accepted by CVPR23"},{"id":"http://arxiv.org/abs/2303.13220v1","updated":"2023-03-23T12:34:30Z","published":"2023-03-23T12:34:30Z","title":"Parameter-Efficient Sparse Retrievers and Rerankers using Adapters","summary":"  Parameter-Efficient transfer learning with Adapters have been studied in\nNatural Language Processing (NLP) as an alternative to full fine-tuning.\nAdapters are memory-efficient and scale well with downstream tasks by training\nsmall bottle-neck layers added between transformer layers while keeping the\nlarge pretrained language model (PLMs) frozen. In spite of showing promising\nresults in NLP, these methods are under-explored in Information Retrieval.\nWhile previous studies have only experimented with dense retriever or in a\ncross lingual retrieval scenario, in this paper we aim to complete the picture\non the use of adapters in IR. First, we study adapters for SPLADE, a sparse\nretriever, for which adapters not only retain the efficiency and effectiveness\notherwise achieved by finetuning, but are memory-efficient and orders of\nmagnitude lighter to train. We observe that Adapters-SPLADE not only optimizes\njust 2\\% of training parameters, but outperforms fully fine-tuned counterpart\nand existing parameter-efficient dense IR models on IR benchmark datasets.\nSecondly, we address domain adaptation of neural retrieval thanks to adapters\non cross-domain BEIR datasets and TripClick. Finally, we also consider\nknowledge sharing between rerankers and first stage rankers. Overall, our study\ncomplete the examination of adapters for neural IR\n","authors":["Vaishali Pal","Carlos Lassance","Hervé Déjean","Stéphane Clinchant"],"pdf_url":"https://arxiv.org/pdf/2303.13220v1.pdf","comment":"accepted at ECIR'23"},{"id":"http://arxiv.org/abs/2303.13217v1","updated":"2023-03-23T12:28:25Z","published":"2023-03-23T12:28:25Z","title":"Fairness-guided Few-shot Prompting for Large Language Models","summary":"  Large language models have demonstrated surprising ability to perform\nin-context learning, i.e., these models can be directly applied to solve\nnumerous downstream tasks by conditioning on a prompt constructed by a few\ninput-output examples. However, prior research has shown that in-context\nlearning can suffer from high instability due to variations in training\nexamples, example order, and prompt formats. Therefore, the construction of an\nappropriate prompt is essential for improving the performance of in-context\nlearning. In this paper, we revisit this problem from the view of predictive\nbias. Specifically, we introduce a metric to evaluate the predictive bias of a\nfixed prompt against labels or a given attributes. Then we empirically show\nthat prompts with higher bias always lead to unsatisfactory predictive quality.\nBased on this observation, we propose a novel search strategy based on the\ngreedy search to identify the near-optimal prompt for improving the performance\nof in-context learning. We perform comprehensive experiments with\nstate-of-the-art mainstream models such as GPT-3 on various downstream tasks.\nOur results indicate that our method can enhance the model's in-context\nlearning performance in an effective and interpretable manner.\n","authors":["Huan Ma","Changqing Zhang","Yatao Bian","Lemao Liu","Zhirui Zhang","Peilin Zhao","Shu Zhang","Huazhu Fu","Qinghua Hu","Bingzhe Wu"],"pdf_url":"https://arxiv.org/pdf/2303.13217v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13112v1","updated":"2023-03-23T09:00:07Z","published":"2023-03-23T09:00:07Z","title":"A Simple Explanation for the Phase Transition in Large Language Models\n  with List Decoding","summary":"  Various recent experimental results show that large language models (LLM)\nexhibit emergent abilities that are not present in small models. System\nperformance is greatly improved after passing a certain critical threshold of\nscale. In this letter, we provide a simple explanation for such a phase\ntransition phenomenon. For this, we model an LLM as a sequence-to-sequence\nrandom function. Instead of using instant generation at each step, we use a\nlist decoder that keeps a list of candidate sequences at each step and defers\nthe generation of the output sequence at the end. We show that there is a\ncritical threshold such that the expected number of erroneous candidate\nsequences remains bounded when an LLM is below the threshold, and it grows\nexponentially when an LLM is above the threshold. Such a threshold is related\nto the basic reproduction number in a contagious disease.\n","authors":["Cheng-Shang Chang"],"pdf_url":"https://arxiv.org/pdf/2303.13112v1.pdf","comment":"5 pages, 1 figure"},{"id":"http://arxiv.org/abs/2303.13099v1","updated":"2023-03-23T08:30:35Z","published":"2023-03-23T08:30:35Z","title":"Multi-View Zero-Shot Open Intent Induction from Dialogues: Multi Domain\n  Batch and Proxy Gradient Transfer","summary":"  In Task Oriented Dialogue (TOD) system, detecting and inducing new intents\nare two main challenges to apply the system in the real world. In this paper,\nwe suggest the semantic multi-view model to resolve these two challenges: (1)\nSBERT for General Embedding (GE), (2) Multi Domain Batch (MDB) for dialogue\ndomain knowledge, and (3) Proxy Gradient Transfer (PGT) for cluster-specialized\nsemantic. MDB feeds diverse dialogue datasets to the model at once to tackle\nthe multi-domain problem by learning the multiple domain knowledge. We\nintroduce a novel method PGT, which employs the Siamese network to fine-tune\nthe model with a clustering method directly.Our model can learn how to cluster\ndialogue utterances by using PGT. Experimental results demonstrate that our\nmulti-view model with MDB and PGT significantly improves the Open Intent\nInduction performance compared to baseline systems.\n","authors":["Hyukhun Koh","Haesung Pyun","Nakyeong Yang","Kyomin Jung"],"pdf_url":"https://arxiv.org/pdf/2303.13099v1.pdf","comment":"8 pages, 3 figures, ACL 2023 workshop (DSTC)"},{"id":"http://arxiv.org/abs/2303.13072v1","updated":"2023-03-23T06:54:37Z","published":"2023-03-23T06:54:37Z","title":"Beyond Universal Transformer: block reusing with adaptor in Transformer\n  for automatic speech recognit","summary":"  Transformer-based models have recently made significant achievements in the\napplication of end-to-end (E2E) automatic speech recognition (ASR). It is\npossible to deploy the E2E ASR system on smart devices with the help of\nTransformer-based models. While these models still have the disadvantage of\nrequiring a large number of model parameters. To overcome the drawback of\nuniversal Transformer models for the application of ASR on edge devices, we\npropose a solution that can reuse the block in Transformer models for the\noccasion of the small footprint ASR system, which meets the objective of\naccommodating resource limitations without compromising recognition accuracy.\nSpecifically, we design a novel block-reusing strategy for speech Transformer\n(BRST) to enhance the effectiveness of parameters and propose an adapter module\n(ADM) that can produce a compact and adaptable model with only a few additional\ntrainable parameters accompanying each reusing block. We conducted an\nexperiment with the proposed method on the public AISHELL-1 corpus, and the\nresults show that the proposed approach achieves the character error rate (CER)\nof 9.3%/6.63% with only 7.6M/8.3M parameters without and with the ADM,\nrespectively. In addition, we also make a deeper analysis to show the effect of\nADM in the general block-reusing method.\n","authors":["Haoyu Tang","Zhaoyi Liu","Chang Zeng","Xinfeng Li"],"pdf_url":"https://arxiv.org/pdf/2303.13072v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13065v1","updated":"2023-03-23T06:33:06Z","published":"2023-03-23T06:33:06Z","title":"Retrieval-Augmented Classification with Decoupled Representation","summary":"  Pretrained language models (PLMs) have shown marvelous improvements across\nvarious NLP tasks. Most Chinese PLMs simply treat an input text as a sequence\nof characters, and completely ignore word information. Although Whole Word\nMasking can alleviate this, the semantics in words is still not well\nrepresented. In this paper, we revisit the segmentation granularity of Chinese\nPLMs. We propose a mixed-granularity Chinese BERT (MigBERT) by considering both\ncharacters and words. To achieve this, we design objective functions for\nlearning both character and word-level representations. We conduct extensive\nexperiments on various Chinese NLP tasks to evaluate existing PLMs as well as\nthe proposed MigBERT. Experimental results show that MigBERT achieves new SOTA\nperformance on all these tasks. Further analysis demonstrates that words are\nsemantically richer than characters. More interestingly, we show that MigBERT\nalso works with Japanese. Our code has been released\nhere~\\footnote{\\url{https://github.com/xnliang98/MigBERT}} and you can download\nour model here~\\footnote{\\url{https://huggingface.co/xnliang/MigBERT-large/}}.\n","authors":["Xinnian Liang","Shuangzhi Wu","Hui Huang","Jiaqi Bai","Chao Bian","Zhoujun Li"],"pdf_url":"https://arxiv.org/pdf/2303.13065v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2303.13035v1","updated":"2023-03-23T04:47:46Z","published":"2023-03-23T04:47:46Z","title":"SPeC: A Soft Prompt-Based Calibration on Mitigating Performance\n  Variability in Clinical Notes Summarization","summary":"  Electronic health records (EHRs) store an extensive array of patient\ninformation, encompassing medical histories, diagnoses, treatments, and test\noutcomes. These records are crucial for enabling healthcare providers to make\nwell-informed decisions regarding patient care. Summarizing clinical notes\nfurther assists healthcare professionals in pinpointing potential health risks\nand making better-informed decisions. This process contributes to reducing\nerrors and enhancing patient outcomes by ensuring providers have access to the\nmost pertinent and current patient data. Recent research has shown that\nincorporating prompts with large language models (LLMs) substantially boosts\nthe efficacy of summarization tasks. However, we show that this approach also\nleads to increased output variance, resulting in notably divergent outputs even\nwhen prompts share similar meanings. To tackle this challenge, we introduce a\nmodel-agnostic Soft Prompt-Based Calibration (SPeC) pipeline that employs soft\nprompts to diminish variance while preserving the advantages of prompt-based\nsummarization. Experimental findings on multiple clinical note tasks and LLMs\nindicate that our method not only bolsters performance but also effectively\ncurbs variance for various LLMs, providing a more uniform and dependable\nsolution for summarizing vital medical information.\n","authors":["Yu-Neng Chuang","Ruixiang Tang","Xiaoqian Jiang","Xia Hu"],"pdf_url":"https://arxiv.org/pdf/2303.13035v1.pdf","comment":null},{"id":"http://arxiv.org/abs/1908.04531v2","updated":"2023-03-23T04:24:09Z","published":"2019-08-13T08:29:48Z","title":"Offensive Language and Hate Speech Detection for Danish","summary":"  The presence of offensive language on social media platforms and the\nimplications this poses is becoming a major concern in modern society. Given\nthe enormous amount of content created every day, automatic methods are\nrequired to detect and deal with this type of content. Until now, most of the\nresearch has focused on solving the problem for the English language, while the\nproblem is multilingual.\n  We construct a Danish dataset containing user-generated comments from\n\\textit{Reddit} and \\textit{Facebook}. It contains user generated comments from\nvarious social media platforms, and to our knowledge, it is the first of its\nkind. Our dataset is annotated to capture various types and target of offensive\nlanguage. We develop four automatic classification systems, each designed to\nwork for both the English and the Danish language. In the detection of\noffensive language in English, the best performing system achieves a macro\naveraged F1-score of $0.74$, and the best performing system for Danish achieves\na macro averaged F1-score of $0.70$. In the detection of whether or not an\noffensive post is targeted, the best performing system for English achieves a\nmacro averaged F1-score of $0.62$, while the best performing system for Danish\nachieves a macro averaged F1-score of $0.73$. Finally, in the detection of the\ntarget type in a targeted offensive post, the best performing system for\nEnglish achieves a macro averaged F1-score of $0.56$, and the best performing\nsystem for Danish achieves a macro averaged F1-score of $0.63$.\n  Our work for both the English and the Danish language captures the type and\ntargets of offensive language, and present automatic methods for detecting\ndifferent kinds of offensive language such as hate speech and cyberbullying.\n","authors":["Gudbjartur Ingi Sigurbergsson","Leon Derczynski"],"pdf_url":"https://arxiv.org/pdf/1908.04531v2.pdf","comment":"Proceedings of the Twelfth Language Resources and Evaluation\n  Conference"},{"id":"http://arxiv.org/abs/2012.06431v2","updated":"2023-03-23T04:22:08Z","published":"2020-12-11T15:46:15Z","title":"Discriminating Between Similar Nordic Languages","summary":"  Automatic language identification is a challenging problem. Discriminating\nbetween closely related languages is especially difficult. This paper presents\na machine learning approach for automatic language identification for the\nNordic languages, which often suffer miscategorisation by existing\nstate-of-the-art tools. Concretely we will focus on discrimination between six\nNordic languages: Danish, Swedish, Norwegian (Nynorsk), Norwegian (Bokm{\\aa}l),\nFaroese and Icelandic.\n","authors":["René Haas","Leon Derczynski"],"pdf_url":"https://arxiv.org/pdf/2012.06431v2.pdf","comment":"Proceedings of the Eighth Workshop on NLP for Similar Languages,\n  Varieties and Dialects"},{"id":"http://arxiv.org/abs/2303.09892v2","updated":"2023-03-23T03:52:05Z","published":"2023-03-17T11:13:30Z","title":"Memotion 3: Dataset on Sentiment and Emotion Analysis of Codemixed\n  Hindi-English Memes","summary":"  Memes are the new-age conveyance mechanism for humor on social media sites.\nMemes often include an image and some text. Memes can be used to promote\ndisinformation or hatred, thus it is crucial to investigate in details. We\nintroduce Memotion 3, a new dataset with 10,000 annotated memes. Unlike other\nprevalent datasets in the domain, including prior iterations of Memotion,\nMemotion 3 introduces Hindi-English Codemixed memes while prior works in the\narea were limited to only the English memes. We describe the Memotion task, the\ndata collection and the dataset creation methodologies. We also provide a\nbaseline for the task. The baseline code and dataset will be made available at\nhttps://github.com/Shreyashm16/Memotion-3.0\n","authors":["Shreyash Mishra","S Suryavardan","Parth Patwa","Megha Chakraborty","Anku Rani","Aishwarya Reganti","Aman Chadha","Amitava Das","Amit Sheth","Manoj Chinnakotla","Asif Ekbal","Srijan Kumar"],"pdf_url":"https://arxiv.org/pdf/2303.09892v2.pdf","comment":"Defactify2 @AAAI"},{"id":"http://arxiv.org/abs/2303.13013v1","updated":"2023-03-23T03:30:30Z","published":"2023-03-23T03:30:30Z","title":"GesGPT: Speech Gesture Synthesis With Text Parsing from GPT","summary":"  Gesture synthesis has gained significant attention as a critical research\narea, focusing on producing contextually appropriate and natural gestures\ncorresponding to speech or textual input. Although deep learning-based\napproaches have achieved remarkable progress, they often overlook the rich\nsemantic information present in the text, leading to less expressive and\nmeaningful gestures. We propose GesGPT, a novel approach to gesture generation\nthat leverages the semantic analysis capabilities of Large Language Models\n(LLMs), such as GPT. By capitalizing on the strengths of LLMs for text\nanalysis, we design prompts to extract gesture-related information from textual\ninput. Our method entails developing prompt principles that transform gesture\ngeneration into an intention classification problem based on GPT, and utilizing\na curated gesture library and integration module to produce semantically rich\nco-speech gestures. Experimental results demonstrate that GesGPT effectively\ngenerates contextually appropriate and expressive gestures, offering a new\nperspective on semantic co-speech gesture generation.\n","authors":["Nan Gao","Zeyu Zhao","Zhi Zeng","Shuwu Zhang","Dongdong Weng"],"pdf_url":"https://arxiv.org/pdf/2303.13013v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13001v1","updated":"2023-03-23T02:50:38Z","published":"2023-03-23T02:50:38Z","title":"Is ChatGPT A Good Keyphrase Generator? A Preliminary Study","summary":"  The emergence of ChatGPT has recently garnered significant attention from the\ncomputational linguistics community. To demonstrate its capabilities as a\nkeyphrase generator, we conduct a preliminary evaluation of ChatGPT for the\nkeyphrase generation task. We evaluate its performance in various aspects,\nincluding keyphrase generation prompts, keyphrase generation diversity,\nmulti-domain keyphrase generation, and long document understanding. Our\nevaluation is based on six benchmark datasets, and we adopt the prompt\nsuggested by OpenAI while extending it to six candidate prompts. We find that\nChatGPT performs exceptionally well on all six candidate prompts, with minor\nperformance differences observed across the datasets. Based on our findings, we\nconclude that ChatGPT has great potential for keyphrase generation. Moreover,\nwe discover that ChatGPT still faces challenges when it comes to generating\nabsent keyphrases. Meanwhile, in the final section, we also present some\nlimitations and future expansions of this report.\n","authors":["Mingyang Song","Haiyun Jiang","Shuming Shi","Songfang Yao","Shilong Lu","Yi Feng","Huafeng Liu","Liping Jing"],"pdf_url":"https://arxiv.org/pdf/2303.13001v1.pdf","comment":"Technical Report, 7 pages"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2303.13519v1","updated":"2023-03-23T17:59:54Z","published":"2023-03-23T17:59:54Z","title":"Learning and Verification of Task Structure in Instructional Videos","summary":"  Given the enormous number of instructional videos available online, learning\na diverse array of multi-step task models from videos is an appealing goal. We\nintroduce a new pre-trained video model, VideoTaskformer, focused on\nrepresenting the semantics and structure of instructional videos. We pre-train\nVideoTaskformer using a simple and effective objective: predicting weakly\nsupervised textual labels for steps that are randomly masked out from an\ninstructional video (masked step modeling). Compared to prior work which learns\nstep representations locally, our approach involves learning them globally,\nleveraging video of the entire surrounding task as context. From these learned\nrepresentations, we can verify if an unseen video correctly executes a given\ntask, as well as forecast which steps are likely to be taken after a given\nstep. We introduce two new benchmarks for detecting mistakes in instructional\nvideos, to verify if there is an anomalous step and if steps are executed in\nthe right order. We also introduce a long-term forecasting benchmark, where the\ngoal is to predict long-range future steps from a given step. Our method\noutperforms previous baselines on these tasks, and we believe the tasks will be\na valuable way for the community to measure the quality of step\nrepresentations. Additionally, we evaluate VideoTaskformer on 3 existing\nbenchmarks -- procedural activity recognition, step classification, and step\nforecasting -- and demonstrate on each that our method outperforms existing\nbaselines and achieves new state-of-the-art performance.\n","authors":["Medhini Narasimhan","Licheng Yu","Sean Bell","Ning Zhang","Trevor Darrell"],"pdf_url":"https://arxiv.org/pdf/2303.13519v1.pdf","comment":"Wesbite at https://medhini.github.io/task_structure"},{"id":"http://arxiv.org/abs/2303.13518v1","updated":"2023-03-23T17:59:53Z","published":"2023-03-23T17:59:53Z","title":"Three ways to improve feature alignment for open vocabulary detection","summary":"  The core problem in zero-shot open vocabulary detection is how to align\nvisual and text features, so that the detector performs well on unseen classes.\nPrevious approaches train the feature pyramid and detection head from scratch,\nwhich breaks the vision-text feature alignment established during pretraining,\nand struggles to prevent the language model from forgetting unseen classes.\n  We propose three methods to alleviate these issues. Firstly, a simple scheme\nis used to augment the text embeddings which prevents overfitting to a small\nnumber of classes seen during training, while simultaneously saving memory and\ncomputation. Secondly, the feature pyramid network and the detection head are\nmodified to include trainable gated shortcuts, which encourages vision-text\nfeature alignment and guarantees it at the start of detection training.\nFinally, a self-training approach is used to leverage a larger corpus of\nimage-text pairs thus improving detection performance on classes with no human\nannotated bounding boxes.\n  Our three methods are evaluated on the zero-shot version of the LVIS\nbenchmark, each of them showing clear and significant benefits. Our final\nnetwork achieves the new stateof-the-art on the mAP-all metric and demonstrates\ncompetitive performance for mAP-rare, as well as superior transfer to COCO and\nObjects365.\n","authors":["Relja Arandjelović","Alex Andonian","Arthur Mensch","Olivier J. Hénaff","Jean-Baptiste Alayrac","Andrew Zisserman"],"pdf_url":"https://arxiv.org/pdf/2303.13518v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13516v1","updated":"2023-03-23T17:59:42Z","published":"2023-03-23T17:59:42Z","title":"Ablating Concepts in Text-to-Image Diffusion Models","summary":"  Large-scale text-to-image diffusion models can generate high-fidelity images\nwith powerful compositional ability. However, these models are typically\ntrained on an enormous amount of Internet data, often containing copyrighted\nmaterial, licensed images, and personal photos. Furthermore, they have been\nfound to replicate the style of various living artists or memorize exact\ntraining samples. How can we remove such copyrighted concepts or images without\nretraining the model from scratch? To achieve this goal, we propose an\nefficient method of ablating concepts in the pretrained model, i.e., preventing\nthe generation of a target concept. Our algorithm learns to match the image\ndistribution for a target style, instance, or text prompt we wish to ablate to\nthe distribution corresponding to an anchor concept. This prevents the model\nfrom generating target concepts given its text condition. Extensive experiments\nshow that our method can successfully prevent the generation of the ablated\nconcept while preserving closely related concepts in the model.\n","authors":["Nupur Kumari","Bingliang Zhang","Sheng-Yu Wang","Eli Shechtman","Richard Zhang","Jun-Yan Zhu"],"pdf_url":"https://arxiv.org/pdf/2303.13516v1.pdf","comment":"project website: https://www.cs.cmu.edu/~concept-ablation/"},{"id":"http://arxiv.org/abs/2303.13515v1","updated":"2023-03-23T17:59:40Z","published":"2023-03-23T17:59:40Z","title":"Persistent Nature: A Generative Model of Unbounded 3D Worlds","summary":"  Despite increasingly realistic image quality, recent 3D image generative\nmodels often operate on 3D volumes of fixed extent with limited camera motions.\nWe investigate the task of unconditionally synthesizing unbounded nature\nscenes, enabling arbitrarily large camera motion while maintaining a persistent\n3D world model. Our scene representation consists of an extendable, planar\nscene layout grid, which can be rendered from arbitrary camera poses via a 3D\ndecoder and volume rendering, and a panoramic skydome. Based on this\nrepresentation, we learn a generative world model solely from single-view\ninternet photos. Our method enables simulating long flights through 3D\nlandscapes, while maintaining global scene consistency--for instance, returning\nto the starting point yields the same view of the scene. Our approach enables\nscene extrapolation beyond the fixed bounds of current 3D generative models,\nwhile also supporting a persistent, camera-independent world representation\nthat stands in contrast to auto-regressive 3D prediction models. Our project\npage: https://chail.github.io/persistent-nature/.\n","authors":["Lucy Chai","Richard Tucker","Zhengqi Li","Phillip Isola","Noah Snavely"],"pdf_url":"https://arxiv.org/pdf/2303.13515v1.pdf","comment":"CVPR camera ready version, project page:\n  https://chail.github.io/persistent-nature/"},{"id":"http://arxiv.org/abs/2303.13514v1","updated":"2023-03-23T17:59:35Z","published":"2023-03-23T17:59:35Z","title":"SAOR: Single-View Articulated Object Reconstruction","summary":"  We introduce SAOR, a novel approach for estimating the 3D shape, texture, and\nviewpoint of an articulated object from a single image captured in the wild.\nUnlike prior approaches that rely on pre-defined category-specific 3D templates\nor tailored 3D skeletons, SAOR learns to articulate shapes from single-view\nimage collections with a skeleton-free part-based model without requiring any\n3D object shape priors. To prevent ill-posed solutions, we propose a\ncross-instance consistency loss that exploits disentangled object shape\ndeformation and articulation. This is helped by a new silhouette-based sampling\nmechanism to enhance viewpoint diversity during training. Our method only\nrequires estimated object silhouettes and relative depth maps from\noff-the-shelf pre-trained networks during training. At inference time, given a\nsingle-view image, it efficiently outputs an explicit mesh representation. We\nobtain improved qualitative and quantitative results on challenging quadruped\nanimals compared to relevant existing work.\n","authors":["Mehmet Aygün","Oisin Mac Aodha"],"pdf_url":"https://arxiv.org/pdf/2303.13514v1.pdf","comment":"https://mehmetaygun.github.io/saor"},{"id":"http://arxiv.org/abs/2303.13511v1","updated":"2023-03-23T17:59:10Z","published":"2023-03-23T17:59:10Z","title":"Neural Preset for Color Style Transfer","summary":"  In this paper, we present a Neural Preset technique to address the\nlimitations of existing color style transfer methods, including visual\nartifacts, vast memory requirement, and slow style switching speed. Our method\nis based on two core designs. First, we propose Deterministic Neural Color\nMapping (DNCM) to consistently operate on each pixel via an image-adaptive\ncolor mapping matrix, avoiding artifacts and supporting high-resolution inputs\nwith a small memory footprint. Second, we develop a two-stage pipeline by\ndividing the task into color normalization and stylization, which allows\nefficient style switching by extracting color styles as presets and reusing\nthem on normalized input images. Due to the unavailability of pairwise\ndatasets, we describe how to train Neural Preset via a self-supervised\nstrategy. Various advantages of Neural Preset over existing methods are\ndemonstrated through comprehensive evaluations. Besides, we show that our\ntrained model can naturally support multiple applications without fine-tuning,\nincluding low-light image enhancement, underwater image correction, image\ndehazing, and image harmonization.\n","authors":["Zhanghan Ke","Yuhao Liu","Lei Zhu","Nanxuan Zhao","Rynson W. H. Lau"],"pdf_url":"https://arxiv.org/pdf/2303.13511v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13509v1","updated":"2023-03-23T17:59:02Z","published":"2023-03-23T17:59:02Z","title":"Position-Guided Point Cloud Panoptic Segmentation Transformer","summary":"  DEtection TRansformer (DETR) started a trend that uses a group of learnable\nqueries for unified visual perception. This work begins by applying this\nappealing paradigm to LiDAR-based point cloud segmentation and obtains a simple\nyet effective baseline. Although the naive adaptation obtains fair results, the\ninstance segmentation performance is noticeably inferior to previous works. By\ndiving into the details, we observe that instances in the sparse point clouds\nare relatively small to the whole scene and often have similar geometry but\nlack distinctive appearance for segmentation, which are rare in the image\ndomain. Considering instances in 3D are more featured by their positional\ninformation, we emphasize their roles during the modeling and design a robust\nMixed-parameterized Positional Embedding (MPE) to guide the segmentation\nprocess. It is embedded into backbone features and later guides the mask\nprediction and query update processes iteratively, leading to Position-Aware\nSegmentation (PA-Seg) and Masked Focal Attention (MFA). All these designs impel\nthe queries to attend to specific regions and identify various instances. The\nmethod, named Position-guided Point cloud Panoptic segmentation transFormer\n(P3Former), outperforms previous state-of-the-art methods by 3.4% and 1.2% PQ\non SemanticKITTI and nuScenes benchmark, respectively. The source code and\nmodels are available at https://github.com/SmartBot-PJLab/P3Former .\n","authors":["Zeqi Xiao","Wenwei Zhang","Tai Wang","Chen Change Loy","Dahua Lin","Jiangmiao Pang"],"pdf_url":"https://arxiv.org/pdf/2303.13509v1.pdf","comment":"Project page: https://github.com/SmartBot-PJLab/P3Former"},{"id":"http://arxiv.org/abs/2303.13510v1","updated":"2023-03-23T17:59:02Z","published":"2023-03-23T17:59:02Z","title":"MV-JAR: Masked Voxel Jigsaw and Reconstruction for LiDAR-Based\n  Self-Supervised Pre-Training","summary":"  This paper introduces the Masked Voxel Jigsaw and Reconstruction (MV-JAR)\nmethod for LiDAR-based self-supervised pre-training and a carefully designed\ndata-efficient 3D object detection benchmark on the Waymo dataset. Inspired by\nthe scene-voxel-point hierarchy in downstream 3D object detectors, we design\nmasking and reconstruction strategies accounting for voxel distributions in the\nscene and local point distributions within the voxel. We employ a\nReversed-Furthest-Voxel-Sampling strategy to address the uneven distribution of\nLiDAR points and propose MV-JAR, which combines two techniques for modeling the\naforementioned distributions, resulting in superior performance. Our\nexperiments reveal limitations in previous data-efficient experiments, which\nuniformly sample fine-tuning splits with varying data proportions from each\nLiDAR sequence, leading to similar data diversity across splits. To address\nthis, we propose a new benchmark that samples scene sequences for diverse\nfine-tuning splits, ensuring adequate model convergence and providing a more\naccurate evaluation of pre-training methods. Experiments on our Waymo benchmark\nand the KITTI dataset demonstrate that MV-JAR consistently and significantly\nimproves 3D detection performance across various data scales, achieving up to a\n6.3% increase in mAPH compared to training from scratch. Codes and the\nbenchmark will be available at https://github.com/SmartBot-PJLab/MV-JAR .\n","authors":["Runsen Xu","Tai Wang","Wenwei Zhang","Runjian Chen","Jinkun Cao","Jiangmiao Pang","Dahua Lin"],"pdf_url":"https://arxiv.org/pdf/2303.13510v1.pdf","comment":"Accepted by CVPR 2023 with a carefully designed benchmark on Waymo.\n  Codes and the benchmark will be available at\n  https://github.com/SmartBot-PJLab/MV-JAR"},{"id":"http://arxiv.org/abs/2303.13508v1","updated":"2023-03-23T17:59:00Z","published":"2023-03-23T17:59:00Z","title":"DreamBooth3D: Subject-Driven Text-to-3D Generation","summary":"  We present DreamBooth3D, an approach to personalize text-to-3D generative\nmodels from as few as 3-6 casually captured images of a subject. Our approach\ncombines recent advances in personalizing text-to-image models (DreamBooth)\nwith text-to-3D generation (DreamFusion). We find that naively combining these\nmethods fails to yield satisfactory subject-specific 3D assets due to\npersonalized text-to-image models overfitting to the input viewpoints of the\nsubject. We overcome this through a 3-stage optimization strategy where we\njointly leverage the 3D consistency of neural radiance fields together with the\npersonalization capability of text-to-image models. Our method can produce\nhigh-quality, subject-specific 3D assets with text-driven modifications such as\nnovel poses, colors and attributes that are not seen in any of the input images\nof the subject.\n","authors":["Amit Raj","Srinivas Kaza","Ben Poole","Michael Niemeyer","Nataniel Ruiz","Ben Mildenhall","Shiran Zada","Kfir Aberman","Michael Rubinstein","Jonathan Barron","Yuanzhen Li","Varun Jampani"],"pdf_url":"https://arxiv.org/pdf/2303.13508v1.pdf","comment":"Project page at https://dreambooth3d.github.io/ Video Summary at\n  https://youtu.be/TFtaoAqSkEA"},{"id":"http://arxiv.org/abs/2303.13504v1","updated":"2023-03-23T17:58:05Z","published":"2023-03-23T17:58:05Z","title":"ReBotNet: Fast Real-time Video Enhancement","summary":"  Most video restoration networks are slow, have high computational load, and\ncan't be used for real-time video enhancement. In this work, we design an\nefficient and fast framework to perform real-time video enhancement for\npractical use-cases like live video calls and video streams. Our proposed\nmethod, called Recurrent Bottleneck Mixer Network (ReBotNet), employs a\ndual-branch framework. The first branch learns spatio-temporal features by\ntokenizing the input frames along the spatial and temporal dimensions using a\nConvNext-based encoder and processing these abstract tokens using a bottleneck\nmixer. To further improve temporal consistency, the second branch employs a\nmixer directly on tokens extracted from individual frames. A common decoder\nthen merges the features form the two branches to predict the enhanced frame.\nIn addition, we propose a recurrent training approach where the last frame's\nprediction is leveraged to efficiently enhance the current frame while\nimproving temporal consistency. To evaluate our method, we curate two new\ndatasets that emulate real-world video call and streaming scenarios, and show\nextensive results on multiple datasets where ReBotNet outperforms existing\napproaches with lower computations, reduced memory requirements, and faster\ninference time.\n","authors":["Jeya Maria Jose Valanarasu","Rahul Garg","Andeep Toor","Xin Tong","Weijuan Xi","Andreas Lugmayr","Vishal M. Patel","Anne Menini"],"pdf_url":"https://arxiv.org/pdf/2303.13504v1.pdf","comment":"Project Website: https://jeya-maria-jose.github.io/rebotnet-web/"},{"id":"http://arxiv.org/abs/2303.13505v1","updated":"2023-03-23T17:58:05Z","published":"2023-03-23T17:58:05Z","title":"A Large-scale Study of Spatiotemporal Representation Learning with a New\n  Benchmark on Action Recognition","summary":"  The goal of building a benchmark (suite of datasets) is to provide a unified\nprotocol for fair evaluation and thus facilitate the evolution of a specific\narea. Nonetheless, we point out that existing protocols of action recognition\ncould yield partial evaluations due to several limitations. To comprehensively\nprobe the effectiveness of spatiotemporal representation learning, we introduce\nBEAR, a new BEnchmark on video Action Recognition. BEAR is a collection of 18\nvideo datasets grouped into 5 categories (anomaly, gesture, daily, sports, and\ninstructional), which covers a diverse set of real-world applications. With\nBEAR, we thoroughly evaluate 6 common spatiotemporal models pre-trained by both\nsupervised and self-supervised learning. We also report transfer performance\nvia standard finetuning, few-shot finetuning, and unsupervised domain\nadaptation. Our observation suggests that current state-of-the-art cannot\nsolidly guarantee high performance on datasets close to real-world\napplications, and we hope BEAR can serve as a fair and challenging evaluation\nbenchmark to gain insights on building next-generation spatiotemporal learners.\nOur dataset, code, and models are released at:\nhttps://github.com/AndongDeng/BEAR\n","authors":["Andong Deng","Taojiannan Yang","Chen Chen"],"pdf_url":"https://arxiv.org/pdf/2303.13505v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13501v1","updated":"2023-03-23T17:57:28Z","published":"2023-03-23T17:57:28Z","title":"Chordal Averaging on Flag Manifolds and Its Applications","summary":"  This paper presents a new, provably-convergent algorithm for computing the\nflag-mean and flag-median of a set of points on a flag manifold under the\nchordal metric. The flag manifold is a mathematical space consisting of flags,\nwhich are sequences of nested subspaces of a vector space that increase in\ndimension. The flag manifold is a superset of a wide range of known matrix\ngroups, including Stiefel and Grassmanians, making it a general object that is\nuseful in a wide variety computer vision problems.\n  To tackle the challenge of computing first order flag statistics, we first\ntransform the problem into one that involves auxiliary variables constrained to\nthe Stiefel manifold. The Stiefel manifold is a space of orthogonal frames, and\nleveraging the numerical stability and efficiency of Stiefel-manifold\noptimization enables us to compute the flag-mean effectively. Through a series\nof experiments, we show the competence of our method in Grassmann and rotation\naveraging, as well as principal component analysis.\n","authors":["Nathan Mankovich","Tolga Birdal"],"pdf_url":"https://arxiv.org/pdf/2303.13501v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.05845v2","updated":"2023-03-23T17:56:52Z","published":"2022-08-11T14:28:21Z","title":"A Comprehensive Analysis of AI Biases in DeepFake Detection With\n  Massively Annotated Databases","summary":"  In recent years, image and video manipulations with Deepfake have become a\nsevere concern for security and society. Many detection models and datasets\nhave been proposed to detect Deepfake data reliably. However, there is an\nincreased concern that these models and training databases might be biased and,\nthus, cause Deepfake detectors to fail. In this work, we investigate the bias\nissue caused by public Deepfake datasets by (a) providing large-scale\ndemographic and non-demographic attribute annotations of 47 different\nattributes for five popular Deepfake datasets and (b) comprehensively analysing\nAI-bias of three state-of-the-art Deepfake detection backbone models on these\ndatasets. The investigation analyses the influence of a large variety of\ndistinctive attributes (from over 65M labels) on the detection performance,\nincluding demographic (age, gender, ethnicity) and non-demographic (hair, skin,\naccessories, etc.) information. The results indicate that investigated\ndatabases lack diversity and, more importantly, show that the utilised Deepfake\ndetection backbone models are strongly biased towards many investigated\nattributes. The Deepfake detection backbone methods, which are trained with\nbiased datasets, might output incorrect detection results, thereby leading to\ngeneralisability, fairness, and security issues. We hope that the findings of\nthis study and the annotation databases will help to evaluate and mitigate bias\nin future Deepfake detection techniques. The annotation datasets are publicly\navailable.\n","authors":["Ying Xu","Philipp Terhörst","Kiran Raja","Marius Pedersen"],"pdf_url":"https://arxiv.org/pdf/2208.05845v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13497v1","updated":"2023-03-23T17:56:20Z","published":"2023-03-23T17:56:20Z","title":"TriPlaneNet: An Encoder for EG3D Inversion","summary":"  Recent progress in NeRF-based GANs has introduced a number of approaches for\nhigh-resolution and high-fidelity generative modeling of human heads with a\npossibility for novel view rendering. At the same time, one must solve an\ninverse problem to be able to re-render or modify an existing image or video.\nDespite the success of universal optimization-based methods for 2D GAN\ninversion, those, applied to 3D GANs, may fail to produce 3D-consistent\nrenderings. Fast encoder-based techniques, such as those developed for\nStyleGAN, may also be less appealing due to the lack of identity preservation.\nIn our work, we introduce a real-time method that bridges the gap between the\ntwo approaches by directly utilizing the tri-plane representation introduced\nfor EG3D generative model. In particular, we build upon a feed-forward\nconvolutional encoder for the latent code and extend it with a\nfully-convolutional predictor of tri-plane numerical offsets. As shown in our\nwork, the renderings are similar in quality to optimization-based techniques\nand significantly outperform the baselines for novel view. As we empirically\nprove, this is a consequence of directly operating in the tri-plane space, not\nin the GAN parameter space, while making use of an encoder-based trainable\napproach.\n","authors":["Ananta R. Bhattarai","Matthias Nießner","Artem Sevastopolsky"],"pdf_url":"https://arxiv.org/pdf/2303.13497v1.pdf","comment":"Video: https://youtu.be/GpmSswHMeWU Project page:\n  https://anantarb.github.io/triplanenet"},{"id":"http://arxiv.org/abs/2303.13496v1","updated":"2023-03-23T17:56:12Z","published":"2023-03-23T17:56:12Z","title":"The effectiveness of MAE pre-pretraining for billion-scale pretraining","summary":"  This paper revisits the standard pretrain-then-finetune paradigm used in\ncomputer vision for visual recognition tasks. Typically, state-of-the-art\nfoundation models are pretrained using large scale (weakly) supervised datasets\nwith billions of images. We introduce an additional pre-pretraining stage that\nis simple and uses the self-supervised MAE technique to initialize the model.\nWhile MAE has only been shown to scale with the size of models, we find that it\nscales with the size of the training dataset as well. Thus, our MAE-based\npre-pretraining scales with both model and data size making it applicable for\ntraining foundation models. Pre-pretraining consistently improves both the\nmodel convergence and the downstream transfer performance across a range of\nmodel scales (millions to billions of parameters), and dataset sizes (millions\nto billions of images). We measure the effectiveness of pre-pretraining on 10\ndifferent visual recognition tasks spanning image classification, video\nrecognition, object detection, low-shot classification and zero-shot\nrecognition. Our largest model achieves new state-of-the-art results on\niNaturalist-18 (91.3%), 1-shot ImageNet-1k (62.1%), and zero-shot transfer on\nFood-101 (96.0%). Our study reveals that model initialization plays a\nsignificant role, even for web-scale pretraining with billions of images.\n","authors":["Mannat Singh","Quentin Duval","Kalyan Vasudev Alwala","Haoqi Fan","Vaibhav Aggarwal","Aaron Adcock","Armand Joulin","Piotr Dollár","Christoph Feichtenhofer","Ross Girshick","Rohit Girdhar","Ishan Misra"],"pdf_url":"https://arxiv.org/pdf/2303.13496v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13495v1","updated":"2023-03-23T17:56:10Z","published":"2023-03-23T17:56:10Z","title":"ReVersion: Diffusion-Based Relation Inversion from Images","summary":"  Diffusion models gain increasing popularity for their generative\ncapabilities. Recently, there have been surging needs to generate customized\nimages by inverting diffusion models from exemplar images. However, existing\ninversion methods mainly focus on capturing object appearances. How to invert\nobject relations, another important pillar in the visual world, remains\nunexplored. In this work, we propose ReVersion for the Relation Inversion task,\nwhich aims to learn a specific relation (represented as \"relation prompt\") from\nexemplar images. Specifically, we learn a relation prompt from a frozen\npre-trained text-to-image diffusion model. The learned relation prompt can then\nbe applied to generate relation-specific images with new objects, backgrounds,\nand styles. Our key insight is the \"preposition prior\" - real-world relation\nprompts can be sparsely activated upon a set of basis prepositional words.\nSpecifically, we propose a novel relation-steering contrastive learning scheme\nto impose two critical properties of the relation prompt: 1) The relation\nprompt should capture the interaction between objects, enforced by the\npreposition prior. 2) The relation prompt should be disentangled away from\nobject appearances. We further devise relation-focal importance sampling to\nemphasize high-level interactions over low-level appearances (e.g., texture,\ncolor). To comprehensively evaluate this new task, we contribute ReVersion\nBenchmark, which provides various exemplar images with diverse relations.\nExtensive experiments validate the superiority of our approach over existing\nmethods across a wide range of visual relations.\n","authors":["Ziqi Huang","Tianxing Wu","Yuming Jiang","Kelvin C. K. Chan","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2303.13495v1.pdf","comment":"First two authors contributed equally. Project page:\n  https://ziqihuangg.github.io/projects/reversion.html Code:\n  https://github.com/ziqihuangg/ReVersion"},{"id":"http://arxiv.org/abs/2303.13483v1","updated":"2023-03-23T17:50:40Z","published":"2023-03-23T17:50:40Z","title":"NS3D: Neuro-Symbolic Grounding of 3D Objects and Relations","summary":"  Grounding object properties and relations in 3D scenes is a prerequisite for\na wide range of artificial intelligence tasks, such as visually grounded\ndialogues and embodied manipulation. However, the variability of the 3D domain\ninduces two fundamental challenges: 1) the expense of labeling and 2) the\ncomplexity of 3D grounded language. Hence, essential desiderata for models are\nto be data-efficient, generalize to different data distributions and tasks with\nunseen semantic forms, as well as ground complex language semantics (e.g.,\nview-point anchoring and multi-object reference). To address these challenges,\nwe propose NS3D, a neuro-symbolic framework for 3D grounding. NS3D translates\nlanguage into programs with hierarchical structures by leveraging large\nlanguage-to-code models. Different functional modules in the programs are\nimplemented as neural networks. Notably, NS3D extends prior neuro-symbolic\nvisual reasoning methods by introducing functional modules that effectively\nreason about high-arity relations (i.e., relations among more than two\nobjects), key in disambiguating objects in complex 3D scenes. Modular and\ncompositional architecture enables NS3D to achieve state-of-the-art results on\nthe ReferIt3D view-dependence task, a 3D referring expression comprehension\nbenchmark. Importantly, NS3D shows significantly improved performance on\nsettings of data-efficiency and generalization, and demonstrate zero-shot\ntransfer to an unseen 3D question-answering task.\n","authors":["Joy Hsu","Jiayuan Mao","Jiajun Wu"],"pdf_url":"https://arxiv.org/pdf/2303.13483v1.pdf","comment":"In CVPR 2023"},{"id":"http://arxiv.org/abs/2303.13482v1","updated":"2023-03-23T17:50:09Z","published":"2023-03-23T17:50:09Z","title":"TactoFind: A Tactile Only System for Object Retrieval","summary":"  We study the problem of object retrieval in scenarios where visual sensing is\nabsent, object shapes are unknown beforehand and objects can move freely, like\ngrabbing objects out of a drawer. Successful solutions require localizing free\nobjects, identifying specific object instances, and then grasping the\nidentified objects, only using touch feedback. Unlike vision, where cameras can\nobserve the entire scene, touch sensors are local and only observe parts of the\nscene that are in contact with the manipulator. Moreover, information gathering\nvia touch sensors necessitates applying forces on the touched surface which may\ndisturb the scene itself. Reasoning with touch, therefore, requires careful\nexploration and integration of information over time -- a challenge we tackle.\nWe present a system capable of using sparse tactile feedback from fingertip\ntouch sensors on a dexterous hand to localize, identify and grasp novel objects\nwithout any visual feedback. Videos are available at\nhttps://taochenshh.github.io/projects/tactofind.\n","authors":["Sameer Pai","Tao Chen","Megha Tippur","Edward Adelson","Abhishek Gupta","Pulkit Agrawal"],"pdf_url":"https://arxiv.org/pdf/2303.13482v1.pdf","comment":"Accepted in ICRA 2023"},{"id":"http://arxiv.org/abs/2303.13479v1","updated":"2023-03-23T17:48:12Z","published":"2023-03-23T17:48:12Z","title":"Prior-free Category-level Pose Estimation with Implicit Space\n  Transformation","summary":"  Category-level 6D pose estimation aims to predict the poses and sizes of\nunseen objects from a specific category. Thanks to prior deformation, which\nexplicitly adapts a category-specific 3D prior (i.e., a 3D template) to a given\nobject instance, prior-based methods attained great success and have become a\nmajor research stream. However, obtaining category-specific priors requires\ncollecting a large amount of 3D models, which is labor-consuming and often not\naccessible in practice. This motivates us to investigate whether priors are\nnecessary to make prior-based methods effective. Our empirical study shows that\nthe 3D prior itself is not the credit to the high performance. The keypoint\nactually is the explicit deformation process, which aligns camera and world\ncoordinates supervised by world-space 3D models (also called canonical space).\nInspired by these observation, we introduce a simple prior-free implicit space\ntransformation network, namely IST-Net, to transform camera-space features to\nworld-space counterparts and build correspondence between them in an implicit\nmanner without relying on 3D priors. Besides, we design camera- and world-space\nenhancers to enrich the features with pose-sensitive information and\ngeometrical constraints, respectively. Albeit simple, IST-Net becomes the first\nprior-free method that achieves state-of-the-art performance, with top\ninference speed on the REAL275 dataset. Our code and models will be publicly\navailable.\n","authors":["Jianhui Liu","Yukang Chen","Xiaoqing Ye","Xiaojuan Qi"],"pdf_url":"https://arxiv.org/pdf/2303.13479v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13477v1","updated":"2023-03-23T17:46:54Z","published":"2023-03-23T17:46:54Z","title":"TransPoser: Transformer as an Optimizer for Joint Object Shape and Pose\n  Estimation","summary":"  We propose a novel method for joint estimation of shape and pose of rigid\nobjects from their sequentially observed RGB-D images. In sharp contrast to\npast approaches that rely on complex non-linear optimization, we propose to\nformulate it as a neural optimization that learns to efficiently estimate the\nshape and pose. We introduce Deep Directional Distance Function (DeepDDF), a\nneural network that directly outputs the depth image of an object given the\ncamera viewpoint and viewing direction, for efficient error computation in 2D\nimage space. We formulate the joint estimation itself as a Transformer which we\nrefer to as TransPoser. We fully leverage the tokenization and multi-head\nattention to sequentially process the growing set of observations and to\nefficiently update the shape and pose with a learned momentum, respectively.\nExperimental results on synthetic and real data show that DeepDDF achieves high\naccuracy as a category-level object shape representation and TransPoser\nachieves state-of-the-art accuracy efficiently for joint shape and pose\nestimation.\n","authors":["Yuta Yoshitake","Mai Nishimura","Shohei Nobuhara","Ko Nishino"],"pdf_url":"https://arxiv.org/pdf/2303.13477v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13472v1","updated":"2023-03-23T17:43:17Z","published":"2023-03-23T17:43:17Z","title":"Plotting Behind the Scenes: Towards Learnable Game Engines","summary":"  Game engines are powerful tools in computer graphics. Their power comes at\nthe immense cost of their development. In this work, we present a framework to\ntrain game-engine-like neural models, solely from monocular annotated videos.\nThe result-a Learnable Game Engine (LGE)-maintains states of the scene, objects\nand agents in it, and enables rendering the environment from a controllable\nviewpoint. Similarly to a game engine, it models the logic of the game and the\nunderlying rules of physics, to make it possible for a user to play the game by\nspecifying both high- and low-level action sequences. Most captivatingly, our\nLGE unlocks the director's mode, where the game is played by plotting behind\nthe scenes, specifying high-level actions and goals for the agents in the form\nof language and desired states. This requires learning \"game AI\", encapsulated\nby our animation model, to navigate the scene using high-level constraints,\nplay against an adversary, devise the strategy to win a point. The key to\nlearning such game AI is the exploitation of a large and diverse text corpus,\ncollected in this work, describing detailed actions in a game and used to train\nour animation model. To render the resulting state of the environment and its\nagents, we use a compositional NeRF representation used in our synthesis model.\nTo foster future research, we present newly collected, annotated and calibrated\nlarge-scale Tennis and Minecraft datasets. Our method significantly outperforms\nexisting neural video game simulators in terms of rendering quality. Besides,\nour LGEs unlock applications beyond capabilities of the current state of the\nart. Our framework, data, and models are available at\nhttps://learnable-game-engines.github.io/lge-website.\n","authors":["Willi Menapace","Aliaksandr Siarohin","Stéphane Lathuilière","Panos Achlioptas","Vladislav Golyanik","Elisa Ricci","Sergey Tulyakov"],"pdf_url":"https://arxiv.org/pdf/2303.13472v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13471v1","updated":"2023-03-23T17:43:11Z","published":"2023-03-23T17:43:11Z","title":"Egocentric Audio-Visual Object Localization","summary":"  Humans naturally perceive surrounding scenes by unifying sound and sight in a\nfirst-person view. Likewise, machines are advanced to approach human\nintelligence by learning with multisensory inputs from an egocentric\nperspective. In this paper, we explore the challenging egocentric audio-visual\nobject localization task and observe that 1) egomotion commonly exists in\nfirst-person recordings, even within a short duration; 2) The out-of-view sound\ncomponents can be created while wearers shift their attention. To address the\nfirst problem, we propose a geometry-aware temporal aggregation module to\nhandle the egomotion explicitly. The effect of egomotion is mitigated by\nestimating the temporal geometry transformation and exploiting it to update\nvisual representations. Moreover, we propose a cascaded feature enhancement\nmodule to tackle the second issue. It improves cross-modal localization\nrobustness by disentangling visually-indicated audio representation. During\ntraining, we take advantage of the naturally available audio-visual temporal\nsynchronization as the ``free'' self-supervision to avoid costly labeling. We\nalso annotate and create the Epic Sounding Object dataset for evaluation\npurposes. Extensive experiments show that our method achieves state-of-the-art\nlocalization performance in egocentric videos and can be generalized to diverse\naudio-visual scenes.\n","authors":["Chao Huang","Yapeng Tian","Anurag Kumar","Chenliang Xu"],"pdf_url":"https://arxiv.org/pdf/2303.13471v1.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2209.04634v2","updated":"2023-03-23T17:31:25Z","published":"2022-09-10T10:35:53Z","title":"Real-time event simulation with frame-based cameras","summary":"  Event cameras are becoming increasingly popular in robotics and computer\nvision due to their beneficial properties, e.g., high temporal resolution, high\nbandwidth, almost no motion blur, and low power consumption. However, these\ncameras remain expensive and scarce in the market, making them inaccessible to\nthe majority. Using event simulators minimizes the need for real event cameras\nto develop novel algorithms. However, due to the computational complexity of\nthe simulation, the event streams of existing simulators cannot be generated in\nreal-time but rather have to be pre-calculated from existing video sequences or\npre-rendered and then simulated from a virtual 3D scene. Although these offline\ngenerated event streams can be used as training data for learning tasks, all\nresponse time dependent applications cannot benefit from these simulators yet,\nas they still require an actual event camera. This work proposes simulation\nmethods that improve the performance of event simulation by two orders of\nmagnitude (making them real-time capable) while remaining competitive in the\nquality assessment.\n","authors":["Andreas Ziegler","Daniel Teigland","Jonas Tebbe","Thomas Gossard","Andreas Zell"],"pdf_url":"https://arxiv.org/pdf/2209.04634v2.pdf","comment":"Accepted for 2023 IEEE International Conference on Robotics and\n  Automation (ICRA 2023). Project web page:\n  https://cogsys-tuebingen.github.io/realtime_event_simulator/"},{"id":"http://arxiv.org/abs/2303.13455v1","updated":"2023-03-23T17:24:31Z","published":"2023-03-23T17:24:31Z","title":"CoBIT: A Contrastive Bi-directional Image-Text Generation Model","summary":"  The field of vision and language has witnessed a proliferation of pre-trained\nfoundation models. Most existing methods are independently pre-trained with\ncontrastive objective like CLIP, image-to-text generative objective like PaLI,\nor text-to-image generative objective like Parti. However, the three objectives\ncan be pre-trained on the same data, image-text pairs, and intuitively they\ncomplement each other as contrasting provides global alignment capacity and\ngeneration grants fine-grained understanding. In this work, we present a\nContrastive Bi-directional Image-Text generation model (CoBIT), which attempts\nto unify the three pre-training objectives in one framework. Specifically,\nCoBIT employs a novel unicoder-decoder structure, consisting of an image\nunicoder, a text unicoder and a cross-modal decoder. The image/text unicoders\ncan switch between encoding and decoding in different tasks, enabling\nflexibility and shared knowledge that benefits both image-to-text and\ntext-to-image generations. CoBIT achieves superior performance in image\nunderstanding, image-text understanding (Retrieval, Captioning, VQA, SNLI-VE)\nand text-based content creation, particularly in zero-shot scenarios. For\ninstance, 82.7% in zero-shot ImageNet classification, 9.37 FID score in\nzero-shot text-to-image generation and 44.8 CIDEr in zero-shot captioning.\n","authors":["Haoxuan You","Mandy Guo","Zhecan Wang","Kai-Wei Chang","Jason Baldridge","Jiahui Yu"],"pdf_url":"https://arxiv.org/pdf/2303.13455v1.pdf","comment":"14 pages, 5 figures"},{"id":"http://arxiv.org/abs/2303.13450v1","updated":"2023-03-23T17:17:29Z","published":"2023-03-23T17:17:29Z","title":"Set-the-Scene: Global-Local Training for Generating Controllable NeRF\n  Scenes","summary":"  Recent breakthroughs in text-guided image generation have led to remarkable\nprogress in the field of 3D synthesis from text. By optimizing neural radiance\nfields (NeRF) directly from text, recent methods are able to produce remarkable\nresults. Yet, these methods are limited in their control of each object's\nplacement or appearance, as they represent the scene as a whole. This can be a\nmajor issue in scenarios that require refining or manipulating objects in the\nscene. To remedy this deficit, we propose a novel GlobalLocal training\nframework for synthesizing a 3D scene using object proxies. A proxy represents\nthe object's placement in the generated scene and optionally defines its coarse\ngeometry. The key to our approach is to represent each object as an independent\nNeRF. We alternate between optimizing each NeRF on its own and as part of the\nfull scene. Thus, a complete representation of each object can be learned,\nwhile also creating a harmonious scene with style and lighting match. We show\nthat using proxies allows a wide variety of editing options, such as adjusting\nthe placement of each independent object, removing objects from a scene, or\nrefining an object. Our results show that Set-the-Scene offers a powerful\nsolution for scene synthesis and manipulation, filling a crucial gap in\ncontrollable text-to-3D synthesis.\n","authors":["Dana Cohen-Bar","Elad Richardson","Gal Metzer","Raja Giryes","Daniel Cohen-Or"],"pdf_url":"https://arxiv.org/pdf/2303.13450v1.pdf","comment":"project page at https://danacohen95.github.io/Set-the-Scene/"},{"id":"http://arxiv.org/abs/2303.13440v1","updated":"2023-03-23T17:02:00Z","published":"2023-03-23T17:02:00Z","title":"CLIP for All Things Zero-Shot Sketch-Based Image Retrieval, Fine-Grained\n  or Not","summary":"  In this paper, we leverage CLIP for zero-shot sketch based image retrieval\n(ZS-SBIR). We are largely inspired by recent advances on foundation models and\nthe unparalleled generalisation ability they seem to offer, but for the first\ntime tailor it to benefit the sketch community. We put forward novel designs on\nhow best to achieve this synergy, for both the category setting and the\nfine-grained setting (\"all\"). At the very core of our solution is a prompt\nlearning setup. First we show just via factoring in sketch-specific prompts, we\nalready have a category-level ZS-SBIR system that overshoots all prior arts, by\na large margin (24.8%) - a great testimony on studying the CLIP and ZS-SBIR\nsynergy. Moving onto the fine-grained setup is however trickier, and requires a\ndeeper dive into this synergy. For that, we come up with two specific designs\nto tackle the fine-grained matching nature of the problem: (i) an additional\nregularisation loss to ensure the relative separation between sketches and\nphotos is uniform across categories, which is not the case for the gold\nstandard standalone triplet loss, and (ii) a clever patch shuffling technique\nto help establishing instance-level structural correspondences between\nsketch-photo pairs. With these designs, we again observe significant\nperformance gains in the region of 26.9% over previous state-of-the-art. The\ntake-home message, if any, is the proposed CLIP and prompt learning paradigm\ncarries great promise in tackling other sketch-related tasks (not limited to\nZS-SBIR) where data scarcity remains a great challenge. Code and models will be\nmade available.\n","authors":["Aneeshan Sain","Ayan Kumar Bhunia","Pinaki Nath Chowdhury","Subhadeep Koley","Tao Xiang","Yi-Zhe Song"],"pdf_url":"https://arxiv.org/pdf/2303.13440v1.pdf","comment":"Accepted in Computer Vision and Pattern Recognition (CVPR), 2023"},{"id":"http://arxiv.org/abs/2303.13439v1","updated":"2023-03-23T17:01:59Z","published":"2023-03-23T17:01:59Z","title":"Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video\n  Generators","summary":"  Recent text-to-video generation approaches rely on computationally heavy\ntraining and require large-scale video datasets. In this paper, we introduce a\nnew task of zero-shot text-to-video generation and propose a low-cost approach\n(without any training or optimization) by leveraging the power of existing\ntext-to-image synthesis methods (e.g., Stable Diffusion), making them suitable\nfor the video domain.\n  Our key modifications include (i) enriching the latent codes of the generated\nframes with motion dynamics to keep the global scene and the background time\nconsistent; and (ii) reprogramming frame-level self-attention using a new\ncross-frame attention of each frame on the first frame, to preserve the\ncontext, appearance, and identity of the foreground object.\n  Experiments show that this leads to low overhead, yet high-quality and\nremarkably consistent video generation. Moreover, our approach is not limited\nto text-to-video synthesis but is also applicable to other tasks such as\nconditional and content-specialized video generation, and Video\nInstruct-Pix2Pix, i.e., instruction-guided video editing.\n  As experiments show, our method performs comparably or sometimes better than\nrecent approaches, despite not being trained on additional video data. Our code\nwill be open sourced at: https://github.com/Picsart-AI-Research/Text2Video-Zero .\n","authors":["Levon Khachatryan","Andranik Movsisyan","Vahram Tadevosyan","Roberto Henschel","Zhangyang Wang","Shant Navasardyan","Humphrey Shi"],"pdf_url":"https://arxiv.org/pdf/2303.13439v1.pdf","comment":"The project is available at:\n  https://github.com/Picsart-AI-Research/Text2Video-Zero"},{"id":"http://arxiv.org/abs/2302.06514v4","updated":"2023-03-23T16:58:41Z","published":"2023-02-13T16:49:27Z","title":"Multiple Appropriate Facial Reaction Generation in Dyadic Interaction\n  Settings: What, Why and How?","summary":"  According to the Stimulus Organism Response (SOR) theory, all human\nbehavioral reactions are stimulated by context, where people will process the\nreceived stimulus and produce an appropriate reaction. This implies that in a\nspecific context for a given input stimulus, a person can react differently\naccording to their internal state and other contextual factors. Analogously, in\ndyadic interactions, humans communicate using verbal and nonverbal cues, where\na broad spectrum of listeners' non-verbal reactions might be appropriate for\nresponding to a specific speaker behaviour. There already exists a body of work\nthat investigated the problem of automatically generating an appropriate\nreaction for a given input. However, none attempted to automatically generate\nmultiple appropriate reactions in the context of dyadic interactions and\nevaluate the appropriateness of those reactions using objective measures. This\npaper starts by defining the facial Multiple Appropriate Reaction Generation\n(fMARG) task for the first time in the literature and proposes a new set of\nobjective evaluation metrics to evaluate the appropriateness of the generated\nreactions. The paper subsequently introduces a framework to predict, generate,\nand evaluate multiple appropriate facial reactions.\n","authors":["Siyang Song","Micol Spitale","Yiming Luo","Batuhan Bal","Hatice Gunes"],"pdf_url":"https://arxiv.org/pdf/2302.06514v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13434v1","updated":"2023-03-23T16:56:01Z","published":"2023-03-23T16:56:01Z","title":"Patch-Mix Transformer for Unsupervised Domain Adaptation: A Game\n  Perspective","summary":"  Endeavors have been recently made to leverage the vision transformer (ViT)\nfor the challenging unsupervised domain adaptation (UDA) task. They typically\nadopt the cross-attention in ViT for direct domain alignment. However, as the\nperformance of cross-attention highly relies on the quality of pseudo labels\nfor targeted samples, it becomes less effective when the domain gap becomes\nlarge. We solve this problem from a game theory's perspective with the proposed\nmodel dubbed as PMTrans, which bridges source and target domains with an\nintermediate domain. Specifically, we propose a novel ViT-based module called\nPatchMix that effectively builds up the intermediate domain, i.e., probability\ndistribution, by learning to sample patches from both domains based on the\ngame-theoretical models. This way, it learns to mix the patches from the source\nand target domains to maximize the cross entropy (CE), while exploiting two\nsemi-supervised mixup losses in the feature and label spaces to minimize it. As\nsuch, we interpret the process of UDA as a min-max CE game with three players,\nincluding the feature extractor, classifier, and PatchMix, to find the Nash\nEquilibria. Moreover, we leverage attention maps from ViT to re-weight the\nlabel of each patch by its importance, making it possible to obtain more\ndomain-discriminative feature representations. We conduct extensive experiments\non four benchmark datasets, and the results show that PMTrans significantly\nsurpasses the ViT-based and CNN-based SoTA methods by +3.6% on Office-Home,\n+1.4% on Office-31, and +17.7% on DomainNet, respectively.\n","authors":["Jinjing Zhu","Haotian Bai","Lin Wang"],"pdf_url":"https://arxiv.org/pdf/2303.13434v1.pdf","comment":"Accepted by CVPR 2023 (Highlight)"},{"id":"http://arxiv.org/abs/2303.13430v1","updated":"2023-03-23T16:50:19Z","published":"2023-03-23T16:50:19Z","title":"Medical diffusion on a budget: textual inversion for medical image\n  generation","summary":"  Diffusion-based models for text-to-image generation have gained immense\npopularity due to recent advancements in efficiency, accessibility, and\nquality. Although it is becoming increasingly feasible to perform inference\nwith these systems using consumer-grade GPUs, training them from scratch still\nrequires access to large datasets and significant computational resources. In\nthe case of medical image generation, the availability of large, publicly\naccessible datasets that include text reports is limited due to legal and\nethical concerns. While training a diffusion model on a private dataset may\naddress this issue, it is not always feasible for institutions lacking the\nnecessary computational resources. This work demonstrates that pre-trained\nStable Diffusion models, originally trained on natural images, can be adapted\nto various medical imaging modalities by training text embeddings with textual\ninversion. In this study, we conducted experiments using medical datasets\ncomprising only 100 samples from three medical modalities. Embeddings were\ntrained in a matter of hours, while still retaining diagnostic relevance in\nimage generation. Experiments were designed to achieve several objectives.\nFirstly, we fine-tuned the training and inference processes of textual\ninversion, revealing that larger embeddings and more examples are required.\nSecondly, we validated our approach by demonstrating a 2\\% increase in the\ndiagnostic accuracy (AUC) for detecting prostate cancer on MRI, which is a\nchallenging multi-modal imaging modality, from 0.78 to 0.80. Thirdly, we\nperformed simulations by interpolating between healthy and diseased states,\ncombining multiple pathologies, and inpainting to show embedding flexibility\nand control of disease appearance. Finally, the embeddings trained in this\nstudy are small (less than 1 MB), which facilitates easy sharing of medical\ndata with reduced privacy concerns.\n","authors":["Bram de Wilde","Anindo Saha","Richard P. G. ten Broek","Henkjan Huisman"],"pdf_url":"https://arxiv.org/pdf/2303.13430v1.pdf","comment":null},{"id":"http://arxiv.org/abs/1909.12913v5","updated":"2023-03-23T16:43:29Z","published":"2019-09-18T15:46:48Z","title":"Student Engagement Detection Using Emotion Analysis, Eye Tracking and\n  Head Movement with Machine Learning","summary":"  With the increase of distance learning, in general, and e-learning, in\nparticular, having a system capable of determining the engagement of students\nis of primordial importance, and one of the biggest challenges, both for\nteachers, researchers and policy makers. Here, we present a system to detect\nthe engagement level of the students. It uses only information provided by the\ntypical built-in web-camera present in a laptop computer, and was designed to\nwork in real time. We combine information about the movements of the eyes and\nhead, and facial emotions to produce a concentration index with three classes\nof engagement: \"very engaged\", \"nominally engaged\" and \"not engaged at all\".\nThe system was tested in a typical e-learning scenario, and the results show\nthat it correctly identifies each period of time where students were \"very\nengaged\", \"nominally engaged\" and \"not engaged at all\". Additionally, the\nresults also show that the students with best scores also have higher\nconcentration indexes.\n","authors":["Prabin Sharma","Shubham Joshi","Subash Gautam","Sneha Maharjan","Salik Ram Khanal","Manuel Cabral Reis","João Barroso","Vítor Manuel de Jesus Filipe"],"pdf_url":"https://arxiv.org/pdf/1909.12913v5.pdf","comment":"9 pages, 9 Figures, 2 tables"},{"id":"http://arxiv.org/abs/2303.13412v1","updated":"2023-03-23T16:32:49Z","published":"2023-03-23T16:32:49Z","title":"Low-Light Image Enhancement by Learning Contrastive Representations in\n  Spatial and Frequency Domains","summary":"  Images taken under low-light conditions tend to suffer from poor visibility,\nwhich can decrease image quality and even reduce the performance of the\ndownstream tasks. It is hard for a CNN-based method to learn generalized\nfeatures that can recover normal images from the ones under various unknow\nlow-light conditions. In this paper, we propose to incorporate the contrastive\nlearning into an illumination correction network to learn abstract\nrepresentations to distinguish various low-light conditions in the\nrepresentation space, with the purpose of enhancing the generalizability of the\nnetwork. Considering that light conditions can change the frequency components\nof the images, the representations are learned and compared in both spatial and\nfrequency domains to make full advantage of the contrastive learning. The\nproposed method is evaluated on LOL and LOL-V2 datasets, the results show that\nthe proposed method achieves better qualitative and quantitative results\ncompared with other state-of-the-arts.\n","authors":["Yi Huang","Xiaoguang Tu","Gui Fu","Tingting Liu","Bokai Liu","Ming Yang","Ziliang Feng"],"pdf_url":"https://arxiv.org/pdf/2303.13412v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.12870v2","updated":"2023-03-23T16:29:04Z","published":"2022-11-23T11:21:00Z","title":"ActMAD: Activation Matching to Align Distributions for\n  Test-Time-Training","summary":"  Test-Time-Training (TTT) is an approach to cope with out-of-distribution\n(OOD) data by adapting a trained model to distribution shifts occurring at\ntest-time. We propose to perform this adaptation via Activation Matching\n(ActMAD): We analyze activations of the model and align activation statistics\nof the OOD test data to those of the training data. In contrast to existing\nmethods, which model the distribution of entire channels in the ultimate layer\nof the feature extractor, we model the distribution of each feature in multiple\nlayers across the network. This results in a more fine-grained supervision and\nmakes ActMAD attain state of the art performance on CIFAR-100C and Imagenet-C.\nActMAD is also architecture- and task-agnostic, which lets us go beyond image\nclassification, and score 15.4% improvement over previous approaches when\nevaluating a KITTI-trained object detector on KITTI-Fog. Our experiments\nhighlight that ActMAD can be applied to online adaptation in realistic\nscenarios, requiring little data to attain its full performance.\n","authors":["Muhammad Jehanzeb Mirza","Pol Jané Soneira","Wei Lin","Mateusz Kozinski","Horst Possegger","Horst Bischof"],"pdf_url":"https://arxiv.org/pdf/2211.12870v2.pdf","comment":"CVPR 2023 - Project Page: https://jmiemirza.github.io/ActMAD/"},{"id":"http://arxiv.org/abs/2303.13405v1","updated":"2023-03-23T16:28:15Z","published":"2023-03-23T16:28:15Z","title":"SC-MIL: Supervised Contrastive Multiple Instance Learning for Imbalanced\n  Classification in Pathology","summary":"  Multiple Instance learning (MIL) models have been extensively used in\npathology to predict biomarkers and risk-stratify patients from gigapixel-sized\nimages. Machine learning problems in medical imaging often deal with rare\ndiseases, making it important for these models to work in a label-imbalanced\nsetting. Furthermore, these imbalances can occur in out-of-distribution (OOD)\ndatasets when the models are deployed in the real-world. We leverage the idea\nthat decoupling feature and classifier learning can lead to improved decision\nboundaries for label imbalanced datasets. To this end, we investigate the\nintegration of supervised contrastive learning with multiple instance learning\n(SC-MIL). Specifically, we propose a joint-training MIL framework in the\npresence of label imbalance that progressively transitions from learning\nbag-level representations to optimal classifier learning. We perform\nexperiments with different imbalance settings for two well-studied problems in\ncancer pathology: subtyping of non-small cell lung cancer and subtyping of\nrenal cell carcinoma. SC-MIL provides large and consistent improvements over\nother techniques on both in-distribution (ID) and OOD held-out sets across\nmultiple imbalanced settings.\n","authors":["Dinkar Juyal","Siddhant Shingi","Syed Ashar Javed","Harshith Padigela","Chintan Shah","Anand Sampat","Archit Khosla","John Abel","Amaro Taylor-Weiner"],"pdf_url":"https://arxiv.org/pdf/2303.13405v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13404v1","updated":"2023-03-23T16:27:30Z","published":"2023-03-23T16:27:30Z","title":"MSFA-Frequency-Aware Transformer for Hyperspectral Images Demosaicing","summary":"  Hyperspectral imaging systems that use multispectral filter arrays (MSFA)\ncapture only one spectral component in each pixel. Hyperspectral demosaicing is\nused to recover the non-measured components. While deep learning methods have\nshown promise in this area, they still suffer from several challenges,\nincluding limited modeling of non-local dependencies, lack of consideration of\nthe periodic MSFA pattern that could be linked to periodic artifacts, and\ndifficulty in recovering high-frequency details. To address these challenges,\nthis paper proposes a novel de-mosaicing framework, the MSFA-frequency-aware\nTransformer network (FDM-Net). FDM-Net integrates a novel MSFA-frequency-aware\nmulti-head self-attention mechanism (MaFormer) and a filter-based Fourier\nzero-padding method to reconstruct high pass components with greater difficulty\nand low pass components with relative ease, separately. The advantage of\nMaformer is that it can leverage the MSFA information and non-local\ndependencies present in the data. Additionally, we introduce a joint spatial\nand frequency loss to transfer MSFA information and enhance training on\nfrequency components that are hard to recover. Our experimental results\ndemonstrate that FDM-Net outperforms state-of-the-art methods with 6dB PSNR,\nand reconstructs high-fidelity details successfully.\n","authors":["Haijin Zeng","Kai Feng","Shaoguang Huang","Jiezhang Cao","Yongyong Chen","Hongyan Zhang","Hiep Luong","Wilfried Philips"],"pdf_url":"https://arxiv.org/pdf/2303.13404v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.10156v2","updated":"2023-03-23T16:26:08Z","published":"2022-12-20T10:47:53Z","title":"Planning-oriented Autonomous Driving","summary":"  Modern autonomous driving system is characterized as modular tasks in\nsequential order, i.e., perception, prediction, and planning. In order to\nperform a wide diversity of tasks and achieve advanced-level intelligence,\ncontemporary approaches either deploy standalone models for individual tasks,\nor design a multi-task paradigm with separate heads. However, they might suffer\nfrom accumulative errors or deficient task coordination. Instead, we argue that\na favorable framework should be devised and optimized in pursuit of the\nultimate goal, i.e., planning of the self-driving car. Oriented at this, we\nrevisit the key components within perception and prediction, and prioritize the\ntasks such that all these tasks contribute to planning. We introduce Unified\nAutonomous Driving (UniAD), a comprehensive framework up-to-date that\nincorporates full-stack driving tasks in one network. It is exquisitely devised\nto leverage advantages of each module, and provide complementary feature\nabstractions for agent interaction from a global perspective. Tasks are\ncommunicated with unified query interfaces to facilitate each other toward\nplanning. We instantiate UniAD on the challenging nuScenes benchmark. With\nextensive ablations, the effectiveness of using such a philosophy is proven by\nsubstantially outperforming previous state-of-the-arts in all aspects. Code and\nmodels are public.\n","authors":["Yihan Hu","Jiazhi Yang","Li Chen","Keyu Li","Chonghao Sima","Xizhou Zhu","Siqi Chai","Senyao Du","Tianwei Lin","Wenhai Wang","Lewei Lu","Xiaosong Jia","Qiang Liu","Jifeng Dai","Yu Qiao","Hongyang Li"],"pdf_url":"https://arxiv.org/pdf/2212.10156v2.pdf","comment":"CVPR 2023 award candidate. Project page:\n  https://opendrivelab.github.io/UniAD/"},{"id":"http://arxiv.org/abs/2303.13401v1","updated":"2023-03-23T16:22:59Z","published":"2023-03-23T16:22:59Z","title":"Optimization and Optimizers for Adversarial Robustness","summary":"  Empirical robustness evaluation (RE) of deep learning models against\nadversarial perturbations entails solving nontrivial constrained optimization\nproblems. Existing numerical algorithms that are commonly used to solve them in\npractice predominantly rely on projected gradient, and mostly handle\nperturbations modeled by the $\\ell_1$, $\\ell_2$ and $\\ell_\\infty$ distances. In\nthis paper, we introduce a novel algorithmic framework that blends a\ngeneral-purpose constrained-optimization solver PyGRANSO with Constraint\nFolding (PWCF), which can add more reliability and generality to the\nstate-of-the-art RE packages, e.g., AutoAttack. Regarding reliability, PWCF\nprovides solutions with stationarity measures and feasibility tests to assess\nthe solution quality. For generality, PWCF can handle perturbation models that\nare typically inaccessible to the existing projected gradient methods; the main\nrequirement is the distance metric to be almost everywhere differentiable.\nTaking advantage of PWCF and other existing numerical algorithms, we further\nexplore the distinct patterns in the solutions found for solving these\noptimization problems using various combinations of losses, perturbation\nmodels, and optimization algorithms. We then discuss the implications of these\npatterns on the current robustness evaluation and adversarial training.\n","authors":["Hengyue Liang","Buyun Liang","Le Peng","Ying Cui","Tim Mitchell","Ju Sun"],"pdf_url":"https://arxiv.org/pdf/2303.13401v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13399v1","updated":"2023-03-23T16:19:43Z","published":"2023-03-23T16:19:43Z","title":"Multi-granularity Interaction Simulation for Unsupervised Interactive\n  Segmentation","summary":"  Interactive segmentation enables users to segment as needed by providing cues\nof objects, which introduces human-computer interaction for many fields, such\nas image editing and medical image analysis. Typically, massive and expansive\npixel-level annotations are spent to train deep models by object-oriented\ninteractions with manually labeled object masks. In this work, we reveal that\ninformative interactions can be made by simulation with semantic-consistent yet\ndiverse region exploration in an unsupervised paradigm. Concretely, we\nintroduce a Multi-granularity Interaction Simulation (MIS) approach to open up\na promising direction for unsupervised interactive segmentation. Drawing on the\nhigh-quality dense features produced by recent self-supervised models, we\npropose to gradually merge patches or regions with similar features to form\nmore extensive regions and thus, every merged region serves as a\nsemantic-meaningful multi-granularity proposal. By randomly sampling these\nproposals and simulating possible interactions based on them, we provide\nmeaningful interaction at multiple granularities to teach the model to\nunderstand interactions. Our MIS significantly outperforms non-deep learning\nunsupervised methods and is even comparable with some previous deep-supervised\nmethods without any annotation.\n","authors":["Kehan Li","Yian Zhao","Zhennan Wang","Zesen Cheng","Peng Jin","Xiangyang Ji","Li Yuan","Chang Liu","Jie Chen"],"pdf_url":"https://arxiv.org/pdf/2303.13399v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13397v1","updated":"2023-03-23T16:15:18Z","published":"2023-03-23T16:15:18Z","title":"DDT: A Diffusion-Driven Transformer-based Framework for Human Mesh\n  Recovery from a Video","summary":"  Human mesh recovery (HMR) provides rich human body information for various\nreal-world applications such as gaming, human-computer interaction, and virtual\nreality. Compared to single image-based methods, video-based methods can\nutilize temporal information to further improve performance by incorporating\nhuman body motion priors. However, many-to-many approaches such as VIBE suffer\nfrom motion smoothness and temporal inconsistency. While many-to-one approaches\nsuch as TCMR and MPS-Net rely on the future frames, which is non-causal and\ntime inefficient during inference. To address these challenges, a novel\nDiffusion-Driven Transformer-based framework (DDT) for video-based HMR is\npresented. DDT is designed to decode specific motion patterns from the input\nsequence, enhancing motion smoothness and temporal consistency. As a\nmany-to-many approach, the decoder of our DDT outputs the human mesh of all the\nframes, making DDT more viable for real-world applications where time\nefficiency is crucial and a causal model is desired. Extensive experiments are\nconducted on the widely used datasets (Human3.6M, MPI-INF-3DHP, and 3DPW),\nwhich demonstrated the effectiveness and efficiency of our DDT.\n","authors":["Ce Zheng","Guo-Jun Qi","Chen Chen"],"pdf_url":"https://arxiv.org/pdf/2303.13397v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13396v1","updated":"2023-03-23T16:15:07Z","published":"2023-03-23T16:15:07Z","title":"Zero-guidance Segmentation Using Zero Segment Labels","summary":"  CLIP has enabled new and exciting joint vision-language applications, one of\nwhich is open-vocabulary segmentation, which can locate any segment given an\narbitrary text query. In our research, we ask whether it is possible to\ndiscover semantic segments without any user guidance in the form of text\nqueries or predefined classes, and label them using natural language\nautomatically? We propose a novel problem zero-guidance segmentation and the\nfirst baseline that leverages two pre-trained generalist models, DINO and CLIP,\nto solve this problem without any fine-tuning or segmentation dataset. The\ngeneral idea is to first segment an image into small over-segments, encode them\ninto CLIP's visual-language space, translate them into text labels, and merge\nsemantically similar segments together. The key challenge, however, is how to\nencode a visual segment into a segment-specific embedding that balances global\nand local context information, both useful for recognition. Our main\ncontribution is a novel attention-masking technique that balances the two\ncontexts by analyzing the attention layers inside CLIP. We also introduce\nseveral metrics for the evaluation of this new task. With CLIP's innate\nknowledge, our method can precisely locate the Mona Lisa painting among a\nmuseum crowd. Project page: https://zero-guide-seg.github.io/.\n","authors":["Pitchaporn Rewatbowornwong","Nattanat Chatthee","Ekapol Chuangsuwanich","Supasorn Suwajanakorn"],"pdf_url":"https://arxiv.org/pdf/2303.13396v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13391v1","updated":"2023-03-23T16:07:31Z","published":"2023-03-23T16:07:31Z","title":"Xplainer: From X-Ray Observations to Explainable Zero-Shot Diagnosis","summary":"  Automated diagnosis prediction from medical images is a valuable resource to\nsupport clinical decision-making. However, such systems usually need to be\ntrained on large amounts of annotated data, which often is scarce in the\nmedical domain. Zero-shot methods address this challenge by allowing a flexible\nadaption to new settings with different clinical findings without relying on\nlabeled data. Further, to integrate automated diagnosis in the clinical\nworkflow, methods should be transparent and explainable, increasing medical\nprofessionals' trust and facilitating correctness verification. In this work,\nwe introduce Xplainer, a novel framework for explainable zero-shot diagnosis in\nthe clinical setting. Xplainer adapts the classification-by-description\napproach of contrastive vision-language models to the multi-label medical\ndiagnosis task. Specifically, instead of directly predicting a diagnosis, we\nprompt the model to classify the existence of descriptive observations, which a\nradiologist would look for on an X-Ray scan, and use the descriptor\nprobabilities to estimate the likelihood of a diagnosis. Our model is\nexplainable by design, as the final diagnosis prediction is directly based on\nthe prediction of the underlying descriptors. We evaluate Xplainer on two chest\nX-ray datasets, CheXpert and ChestX-ray14, and demonstrate its effectiveness in\nimproving the performance and explainability of zero-shot diagnosis. Our\nresults suggest that Xplainer provides a more detailed understanding of the\ndecision-making process and can be a valuable tool for clinical diagnosis.\n","authors":["Chantal Pellegrini","Matthias Keicher","Ege Özsoy","Petra Jiraskova","Rickmer Braren","Nassir Navab"],"pdf_url":"https://arxiv.org/pdf/2303.13391v1.pdf","comment":"9 pages, 2 figures, 6 tables"},{"id":"http://arxiv.org/abs/2205.15448v3","updated":"2023-03-23T15:48:05Z","published":"2022-05-30T22:09:57Z","title":"FeatER: An Efficient Network for Human Reconstruction via Feature\n  Map-Based TransformER","summary":"  Recently, vision transformers have shown great success in a set of human\nreconstruction tasks such as 2D human pose estimation (2D HPE), 3D human pose\nestimation (3D HPE), and human mesh reconstruction (HMR) tasks. In these tasks,\nfeature map representations of the human structural information are often\nextracted first from the image by a CNN (such as HRNet), and then further\nprocessed by transformer to predict the heatmaps (encodes each joint's location\ninto a feature map with a Gaussian distribution) for HPE or HMR. However,\nexisting transformer architectures are not able to process these feature map\ninputs directly, forcing an unnatural flattening of the location-sensitive\nhuman structural information. Furthermore, much of the performance benefit in\nrecent HPE and HMR methods has come at the cost of ever-increasing computation\nand memory needs. Therefore, to simultaneously address these problems, we\npropose FeatER, a novel transformer design that preserves the inherent\nstructure of feature map representations when modeling attention while reducing\nmemory and computational costs. Taking advantage of FeatER, we build an\nefficient network for a set of human reconstruction tasks including 2D HPE, 3D\nHPE, and HMR. A feature map reconstruction module is applied to improve the\nperformance of the estimated human pose and mesh. Extensive experiments\ndemonstrate the effectiveness of FeatER on various human pose and mesh\ndatasets. For instance, FeatER outperforms the SOTA method MeshGraphormer by\nrequiring 5% of Params and 16% of MACs on Human3.6M and 3DPW datasets. The\nproject webpage is https://zczcwh.github.io/feater_page/.\n","authors":["Ce Zheng","Matias Mendieta","Taojiannan Yang","Guo-Jun Qi","Chen Chen"],"pdf_url":"https://arxiv.org/pdf/2205.15448v3.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.13371v1","updated":"2023-03-23T15:42:05Z","published":"2023-03-23T15:42:05Z","title":"Plug-and-Play Regulators for Image-Text Matching","summary":"  Exploiting fine-grained correspondence and visual-semantic alignments has\nshown great potential in image-text matching. Generally, recent approaches\nfirst employ a cross-modal attention unit to capture latent region-word\ninteractions, and then integrate all the alignments to obtain the final\nsimilarity. However, most of them adopt one-time forward association or\naggregation strategies with complex architectures or additional information,\nwhile ignoring the regulation ability of network feedback. In this paper, we\ndevelop two simple but quite effective regulators which efficiently encode the\nmessage output to automatically contextualize and aggregate cross-modal\nrepresentations. Specifically, we propose (i) a Recurrent Correspondence\nRegulator (RCR) which facilitates the cross-modal attention unit progressively\nwith adaptive attention factors to capture more flexible correspondence, and\n(ii) a Recurrent Aggregation Regulator (RAR) which adjusts the aggregation\nweights repeatedly to increasingly emphasize important alignments and dilute\nunimportant ones. Besides, it is interesting that RCR and RAR are\nplug-and-play: both of them can be incorporated into many frameworks based on\ncross-modal interaction to obtain significant benefits, and their cooperation\nachieves further improvements. Extensive experiments on MSCOCO and Flickr30K\ndatasets validate that they can bring an impressive and consistent R@1 gain on\nmultiple models, confirming the general effectiveness and generalization\nability of the proposed methods. Code and pre-trained models are available at:\nhttps://github.com/Paranioar/RCAR.\n","authors":["Haiwen Diao","Ying Zhang","Wei Liu","Xiang Ruan","Huchuan Lu"],"pdf_url":"https://arxiv.org/pdf/2303.13371v1.pdf","comment":"13 pages, 9 figures, Accepted by TIP2023"},{"id":"http://arxiv.org/abs/2303.13357v1","updated":"2023-03-23T15:36:12Z","published":"2023-03-23T15:36:12Z","title":"POTTER: Pooling Attention Transformer for Efficient Human Mesh Recovery","summary":"  Transformer architectures have achieved SOTA performance on the human mesh\nrecovery (HMR) from monocular images. However, the performance gain has come at\nthe cost of substantial memory and computational overhead. A lightweight and\nefficient model to reconstruct accurate human mesh is needed for real-world\napplications. In this paper, we propose a pure transformer architecture named\nPOoling aTtention TransformER (POTTER) for the HMR task from single images.\nObserving that the conventional attention module is memory and computationally\nexpensive, we propose an efficient pooling attention module, which\nsignificantly reduces the memory and computational cost without sacrificing\nperformance. Furthermore, we design a new transformer architecture by\nintegrating a High-Resolution (HR) stream for the HMR task. The high-resolution\nlocal and global features from the HR stream can be utilized for recovering\nmore accurate human mesh. Our POTTER outperforms the SOTA method METRO by only\nrequiring 7% of total parameters and 14% of the Multiply-Accumulate Operations\non the Human3.6M (PA-MPJPE metric) and 3DPW (all three metrics) datasets. The\nproject webpage is https://zczcwh.github.io/potter_page.\n","authors":["Ce Zheng","Xianpeng Liu","Guo-Jun Qi","Chen Chen"],"pdf_url":"https://arxiv.org/pdf/2303.13357v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.13340v1","updated":"2023-03-23T15:20:05Z","published":"2023-03-23T15:20:05Z","title":"Increasing Textual Context Size Boosts Medical Image-Text Matching","summary":"  This short technical report demonstrates a simple technique that yields state\nof the art results in medical image-text matching tasks. We analyze the use of\nOpenAI's CLIP, a general image-text matching model, and observe that CLIP's\nlimited textual input size has negative impact on downstream performance in the\nmedical domain where encoding longer textual contexts is often required. We\nthus train and release ClipMD, which is trained with a simple sliding window\ntechnique to encode textual captions. ClipMD was tested on two medical\nimage-text datasets and compared with other image-text matching models. The\nresults show that ClipMD outperforms other models on both datasets by a large\nmargin. We make our code and pretrained model publicly available.\n","authors":["Idan Glassberg","Tom Hope"],"pdf_url":"https://arxiv.org/pdf/2303.13340v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13332v1","updated":"2023-03-23T15:13:04Z","published":"2023-03-23T15:13:04Z","title":"Clinically Relevant Latent Space Embedding of Cancer Histopathology\n  Slides through Variational Autoencoder Based Image Compression","summary":"  In this paper, we introduce a Variational Autoencoder (VAE) based training\napproach that can compress and decompress cancer pathology slides at a\ncompression ratio of 1:512, which is better than the previously reported state\nof the art (SOTA) in the literature, while still maintaining accuracy in\nclinical validation tasks. The compression approach was tested on more common\ncomputer vision datasets such as CIFAR10, and we explore which image\ncharacteristics enable this compression ratio on cancer imaging data but not\ngeneric images. We generate and visualize embeddings from the compressed latent\nspace and demonstrate how they are useful for clinical interpretation of data,\nand how in the future such latent embeddings can be used to accelerate search\nof clinical imaging data.\n","authors":["Mohammad Sadegh Nasr","Amir Hajighasemi","Paul Koomey","Parisa Boodaghi Malidarreh","Michael Robben","Jillur Rahman Saurav","Helen H. Shang","Manfred Huber","Jacob M. Luber"],"pdf_url":"https://arxiv.org/pdf/2303.13332v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08356v2","updated":"2023-03-23T15:07:46Z","published":"2022-12-16T09:02:01Z","title":"Cyclical Compound Domain Test-time Adaptation via Continual\n  Domain-Matching Algorithm","summary":"  Test-time adaptation (TTA), a key component of lifelong learning in edge\ndevices, refers to the ability of a pre-trained model to adapt itself to new\nenvironments during test time. Due to its practical ability, TTA has attracted\nsignificant attention and experienced a rapid performance boost these days. In\nthis paper, we present an under-explored yet more realistic TTA scenario and\nprovide a strong baseline favorable to this scenario, named cyclical compound\ndomain (CCD). The CCD represents the real-world scenario in which the target\ndomain contains multiple sub-target domains (i.e., compound domain due to\nweather or time change) and the sub-target domains are likely to rise\ncyclically. Unfortunately, existing works do not faithfully account for this\nplausible scenario, only focusing on adapting to the current sub-target domain\nwhile discarding the past knowledge acquired from repeated sub-target domains.\nTherefore, we first propose a lightweight domain-matching algorithm that allows\nthe TTA model to manage knowledge from the compound domain. This algorithm\nidentifies the type of domain among sub-target domains by continuously matching\nthe current image's distribution with reference domain points. Moreover, our\nnewly proposed regularization method compares the present distribution with\nsource one in order to regularize the adaptation pace according to each data in\nsub-target domains. Qualitatively, we demonstrate that our simple-yet-effective\napproach improves the adaptation performance on various benchmarks, including\nimage classification on ImageNet-C and semantic segmentation on GTA5, C-driving\ndatasets, and Cityscapes with corruptions.\n","authors":["Junha Song","Kwanyong Park","InKyu Shin","Sanghyun Woo","Chaoning Zhang","In So Kweon"],"pdf_url":"https://arxiv.org/pdf/2212.08356v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13325v1","updated":"2023-03-23T15:04:23Z","published":"2023-03-23T15:04:23Z","title":"DARE-GRAM : Unsupervised Domain Adaptation Regression by Aligning\n  Inverse Gram Matrices","summary":"  Unsupervised Domain Adaptation Regression (DAR) aims to bridge the domain gap\nbetween a labeled source dataset and an unlabelled target dataset for\nregression problems. Recent works mostly focus on learning a deep feature\nencoder by minimizing the discrepancy between source and target features. In\nthis work, we present a different perspective for the DAR problem by analyzing\nthe closed-form ordinary least square~(OLS) solution to the linear regressor in\nthe deep domain adaptation context. Rather than aligning the original feature\nembedding space, we propose to align the inverse Gram matrix of the features,\nwhich is motivated by its presence in the OLS solution and the Gram matrix's\nability to capture the feature correlations. Specifically, we propose a simple\nyet effective DAR method which leverages the pseudo-inverse low-rank property\nto align the scale and angle in a selected subspace generated by the\npseudo-inverse Gram matrix of the two domains. We evaluate our method on three\ndomain adaptation regression benchmarks. Experimental results demonstrate that\nour method achieves state-of-the-art performance. Our code is available at\nhttps://github.com/ismailnejjar/DARE-GRAM.\n","authors":["Ismail Nejjar","Qin Wang","Olga Fink"],"pdf_url":"https://arxiv.org/pdf/2303.13325v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13297v1","updated":"2023-03-23T14:27:49Z","published":"2023-03-23T14:27:49Z","title":"Improving Generalization with Domain Convex Game","summary":"  Domain generalization (DG) tends to alleviate the poor generalization\ncapability of deep neural networks by learning model with multiple source\ndomains. A classical solution to DG is domain augmentation, the common belief\nof which is that diversifying source domains will be conducive to the\nout-of-distribution generalization. However, these claims are understood\nintuitively, rather than mathematically. Our explorations empirically reveal\nthat the correlation between model generalization and the diversity of domains\nmay be not strictly positive, which limits the effectiveness of domain\naugmentation. This work therefore aim to guarantee and further enhance the\nvalidity of this strand. To this end, we propose a new perspective on DG that\nrecasts it as a convex game between domains. We first encourage each\ndiversified domain to enhance model generalization by elaborately designing a\nregularization term based on supermodularity. Meanwhile, a sample filter is\nconstructed to eliminate low-quality samples, thereby avoiding the impact of\npotentially harmful information. Our framework presents a new avenue for the\nformal analysis of DG, heuristic analysis and extensive experiments demonstrate\nthe rationality and effectiveness.\n","authors":["Fangrui Lv","Jian Liang","Shuang Li","Jinming Zhang","Di Liu"],"pdf_url":"https://arxiv.org/pdf/2303.13297v1.pdf","comment":"accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2212.07422v2","updated":"2023-03-23T14:27:38Z","published":"2022-12-14T18:59:19Z","title":"ECON: Explicit Clothed humans Optimized via Normal integration","summary":"  The combination of deep learning, artist-curated scans, and Implicit\nFunctions (IF), is enabling the creation of detailed, clothed, 3D humans from\nimages. However, existing methods are far from perfect. IF-based methods\nrecover free-form geometry, but produce disembodied limbs or degenerate shapes\nfor novel poses or clothes. To increase robustness for these cases, existing\nwork uses an explicit parametric body model to constrain surface\nreconstruction, but this limits the recovery of free-form surfaces such as\nloose clothing that deviates from the body. What we want is a method that\ncombines the best properties of implicit representation and explicit body\nregularization. To this end, we make two key observations: (1) current networks\nare better at inferring detailed 2D maps than full-3D surfaces, and (2) a\nparametric model can be seen as a \"canvas\" for stitching together detailed\nsurface patches. Based on these, our method, ECON, has three main steps: (1) It\ninfers detailed 2D normal maps for the front and back side of a clothed person.\n(2) From these, it recovers 2.5D front and back surfaces, called d-BiNI, that\nare equally detailed, yet incomplete, and registers these w.r.t. each other\nwith the help of a SMPL-X body mesh recovered from the image. (3) It \"inpaints\"\nthe missing geometry between d-BiNI surfaces. If the face and hands are noisy,\nthey can optionally be replaced with the ones of SMPL-X. As a result, ECON\ninfers high-fidelity 3D humans even in loose clothes and challenging poses.\nThis goes beyond previous methods, according to the quantitative evaluation on\nthe CAPE and Renderpeople datasets. Perceptual studies also show that ECON's\nperceived realism is better by a large margin. Code and models are available\nfor research purposes at econ.is.tue.mpg.de\n","authors":["Yuliang Xiu","Jinlong Yang","Xu Cao","Dimitrios Tzionas","Michael J. Black"],"pdf_url":"https://arxiv.org/pdf/2212.07422v2.pdf","comment":"Homepage: https://xiuyuliang.cn/econ Code:\n  https://github.com/YuliangXiu/ECON"},{"id":"http://arxiv.org/abs/2303.13294v1","updated":"2023-03-23T14:26:21Z","published":"2023-03-23T14:26:21Z","title":"Considerations on the Evaluation of Biometric Quality Assessment\n  Algorithms","summary":"  Quality assessment algorithms can be used to estimate the utility of a\nbiometric sample for the purpose of biometric recognition. \"Error versus\nDiscard Characteristic\" (EDC) plots, and \"partial Area Under Curve\" (pAUC)\nvalues of curves therein, are generally used by researchers to evaluate the\npredictive performance of such quality assessment algorithms. An EDC curve\ndepends on an error type such as the \"False Non Match Rate\" (FNMR), a quality\nassessment algorithm, a biometric recognition system, a set of comparisons each\ncorresponding to a biometric sample pair, and a comparison score threshold\ncorresponding to a starting error. To compute an EDC curve, comparisons are\nprogressively discarded based on the associated samples' lowest quality scores,\nand the error is computed for the remaining comparisons. Additionally, a\ndiscard fraction limit or range must be selected to compute pAUC values, which\ncan then be used to quantitatively rank quality assessment algorithms.\n  This paper discusses and analyses various details for this kind of quality\nassessment algorithm evaluation, including general EDC properties,\ninterpretability improvements for pAUC values based on a hard lower error limit\nand a soft upper error limit, the use of relative instead of discrete rankings,\nstepwise vs. linear curve interpolation, and normalisation of quality scores to\na [0, 100] integer range. We also analyse the stability of quantitative quality\nassessment algorithm rankings based on pAUC values across varying pAUC discard\nfraction limits and starting errors, concluding that higher pAUC discard\nfraction limits should be preferred. The analyses are conducted both with\nsynthetic data and with real data for a face image quality assessment scenario,\nwith a focus on general modality-independent conclusions for EDC evaluations.\n","authors":["Torsten Schlett","Christian Rathgeb","Juan Tapia","Christoph Busch"],"pdf_url":"https://arxiv.org/pdf/2303.13294v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13293v1","updated":"2023-03-23T14:26:16Z","published":"2023-03-23T14:26:16Z","title":"LABRAD-OR: Lightweight Memory Scene Graphs for Accurate Bimodal\n  Reasoning in Dynamic Operating Rooms","summary":"  Modern surgeries are performed in complex and dynamic settings, including\never-changing interactions between medical staff, patients, and equipment. The\nholistic modeling of the operating room (OR) is, therefore, a challenging but\nessential task, with the potential to optimize the performance of surgical\nteams and aid in developing new surgical technologies to improve patient\noutcomes. The holistic representation of surgical scenes as semantic scene\ngraphs (SGG), where entities are represented as nodes and relations between\nthem as edges, is a promising direction for fine-grained semantic OR\nunderstanding. We propose, for the first time, the use of temporal information\nfor more accurate and consistent holistic OR modeling. Specifically, we\nintroduce memory scene graphs, where the scene graphs of previous time steps\nact as the temporal representation guiding the current prediction. We design an\nend-to-end architecture that intelligently fuses the temporal information of\nour lightweight memory scene graphs with the visual information from point\nclouds and images. We evaluate our method on the 4D-OR dataset and demonstrate\nthat integrating temporality leads to more accurate and consistent results\nachieving an +5% increase and a new SOTA of 0.88 in macro F1. This work opens\nthe path for representing the entire surgery history with memory scene graphs\nand improves the holistic understanding in the OR. Introducing scene graphs as\nmemory representations can offer a valuable tool for many temporal\nunderstanding tasks.\n","authors":["Ege Özsoy","Tobias Czempiel","Felix Holm","Chantal Pellegrini","Nassir Navab"],"pdf_url":"https://arxiv.org/pdf/2303.13293v1.pdf","comment":"11 pages, 3 figures"},{"id":"http://arxiv.org/abs/2211.14086v2","updated":"2023-03-23T14:21:24Z","published":"2022-11-25T13:14:56Z","title":"ShadowNeuS: Neural SDF Reconstruction by Shadow Ray Supervision","summary":"  By supervising camera rays between a scene and multi-view image planes, NeRF\nreconstructs a neural scene representation for the task of novel view\nsynthesis. On the other hand, shadow rays between the light source and the\nscene have yet to be considered. Therefore, we propose a novel shadow ray\nsupervision scheme that optimizes both the samples along the ray and the ray\nlocation. By supervising shadow rays, we successfully reconstruct a neural SDF\nof the scene from single-view images under multiple lighting conditions. Given\nsingle-view binary shadows, we train a neural network to reconstruct a complete\nscene not limited by the camera's line of sight. By further modeling the\ncorrelation between the image colors and the shadow rays, our technique can\nalso be effectively extended to RGB inputs. We compare our method with previous\nworks on challenging tasks of shape reconstruction from single-view binary\nshadow or RGB images and observe significant improvements. The code and data\nare available at https://github.com/gerwang/ShadowNeuS.\n","authors":["Jingwang Ling","Zhibo Wang","Feng Xu"],"pdf_url":"https://arxiv.org/pdf/2211.14086v2.pdf","comment":"CVPR 2023. Project page: https://gerwang.github.io/shadowneus/"},{"id":"http://arxiv.org/abs/2103.10427v4","updated":"2023-03-23T14:21:02Z","published":"2021-03-18T17:58:02Z","title":"The Low-Rank Simplicity Bias in Deep Networks","summary":"  Modern deep neural networks are highly over-parameterized compared to the\ndata on which they are trained, yet they often generalize remarkably well. A\nflurry of recent work has asked: why do deep networks not overfit to their\ntraining data? In this work, we make a series of empirical observations that\ninvestigate and extend the hypothesis that deeper networks are inductively\nbiased to find solutions with lower effective rank embeddings. We conjecture\nthat this bias exists because the volume of functions that maps to low\neffective rank embedding increases with depth. We show empirically that our\nclaim holds true on finite width linear and non-linear models on practical\nlearning paradigms and show that on natural data, these are often the solutions\nthat generalize well. We then show that the simplicity bias exists at both\ninitialization and after training and is resilient to hyper-parameters and\nlearning methods. We further demonstrate how linear over-parameterization of\ndeep non-linear models can be used to induce low-rank bias, improving\ngeneralization performance on CIFAR and ImageNet without changing the modeling\ncapacity.\n","authors":["Minyoung Huh","Hossein Mobahi","Richard Zhang","Brian Cheung","Pulkit Agrawal","Phillip Isola"],"pdf_url":"https://arxiv.org/pdf/2103.10427v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13290v1","updated":"2023-03-23T14:18:06Z","published":"2023-03-23T14:18:06Z","title":"Unsupervised Deep Probabilistic Approach for Partial Point Cloud\n  Registration","summary":"  Deep point cloud registration methods face challenges to partial overlaps and\nrely on labeled data. To address these issues, we propose UDPReg, an\nunsupervised deep probabilistic registration framework for point clouds with\npartial overlaps. Specifically, we first adopt a network to learn posterior\nprobability distributions of Gaussian mixture models (GMMs) from point clouds.\nTo handle partial point cloud registration, we apply the Sinkhorn algorithm to\npredict the distribution-level correspondences under the constraint of the\nmixing weights of GMMs. To enable unsupervised learning, we design three\ndistribution consistency-based losses: self-consistency, cross-consistency, and\nlocal contrastive. The self-consistency loss is formulated by encouraging GMMs\nin Euclidean and feature spaces to share identical posterior distributions. The\ncross-consistency loss derives from the fact that the points of two partially\noverlapping point clouds belonging to the same clusters share the cluster\ncentroids. The cross-consistency loss allows the network to flexibly learn a\ntransformation-invariant posterior distribution of two aligned point clouds.\nThe local contrastive loss facilitates the network to extract discriminative\nlocal features. Our UDPReg achieves competitive performance on the\n3DMatch/3DLoMatch and ModelNet/ModelLoNet benchmarks.\n","authors":["Guofeng Mei","Hao Tang","Xiaoshui Huang","Weijie Wang","Juan Liu","Jian Zhang","Luc Van Gool","Qiang Wu"],"pdf_url":"https://arxiv.org/pdf/2303.13290v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.12710v2","updated":"2023-03-23T14:13:05Z","published":"2023-03-09T04:35:00Z","title":"A Unified Arbitrary Style Transfer Framework via Adaptive Contrastive\n  Learning","summary":"  We present Unified Contrastive Arbitrary Style Transfer (UCAST), a novel\nstyle representation learning and transfer framework, which can fit in most\nexisting arbitrary image style transfer models, e.g., CNN-based, ViT-based, and\nflow-based methods. As the key component in image style transfer tasks, a\nsuitable style representation is essential to achieve satisfactory results.\nExisting approaches based on deep neural network typically use second-order\nstatistics to generate the output. However, these hand-crafted features\ncomputed from a single image cannot leverage style information sufficiently,\nwhich leads to artifacts such as local distortions and style inconsistency. To\naddress these issues, we propose to learn style representation directly from a\nlarge amount of images based on contrastive learning, by taking the\nrelationships between specific styles and the holistic style distribution into\naccount. Specifically, we present an adaptive contrastive learning scheme for\nstyle transfer by introducing an input-dependent temperature. Our framework\nconsists of three key components, i.e., a parallel contrastive learning scheme\nfor style representation and style transfer, a domain enhancement module for\neffective learning of style distribution, and a generative network for style\ntransfer. We carry out qualitative and quantitative evaluations to show that\nour approach produces superior results than those obtained via state-of-the-art\nmethods.\n","authors":["Yuxin Zhang","Fan Tang","Weiming Dong","Haibin Huang","Chongyang Ma","Tong-Yee Lee","Changsheng Xu"],"pdf_url":"https://arxiv.org/pdf/2303.12710v2.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2205.09542"},{"id":"http://arxiv.org/abs/2303.13283v1","updated":"2023-03-23T14:04:23Z","published":"2023-03-23T14:04:23Z","title":"Visual-Language Prompt Tuning with Knowledge-guided Context Optimization","summary":"  Prompt tuning is an effective way to adapt the pre-trained visual-language\nmodel (VLM) to the downstream task using task-related textual tokens.\nRepresentative CoOp-based work combines the learnable textual tokens with the\nclass tokens to obtain specific textual knowledge. However, the specific\ntextual knowledge is the worse generalization to the unseen classes because it\nforgets the essential general textual knowledge having a strong generalization\nability. To tackle this issue, we introduce a novel Knowledge-guided Context\nOptimization (KgCoOp) to enhance the generalization ability of the learnable\nprompt for unseen classes. The key insight of KgCoOp is that forgetting about\nessential knowledge can be alleviated by reducing the discrepancy between the\nlearnable prompt and the hand-crafted prompt. Especially, KgCoOp minimizes the\ndiscrepancy between the textual embeddings generated by learned prompts and the\nhand-crafted prompts. Finally, adding the KgCoOp upon the contrastive loss can\nmake a discriminative prompt for both seen and unseen tasks. Extensive\nevaluation of several benchmarks demonstrates that the proposed\nKnowledge-guided Context Optimization is an efficient method for prompt tuning,\n\\emph{i.e.,} achieves better performance with less training time.\n","authors":["Hantao Yao","Rui Zhang","Changsheng Xu"],"pdf_url":"https://arxiv.org/pdf/2303.13283v1.pdf","comment":"accepted by CVPR23"},{"id":"http://arxiv.org/abs/2211.11635v4","updated":"2023-03-23T14:02:21Z","published":"2022-11-21T16:49:47Z","title":"Understanding and Improving Visual Prompting: A Label-Mapping\n  Perspective","summary":"  We revisit and advance visual prompting (VP), an input prompting technique\nfor vision tasks. VP can reprogram a fixed, pre-trained source model to\naccomplish downstream tasks in the target domain by simply incorporating\nuniversal prompts (in terms of input perturbation patterns) into downstream\ndata points. Yet, it remains elusive why VP stays effective even given a\nruleless label mapping (LM) between the source classes and the target classes.\nInspired by the above, we ask: How is LM interrelated with VP? And how to\nexploit such a relationship to improve its accuracy on target tasks? We peer\ninto the influence of LM on VP and provide an affirmative answer that a better\n'quality' of LM (assessed by mapping precision and explanation) can\nconsistently improve the effectiveness of VP. This is in contrast to the prior\nart where the factor of LM was missing. To optimize LM, we propose a new VP\nframework, termed ILM-VP (iterative label mapping-based visual prompting),\nwhich automatically re-maps the source labels to the target labels and\nprogressively improves the target task accuracy of VP. Further, when using a\ncontrastive language-image pretrained (CLIP) model, we propose to integrate an\nLM process to assist the text prompt selection of CLIP and to improve the\ntarget task accuracy. Extensive experiments demonstrate that our proposal\nsignificantly outperforms state-of-the-art VP methods. As highlighted below, we\nshow that when reprogramming an ImageNet-pretrained ResNet-18 to 13 target\ntasks, our method outperforms baselines by a substantial margin, e.g., 7.9% and\n6.7% accuracy improvements in transfer learning to the target Flowers102 and\nCIFAR100 datasets. Besides, our proposal on CLIP-based VP provides 13.7% and\n7.1% accuracy improvements on Flowers102 and DTD respectively. Our code is\navailable at https://github.com/OPTML-Group/ILM-VP.\n","authors":["Aochuan Chen","Yuguang Yao","Pin-Yu Chen","Yihua Zhang","Sijia Liu"],"pdf_url":"https://arxiv.org/pdf/2211.11635v4.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.13278v1","updated":"2023-03-23T13:59:57Z","published":"2023-03-23T13:59:57Z","title":"Improved Anisotropic Gaussian Filters","summary":"  Elongated anisotropic Gaussian filters are used for the orientation\nestimation of fibers. In cases where computed tomography images are noisy,\nroughly resolved, and of low contrast, they are the method of choice even if\nbeing efficient only in virtual 2D slices. However, minor inaccuracies in the\nanisotropic Gaussian filters can carry over to the orientation estimation.\nTherefore, we propose a modified algorithm for 2D anisotropic Gaussian filters\nand show that this improves their precision. Applied to synthetic images of\nfiber bundles, it is more accurate and robust to noise. Finally, we demonstrate\nthe effectiveness of our approach by applying it to real-world images of sheet\nmolding compounds.\n","authors":["Alex Keilmann","Michael Godehardt","Ali Moghiseh","Claudia Redenbach","Katja Schladitz"],"pdf_url":"https://arxiv.org/pdf/2303.13278v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13277v1","updated":"2023-03-23T13:58:11Z","published":"2023-03-23T13:58:11Z","title":"SINE: Semantic-driven Image-based NeRF Editing with Prior-guided Editing\n  Field","summary":"  Despite the great success in 2D editing using user-friendly tools, such as\nPhotoshop, semantic strokes, or even text prompts, similar capabilities in 3D\nareas are still limited, either relying on 3D modeling skills or allowing\nediting within only a few categories.In this paper, we present a novel\nsemantic-driven NeRF editing approach, which enables users to edit a neural\nradiance field with a single image, and faithfully delivers edited novel views\nwith high fidelity and multi-view consistency.To achieve this goal, we propose\na prior-guided editing field to encode fine-grained geometric and texture\nediting in 3D space, and develop a series of techniques to aid the editing\nprocess, including cyclic constraints with a proxy mesh to facilitate geometric\nsupervision, a color compositing mechanism to stabilize semantic-driven texture\nediting, and a feature-cluster-based regularization to preserve the irrelevant\ncontent unchanged.Extensive experiments and editing examples on both real-world\nand synthetic data demonstrate that our method achieves photo-realistic 3D\nediting using only a single edited image, pushing the bound of semantic-driven\nediting in 3D real-world scenes. Our project webpage:\nhttps://zju3dv.github.io/sine/.\n","authors":["Chong Bao","Yinda Zhang","Bangbang Yang","Tianxing Fan","Zesong Yang","Hujun Bao","Guofeng Zhang","Zhaopeng Cui"],"pdf_url":"https://arxiv.org/pdf/2303.13277v1.pdf","comment":"Accepted to CVPR 2023. Project Page: https://zju3dv.github.io/sine/"},{"id":"http://arxiv.org/abs/2303.13273v1","updated":"2023-03-23T13:53:16Z","published":"2023-03-23T13:53:16Z","title":"TAPS3D: Text-Guided 3D Textured Shape Generation from Pseudo Supervision","summary":"  In this paper, we investigate an open research task of generating\ncontrollable 3D textured shapes from the given textual descriptions. Previous\nworks either require ground truth caption labeling or extensive optimization\ntime. To resolve these issues, we present a novel framework, TAPS3D, to train a\ntext-guided 3D shape generator with pseudo captions. Specifically, based on\nrendered 2D images, we retrieve relevant words from the CLIP vocabulary and\nconstruct pseudo captions using templates. Our constructed captions provide\nhigh-level semantic supervision for generated 3D shapes. Further, in order to\nproduce fine-grained textures and increase geometry diversity, we propose to\nadopt low-level image regularization to enable fake-rendered images to align\nwith the real ones. During the inference phase, our proposed model can generate\n3D textured shapes from the given text without any additional optimization. We\nconduct extensive experiments to analyze each of our proposed components and\nshow the efficacy of our framework in generating high-fidelity 3D textured and\ntext-relevant shapes.\n","authors":["Jiacheng Wei","Hao Wang","Jiashi Feng","Guosheng Lin","Kim-Hui Yap"],"pdf_url":"https://arxiv.org/pdf/2303.13273v1.pdf","comment":"Accepted to CVPR2023"},{"id":"http://arxiv.org/abs/2303.13269v1","updated":"2023-03-23T13:50:46Z","published":"2023-03-23T13:50:46Z","title":"Disguise without Disruption: Utility-Preserving Face De-Identification","summary":"  With the increasing ubiquity of cameras and smart sensors, humanity is\ngenerating data at an exponential rate. Access to this trove of information,\noften covering yet-underrepresented use-cases (e.g., AI in medical settings)\ncould fuel a new generation of deep-learning tools. However, eager data\nscientists should first provide satisfying guarantees w.r.t. the privacy of\nindividuals present in these untapped datasets. This is especially important\nfor images or videos depicting faces, as their biometric information is the\ntarget of most identification methods. While a variety of solutions have been\nproposed to de-identify such images, they often corrupt other non-identifying\nfacial attributes that would be relevant for downstream tasks. In this paper,\nwe propose Disguise, a novel algorithm to seamlessly de-identify facial images\nwhile ensuring the usability of the altered data. Unlike prior arts, we ground\nour solution in both differential privacy and ensemble-learning research\ndomains. Our method extracts and swaps depicted identities with fake ones,\nsynthesized via variational mechanisms to maximize obfuscation and\nnon-invertibility; while leveraging the supervision from a mixture-of-experts\nto disentangle and preserve other utility attributes. We extensively evaluate\nour method on multiple datasets, demonstrating higher de-identification rate\nand superior consistency than prior art w.r.t. various downstream tasks.\n","authors":["Zikui Cai","Zhongpai Gao","Benjamin Planche","Meng Zheng","Terrence Chen","M. Salman Asif","Ziyan Wu"],"pdf_url":"https://arxiv.org/pdf/2303.13269v1.pdf","comment":"paper + supplementary material"},{"id":"http://arxiv.org/abs/2303.13251v1","updated":"2023-03-23T13:33:58Z","published":"2023-03-23T13:33:58Z","title":"A Bag-of-Prototypes Representation for Dataset-Level Applications","summary":"  This work investigates dataset vectorization for two dataset-level tasks:\nassessing training set suitability and test set difficulty. The former measures\nhow suitable a training set is for a target domain, while the latter studies\nhow challenging a test set is for a learned model. Central to the two tasks is\nmeasuring the underlying relationship between datasets. This needs a desirable\ndataset vectorization scheme, which should preserve as much discriminative\ndataset information as possible so that the distance between the resulting\ndataset vectors can reflect dataset-to-dataset similarity. To this end, we\npropose a bag-of-prototypes (BoP) dataset representation that extends the\nimage-level bag consisting of patch descriptors to dataset-level bag consisting\nof semantic prototypes. Specifically, we develop a codebook consisting of K\nprototypes clustered from a reference dataset. Given a dataset to be encoded,\nwe quantize each of its image features to a certain prototype in the codebook\nand obtain a K-dimensional histogram. Without assuming access to dataset\nlabels, the BoP representation provides a rich characterization of the dataset\nsemantic distribution. Furthermore, BoP representations cooperate well with\nJensen-Shannon divergence for measuring dataset-to-dataset similarity. Although\nvery simple, BoP consistently shows its advantage over existing representations\non a series of benchmarks for two dataset-level tasks.\n","authors":["Weijie Tu","Weijian Deng","Tom Gedeon","Liang Zheng"],"pdf_url":"https://arxiv.org/pdf/2303.13251v1.pdf","comment":"CVPR 2023 camera-ready"},{"id":"http://arxiv.org/abs/2303.13245v1","updated":"2023-03-23T13:24:16Z","published":"2023-03-23T13:24:16Z","title":"CrOC: Cross-View Online Clustering for Dense Visual Representation\n  Learning","summary":"  Learning dense visual representations without labels is an arduous task and\nmore so from scene-centric data. We propose to tackle this challenging problem\nby proposing a Cross-view consistency objective with an Online Clustering\nmechanism (CrOC) to discover and segment the semantics of the views. In the\nabsence of hand-crafted priors, the resulting method is more generalizable and\ndoes not require a cumbersome pre-processing step. More importantly, the\nclustering algorithm conjointly operates on the features of both views, thereby\nelegantly bypassing the issue of content not represented in both views and the\nambiguous matching of objects from one crop to the other. We demonstrate\nexcellent performance on linear and unsupervised segmentation transfer tasks on\nvarious datasets and similarly for video object segmentation. Our code and\npre-trained models are publicly available at https://github.com/stegmuel/CrOC.\n","authors":["Thomas Stegmüller","Tim Lebailly","Behzad Bozorgtabar","Tinne Tuytelaars","Jean-Philippe Thiran"],"pdf_url":"https://arxiv.org/pdf/2303.13245v1.pdf","comment":"Accepted at CVPR 2023, * denotes equal contribution"},{"id":"http://arxiv.org/abs/2303.13241v1","updated":"2023-03-23T13:18:05Z","published":"2023-03-23T13:18:05Z","title":"6D Object Pose Estimation from Approximate 3D Models for Orbital\n  Robotics","summary":"  We present a novel technique to estimate the 6D pose of objects from single\nimages where the 3D geometry of the object is only given approximately and not\nas a precise 3D model. To achieve this, we employ a dense 2D-to-3D\ncorrespondence predictor that regresses 3D model coordinates for every pixel.\nIn addition to the 3D coordinates, our model also estimates the pixel-wise\ncoordinate error to discard correspondences that are likely wrong. This allows\nus to generate multiple 6D pose hypotheses of the object, which we then refine\niteratively using a highly efficient region-based approach. We also introduce a\nnovel pixel-wise posterior formulation by which we can estimate the probability\nfor each hypothesis and select the most likely one. As we show in experiments,\nour approach is capable of dealing with extreme visual conditions including\noverexposure, high contrast, or low signal-to-noise ratio. This makes it a\npowerful technique for the particularly challenging task of estimating the pose\nof tumbling satellites for in-orbit robotic applications. Our method achieves\nstate-of-the-art performance on the SPEED+ dataset and has won the SPEC2021\npost-mortem competition.\n","authors":["Maximilian Ulmer","Maximilian Durner","Martin Sundermeyer","Manuel Stoiber","Rudolph Triebel"],"pdf_url":"https://arxiv.org/pdf/2303.13241v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2211.12499v2","updated":"2023-03-23T13:16:06Z","published":"2022-11-22T18:59:46Z","title":"Instant Volumetric Head Avatars","summary":"  We present Instant Volumetric Head Avatars (INSTA), a novel approach for\nreconstructing photo-realistic digital avatars instantaneously. INSTA models a\ndynamic neural radiance field based on neural graphics primitives embedded\naround a parametric face model. Our pipeline is trained on a single monocular\nRGB portrait video that observes the subject under different expressions and\nviews. While state-of-the-art methods take up to several days to train an\navatar, our method can reconstruct a digital avatar in less than 10 minutes on\nmodern GPU hardware, which is orders of magnitude faster than previous\nsolutions. In addition, it allows for the interactive rendering of novel poses\nand expressions. By leveraging the geometry prior of the underlying parametric\nface model, we demonstrate that INSTA extrapolates to unseen poses. In\nquantitative and qualitative studies on various subjects, INSTA outperforms\nstate-of-the-art methods regarding rendering quality and training time.\n","authors":["Wojciech Zielonka","Timo Bolkart","Justus Thies"],"pdf_url":"https://arxiv.org/pdf/2211.12499v2.pdf","comment":"Website: https://zielon.github.io/insta/ Video:\n  https://youtu.be/HOgaeWTih7Q Accepted to CVPR2023"},{"id":"http://arxiv.org/abs/2210.03591v3","updated":"2023-03-23T13:15:27Z","published":"2022-10-07T14:46:32Z","title":"Modeling Inter-Class and Intra-Class Constraints in Novel Class\n  Discovery","summary":"  Novel class discovery (NCD) aims at learning a model that transfers the\ncommon knowledge from a class-disjoint labelled dataset to another unlabelled\ndataset and discovers new classes (clusters) within it. Many methods, as well\nas elaborate training pipelines and appropriate objectives, have been proposed\nand considerably boosted performance on NCD tasks. Despite all this, we find\nthat the existing methods do not sufficiently take advantage of the essence of\nthe NCD setting. To this end, in this paper, we propose to model both\ninter-class and intra-class constraints in NCD based on the symmetric\nKullback-Leibler divergence (sKLD). Specifically, we propose an inter-class\nsKLD constraint to effectively exploit the disjoint relationship between\nlabelled and unlabelled classes, enforcing the separability for different\nclasses in the embedding space. In addition, we present an intra-class sKLD\nconstraint to explicitly constrain the intra-relationship between a sample and\nits augmentations and ensure the stability of the training process at the same\ntime. We conduct extensive experiments on the popular CIFAR10, CIFAR100 and\nImageNet benchmarks and successfully demonstrate that our method can establish\na new state of the art and can achieve significant performance improvements,\ne.g., 3.5%/3.7% clustering accuracy improvements on CIFAR100-50 dataset split\nunder the task-aware/-agnostic evaluation protocol, over previous\nstate-of-the-art methods. Code is available at\nhttps://github.com/FanZhichen/NCD-IIC.\n","authors":["Wenbin Li","Zhichen Fan","Jing Huo","Yang Gao"],"pdf_url":"https://arxiv.org/pdf/2210.03591v3.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2303.13233v1","updated":"2023-03-23T13:06:38Z","published":"2023-03-23T13:06:38Z","title":"Visually-Prompted Language Model for Fine-Grained Scene Graph Generation\n  in an Open World","summary":"  Scene Graph Generation (SGG) aims to extract <subject, predicate, object>\nrelationships in images for vision understanding. Although recent works have\nmade steady progress on SGG, they still suffer long-tail distribution issues\nthat tail-predicates are more costly to train and hard to distinguish due to a\nsmall amount of annotated data compared to frequent predicates. Existing\nre-balancing strategies try to haddle it via prior rules but are still confined\nto pre-defined conditions, which are not scalable for various models and\ndatasets. In this paper, we propose a Cross-modal prediCate boosting (CaCao)\nframework, where a visually-prompted language model is learned to generate\ndiverse fine-grained predicates in a low-resource way. The proposed CaCao can\nbe applied in a plug-and-play fashion and automatically strengthen existing SGG\nto tackle the long-tailed problem. Based on that, we further introduce a novel\nEntangled cross-modal prompt approach for open-world predicate scene graph\ngeneration (Epic), where models can generalize to unseen predicates in a\nzero-shot manner. Comprehensive experiments on three benchmark datasets show\nthat CaCao consistently boosts the performance of multiple scene graph\ngeneration models in a model-agnostic way. Moreover, our Epic achieves\ncompetitive performance on open-world predicate prediction.\n","authors":["Qifan Yu","Juncheng Li","Yu Wu","Siliang Tang","Wei Ji","Yueting Zhuang"],"pdf_url":"https://arxiv.org/pdf/2303.13233v1.pdf","comment":"21 pages, 16 figures"},{"id":"http://arxiv.org/abs/2303.13232v1","updated":"2023-03-23T13:05:57Z","published":"2023-03-23T13:05:57Z","title":"Transforming Radiance Field with Lipschitz Network for Photorealistic 3D\n  Scene Stylization","summary":"  Recent advances in 3D scene representation and novel view synthesis have\nwitnessed the rise of Neural Radiance Fields (NeRFs). Nevertheless, it is not\ntrivial to exploit NeRF for the photorealistic 3D scene stylization task, which\naims to generate visually consistent and photorealistic stylized scenes from\nnovel views. Simply coupling NeRF with photorealistic style transfer (PST) will\nresult in cross-view inconsistency and degradation of stylized view syntheses.\nThrough a thorough analysis, we demonstrate that this non-trivial task can be\nsimplified in a new light: When transforming the appearance representation of a\npre-trained NeRF with Lipschitz mapping, the consistency and photorealism\nacross source views will be seamlessly encoded into the syntheses. That\nmotivates us to build a concise and flexible learning framework namely LipRF,\nwhich upgrades arbitrary 2D PST methods with Lipschitz mapping tailored for the\n3D scene. Technically, LipRF first pre-trains a radiance field to reconstruct\nthe 3D scene, and then emulates the style on each view by 2D PST as the prior\nto learn a Lipschitz network to stylize the pre-trained appearance. In view of\nthat Lipschitz condition highly impacts the expressivity of the neural network,\nwe devise an adaptive regularization to balance the reconstruction and\nstylization. A gradual gradient aggregation strategy is further introduced to\noptimize LipRF in a cost-efficient manner. We conduct extensive experiments to\nshow the high quality and robust performance of LipRF on both photorealistic 3D\nstylization and object appearance editing.\n","authors":["Zicheng Zhang","Yinglu Liu","Congying Han","Yingwei Pan","Tiande Guo","Ting Yao"],"pdf_url":"https://arxiv.org/pdf/2303.13232v1.pdf","comment":"CVPR 2023, Highlight"},{"id":"http://arxiv.org/abs/2210.00924v2","updated":"2023-03-23T13:02:42Z","published":"2022-10-03T13:30:40Z","title":"Multi-view object pose estimation from correspondence distributions and\n  epipolar geometry","summary":"  In many automation tasks involving manipulation of rigid objects, the poses\nof the objects must be acquired. Vision-based pose estimation using a single\nRGB or RGB-D sensor is especially popular due to its broad applicability.\nHowever, single-view pose estimation is inherently limited by depth ambiguity\nand ambiguities imposed by various phenomena like occlusion, self-occlusion,\nreflections, etc. Aggregation of information from multiple views can\npotentially resolve these ambiguities, but the current state-of-the-art\nmulti-view pose estimation method only uses multiple views to aggregate\nsingle-view pose estimates, and thus rely on obtaining good single-view\nestimates. We present a multi-view pose estimation method which aggregates\nlearned 2D-3D distributions from multiple views for both the initial estimate\nand optional refinement. Our method performs probabilistic sampling of 3D-3D\ncorrespondences under epipolar constraints using learned 2D-3D correspondence\ndistributions which are implicitly trained to respect visual ambiguities such\nas symmetry. Evaluation on the T-LESS dataset shows that our method reduces\npose estimation errors by 80-91% compared to the best single-view method, and\nwe present state-of-the-art results on T-LESS with four views, even compared\nwith methods using five and eight views.\n","authors":["Rasmus Laurvig Haugaard","Thorbjørn Mosekjær Iversen"],"pdf_url":"https://arxiv.org/pdf/2210.00924v2.pdf","comment":"7 pages, 2 figures, 1 table, ICRA 2023"},{"id":"http://arxiv.org/abs/2301.12900v2","updated":"2023-03-23T12:55:02Z","published":"2023-01-30T14:02:33Z","title":"DepGraph: Towards Any Structural Pruning","summary":"  Structural pruning enables model acceleration by removing\nstructurally-grouped parameters from neural networks. However, the\nparameter-grouping patterns vary widely across different models, making\narchitecture-specific pruners, which rely on manually-designed grouping\nschemes, non-generalizable to new architectures. In this work, we study a\nhighly-challenging yet barely-explored task, any structural pruning, to tackle\ngeneral structural pruning of arbitrary architecture like CNNs, RNNs, GNNs and\nTransformers. The most prominent obstacle towards this goal lies in the\nstructural coupling, which not only forces different layers to be pruned\nsimultaneously, but also expects all removed parameters to be consistently\nunimportant, thereby avoiding structural issues and significant performance\ndegradation after pruning. To address this problem, we propose a general and\n{fully automatic} method, \\emph{Dependency Graph} (DepGraph), to explicitly\nmodel the dependency between layers and comprehensively group coupled\nparameters for pruning. In this work, we extensively evaluate our method on\nseveral architectures and tasks, including ResNe(X)t, DenseNet, MobileNet and\nVision transformer for images, GAT for graph, DGCNN for 3D point cloud,\nalongside LSTM for language, and demonstrate that, even with a simple\nnorm-based criterion, the proposed method consistently yields gratifying\nperformances.\n","authors":["Gongfan Fang","Xinyin Ma","Mingli Song","Michael Bi Mi","Xinchao Wang"],"pdf_url":"https://arxiv.org/pdf/2301.12900v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07150v2","updated":"2023-03-23T12:49:39Z","published":"2023-03-13T14:23:39Z","title":"Multi PILOT: Learned Feasible Multiple Acquisition Trajectories for\n  Dynamic MRI","summary":"  Dynamic Magnetic Resonance Imaging (MRI) is known to be a powerful and\nreliable technique for the dynamic imaging of internal organs and tissues,\nmaking it a leading diagnostic tool. A major difficulty in using MRI in this\nsetting is the relatively long acquisition time (and, hence, increased cost)\nrequired for imaging in high spatio-temporal resolution, leading to the\nappearance of related motion artifacts and decrease in resolution. Compressed\nSensing (CS) techniques have become a common tool to reduce MRI acquisition\ntime by subsampling images in the k-space according to some acquisition\ntrajectory. Several studies have particularly focused on applying deep learning\ntechniques to learn these acquisition trajectories in order to attain better\nimage reconstruction, rather than using some predefined set of trajectories. To\nthe best of our knowledge, learning acquisition trajectories has been only\nexplored in the context of static MRI. In this study, we consider acquisition\ntrajectory learning in the dynamic imaging setting. We design an end-to-end\npipeline for the joint optimization of multiple per-frame acquisition\ntrajectories along with a reconstruction neural network, and demonstrate\nimproved image reconstruction quality in shorter acquisition times. The code\nfor reproducing all experiments is accessible at\nhttps://github.com/tamirshor7/MultiPILOT.\n","authors":["Tamir Shor","Tomer Weiss","Dor Noti","Alex Bronstein"],"pdf_url":"https://arxiv.org/pdf/2303.07150v2.pdf","comment":"Accepted For MIDL 2023"},{"id":"http://arxiv.org/abs/2303.13227v1","updated":"2023-03-23T12:48:47Z","published":"2023-03-23T12:48:47Z","title":"Confidence-Aware and Self-Supervised Image Anomaly Localisation","summary":"  Universal anomaly detection still remains a challenging problem in machine\nlearning and medical image analysis. It is possible to learn an expected\ndistribution from a single class of normative samples, e.g., through epistemic\nuncertainty estimates, auto-encoding models, or from synthetic anomalies in a\nself-supervised way. The performance of self-supervised anomaly detection\napproaches is still inferior compared to methods that use examples from known\nunknown classes to shape the decision boundary. However, outlier exposure\nmethods often do not identify unknown unknowns. Here we discuss an improved\nself-supervised single-class training strategy that supports the approximation\nof probabilistic inference with loosen feature locality constraints. We show\nthat up-scaling of gradients with histogram-equalised images is beneficial for\nrecently proposed self-supervision tasks. Our method is integrated into several\nout-of-distribution (OOD) detection models and we show evidence that our method\noutperforms the state-of-the-art on various benchmark datasets. Source code\nwill be publicly available by the time of the conference.\n","authors":["Johanna P. Müller","Matthew Baugh","Jeremy Tan","Mischa Dombrowski","Bernhard Kainz"],"pdf_url":"https://arxiv.org/pdf/2303.13227v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2303.13223v1","updated":"2023-03-23T12:39:20Z","published":"2023-03-23T12:39:20Z","title":"Exploring Structured Semantic Prior for Multi Label Recognition with\n  Incomplete Labels","summary":"  Multi-label recognition (MLR) with incomplete labels is very challenging.\nRecent works strive to explore the image-to-label correspondence in the\nvision-language model, \\ie, CLIP~\\cite{radford2021clip}, to compensate for\ninsufficient annotations. In spite of promising performance, they generally\noverlook the valuable prior about the label-to-label correspondence. In this\npaper, we advocate remedying the deficiency of label supervision for the MLR\nwith incomplete labels by deriving a structured semantic prior about the\nlabel-to-label correspondence via a semantic prior prompter. We then present a\nnovel Semantic Correspondence Prompt Network (SCPNet), which can thoroughly\nexplore the structured semantic prior. A Prior-Enhanced Self-Supervised\nLearning method is further introduced to enhance the use of the prior.\nComprehensive experiments and analyses on several widely used benchmark\ndatasets show that our method significantly outperforms existing methods on all\ndatasets, well demonstrating the effectiveness and the superiority of our\nmethod. Our code will be available at https://github.com/jameslahm/SCPNet.\n","authors":["Zixuan Ding","Ao Wang","Hui Chen","Qiang Zhang","Pengzhang Liu","Yongjun Bao","Weipeng Yan","Jungong Han"],"pdf_url":"https://arxiv.org/pdf/2303.13223v1.pdf","comment":"Accepted by IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2023"},{"id":"http://arxiv.org/abs/2303.13221v1","updated":"2023-03-23T12:34:52Z","published":"2023-03-23T12:34:52Z","title":"Explore the Power of Synthetic Data on Few-shot Object Detection","summary":"  Few-shot object detection (FSOD) aims to expand an object detector for novel\ncategories given only a few instances for training. The few training samples\nrestrict the performance of FSOD model. Recent text-to-image generation models\nhave shown promising results in generating high-quality images. How applicable\nthese synthetic images are for FSOD tasks remains under-explored. This work\nextensively studies how synthetic images generated from state-of-the-art\ntext-to-image generators benefit FSOD tasks. We focus on two perspectives: (1)\nHow to use synthetic data for FSOD? (2) How to find representative samples from\nthe large-scale synthetic dataset? We design a copy-paste-based pipeline for\nusing synthetic data. Specifically, saliency object detection is applied to the\noriginal generated image, and the minimum enclosing box is used for cropping\nthe main object based on the saliency map. After that, the cropped object is\nrandomly pasted on the image, which comes from the base dataset. We also study\nthe influence of the input text of text-to-image generator and the number of\nsynthetic images used. To construct a representative synthetic training\ndataset, we maximize the diversity of the selected images via a sample-based\nand cluster-based method. However, the severe problem of high false positives\n(FP) ratio of novel categories in FSOD can not be solved by using synthetic\ndata. We propose integrating CLIP, a zero-shot recognition model, into the FSOD\npipeline, which can filter 90% of FP by defining a threshold for the similarity\nscore between the detected object and the text of the predicted category.\nExtensive experiments on PASCAL VOC and MS COCO validate the effectiveness of\nour method, in which performance gain is up to 21.9% compared to the few-shot\nbaseline.\n","authors":["Shaobo Lin","Kun Wang","Xingyu Zeng","Rui Zhao"],"pdf_url":"https://arxiv.org/pdf/2303.13221v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.01047v2","updated":"2023-03-23T12:30:35Z","published":"2023-02-02T12:21:10Z","title":"Real-Time Evaluation in Online Continual Learning: A New Hope","summary":"  Current evaluations of Continual Learning (CL) methods typically assume that\nthere is no constraint on training time and computation. This is an unrealistic\nassumption for any real-world setting, which motivates us to propose: a\npractical real-time evaluation of continual learning, in which the stream does\nnot wait for the model to complete training before revealing the next data for\npredictions. To do this, we evaluate current CL methods with respect to their\ncomputational costs. We conduct extensive experiments on CLOC, a large-scale\ndataset containing 39 million time-stamped images with geolocation labels. We\nshow that a simple baseline outperforms state-of-the-art CL methods under this\nevaluation, questioning the applicability of existing methods in realistic\nsettings. In addition, we explore various CL components commonly used in the\nliterature, including memory sampling strategies and regularization approaches.\nWe find that all considered methods fail to be competitive against our simple\nbaseline. This surprisingly suggests that the majority of existing CL\nliterature is tailored to a specific class of streams that is not practical. We\nhope that the evaluation we provide will be the first step towards a paradigm\nshift to consider the computational cost in the development of online continual\nlearning methods.\n","authors":["Yasir Ghunaim","Adel Bibi","Kumail Alhamoud","Motasem Alfarra","Hasan Abed Al Kader Hammoud","Ameya Prabhu","Philip H. S. Torr","Bernard Ghanem"],"pdf_url":"https://arxiv.org/pdf/2302.01047v2.pdf","comment":"Accepted at CVPR'23 as Highlight (Top 2.5%)"},{"id":"http://arxiv.org/abs/2301.07702v2","updated":"2023-03-23T12:25:12Z","published":"2023-01-18T18:47:46Z","title":"Learning 3D-aware Image Synthesis with Unknown Pose Distribution","summary":"  Existing methods for 3D-aware image synthesis largely depend on the 3D pose\ndistribution pre-estimated on the training set. An inaccurate estimation may\nmislead the model into learning faulty geometry. This work proposes PoF3D that\nfrees generative radiance fields from the requirements of 3D pose priors. We\nfirst equip the generator with an efficient pose learner, which is able to\ninfer a pose from a latent code, to approximate the underlying true pose\ndistribution automatically. We then assign the discriminator a task to learn\npose distribution under the supervision of the generator and to differentiate\nreal and synthesized images with the predicted pose as the condition. The\npose-free generator and the pose-aware discriminator are jointly trained in an\nadversarial manner. Extensive results on a couple of datasets confirm that the\nperformance of our approach, regarding both image quality and geometry quality,\nis on par with state of the art. To our best knowledge, PoF3D demonstrates the\nfeasibility of learning high-quality 3D-aware image synthesis without using 3D\npose priors for the first time.\n","authors":["Zifan Shi","Yujun Shen","Yinghao Xu","Sida Peng","Yiyi Liao","Sheng Guo","Qifeng Chen","Dit-Yan Yeung"],"pdf_url":"https://arxiv.org/pdf/2301.07702v2.pdf","comment":"CVPR 2023. Project page: https://vivianszf.github.io/pof3d/"},{"id":"http://arxiv.org/abs/2303.13212v1","updated":"2023-03-23T12:13:29Z","published":"2023-03-23T12:13:29Z","title":"A Simple and Generic Framework for Feature Distillation via Channel-wise\n  Transformation","summary":"  Knowledge distillation is a popular technique for transferring the knowledge\nfrom a large teacher model to a smaller student model by mimicking. However,\ndistillation by directly aligning the feature maps between teacher and student\nmay enforce overly strict constraints on the student thus degrade the\nperformance of the student model. To alleviate the above feature misalignment\nissue, existing works mainly focus on spatially aligning the feature maps of\nthe teacher and the student, with pixel-wise transformation. In this paper, we\nnewly find that aligning the feature maps between teacher and student along the\nchannel-wise dimension is also effective for addressing the feature\nmisalignment issue. Specifically, we propose a learnable nonlinear channel-wise\ntransformation to align the features of the student and the teacher model.\nBased on it, we further propose a simple and generic framework for feature\ndistillation, with only one hyper-parameter to balance the distillation loss\nand the task specific loss. Extensive experimental results show that our method\nachieves significant performance improvements in various computer vision tasks\nincluding image classification (+3.28% top-1 accuracy for MobileNetV1 on\nImageNet-1K), object detection (+3.9% bbox mAP for ResNet50-based Faster-RCNN\non MS COCO), instance segmentation (+2.8% Mask mAP for ResNet50-based\nMask-RCNN), and semantic segmentation (+4.66% mIoU for ResNet18-based PSPNet in\nsemantic segmentation on Cityscapes), which demonstrates the effectiveness and\nthe versatility of the proposed method. The code will be made publicly\navailable.\n","authors":["Ziwei Liu","Yongtao Wang","Xiaojie Chu"],"pdf_url":"https://arxiv.org/pdf/2303.13212v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2303.13211v1","updated":"2023-03-23T12:11:24Z","published":"2023-03-23T12:11:24Z","title":"Don't FREAK Out: A Frequency-Inspired Approach to Detecting Backdoor\n  Poisoned Samples in DNNs","summary":"  In this paper we investigate the frequency sensitivity of Deep Neural\nNetworks (DNNs) when presented with clean samples versus poisoned samples. Our\nanalysis shows significant disparities in frequency sensitivity between these\ntwo types of samples. Building on these findings, we propose FREAK, a\nfrequency-based poisoned sample detection algorithm that is simple yet\neffective. Our experimental results demonstrate the efficacy of FREAK not only\nagainst frequency backdoor attacks but also against some spatial attacks. Our\nwork is just the first step in leveraging these insights. We believe that our\nanalysis and proposed defense mechanism will provide a foundation for future\nresearch and development of backdoor defenses.\n","authors":["Hasan Abed Al Kader Hammoud","Adel Bibi","Philip H. S. Torr","Bernard Ghanem"],"pdf_url":"https://arxiv.org/pdf/2303.13211v1.pdf","comment":"Accepted at CVPRW (The Art of Robustness)"},{"id":"http://arxiv.org/abs/2303.13209v1","updated":"2023-03-23T12:08:10Z","published":"2023-03-23T12:08:10Z","title":"Taking A Closer Look at Visual Relation: Unbiased Video Scene Graph\n  Generation with Decoupled Label Learning","summary":"  Current video-based scene graph generation (VidSGG) methods have been found\nto perform poorly on predicting predicates that are less represented due to the\ninherent biased distribution in the training data. In this paper, we take a\ncloser look at the predicates and identify that most visual relations (e.g.\nsit_above) involve both actional pattern (sit) and spatial pattern (above),\nwhile the distribution bias is much less severe at the pattern level. Based on\nthis insight, we propose a decoupled label learning (DLL) paradigm to address\nthe intractable visual relation prediction from the pattern-level perspective.\nSpecifically, DLL decouples the predicate labels and adopts separate\nclassifiers to learn actional and spatial patterns respectively. The patterns\nare then combined and mapped back to the predicate. Moreover, we propose a\nknowledge-level label decoupling method to transfer non-target knowledge from\nhead predicates to tail predicates within the same pattern to calibrate the\ndistribution of tail classes. We validate the effectiveness of DLL on the\ncommonly used VidSGG benchmark, i.e. VidVRD. Extensive experiments demonstrate\nthat the DLL offers a remarkably simple but highly effective solution to the\nlong-tailed problem, achieving the state-of-the-art VidSGG performance.\n","authors":["Wenqing Wang","Yawei Luo","Zhiqing Chen","Tao Jiang","Lei Chen","Yi Yang","Jun Xiao"],"pdf_url":"https://arxiv.org/pdf/2303.13209v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.05725v3","updated":"2023-03-23T12:00:33Z","published":"2023-03-10T06:12:36Z","title":"CVT-SLR: Contrastive Visual-Textual Transformation for Sign Language\n  Recognition with Variational Alignment","summary":"  Sign language recognition (SLR) is a weakly supervised task that annotates\nsign videos as textual glosses. Recent studies show that insufficient training\ncaused by the lack of large-scale available sign datasets becomes the main\nbottleneck for SLR. Most SLR works thereby adopt pretrained visual modules and\ndevelop two mainstream solutions. The multi-stream architectures extend\nmulti-cue visual features, yielding the current SOTA performances but requiring\ncomplex designs and might introduce potential noise. Alternatively, the\nadvanced single-cue SLR frameworks using explicit cross-modal alignment between\nvisual and textual modalities are simple and effective, potentially competitive\nwith the multi-cue framework. In this work, we propose a novel contrastive\nvisual-textual transformation for SLR, CVT-SLR, to fully explore the pretrained\nknowledge of both the visual and language modalities. Based on the single-cue\ncross-modal alignment framework, we propose a variational autoencoder (VAE) for\npretrained contextual knowledge while introducing the complete pretrained\nlanguage module. The VAE implicitly aligns visual and textual modalities while\nbenefiting from pretrained contextual knowledge as the traditional contextual\nmodule. Meanwhile, a contrastive cross-modal alignment algorithm is designed to\nexplicitly enhance the consistency constraints. Extensive experiments on public\ndatasets (PHOENIX-2014 and PHOENIX-2014T) demonstrate that our proposed CVT-SLR\nconsistently outperforms existing single-cue methods and even outperforms SOTA\nmulti-cue methods.\n","authors":["Jiangbin Zheng","Yile Wang","Cheng Tan","Siyuan Li","Ge Wang","Jun Xia","Yidong Chen","Stan Z. Li"],"pdf_url":"https://arxiv.org/pdf/2303.05725v3.pdf","comment":"Accepted to CVPR 2023 (Highlight Paper Top 2.5%)"},{"id":"http://arxiv.org/abs/2303.13203v1","updated":"2023-03-23T11:57:50Z","published":"2023-03-23T11:57:50Z","title":"A Confident Labelling Strategy Based on Deep Learning for Improving\n  Early Detection of Knee OsteoArthritis","summary":"  Knee OsteoArthritis (KOA) is a prevalent musculoskeletal disorder that causes\ndecreased mobility in seniors. The diagnosis provided by physicians is\nsubjective, however, as it relies on personal experience and the\nsemi-quantitative Kellgren-Lawrence (KL) scoring system. KOA has been\nsuccessfully diagnosed by Computer-Aided Diagnostic (CAD) systems that use deep\nlearning techniques like Convolutional Neural Networks (CNN). In this paper, we\npropose a novel Siamese-based network, and we introduce a new hybrid loss\nstrategy for the early detection of KOA. The model extends the classical\nSiamese network by integrating a collection of Global Average Pooling (GAP)\nlayers for feature extraction at each level. Then, to improve the\nclassification performance, a novel training strategy that partitions each\ntraining batch into low-, medium- and high-confidence subsets, and a specific\nhybrid loss function are used for each new label attributed to each sample. The\nfinal loss function is then derived by combining the latter loss functions with\noptimized weights. Our test results demonstrate that our proposed approach\nsignificantly improves the detection performance.\n","authors":["Zhe Wang","Aladine Chetouani","Rachid Jennane"],"pdf_url":"https://arxiv.org/pdf/2303.13203v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13199v1","updated":"2023-03-23T11:54:41Z","published":"2023-03-23T11:54:41Z","title":"First Session Adaptation: A Strong Replay-Free Baseline for\n  Class-Incremental Learning","summary":"  In Class-Incremental Learning (CIL) an image classification system is exposed\nto new classes in each learning session and must be updated incrementally.\nMethods approaching this problem have updated both the classification head and\nthe feature extractor body at each session of CIL. In this work, we develop a\nbaseline method, First Session Adaptation (FSA), that sheds light on the\nefficacy of existing CIL approaches and allows us to assess the relative\nperformance contributions from head and body adaption. FSA adapts a pre-trained\nneural network body only on the first learning session and fixes it thereafter;\na head based on linear discriminant analysis (LDA), is then placed on top of\nthe adapted body, allowing exact updates through CIL. FSA is replay-free\ni.e.~it does not memorize examples from previous sessions of continual\nlearning. To empirically motivate FSA, we first consider a diverse selection of\n22 image-classification datasets, evaluating different heads and body\nadaptation techniques in high/low-shot offline settings. We find that the LDA\nhead performs well and supports CIL out-of-the-box. We also find that\nFeaturewise Layer Modulation (FiLM) adapters are highly effective in the\nfew-shot setting, and full-body adaption in the high-shot setting. Second, we\nempirically investigate various CIL settings including high-shot CIL and\nfew-shot CIL, including settings that have previously been used in the\nliterature. We show that FSA significantly improves over the state-of-the-art\nin 15 of the 16 settings considered. FSA with FiLM adapters is especially\nperformant in the few-shot setting. These results indicate that current\napproaches to continuous body adaptation are not working as expected. Finally,\nwe propose a measure that can be applied to a set of unlabelled inputs which is\npredictive of the benefits of body adaptation.\n","authors":["Aristeidis Panos","Yuriko Kobe","Daniel Olmeda Reino","Rahaf Aljundi","Richard E. Turner"],"pdf_url":"https://arxiv.org/pdf/2303.13199v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13194v1","updated":"2023-03-23T11:52:17Z","published":"2023-03-23T11:52:17Z","title":"Complementary Pseudo Multimodal Feature for Point Cloud Anomaly\n  Detection","summary":"  Point cloud (PCD) anomaly detection steadily emerges as a promising research\narea. This study aims to improve PCD anomaly detection performance by combining\nhandcrafted PCD descriptions with powerful pre-trained 2D neural networks. To\nthis end, this study proposes Complementary Pseudo Multimodal Feature (CPMF)\nthat incorporates local geometrical information in 3D modality using\nhandcrafted PCD descriptors and global semantic information in the generated\npseudo 2D modality using pre-trained 2D neural networks. For global semantics\nextraction, CPMF projects the origin PCD into a pseudo 2D modality containing\nmulti-view images. These images are delivered to pre-trained 2D neural networks\nfor informative 2D modality feature extraction. The 3D and 2D modality features\nare aggregated to obtain the CPMF for PCD anomaly detection. Extensive\nexperiments demonstrate the complementary capacity between 2D and 3D modality\nfeatures and the effectiveness of CPMF, with 95.15% image-level AU-ROC and\n92.93% pixel-level PRO on the MVTec3D benchmark. Code is available on\nhttps://github.com/caoyunkang/CPMF.\n","authors":["Yunkang Cao","Xiaohao Xu","Weiming Shen"],"pdf_url":"https://arxiv.org/pdf/2303.13194v1.pdf","comment":"Submitted to Pattern Recognition. Code is available on\n  https://github.com/caoyunkang/CPMF"},{"id":"http://arxiv.org/abs/2303.13193v1","updated":"2023-03-23T11:50:44Z","published":"2023-03-23T11:50:44Z","title":"VADER: Video Alignment Differencing and Retrieval","summary":"  We propose VADER, a spatio-temporal matching, alignment, and change\nsummarization method to help fight misinformation spread via manipulated\nvideos. VADER matches and coarsely aligns partial video fragments to candidate\nvideos using a robust visual descriptor and scalable search over adaptively\nchunked video content. A transformer-based alignment module then refines the\ntemporal localization of the query fragment within the matched video. A\nspace-time comparator module identifies regions of manipulation between aligned\ncontent, invariant to any changes due to any residual temporal misalignments or\nartifacts arising from non-editorial changes of the content. Robustly matching\nvideo to a trusted source enables conclusions to be drawn on video provenance,\nenabling informed trust decisions on content encountered.\n","authors":["Alexander Black","Simon Jenni","Tu Bui","Md. Mehrab Tanjim","Stefano Petrangeli","Ritwik Sinha","Viswanathan Swaminathan","John Collomosse"],"pdf_url":"https://arxiv.org/pdf/2303.13193v1.pdf","comment":"Submitted to ICCV2023"},{"id":"http://arxiv.org/abs/2209.04747v4","updated":"2023-03-23T11:42:58Z","published":"2022-09-10T22:00:30Z","title":"Diffusion Models in Vision: A Survey","summary":"  Denoising diffusion models represent a recent emerging topic in computer\nvision, demonstrating remarkable results in the area of generative modeling. A\ndiffusion model is a deep generative model that is based on two stages, a\nforward diffusion stage and a reverse diffusion stage. In the forward diffusion\nstage, the input data is gradually perturbed over several steps by adding\nGaussian noise. In the reverse stage, a model is tasked at recovering the\noriginal input data by learning to gradually reverse the diffusion process,\nstep by step. Diffusion models are widely appreciated for the quality and\ndiversity of the generated samples, despite their known computational burdens,\ni.e. low speeds due to the high number of steps involved during sampling. In\nthis survey, we provide a comprehensive review of articles on denoising\ndiffusion models applied in vision, comprising both theoretical and practical\ncontributions in the field. First, we identify and present three generic\ndiffusion modeling frameworks, which are based on denoising diffusion\nprobabilistic models, noise conditioned score networks, and stochastic\ndifferential equations. We further discuss the relations between diffusion\nmodels and other deep generative models, including variational auto-encoders,\ngenerative adversarial networks, energy-based models, autoregressive models and\nnormalizing flows. Then, we introduce a multi-perspective categorization of\ndiffusion models applied in computer vision. Finally, we illustrate the current\nlimitations of diffusion models and envision some interesting directions for\nfuture research.\n","authors":["Florinel-Alin Croitoru","Vlad Hondru","Radu Tudor Ionescu","Mubarak Shah"],"pdf_url":"https://arxiv.org/pdf/2209.04747v4.pdf","comment":"Accepted in IEEE Transactions on Pattern Analysis and Machine\n  Intelligence. 25 pages, 3 figures"},{"id":"http://arxiv.org/abs/2303.13190v1","updated":"2023-03-23T11:42:35Z","published":"2023-03-23T11:42:35Z","title":"Marching-Primitives: Shape Abstraction from Signed Distance Function","summary":"  Representing complex objects with basic geometric primitives has long been a\ntopic in computer vision. Primitive-based representations have the merits of\ncompactness and computational efficiency in higher-level tasks such as physics\nsimulation, collision checking, and robotic manipulation. Unlike previous works\nwhich extract polygonal meshes from a signed distance function (SDF), in this\npaper, we present a novel method, named Marching-Primitives, to obtain a\nprimitive-based abstraction directly from an SDF. Our method grows geometric\nprimitives (such as superquadrics) iteratively by analyzing the connectivity of\nvoxels while marching at different levels of signed distance. For each valid\nconnected volume of interest, we march on the scope of voxels from which a\nprimitive is able to be extracted in a probabilistic sense and simultaneously\nsolve for the parameters of the primitive to capture the underlying local\ngeometry. We evaluate the performance of our method on both synthetic and\nreal-world datasets. The results show that the proposed method outperforms the\nstate-of-the-art in terms of accuracy, and is directly generalizable among\ndifferent categories and scales. The code is open-sourced at\nhttps://github.com/ChirikjianLab/Marching-Primitives.git.\n","authors":["Weixiao Liu","Yuwei Wu","Sipu Ruan","Gregory S. Chirikjian"],"pdf_url":"https://arxiv.org/pdf/2303.13190v1.pdf","comment":"Accepted to CVPR2023 Highlight"},{"id":"http://arxiv.org/abs/2302.06081v2","updated":"2023-03-23T11:38:53Z","published":"2023-02-13T03:34:49Z","title":"Correspondence-Free Domain Alignment for Unsupervised Cross-Domain Image\n  Retrieval","summary":"  Cross-domain image retrieval aims at retrieving images across different\ndomains to excavate cross-domain classificatory or correspondence\nrelationships. This paper studies a less-touched problem of cross-domain image\nretrieval, i.e., unsupervised cross-domain image retrieval, considering the\nfollowing practical assumptions: (i) no correspondence relationship, and (ii)\nno category annotations. It is challenging to align and bridge distinct domains\nwithout cross-domain correspondence. To tackle the challenge, we present a\nnovel Correspondence-free Domain Alignment (CoDA) method to effectively\neliminate the cross-domain gap through In-domain Self-matching Supervision\n(ISS) and Cross-domain Classifier Alignment (CCA). To be specific, ISS is\npresented to encapsulate discriminative information into the latent common\nspace by elaborating a novel self-matching supervision mechanism. To alleviate\nthe cross-domain discrepancy, CCA is proposed to align distinct domain-specific\nclassifiers. Thanks to the ISS and CCA, our method could encode the\ndiscrimination into the domain-invariant embedding space for unsupervised\ncross-domain image retrieval. To verify the effectiveness of the proposed\nmethod, extensive experiments are conducted on four benchmark datasets compared\nwith six state-of-the-art methods.\n","authors":["Xu Wang","Dezhong Peng","Ming Yan","Peng Hu"],"pdf_url":"https://arxiv.org/pdf/2302.06081v2.pdf","comment":"AAAI 2023"},{"id":"http://arxiv.org/abs/2303.13186v1","updated":"2023-03-23T11:36:14Z","published":"2023-03-23T11:36:14Z","title":"ScanERU: Interactive 3D Visual Grounding based on Embodied Reference\n  Understanding","summary":"  Aiming to link natural language descriptions to specific regions in a 3D\nscene represented as 3D point clouds, 3D visual grounding is a very fundamental\ntask for human-robot interaction. The recognition errors can significantly\nimpact the overall accuracy and then degrade the operation of AI systems.\nDespite their effectiveness, existing methods suffer from the difficulty of low\nrecognition accuracy in cases of multiple adjacent objects with similar\nappearances.To address this issue, this work intuitively introduces the\nhuman-robot interaction as a cue to facilitate the development of 3D visual\ngrounding. Specifically, a new task termed Embodied Reference Understanding\n(ERU) is first designed for this concern. Then a new dataset called ScanERU is\nconstructed to evaluate the effectiveness of this idea. Different from existing\ndatasets, our ScanERU is the first to cover semi-synthetic scene integration\nwith textual, real-world visual, and synthetic gestural information.\nAdditionally, this paper formulates a heuristic framework based on attention\nmechanisms and human body movements to enlighten the research of ERU.\nExperimental results demonstrate the superiority of the proposed method,\nespecially in the recognition of multiple identical objects. Our codes and\ndataset are ready to be available publicly.\n","authors":["Ziyang Lu","Yunqiang Pei","Guoqing Wang","Yang Yang","Zheng Wang","Heng Tao Shen"],"pdf_url":"https://arxiv.org/pdf/2303.13186v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13182v1","updated":"2023-03-23T11:29:31Z","published":"2023-03-23T11:29:31Z","title":"CMG-Net: An End-to-End Contact-Based Multi-Finger Dexterous Grasping\n  Network","summary":"  In this paper, we propose a novel representation for grasping using contacts\nbetween multi-finger robotic hands and objects to be manipulated. This\nrepresentation significantly reduces the prediction dimensions and accelerates\nthe learning process. We present an effective end-to-end network, CMG-Net, for\ngrasping unknown objects in a cluttered environment by efficiently predicting\nmulti-finger grasp poses and hand configurations from a single-shot point\ncloud. Moreover, we create a synthetic grasp dataset that consists of five\nthousand cluttered scenes, 80 object categories, and 20 million annotations. We\nperform a comprehensive empirical study and demonstrate the effectiveness of\nour grasping representation and CMG-Net. Our work significantly outperforms the\nstate-of-the-art for three-finger robotic hands. We also demonstrate that the\nmodel trained using synthetic data performs very well for real robots.\n","authors":["Mingze Wei","Yaomin Huang","Zhiyuan Xu","Ning Liu","Zhengping Che","Xinyu Zhang","Chaomin Shen","Feifei Feng","Chun Shan","Jian Tang"],"pdf_url":"https://arxiv.org/pdf/2303.13182v1.pdf","comment":"The first two authors are with equal contributions. Paper accepted by\n  ICRA 2023"},{"id":"http://arxiv.org/abs/2206.06067v4","updated":"2023-03-23T11:17:31Z","published":"2022-06-13T11:52:13Z","title":"Better Teacher Better Student: Dynamic Prior Knowledge for Knowledge\n  Distillation","summary":"  Knowledge distillation (KD) has shown very promising capabilities in\ntransferring learning representations from large models (teachers) to small\nmodels (students). However, as the capacity gap between students and teachers\nbecomes larger, existing KD methods fail to achieve better results. Our work\nshows that the `prior knowledge' is vital to KD, especially when applying large\nteachers. Particularly, we propose the dynamic prior knowledge (DPK), which\nintegrates part of teacher's features as the prior knowledge before the feature\ndistillation. This means that our method also takes the teacher's feature as\n`input', not just `target'. Besides, we dynamically adjust the ratio of the\nprior knowledge during the training phase according to the feature gap, thus\nguiding the student in an appropriate difficulty. To evaluate the proposed\nmethod, we conduct extensive experiments on two image classification benchmarks\n(i.e. CIFAR100 and ImageNet) and an object detection benchmark (i.e. MS COCO.\nThe results demonstrate the superiority of our method in performance under\nvarying settings. Besides, our DPK makes the performance of the student model\npositively correlated with that of the teacher model, which means that we can\nfurther boost the accuracy of students by applying larger teachers. More\nimportantly, DPK provides a fast solution in teacher model selection for any\ngiven model.\n","authors":["Zengyu Qiu","Xinzhu Ma","Kunlin Yang","Chunya Liu","Jun Hou","Shuai Yi","Wanli Ouyang"],"pdf_url":"https://arxiv.org/pdf/2206.06067v4.pdf","comment":"ICLR'23 accepted"},{"id":"http://arxiv.org/abs/2211.13654v2","updated":"2023-03-23T11:14:35Z","published":"2022-11-24T15:09:33Z","title":"Cross Aggregation Transformer for Image Restoration","summary":"  Recently, Transformer architecture has been introduced into image restoration\nto replace convolution neural network (CNN) with surprising results.\nConsidering the high computational complexity of Transformer with global\nattention, some methods use the local square window to limit the scope of\nself-attention. However, these methods lack direct interaction among different\nwindows, which limits the establishment of long-range dependencies. To address\nthe above issue, we propose a new image restoration model, Cross Aggregation\nTransformer (CAT). The core of our CAT is the Rectangle-Window Self-Attention\n(Rwin-SA), which utilizes horizontal and vertical rectangle window attention in\ndifferent heads parallelly to expand the attention area and aggregate the\nfeatures cross different windows. We also introduce the Axial-Shift operation\nfor different window interactions. Furthermore, we propose the Locality\nComplementary Module to complement the self-attention mechanism, which\nincorporates the inductive bias of CNN (e.g., translation invariance and\nlocality) into Transformer, enabling global-local coupling. Extensive\nexperiments demonstrate that our CAT outperforms recent state-of-the-art\nmethods on several image restoration applications. The code and models are\navailable at https://github.com/zhengchen1999/CAT.\n","authors":["Zheng Chen","Yulun Zhang","Jinjin Gu","Yongbing Zhang","Linghe Kong","Xin Yuan"],"pdf_url":"https://arxiv.org/pdf/2211.13654v2.pdf","comment":"Accepted to NeurIPS 2022. Code is available at\n  https://github.com/zhengchen1999/CAT"},{"id":"http://arxiv.org/abs/2303.13175v1","updated":"2023-03-23T11:11:41Z","published":"2023-03-23T11:11:41Z","title":"Enhancement of theColor Image Compression Using a New Algorithm based on\n  Discrete Hermite Wavelet Transform","summary":"  The Internet has turned the entire world into a small village;this is because\nit has made it possible to share millions of images and videos. However,\nsending and receiving a huge amount of data is considered to be a main\nchallenge. To address this issue, a new algorithm is required to reduce image\nbits and represent the data in a compressed form. Nevertheless, image\ncompression is an important application for transferring large files and\nimages. This requires appropriate and efficient transfers in this field to\nachieve the task and reach the best results. In this work, we propose a new\nalgorithm based on discrete Hermite wavelets transformation (DHWT) that shows\nthe efficiency and quality of the color images. By compressing the color image,\nthis method analyzes it and divides it into approximate coefficients and detail\ncoefficients after adding the wavelets into MATLAB. With Multi-Resolution\nAnalyses (MRA), the appropriate filter is derived, and the mathematical aspects\nprove to be validated by testing a new filter and performing its operation.\nAfter the decomposition of the rows and upon the process of the reconstruction,\ntaking the inverse of the filter and dealing with the columns of the matrix,\nthe original matrix is improved by measuring the parameters of the image to\nachieve the best quality of the resulting image, such as the peak\nsignal-to-noise ratio (PSNR), compression ratio (CR), bits per pixel (BPP), and\nmean square error (MSE).\n","authors":["Hassan Mohamed Muhi-Aldeen","Asma A. Abdulrahman","Jabbar Abed Eleiwy","Fouad S. Tahir","Yurii Khlaponin"],"pdf_url":"https://arxiv.org/pdf/2303.13175v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13174v1","updated":"2023-03-23T11:03:18Z","published":"2023-03-23T11:03:18Z","title":"3D-POP -- An automated annotation approach to facilitate markerless\n  2D-3D tracking of freely moving birds with marker-based motion capture","summary":"  Recent advances in machine learning and computer vision are revolutionizing\nthe field of animal behavior by enabling researchers to track the poses and\nlocations of freely moving animals without any marker attachment. However,\nlarge datasets of annotated images of animals for markerless pose tracking,\nespecially high-resolution images taken from multiple angles with accurate 3D\nannotations, are still scant. Here, we propose a method that uses a motion\ncapture (mo-cap) system to obtain a large amount of annotated data on animal\nmovement and posture (2D and 3D) in a semi-automatic manner. Our method is\nnovel in that it extracts the 3D positions of morphological keypoints (e.g\neyes, beak, tail) in reference to the positions of markers attached to the\nanimals. Using this method, we obtained, and offer here, a new dataset - 3D-POP\nwith approximately 300k annotated frames (4 million instances) in the form of\nvideos having groups of one to ten freely moving birds from 4 different camera\nviews in a 3.6m x 4.2m area. 3D-POP is the first dataset of flocking birds with\naccurate keypoint annotations in 2D and 3D along with bounding box and\nindividual identities and will facilitate the development of solutions for\nproblems of 2D to 3D markerless pose, trajectory tracking, and identification\nin birds.\n","authors":["Hemal Naik","Alex Hoi Hang Chan","Junran Yang","Mathilde Delacoux","Iain D. Couzin","Fumihiro Kano","Máté Nagy"],"pdf_url":"https://arxiv.org/pdf/2303.13174v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13166v1","updated":"2023-03-23T10:36:10Z","published":"2023-03-23T10:36:10Z","title":"Take 5: Interpretable Image Classification with a Handful of Features","summary":"  Deep Neural Networks use thousands of mostly incomprehensible features to\nidentify a single class, a decision no human can follow. We propose an\ninterpretable sparse and low dimensional final decision layer in a deep neural\nnetwork with measurable aspects of interpretability and demonstrate it on\nfine-grained image classification. We argue that a human can only understand\nthe decision of a machine learning model, if the features are interpretable and\nonly very few of them are used for a single decision. For that matter, the\nfinal layer has to be sparse and, to make interpreting the features feasible,\nlow dimensional. We call a model with a Sparse Low-Dimensional Decision\nSLDD-Model. We show that a SLDD-Model is easier to interpret locally and\nglobally than a dense high-dimensional decision layer while being able to\nmaintain competitive accuracy. Additionally, we propose a loss function that\nimproves a model's feature diversity and accuracy. Our more interpretable\nSLDD-Model only uses 5 out of just 50 features per class, while maintaining 97%\nto 100% of the accuracy on four common benchmark datasets compared to the\nbaseline model with 2048 features.\n","authors":["Thomas Norrenbrock","Marco Rudolph","Bodo Rosenhahn"],"pdf_url":"https://arxiv.org/pdf/2303.13166v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13158v1","updated":"2023-03-23T10:20:19Z","published":"2023-03-23T10:20:19Z","title":"Improvement of Color Image Analysis Using a New Hybrid Face Recognition\n  Algorithm based on Discrete Wavelets and Chebyshev Polynomials","summary":"  This work is unique in the use of discrete wavelets that were built from or\nderived from Chebyshev polynomials of the second and third kind, filter the\nDiscrete Second Chebyshev Wavelets Transform (DSCWT), and derive two effective\nfilters. The Filter Discrete Third Chebyshev Wavelets Transform (FDTCWT) is\nused in the process of analyzing color images and removing noise and impurities\nthat accompany the image, as well as because of the large amount of data that\nmakes up the image as it is taken. These data are massive, making it difficult\nto deal with each other during transmission. However to address this issue, the\nimage compression technique is used, with the image not losing information due\nto the readings that were obtained, and the results were satisfactory. Mean\nSquare Error (MSE), Peak Signal Noise Ratio (PSNR), Bit Per Pixel (BPP), and\nCompression Ratio (CR) Coronavirus is the initial treatment, while the\nprocessing stage is done with network training for Convolutional Neural\nNetworks (CNN) with Discrete Second Chebeshev Wavelets Convolutional Neural\nNetwork (DSCWCNN) and Discrete Third Chebeshev Wavelets Convolutional Neural\nNetwork (DTCWCNN) to create an efficient algorithm for face recognition, and\nthe best results were achieved in accuracy and in the least amount of time. Two\nsamples of color images that were made or implemented were used. The proposed\ntheory was obtained with fast and good results; the results are evident shown\nin the tables below.\n","authors":["Hassan Mohamed Muhi-Aldeen","Maha Ammar Mustafa","Asma A. Abdulrahman","Jabbar Abed Eleiwy","Fouad S. Tahir","Yurii Khlaponin"],"pdf_url":"https://arxiv.org/pdf/2303.13158v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13148v1","updated":"2023-03-23T10:03:12Z","published":"2023-03-23T10:03:12Z","title":"Calibrated Out-of-Distribution Detection with a Generic Representation","summary":"  Out-of-distribution detection is a common issue in deploying vision models in\npractice and solving it is an essential building block in safety critical\napplications. Existing OOD detection solutions focus on improving the OOD\nrobustness of a classification model trained exclusively on in-distribution\n(ID) data. In this work, we take a different approach and propose to leverage\ngeneric pre-trained representations. We first investigate the behaviour of\nsimple classifiers built on top of such representations and show striking\nperformance gains compared to the ID trained representations. We propose a\nnovel OOD method, called GROOD, that achieves excellent performance, predicated\nby the use of a good generic representation. Only a trivial training process is\nrequired for adapting GROOD to a particular problem. The method is simple,\ngeneral, efficient, calibrated and with only a few hyper-parameters. The method\nachieves state-of-the-art performance on a number of OOD benchmarks, reaching\nnear perfect performance on several of them. The source code is available at\nhttps://github.com/vojirt/GROOD.\n","authors":["Tomas Vojir","Jan Sochman","Rahaf Aljundi","Jiri Matas"],"pdf_url":"https://arxiv.org/pdf/2303.13148v1.pdf","comment":"10 pages, submitted to conference"},{"id":"http://arxiv.org/abs/2210.01612v2","updated":"2023-03-23T10:01:33Z","published":"2022-10-04T13:51:59Z","title":"PlaneDepth: Self-supervised Depth Estimation via Orthogonal Planes","summary":"  Multiple near frontal-parallel planes based depth representation demonstrated\nimpressive results in self-supervised monocular depth estimation (MDE).\nWhereas, such a representation would cause the discontinuity of the ground as\nit is perpendicular to the frontal-parallel planes, which is detrimental to the\nidentification of drivable space in autonomous driving. In this paper, we\npropose the PlaneDepth, a novel orthogonal planes based presentation, including\nvertical planes and ground planes. PlaneDepth estimates the depth distribution\nusing a Laplacian Mixture Model based on orthogonal planes for an input image.\nThese planes are used to synthesize a reference view to provide the\nself-supervision signal. Further, we find that the widely used resizing and\ncropping data augmentation breaks the orthogonality assumptions, leading to\ninferior plane predictions. We address this problem by explicitly constructing\nthe resizing cropping transformation to rectify the predefined planes and\npredicted camera pose. Moreover, we propose an augmented self-distillation loss\nsupervised with a bilateral occlusion mask to boost the robustness of\northogonal planes representation for occlusions. Thanks to our orthogonal\nplanes representation, we can extract the ground plane in an unsupervised\nmanner, which is important for autonomous driving. Extensive experiments on the\nKITTI dataset demonstrate the effectiveness and efficiency of our method. The\ncode is available at https://github.com/svip-lab/PlaneDepth.\n","authors":["Ruoyu Wang","Zehao Yu","Shenghua Gao"],"pdf_url":"https://arxiv.org/pdf/2210.01612v2.pdf","comment":"Accepted by CVPR 2023. Code and models are available at:\n  https://github.com/svip-lab/PlaneDepth"},{"id":"http://arxiv.org/abs/2204.08696v3","updated":"2023-03-23T09:44:22Z","published":"2022-04-19T06:38:29Z","title":"CTCNet: A CNN-Transformer Cooperation Network for Face Image\n  Super-Resolution","summary":"  Recently, deep convolution neural networks (CNNs) steered face\nsuper-resolution methods have achieved great progress in restoring degraded\nfacial details by jointly training with facial priors. However, these methods\nhave some obvious limitations. On the one hand, multi-task joint learning\nrequires additional marking on the dataset, and the introduced prior network\nwill significantly increase the computational cost of the model. On the other\nhand, the limited receptive field of CNN will reduce the fidelity and\nnaturalness of the reconstructed facial images, resulting in suboptimal\nreconstructed images. In this work, we propose an efficient CNN-Transformer\nCooperation Network (CTCNet) for face super-resolution tasks, which uses the\nmulti-scale connected encoder-decoder architecture as the backbone.\nSpecifically, we first devise a novel Local-Global Feature Cooperation Module\n(LGCM), which is composed of a Facial Structure Attention Unit (FSAU) and a\nTransformer block, to promote the consistency of local facial detail and global\nfacial structure restoration simultaneously. Then, we design an efficient\nFeature Refinement Module (FRM) to enhance the encoded features. Finally, to\nfurther improve the restoration of fine facial details, we present a\nMulti-scale Feature Fusion Unit (MFFU) to adaptively fuse the features from\ndifferent stages in the encoder procedure. Extensive evaluations on various\ndatasets have assessed that the proposed CTCNet can outperform other\nstate-of-the-art methods significantly. Source code will be available at\nhttps://github.com/IVIPLab/CTCNet.\n","authors":["Guangwei Gao","Zixiang Xu","Juncheng Li","Jian Yang","Tieyong Zeng","Guo-Jun Qi"],"pdf_url":"https://arxiv.org/pdf/2204.08696v3.pdf","comment":"IEEE Transactions on Image Processing, 12 figures, 9 tables"},{"id":"http://arxiv.org/abs/2303.13133v1","updated":"2023-03-23T09:34:17Z","published":"2023-03-23T09:34:17Z","title":"Generative Image Inpainting with Segmentation Confusion Adversarial\n  Training and Contrastive Learning","summary":"  This paper presents a new adversarial training framework for image inpainting\nwith segmentation confusion adversarial training (SCAT) and contrastive\nlearning. SCAT plays an adversarial game between an inpainting generator and a\nsegmentation network, which provides pixel-level local training signals and can\nadapt to images with free-form holes. By combining SCAT with standard global\nadversarial training, the new adversarial training framework exhibits the\nfollowing three advantages simultaneously: (1) the global consistency of the\nrepaired image, (2) the local fine texture details of the repaired image, and\n(3) the flexibility of handling images with free-form holes. Moreover, we\npropose the textural and semantic contrastive learning losses to stabilize and\nimprove our inpainting model's training by exploiting the feature\nrepresentation space of the discriminator, in which the inpainting images are\npulled closer to the ground truth images but pushed farther from the corrupted\nimages. The proposed contrastive losses better guide the repaired images to\nmove from the corrupted image data points to the real image data points in the\nfeature representation space, resulting in more realistic completed images. We\nconduct extensive experiments on two benchmark datasets, demonstrating our\nmodel's effectiveness and superiority both qualitatively and quantitatively.\n","authors":["Zhiwen Zuo","Lei Zhao","Ailin Li","Zhizhong Wang","Zhanjie Zhang","Jiafu Chen","Wei Xing","Dongming Lu"],"pdf_url":"https://arxiv.org/pdf/2303.13133v1.pdf","comment":"Accepted to AAAI2023, Oral"},{"id":"http://arxiv.org/abs/2303.13132v1","updated":"2023-03-23T09:33:44Z","published":"2023-03-23T09:33:44Z","title":"Masked Image Training for Generalizable Deep Image Denoising","summary":"  When capturing and storing images, devices inevitably introduce noise.\nReducing this noise is a critical task called image denoising. Deep learning\nhas become the de facto method for image denoising, especially with the\nemergence of Transformer-based models that have achieved notable\nstate-of-the-art results on various image tasks. However, deep learning-based\nmethods often suffer from a lack of generalization ability. For example, deep\nmodels trained on Gaussian noise may perform poorly when tested on other noise\ndistributions. To address this issue, we present a novel approach to enhance\nthe generalization performance of denoising networks, known as masked training.\nOur method involves masking random pixels of the input image and reconstructing\nthe missing information during training. We also mask out the features in the\nself-attention layers to avoid the impact of training-testing inconsistency.\nOur approach exhibits better generalization ability than other deep learning\nmodels and is directly applicable to real-world scenarios. Additionally, our\ninterpretability analysis demonstrates the superiority of our method.\n","authors":["Haoyu Chen","Jinjin Gu","Yihao Liu","Salma Abdel Magid","Chao Dong","Qiong Wang","Hanspeter Pfister","Lei Zhu"],"pdf_url":"https://arxiv.org/pdf/2303.13132v1.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2303.13131v1","updated":"2023-03-23T09:33:10Z","published":"2023-03-23T09:33:10Z","title":"Watch Out for the Confusing Faces: Detecting Face Swapping with the\n  Probability Distribution of Face Identification Models","summary":"  Recently, face swapping has been developing rapidly and achieved a surprising\nreality, raising concerns about fake content. As a countermeasure, various\ndetection approaches have been proposed and achieved promising performance.\nHowever, most existing detectors struggle to maintain performance on unseen\nface swapping methods and low-quality images. Apart from the generalization\nproblem, current detection approaches have been shown vulnerable to evasion\nattacks crafted by detection-aware manipulators. Lack of robustness under\nadversary scenarios leaves threats for applying face swapping detection in real\nworld. In this paper, we propose a novel face swapping detection approach based\non face identification probability distributions, coined as IdP_FSD, to improve\nthe generalization and robustness. IdP_FSD is specially designed for detecting\nswapped faces whose identities belong to a finite set, which is meaningful in\nreal-world applications. Compared with previous general detection methods, we\nmake use of the available real faces with concerned identities and require no\nfake samples for training. IdP_FSD exploits face swapping's common nature that\nthe identity of swapped face combines that of two faces involved in swapping.\nWe reflect this nature with the confusion of a face identification model and\nmeasure the confusion with the maximum value of the output probability\ndistribution. What's more, to defend our detector under adversary scenarios, an\nattention-based finetuning scheme is proposed for the face identification\nmodels used in IdP_FSD. Extensive experiments show that the proposed IdP_FSD\nnot only achieves high detection performance on different benchmark datasets\nand image qualities but also raises the bar for manipulators to evade the\ndetection.\n","authors":["Yuxuan Duan","Xuhong Zhang","Chuer Yu","Zonghui Wang","Shouling Ji","Wenzhi Chen"],"pdf_url":"https://arxiv.org/pdf/2303.13131v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13129v1","updated":"2023-03-23T09:31:56Z","published":"2023-03-23T09:31:56Z","title":"Task-Oriented Human-Object Interactions Generation with Implicit Neural\n  Representations","summary":"  Digital human motion synthesis is a vibrant research field with applications\nin movies, AR/VR, and video games. Whereas methods were proposed to generate\nnatural and realistic human motions, most only focus on modeling humans and\nlargely ignore object movements. Generating task-oriented human-object\ninteraction motions in simulation is challenging. For different intents of\nusing the objects, humans conduct various motions, which requires the human\nfirst to approach the objects and then make them move consistently with the\nhuman instead of staying still. Also, to deploy in downstream applications, the\nsynthesized motions are desired to be flexible in length, providing options to\npersonalize the predicted motions for various purposes. To this end, we propose\nTOHO: Task-Oriented Human-Object Interactions Generation with Implicit Neural\nRepresentations, which generates full human-object interaction motions to\nconduct specific tasks, given only the task type, the object, and a starting\nhuman status. TOHO generates human-object motions in three steps: 1) it first\nestimates the keyframe poses of conducting a task given the task type and\nobject information; 2) then, it infills the keyframes and generates continuous\nmotions; 3) finally, it applies a compact closed-form object motion estimation\nto generate the object motion. Our method generates continuous motions that are\nparameterized only by the temporal coordinate, which allows for upsampling or\ndownsampling of the sequence to arbitrary frames and adjusting the motion\nspeeds by designing the temporal coordinate vector. We demonstrate the\neffectiveness of our method, both qualitatively and quantitatively. This work\ntakes a step further toward general human-scene interaction simulation.\n","authors":["Quanzhou Li","Jingbo Wang","Chen Change Loy","Bo Dai"],"pdf_url":"https://arxiv.org/pdf/2303.13129v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13126v1","updated":"2023-03-23T09:30:39Z","published":"2023-03-23T09:30:39Z","title":"MagicFusion: Boosting Text-to-Image Generation Performance by Fusing\n  Diffusion Models","summary":"  The advent of open-source AI communities has produced a cornucopia of\npowerful text-guided diffusion models that are trained on various datasets.\nWhile few explorations have been conducted on ensembling such models to combine\ntheir strengths. In this work, we propose a simple yet effective method called\nSaliency-aware Noise Blending (SNB) that can empower the fused text-guided\ndiffusion models to achieve more controllable generation. Specifically, we\nexperimentally find that the responses of classifier-free guidance are highly\nrelated to the saliency of generated images. Thus we propose to trust different\nmodels in their areas of expertise by blending the predicted noises of two\ndiffusion models in a saliency-aware manner. SNB is training-free and can be\ncompleted within a DDIM sampling process. Additionally, it can automatically\nalign the semantics of two noise spaces without requiring additional\nannotations such as masks. Extensive experiments show the impressive\neffectiveness of SNB in various applications. Project page is available at\nhttps://magicfusion.github.io/.\n","authors":["Jing Zhao","Heliang Zheng","Chaoyue Wang","Long Lan","Wenjing Yang"],"pdf_url":"https://arxiv.org/pdf/2303.13126v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13123v1","updated":"2023-03-23T09:23:57Z","published":"2023-03-23T09:23:57Z","title":"Laplacian Segmentation Networks: Improved Epistemic Uncertainty from\n  Spatial Aleatoric Uncertainty","summary":"  Out of distribution (OOD) medical images are frequently encountered, e.g.\nbecause of site- or scanner differences, or image corruption. OOD images come\nwith a risk of incorrect image segmentation, potentially negatively affecting\ndownstream diagnoses or treatment. To ensure robustness to such incorrect\nsegmentations, we propose Laplacian Segmentation Networks (LSN) that jointly\nmodel epistemic (model) and aleatoric (data) uncertainty in image segmentation.\nWe capture data uncertainty with a spatially correlated logit distribution. For\nmodel uncertainty, we propose the first Laplace approximation of the weight\nposterior that scales to large neural networks with skip connections that have\nhigh-dimensional outputs. Empirically, we demonstrate that modelling spatial\npixel correlation allows the Laplacian Segmentation Network to successfully\nassign high epistemic uncertainty to out-of-distribution objects appearing\nwithin images.\n","authors":["Kilian Zepf","Selma Wanna","Marco Miani","Juston Moore","Jes Frellsen","Søren Hauberg","Aasa Feragen","Frederik Warburg"],"pdf_url":"https://arxiv.org/pdf/2303.13123v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13122v1","updated":"2023-03-23T09:23:52Z","published":"2023-03-23T09:23:52Z","title":"Exploring Visual Prompts for Whole Slide Image Classification with\n  Multiple Instance Learning","summary":"  Multiple instance learning (MIL) has emerged as a popular method for\nclassifying histopathology whole slide images (WSIs). However, existing\napproaches typically rely on pre-trained models from large natural image\ndatasets, such as ImageNet, to generate instance features, which can be\nsub-optimal due to the significant differences between natural images and\nhistopathology images that lead to a domain shift. In this paper, we present a\nnovel, simple yet effective method for learning domain-specific knowledge\ntransformation from pre-trained models to histopathology images. Our approach\nentails using a prompt component to assist the pre-trained model in discerning\ndifferences between the pre-trained dataset and the target histopathology\ndataset, resulting in improved performance of MIL models. We validate our\nmethod on two publicly available datasets, Camelyon16 and TCGA-NSCLC. Extensive\nexperimental results demonstrate the significant performance improvement of our\nmethod for different MIL models and backbones. Upon publication of this paper,\nwe will release the source code for our method.\n","authors":["Yi Lin","Zhongchen Zhao","Zhengjie ZHU","Lisheng Wang","Kwang-Ting Cheng","Hao Chen"],"pdf_url":"https://arxiv.org/pdf/2303.13122v1.pdf","comment":"Submitted to MICCAI 2023"},{"id":"http://arxiv.org/abs/2303.13121v1","updated":"2023-03-23T09:23:11Z","published":"2023-03-23T09:23:11Z","title":"DetOFA: Efficient Training of Once-for-All Networks for Object Detection\n  by Using Pre-trained Supernet and Path Filter","summary":"  We address the challenge of training a large supernet for the object\ndetection task, using a relatively small amount of training data. Specifically,\nwe propose an efficient supernet-based neural architecture search (NAS) method\nthat uses transfer learning and search space pruning. First, the supernet is\npre-trained on a classification task, for which large datasets are available.\nSecond, the search space defined by the supernet is pruned by removing\ncandidate models that are predicted to perform poorly. To effectively remove\nthe candidates over a wide range of resource constraints, we particularly\ndesign a performance predictor, called path filter, which can accurately\npredict the relative performance of the models that satisfy similar resource\nconstraints. Hence, supernet training is more focused on the best-performing\ncandidates. Our path filter handles prediction for paths with different\nresource budgets. Compared to once-for-all, our proposed method reduces the\ncomputational cost of the optimal network architecture by 30% and 63%, while\nyielding better accuracy-floating point operations Pareto front (0.85 and 0.45\npoints of improvement on average precision for Pascal VOC and COCO,\nrespectively).\n","authors":["Yuiko Sakuma","Masato Ishii","Takuya Narihira"],"pdf_url":"https://arxiv.org/pdf/2303.13121v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12274v2","updated":"2023-03-23T09:17:51Z","published":"2023-03-22T02:47:42Z","title":"A Hierarchical Hybrid Learning Framework for Multi-agent Trajectory\n  Prediction","summary":"  Accurate and robust trajectory prediction of neighboring agents is critical\nfor autonomous vehicles traversing in complex scenes. Most methods proposed in\nrecent years are deep learning-based due to their strength in encoding complex\ninteractions. However, unplausible predictions are often generated since they\nrely heavily on past observations and cannot effectively capture the transient\nand contingency interactions from sparse samples. In this paper, we propose a\nhierarchical hybrid framework of deep learning (DL) and reinforcement learning\n(RL) for multi-agent trajectory prediction, to cope with the challenge of\npredicting motions shaped by multi-scale interactions. In the DL stage, the\ntraffic scene is divided into multiple intermediate-scale heterogenous graphs\nbased on which Transformer-style GNNs are adopted to encode heterogenous\ninteractions at intermediate and global levels. In the RL stage, we divide the\ntraffic scene into local sub-scenes utilizing the key future points predicted\nin the DL stage. To emulate the motion planning procedure so as to produce\ntrajectory predictions, a Transformer-based Proximal Policy Optimization (PPO)\nincorporated with a vehicle kinematics model is devised to plan motions under\nthe dominant influence of microscopic interactions. A multi-objective reward is\ndesigned to balance between agent-centric accuracy and scene-wise\ncompatibility. Experimental results show that our proposal matches the\nstate-of-the-arts on the Argoverse forecasting benchmark. It's also revealed by\nthe visualized results that the hierarchical learning framework captures the\nmulti-scale interactions and improves the feasibility and compliance of the\npredicted trajectories.\n","authors":["Yujun Jiao","Mingze Miao","Zhishuai Yin","Chunyuan Lei","Xu Zhu","Linzhen Nie","Bo Tao"],"pdf_url":"https://arxiv.org/pdf/2303.12274v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12644v2","updated":"2023-03-23T09:17:22Z","published":"2023-03-22T15:26:22Z","title":"Feature-Conditioned Cascaded Video Diffusion Models for Precise\n  Echocardiogram Synthesis","summary":"  Image synthesis is expected to provide value for the translation of machine\nlearning methods into clinical practice. Fundamental problems like model\nrobustness, domain transfer, causal modelling, and operator training become\napproachable through synthetic data. Especially, heavily operator-dependant\nmodalities like Ultrasound imaging require robust frameworks for image and\nvideo generation. So far, video generation has only been possible by providing\ninput data that is as rich as the output data, e.g., image sequence plus\nconditioning in, video out. However, clinical documentation is usually scarce\nand only single images are reported and stored, thus retrospective\npatient-specific analysis or the generation of rich training data becomes\nimpossible with current approaches. In this paper, we extend elucidated\ndiffusion models for video modelling to generate plausible video sequences from\nsingle images and arbitrary conditioning with clinical parameters. We explore\nthis idea within the context of echocardiograms by looking into the variation\nof the Left Ventricle Ejection Fraction, the most essential clinical metric\ngained from these examinations. We use the publicly available EchoNet-Dynamic\ndataset for all our experiments. Our image to sequence approach achieves an\n$R^2$ score of 93%, which is 38 points higher than recently proposed sequence\nto sequence generation methods. Code and models will be available at:\nhttps://github.com/HReynaud/EchoDiffusion.\n","authors":["Hadrien Reynaud","Mengyun Qiao","Mischa Dombrowski","Thomas Day","Reza Razavi","Alberto Gomez","Paul Leeson","Bernhard Kainz"],"pdf_url":"https://arxiv.org/pdf/2303.12644v2.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2303.09165v2","updated":"2023-03-23T09:02:33Z","published":"2023-03-16T09:03:52Z","title":"A New Benchmark: On the Utility of Synthetic Data with Blender for Bare\n  Supervised Learning and Downstream Domain Adaptation","summary":"  Deep learning in computer vision has achieved great success with the price of\nlarge-scale labeled training data. However, exhaustive data annotation is\nimpracticable for each task of all domains of interest, due to high labor costs\nand unguaranteed labeling accuracy. Besides, the uncontrollable data collection\nprocess produces non-IID training and test data, where undesired duplication\nmay exist. All these nuisances may hinder the verification of typical theories\nand exposure to new findings. To circumvent them, an alternative is to generate\nsynthetic data via 3D rendering with domain randomization. We in this work push\nforward along this line by doing profound and extensive research on bare\nsupervised learning and downstream domain adaptation. Specifically, under the\nwell-controlled, IID data setting enabled by 3D rendering, we systematically\nverify the typical, important learning insights, e.g., shortcut learning, and\ndiscover the new laws of various data regimes and network architectures in\ngeneralization. We further investigate the effect of image formation factors on\ngeneralization, e.g., object scale, material texture, illumination, camera\nviewpoint, and background in a 3D scene. Moreover, we use the\nsimulation-to-reality adaptation as a downstream task for comparing the\ntransferability between synthetic and real data when used for pre-training,\nwhich demonstrates that synthetic data pre-training is also promising to\nimprove real test results. Lastly, to promote future research, we develop a new\nlarge-scale synthetic-to-real benchmark for image classification, termed S2RDA,\nwhich provides more significant challenges for transfer from simulation to\nreality. The code and datasets are available at\nhttps://github.com/huitangtang/On_the_Utility_of_Synthetic_Data.\n","authors":["Hui Tang","Kui Jia"],"pdf_url":"https://arxiv.org/pdf/2303.09165v2.pdf","comment":"24 pages, 14 figures, 5 tables, accepted by the IEEE/CVF Conference\n  on Computer Vision and Pattern Recognition (CVPR), 2023. The proposed new\n  synthetic-to-real benchmark S2RDA is available at\n  https://pan.baidu.com/s/1fHHaqrEHbUZLXEg9XKpgSg?pwd=w9wa"},{"id":"http://arxiv.org/abs/2303.13113v1","updated":"2023-03-23T09:00:38Z","published":"2023-03-23T09:00:38Z","title":"Adaptive Regularization for Class-Incremental Learning","summary":"  Class-Incremental Learning updates a deep classifier with new categories\nwhile maintaining the previously observed class accuracy. Regularizing the\nneural network weights is a common method to prevent forgetting previously\nlearned classes while learning novel ones. However, existing regularizers use a\nconstant magnitude throughout the learning sessions, which may not reflect the\nvarying levels of difficulty of the tasks encountered during incremental\nlearning. This study investigates the necessity of adaptive regularization in\nClass-Incremental Learning, which dynamically adjusts the regularization\nstrength according to the complexity of the task at hand. We propose a Bayesian\nOptimization-based approach to automatically determine the optimal\nregularization magnitude for each learning task. Our experiments on two\ndatasets via two regularizers demonstrate the importance of adaptive\nregularization for achieving accurate and less forgetful visual incremental\nlearning.\n","authors":["Elif Ceren Gok","Murat Onur Yildirim","Mert Kilickaya","Joaquin Vanschoren"],"pdf_url":"https://arxiv.org/pdf/2303.13113v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13111v1","updated":"2023-03-23T08:59:09Z","published":"2023-03-23T08:59:09Z","title":"A Permutable Hybrid Network for Volumetric Medical Image Segmentation","summary":"  The advent of Vision Transformer (ViT) has brought substantial advancements\nin 3D volumetric benchmarks, particularly in 3D medical image segmentation.\nConcurrently, Multi-Layer Perceptron (MLP) networks have regained popularity\namong researchers due to their comparable results to ViT, albeit with the\nexclusion of the heavy self-attention module. This paper introduces a\npermutable hybrid network for volumetric medical image segmentation, named\nPHNet, which exploits the advantages of convolution neural network (CNN) and\nMLP. PHNet addresses the intrinsic isotropy problem of 3D volumetric data by\nutilizing both 2D and 3D CNN to extract local information. Besides, we propose\nan efficient Multi-Layer Permute Perceptron module, named MLPP, which enhances\nthe original MLP by obtaining long-range dependence while retaining positional\ninformation. Extensive experimental results validate that PHNet outperforms the\nstate-of-the-art methods on two public datasets, namely, COVID-19-20 and\nSynapse. Moreover, the ablation study demonstrates the effectiveness of PHNet\nin harnessing the strengths of both CNN and MLP. The code will be accessible to\nthe public upon acceptance.\n","authors":["Yi Lin","Xiao Fang","Dong Zhang","Kwang-Ting Cheng","Hao Chen"],"pdf_url":"https://arxiv.org/pdf/2303.13111v1.pdf","comment":"Submitted to MICCAI 2023"},{"id":"http://arxiv.org/abs/2303.13110v1","updated":"2023-03-23T08:57:11Z","published":"2023-03-23T08:57:11Z","title":"OCELOT: Overlapped Cell on Tissue Dataset for Histopathology","summary":"  Cell detection is a fundamental task in computational pathology that can be\nused for extracting high-level medical information from whole-slide images. For\naccurate cell detection, pathologists often zoom out to understand the\ntissue-level structures and zoom in to classify cells based on their morphology\nand the surrounding context. However, there is a lack of efforts to reflect\nsuch behaviors by pathologists in the cell detection models, mainly due to the\nlack of datasets containing both cell and tissue annotations with overlapping\nregions. To overcome this limitation, we propose and publicly release OCELOT, a\ndataset purposely dedicated to the study of cell-tissue relationships for cell\ndetection in histopathology. OCELOT provides overlapping cell and tissue\nannotations on images acquired from multiple organs. Within this setting, we\nalso propose multi-task learning approaches that benefit from learning both\ncell and tissue tasks simultaneously. When compared against a model trained\nonly for the cell detection task, our proposed approaches improve cell\ndetection performance on 3 datasets: proposed OCELOT, public TIGER, and\ninternal CARP datasets. On the OCELOT test set in particular, we show up to\n6.79 improvement in F1-score. We believe the contributions of this paper,\nincluding the release of the OCELOT dataset at\nhttps://lunit-io.github.io/research/publications/ocelot are a crucial starting\npoint toward the important research direction of incorporating cell-tissue\nrelationships in computation pathology.\n","authors":["Jeongun Ryu","Aaron Valero Puche","JaeWoong Shin","Seonwook Park","Biagio Brattoli","Jinhee Lee","Wonkyung Jung","Soo Ick Cho","Kyunghyun Paeng","Chan-Young Ock","Donggeun Yoo","Sérgio Pereira"],"pdf_url":"https://arxiv.org/pdf/2303.13110v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.04238v3","updated":"2023-03-23T08:49:30Z","published":"2023-03-07T21:03:48Z","title":"Patch of Invisibility: Naturalistic Black-Box Adversarial Attacks on\n  Object Detectors","summary":"  Adversarial attacks on deep-learning models have been receiving increased\nattention in recent years. Work in this area has mostly focused on\ngradient-based techniques, so-called white-box attacks, wherein the attacker\nhas access to the targeted model's internal parameters; such an assumption is\nusually unrealistic in the real world. Some attacks additionally use the entire\npixel space to fool a given model, which is neither practical nor physical\n(i.e., real-world). On the contrary, we propose herein a gradient-free method\nthat uses the learned image manifold of a pretrained generative adversarial\nnetwork (GAN) to generate naturalistic physical adversarial patches for object\ndetectors. We show that our proposed method works both digitally and\nphysically.\n","authors":["Raz Lapid","Moshe Sipper"],"pdf_url":"https://arxiv.org/pdf/2303.04238v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13102v1","updated":"2023-03-23T08:35:56Z","published":"2023-03-23T08:35:56Z","title":"Keypoint-Guided Optimal Transport","summary":"  Existing Optimal Transport (OT) methods mainly derive the optimal transport\nplan/matching under the criterion of transport cost/distance minimization,\nwhich may cause incorrect matching in some cases. In many applications,\nannotating a few matched keypoints across domains is reasonable or even\neffortless in annotation burden. It is valuable to investigate how to leverage\nthe annotated keypoints to guide the correct matching in OT. In this paper, we\npropose a novel KeyPoint-Guided model by ReLation preservation (KPG-RL) that\nsearches for the optimal matching (i.e., transport plan) guided by the\nkeypoints in OT. To impose the keypoints in OT, first, we propose a mask-based\nconstraint of the transport plan that preserves the matching of keypoint pairs.\nSecond, we propose to preserve the relation of each data point to the keypoints\nto guide the matching. The proposed KPG-RL model can be solved by Sinkhorn's\nalgorithm and is applicable even when distributions are supported in different\nspaces. We further utilize the relation preservation constraint in the\nKantorovich Problem and Gromov-Wasserstein model to impose the guidance of\nkeypoints in them. Meanwhile, the proposed KPG-RL model is extended to the\npartial OT setting. Moreover, we deduce the dual formulation of the KPG-RL\nmodel, which is solved using deep learning techniques. Based on the learned\ntransport plan from dual KPG-RL, we propose a novel manifold barycentric\nprojection to transport source data to the target domain. As applications, we\napply the proposed KPG-RL model to the heterogeneous domain adaptation and\nimage-to-image translation. Experiments verified the effectiveness of the\nproposed approach.\n","authors":["Xiang Gu","Yucheng Yang","Wei Zeng","Jian Sun","Zongben Xu"],"pdf_url":"https://arxiv.org/pdf/2303.13102v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2303.13101v1","updated":"2023-03-23T08:34:24Z","published":"2023-03-23T08:34:24Z","title":"MMFormer: Multimodal Transformer Using Multiscale Self-Attention for\n  Remote Sensing Image Classification","summary":"  To benefit the complementary information between heterogeneous data, we\nintroduce a new Multimodal Transformer (MMFormer) for Remote Sensing (RS) image\nclassification using Hyperspectral Image (HSI) accompanied by another source of\ndata such as Light Detection and Ranging (LiDAR). Compared with traditional\nVision Transformer (ViT) lacking inductive biases of convolutions, we first\nintroduce convolutional layers to our MMFormer to tokenize patches from\nmultimodal data of HSI and LiDAR. Then we propose a Multi-scale Multi-head\nSelf-Attention (MSMHSA) module to address the problem of compatibility which\noften limits to fuse HSI with high spectral resolution and LiDAR with\nrelatively low spatial resolution. The proposed MSMHSA module can incorporate\nHSI to LiDAR data in a coarse-to-fine manner enabling us to learn a\nfine-grained representation. Extensive experiments on widely used benchmarks\n(e.g., Trento and MUUFL) demonstrate the effectiveness and superiority of our\nproposed MMFormer for RS image classification.\n","authors":["Bo Zhang","Zuheng Ming","Wei Feng","Yaqian Liu","Liang He","Kaixing Zhao"],"pdf_url":"https://arxiv.org/pdf/2303.13101v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11108v2","updated":"2023-03-23T08:32:29Z","published":"2023-03-20T13:45:58Z","title":"I2Edit: Towards Multi-turn Interactive Image Editing via Dialogue","summary":"  Although there have been considerable research efforts on controllable facial\nimage editing, the desirable interactive setting where the users can interact\nwith the system to adjust their requirements dynamically hasn't been well\nexplored. This paper focuses on facial image editing via dialogue and\nintroduces a new benchmark dataset, Multi-turn Interactive Image Editing\n(I2Edit), for evaluating image editing quality and interaction ability in\nreal-world interactive facial editing scenarios. The dataset is constructed\nupon the CelebA-HQ dataset with images annotated with a multi-turn dialogue\nthat corresponds to the user editing requirements. I2Edit is challenging, as it\nneeds to 1) track the dynamically updated user requirements and edit the images\naccordingly, as well as 2) generate the appropriate natural language response\nto communicate with the user. To address these challenges, we propose a\nframework consisting of a dialogue module and an image editing module. The\nformer is for user edit requirements tracking and generating the corresponding\nindicative responses, while the latter edits the images conditioned on the\ntracked user edit requirements. In contrast to previous works that simply treat\nmulti-turn interaction as a sequence of single-turn interactions, we extract\nthe user edit requirements from the whole dialogue history instead of the\ncurrent single turn. The extracted global user edit requirements enable us to\ndirectly edit the input raw image to avoid error accumulation and attribute\nforgetting issues. Extensive quantitative and qualitative experiments on the\nI2Edit dataset demonstrate the advantage of our proposed framework over the\nprevious single-turn methods. We believe our new dataset could serve as a\nvaluable resource to push forward the exploration of real-world, complex\ninteractive image editing. Code and data will be made public.\n","authors":["Xing Cui","Zekun Li","Peipei Li","Yibo Hu","Hailin Shi","Zhaofeng He"],"pdf_url":"https://arxiv.org/pdf/2303.11108v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13100v1","updated":"2023-03-23T08:32:10Z","published":"2023-03-23T08:32:10Z","title":"PointGame: Geometrically and Adaptively Masked Auto-Encoder on Point\n  Clouds","summary":"  Self-supervised learning is attracting large attention in point cloud\nunderstanding. However, exploring discriminative and transferable features\nstill remains challenging due to their nature of irregularity and sparsity. We\npropose a geometrically and adaptively masked auto-encoder for self-supervised\nlearning on point clouds, termed \\textit{PointGame}. PointGame contains two\ncore components: GATE and EAT. GATE stands for the geometrical and adaptive\ntoken embedding module; it not only absorbs the conventional wisdom of\ngeometric descriptors that captures the surface shape effectively, but also\nexploits adaptive saliency to focus on the salient part of a point cloud. EAT\nstands for the external attention-based Transformer encoder with linear\ncomputational complexity, which increases the efficiency of the whole pipeline.\nUnlike cutting-edge unsupervised learning models, PointGame leverages geometric\ndescriptors to perceive surface shapes and adaptively mines discriminative\nfeatures from training data. PointGame showcases clear advantages over its\ncompetitors on various downstream tasks under both global and local fine-tuning\nstrategies. The code and pre-trained models will be publicly available.\n","authors":["Yun Liu","Xuefeng Yan","Zhilei Chen","Zhiqi Li","Zeyong Wei","Mingqiang Wei"],"pdf_url":"https://arxiv.org/pdf/2303.13100v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.07680v2","updated":"2023-03-23T08:29:12Z","published":"2022-05-16T13:47:02Z","title":"BBDM: Image-to-image Translation with Brownian Bridge Diffusion Models","summary":"  Image-to-image translation is an important and challenging problem in\ncomputer vision and image processing. Diffusion models (DM) have shown great\npotentials for high-quality image synthesis, and have gained competitive\nperformance on the task of image-to-image translation. However, most of the\nexisting diffusion models treat image-to-image translation as conditional\ngeneration processes, and suffer heavily from the gap between distinct domains.\nIn this paper, a novel image-to-image translation method based on the Brownian\nBridge Diffusion Model (BBDM) is proposed, which models image-to-image\ntranslation as a stochastic Brownian bridge process, and learns the translation\nbetween two domains directly through the bidirectional diffusion process rather\nthan a conditional generation process. To the best of our knowledge, it is the\nfirst work that proposes Brownian Bridge diffusion process for image-to-image\ntranslation. Experimental results on various benchmarks demonstrate that the\nproposed BBDM model achieves competitive performance through both visual\ninspection and measurable metrics.\n","authors":["Bo Li","Kaitao Xue","Bin Liu","Yu-Kun Lai"],"pdf_url":"https://arxiv.org/pdf/2205.07680v2.pdf","comment":"18 pages, 13 figures"},{"id":"http://arxiv.org/abs/2303.13097v1","updated":"2023-03-23T08:25:46Z","published":"2023-03-23T08:25:46Z","title":"CP$^3$: Channel Pruning Plug-in for Point-based Networks","summary":"  Channel pruning can effectively reduce both computational cost and memory\nfootprint of the original network while keeping a comparable accuracy\nperformance. Though great success has been achieved in channel pruning for 2D\nimage-based convolutional networks (CNNs), existing works seldom extend the\nchannel pruning methods to 3D point-based neural networks (PNNs). Directly\nimplementing the 2D CNN channel pruning methods to PNNs undermine the\nperformance of PNNs because of the different representations of 2D images and\n3D point clouds as well as the network architecture disparity. In this paper,\nwe proposed CP$^3$, which is a Channel Pruning Plug-in for Point-based network.\nCP$^3$ is elaborately designed to leverage the characteristics of point clouds\nand PNNs in order to enable 2D channel pruning methods for PNNs. Specifically,\nit presents a coordinate-enhanced channel importance metric to reflect the\ncorrelation between dimensional information and individual channel features,\nand it recycles the discarded points in PNN's sampling process and reconsiders\ntheir potentially-exclusive information to enhance the robustness of channel\npruning. Experiments on various PNN architectures show that CP$^3$ constantly\nimproves state-of-the-art 2D CNN pruning approaches on different point cloud\ntasks. For instance, our compressed PointNeXt-S on ScanObjectNN achieves an\naccuracy of 88.52% with a pruning rate of 57.8%, outperforming the baseline\npruning methods with an accuracy gain of 1.94%.\n","authors":["Yaomin Huang","Ning Liu","Zhengping Che","Zhiyuan Xu","Chaomin Shen","Yaxin Peng","Guixu Zhang","Xinmei Liu","Feifei Feng","Jian Tang"],"pdf_url":"https://arxiv.org/pdf/2303.13097v1.pdf","comment":"Yaomin Huang and Ning Liu are with equal contributions. This paper\n  has been accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2303.13095v1","updated":"2023-03-23T08:21:16Z","published":"2023-03-23T08:21:16Z","title":"Modeling Entities as Semantic Points for Visual Information Extraction\n  in the Wild","summary":"  Recently, Visual Information Extraction (VIE) has been becoming increasingly\nimportant in both the academia and industry, due to the wide range of\nreal-world applications. Previously, numerous works have been proposed to\ntackle this problem. However, the benchmarks used to assess these methods are\nrelatively plain, i.e., scenarios with real-world complexity are not fully\nrepresented in these benchmarks. As the first contribution of this work, we\ncurate and release a new dataset for VIE, in which the document images are much\nmore challenging in that they are taken from real applications, and\ndifficulties such as blur, partial occlusion, and printing shift are quite\ncommon. All these factors may lead to failures in information extraction.\nTherefore, as the second contribution, we explore an alternative approach to\nprecisely and robustly extract key information from document images under such\ntough conditions. Specifically, in contrast to previous methods, which usually\neither incorporate visual information into a multi-modal architecture or train\ntext spotting and information extraction in an end-to-end fashion, we\nexplicitly model entities as semantic points, i.e., center points of entities\nare enriched with semantic information describing the attributes and\nrelationships of different entities, which could largely benefit entity\nlabeling and linking. Extensive experiments on standard benchmarks in this\nfield as well as the proposed dataset demonstrate that the proposed method can\nachieve significantly enhanced performance on entity labeling and linking,\ncompared with previous state-of-the-art models. Dataset is available at\nhttps://www.modelscope.cn/datasets/damo/SIBR/summary.\n","authors":["Zhibo Yang","Rujiao Long","Pengfei Wang","Sibo Song","Humen Zhong","Wenqing Cheng","Xiang Bai","Cong Yao"],"pdf_url":"https://arxiv.org/pdf/2303.13095v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13090v1","updated":"2023-03-23T08:10:25Z","published":"2023-03-23T08:10:25Z","title":"Orthogonal Annotation Benefits Barely-supervised Medical Image\n  Segmentation","summary":"  Recent trends in semi-supervised learning have significantly boosted the\nperformance of 3D semi-supervised medical image segmentation. Compared with 2D\nimages, 3D medical volumes involve information from different directions, e.g.,\ntransverse, sagittal, and coronal planes, so as to naturally provide\ncomplementary views. These complementary views and the intrinsic similarity\namong adjacent 3D slices inspire us to develop a novel annotation way and its\ncorresponding semi-supervised model for effective segmentation. Specifically,\nwe firstly propose the orthogonal annotation by only labeling two orthogonal\nslices in a labeled volume, which significantly relieves the burden of\nannotation. Then, we perform registration to obtain the initial pseudo labels\nfor sparsely labeled volumes. Subsequently, by introducing unlabeled volumes,\nwe propose a dual-network paradigm named Dense-Sparse Co-training (DeSCO) that\nexploits dense pseudo labels in early stage and sparse labels in later stage\nand meanwhile forces consistent output of two networks. Experimental results on\nthree benchmark datasets validated our effectiveness in performance and\nefficiency in annotation. For example, with only 10 annotated slices, our\nmethod reaches a Dice up to 86.93% on KiTS19 dataset.\n","authors":["Heng Cai","Shumeng Li","Lei Qi","Qian Yu","Yinghuan Shi","Yang Gao"],"pdf_url":"https://arxiv.org/pdf/2303.13090v1.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2303.13089v1","updated":"2023-03-23T08:06:10Z","published":"2023-03-23T08:06:10Z","title":"Box-Level Active Detection","summary":"  Active learning selects informative samples for annotation within budget,\nwhich has proven efficient recently on object detection. However, the widely\nused active detection benchmarks conduct image-level evaluation, which is\nunrealistic in human workload estimation and biased towards crowded images.\nFurthermore, existing methods still perform image-level annotation, but equally\nscoring all targets within the same image incurs waste of budget and redundant\nlabels. Having revealed above problems and limitations, we introduce a\nbox-level active detection framework that controls a box-based budget per\ncycle, prioritizes informative targets and avoids redundancy for fair\ncomparison and efficient application.\n  Under the proposed box-level setting, we devise a novel pipeline, namely\nComplementary Pseudo Active Strategy (ComPAS). It exploits both human\nannotations and the model intelligence in a complementary fashion: an efficient\ninput-end committee queries labels for informative objects only; meantime\nwell-learned targets are identified by the model and compensated with\npseudo-labels. ComPAS consistently outperforms 10 competitors under 4 settings\nin a unified codebase. With supervision from labeled data only, it achieves\n100% supervised performance of VOC0712 with merely 19% box annotations. On the\nCOCO dataset, it yields up to 4.3% mAP improvement over the second-best method.\nComPAS also supports training with the unlabeled pool, where it surpasses 90%\nCOCO supervised performance with 85% label reduction. Our source code is\npublicly available at https://github.com/lyumengyao/blad.\n","authors":["Mengyao Lyu","Jundong Zhou","Hui Chen","Yijie Huang","Dongdong Yu","Yaqian Li","Yandong Guo","Yuchen Guo","Liuyu Xiang","Guiguang Ding"],"pdf_url":"https://arxiv.org/pdf/2303.13089v1.pdf","comment":"CVPR 2023 highlight"},{"id":"http://arxiv.org/abs/2211.14068v2","updated":"2023-03-23T08:05:52Z","published":"2022-11-25T12:40:45Z","title":"Fine-Grained Face Swapping via Regional GAN Inversion","summary":"  We present a novel paradigm for high-fidelity face swapping that faithfully\npreserves the desired subtle geometry and texture details. We rethink face\nswapping from the perspective of fine-grained face editing, \\textit{i.e.,\n``editing for swapping'' (E4S)}, and propose a framework that is based on the\nexplicit disentanglement of the shape and texture of facial components.\nFollowing the E4S principle, our framework enables both global and local\nswapping of facial features, as well as controlling the amount of partial\nswapping specified by the user. Furthermore, the E4S paradigm is inherently\ncapable of handling facial occlusions by means of facial masks. At the core of\nour system lies a novel Regional GAN Inversion (RGI) method, which allows the\nexplicit disentanglement of shape and texture. It also allows face swapping to\nbe performed in the latent space of StyleGAN. Specifically, we design a\nmulti-scale mask-guided encoder to project the texture of each facial component\ninto regional style codes. We also design a mask-guided injection module to\nmanipulate the feature maps with the style codes. Based on the disentanglement,\nface swapping is reformulated as a simplified problem of style and mask\nswapping. Extensive experiments and comparisons with current state-of-the-art\nmethods demonstrate the superiority of our approach in preserving texture and\nshape details, as well as working with high resolution images. The project page\nis http://e4s2022.github.io\n","authors":["Zhian Liu","Maomao Li","Yong Zhang","Cairong Wang","Qi Zhang","Jue Wang","Yongwei Nie"],"pdf_url":"https://arxiv.org/pdf/2211.14068v2.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2303.13087v1","updated":"2023-03-23T07:58:48Z","published":"2023-03-23T07:58:48Z","title":"Robust Generalization against Photon-Limited Corruptions via Worst-Case\n  Sharpness Minimization","summary":"  Robust generalization aims to tackle the most challenging data distributions\nwhich are rare in the training set and contain severe noises, i.e.,\nphoton-limited corruptions. Common solutions such as distributionally robust\noptimization (DRO) focus on the worst-case empirical risk to ensure low\ntraining error on the uncommon noisy distributions. However, due to the\nover-parameterized model being optimized on scarce worst-case data, DRO fails\nto produce a smooth loss landscape, thus struggling on generalizing well to the\ntest set. Therefore, instead of focusing on the worst-case risk minimization,\nwe propose SharpDRO by penalizing the sharpness of the worst-case distribution,\nwhich measures the loss changes around the neighbor of learning parameters.\nThrough worst-case sharpness minimization, the proposed method successfully\nproduces a flat loss curve on the corrupted distributions, thus achieving\nrobust generalization. Moreover, by considering whether the distribution\nannotation is available, we apply SharpDRO to two problem settings and design a\nworst-case selection process for robust generalization. Theoretically, we show\nthat SharpDRO has a great convergence guarantee. Experimentally, we simulate\nphoton-limited corruptions using CIFAR10/100 and ImageNet30 datasets and show\nthat SharpDRO exhibits a strong generalization ability against severe\ncorruptions and exceeds well-known baseline methods with large performance\ngains.\n","authors":["Zhuo Huang","Miaoxi Zhu","Xiaobo Xia","Li Shen","Jun Yu","Chen Gong","Bo Han","Bo Du","Tongliang Liu"],"pdf_url":"https://arxiv.org/pdf/2303.13087v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.10848v2","updated":"2023-03-23T07:56:07Z","published":"2023-03-20T03:56:47Z","title":"Weakly-Supervised Text Instance Segmentation","summary":"  Text segmentation is a challenging vision task with many downstream\napplications. Current text segmentation methods require pixel-level\nannotations, which are expensive in the cost of human labor and limited in\napplication scenarios. In this paper, we take the first attempt to perform\nweakly-supervised text instance segmentation by bridging text recognition and\ntext segmentation. The insight is that text recognition methods provide precise\nattention position of each text instance, and the attention location can feed\nto both a text adaptive refinement head (TAR) and a text segmentation head.\nSpecifically, the proposed TAR generates pseudo labels by performing two-stage\niterative refinement operations on the attention location to fit the accurate\nboundaries of the corresponding text instance. Meanwhile, the text segmentation\nhead takes the rough attention location to predict segmentation masks which are\nsupervised by the aforementioned pseudo labels. In addition, we design a\nmask-augmented contrastive learning by treating our segmentation result as an\naugmented version of the input text image, thus improving the visual\nrepresentation and further enhancing the performance of both recognition and\nsegmentation. The experimental results demonstrate that the proposed method\nsignificantly outperforms weakly-supervised instance segmentation methods on\nICDAR13-FST (18.95$\\%$ improvement) and TextSeg (17.80$\\%$ improvement)\nbenchmarks.\n","authors":["Xinyan Zu","Haiyang Yu","Bin Li","Xiangyang Xue"],"pdf_url":"https://arxiv.org/pdf/2303.10848v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.01392v2","updated":"2023-03-23T07:15:53Z","published":"2023-02-02T20:06:58Z","title":"Multi-modal Gated Mixture of Local-to-Global Experts for Dynamic Image\n  Fusion","summary":"  Infrared and visible image fusion aims to integrate comprehensive information\nfrom multiple sources to achieve superior performances on various practical\ntasks, such as detection, over that of a single modality. However, most\nexisting methods directly combined the texture details and object contrast of\ndifferent modalities, ignoring the dynamic changes in reality, which diminishes\nthe visible texture in good lighting conditions and the infrared contrast in\nlow lighting conditions. To fill this gap, we propose a dynamic image fusion\nframework with a multi-modal gated mixture of local-to-global experts, termed\nMoE-Fusion, to dynamically extract effective and comprehensive information from\nthe respective modalities. Our model consists of a Mixture of Local Experts\n(MoLE) and a Mixture of Global Experts (MoGE) guided by a multi-modal gate. The\nMoLE performs specialized learning of multi-modal local features, prompting the\nfused images to retain the local information in a sample-adaptive manner, while\nthe MoGE focuses on the global information that complements the fused image\nwith overall texture detail and contrast. Extensive experiments show that our\nMoE-Fusion outperforms state-of-the-art methods in preserving multi-modal image\ntexture and contrast through the local-to-global dynamic learning paradigm, and\nalso achieves superior performance on detection tasks. Our code will be\navailable: https://github.com/SunYM2020/MoE-Fusion.\n","authors":["Yiming Sun","Bing Cao","Pengfei Zhu","Qinghua Hu"],"pdf_url":"https://arxiv.org/pdf/2302.01392v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2111.15077v2","updated":"2023-03-23T07:15:24Z","published":"2021-11-30T02:35:51Z","title":"Unsupervised Domain Generalization for Person Re-identification: A\n  Domain-specific Adaptive Framework","summary":"  Domain generalization (DG) has attracted much attention in person\nre-identification (ReID) recently. It aims to make a model trained on multiple\nsource domains generalize to an unseen target domain. Although achieving\npromising progress, existing methods usually need the source domains to be\nlabeled, which could be a significant burden for practical ReID tasks. In this\npaper, we turn to investigate unsupervised domain generalization for ReID, by\nassuming that no label is available for any source domains.\n  To address this challenging setting, we propose a simple and efficient\ndomain-specific adaptive framework, and realize it with an adaptive\nnormalization module designed upon the batch and instance normalization\ntechniques. In doing so, we successfully yield reliable pseudo-labels to\nimplement training and also enhance the domain generalization capability of the\nmodel as required. In addition, we show that our framework can even be applied\nto improve person ReID under the settings of supervised domain generalization\nand unsupervised domain adaptation, demonstrating competitive performance with\nrespect to relevant methods. Extensive experimental study on benchmark datasets\nis conducted to validate the proposed framework. A significance of our work\nlies in that it shows the potential of unsupervised domain generalization for\nperson ReID and sets a strong baseline for the further research on this topic.\n","authors":["Lei Qi","Jiaqi Liu","Lei Wang","Yinghuan Shi","Xin Geng"],"pdf_url":"https://arxiv.org/pdf/2111.15077v2.pdf","comment":"Accepted to Pattern Recognition (PR)"},{"id":"http://arxiv.org/abs/2303.13077v1","updated":"2023-03-23T07:14:48Z","published":"2023-03-23T07:14:48Z","title":"Improving the Performance of Spiking Neural Networks on Event-based\n  Datasets with Knowledge Transfer","summary":"  Spiking neural networks (SNNs) have rich spatial-temporal dynamics, which are\nsuitable for processing neuromorphic, event-based data. However, event-based\ndatasets are usually less annotated than static datasets used in traditional\ndeep learning. Small data scale makes SNNs prone to overfitting and limits the\nperformance of the SNN. To enhance the generalizability of SNNs on event-based\ndatasets, we propose a knowledge-transfer framework that leverages static\nimages to assist in the training on neuromorphic datasets. Our method proposes\ndomain loss and semantic loss to exploit both domain-invariant and unique\nfeatures of these two domains, providing SNNs with more generalized knowledge\nfor subsequent targeted training on neuromorphic data. Specifically, domain\nloss aligns the feature space and aims to capture common features between\nstatic and event-based images, while semantic loss emphasizes that the\ndifferences between samples from different categories should be as large as\npossible. Experimental results demonstrate that our method outperforms existing\nmethods on all mainstream neuromorphic vision datasets. In particular, we\nachieve significant performance improvement of 2.7\\% and 9.8\\% when using only\n10\\% training data of CIFAR10-DVS and N-Caltech 101 datasets, respectively.\n","authors":["Xiang He","Dongcheng Zhao","Yang Li","Guobin Shen","Qingqun Kong","Yi Zeng"],"pdf_url":"https://arxiv.org/pdf/2303.13077v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13076v1","updated":"2023-03-23T07:13:57Z","published":"2023-03-23T07:13:57Z","title":"CORA: Adapting CLIP for Open-Vocabulary Detection with Region Prompting\n  and Anchor Pre-Matching","summary":"  Open-vocabulary detection (OVD) is an object detection task aiming at\ndetecting objects from novel categories beyond the base categories on which the\ndetector is trained. Recent OVD methods rely on large-scale visual-language\npre-trained models, such as CLIP, for recognizing novel objects. We identify\nthe two core obstacles that need to be tackled when incorporating these models\ninto detector training: (1) the distribution mismatch that happens when\napplying a VL-model trained on whole images to region recognition tasks; (2)\nthe difficulty of localizing objects of unseen classes. To overcome these\nobstacles, we propose CORA, a DETR-style framework that adapts CLIP for\nOpen-vocabulary detection by Region prompting and Anchor pre-matching. Region\nprompting mitigates the whole-to-region distribution gap by prompting the\nregion features of the CLIP-based region classifier. Anchor pre-matching helps\nlearning generalizable object localization by a class-aware matching mechanism.\nWe evaluate CORA on the COCO OVD benchmark, where we achieve 41.7 AP50 on novel\nclasses, which outperforms the previous SOTA by 2.4 AP50 even without resorting\nto extra training data. When extra training data is available, we train\nCORA$^+$ on both ground-truth base-category annotations and additional pseudo\nbounding box labels computed by CORA. CORA$^+$ achieves 43.1 AP50 on the COCO\nOVD benchmark and 28.1 box APr on the LVIS OVD benchmark.\n","authors":["Xiaoshi Wu","Feng Zhu","Rui Zhao","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2303.13076v1.pdf","comment":"11 pages, 4 figures. Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2211.08332v3","updated":"2023-03-23T07:13:25Z","published":"2022-11-15T17:44:05Z","title":"Versatile Diffusion: Text, Images and Variations All in One Diffusion\n  Model","summary":"  Recent advances in diffusion models have set an impressive milestone in many\ngeneration tasks, and trending works such as DALL-E2, Imagen, and Stable\nDiffusion have attracted great interest. Despite the rapid landscape changes,\nrecent new approaches focus on extensions and performance rather than capacity,\nthus requiring separate models for separate tasks. In this work, we expand the\nexisting single-flow diffusion pipeline into a multi-task multimodal network,\ndubbed Versatile Diffusion (VD), that handles multiple flows of text-to-image,\nimage-to-text, and variations in one unified model. The pipeline design of VD\ninstantiates a unified multi-flow diffusion framework, consisting of sharable\nand swappable layer modules that enable the crossmodal generality beyond images\nand text. Through extensive experiments, we demonstrate that VD successfully\nachieves the following: a) VD outperforms the baseline approaches and handles\nall its base tasks with competitive quality; b) VD enables novel extensions\nsuch as disentanglement of style and semantics, dual- and multi-context\nblending, etc.; c) The success of our multi-flow multimodal framework over\nimages and text may inspire further diffusion-based universal AI research. Our\ncode and models are open-sourced at\nhttps://github.com/SHI-Labs/Versatile-Diffusion.\n","authors":["Xingqian Xu","Zhangyang Wang","Eric Zhang","Kai Wang","Humphrey Shi"],"pdf_url":"https://arxiv.org/pdf/2211.08332v3.pdf","comment":"Github link: https://github.com/SHI-Labs/Versatile-Diffusion"},{"id":"http://arxiv.org/abs/2303.00521v2","updated":"2023-03-23T06:57:56Z","published":"2023-03-01T13:52:40Z","title":"Quality-aware Pre-trained Models for Blind Image Quality Assessment","summary":"  Blind image quality assessment (BIQA) aims to automatically evaluate the\nperceived quality of a single image, whose performance has been improved by\ndeep learning-based methods in recent years. However, the paucity of labeled\ndata somewhat restrains deep learning-based BIQA methods from unleashing their\nfull potential. In this paper, we propose to solve the problem by a pretext\ntask customized for BIQA in a self-supervised learning manner, which enables\nlearning representations from orders of magnitude more data. To constrain the\nlearning process, we propose a quality-aware contrastive loss based on a simple\nassumption: the quality of patches from a distorted image should be similar,\nbut vary from patches from the same image with different degradations and\npatches from different images. Further, we improve the existing degradation\nprocess and form a degradation space with the size of roughly $2\\times10^7$.\nAfter pre-trained on ImageNet using our method, models are more sensitive to\nimage quality and perform significantly better on downstream BIQA tasks.\nExperimental results show that our method obtains remarkable improvements on\npopular BIQA datasets.\n","authors":["Kai Zhao","Kun Yuan","Ming Sun","Mading Li","Xing Wen"],"pdf_url":"https://arxiv.org/pdf/2303.00521v2.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2303.13071v1","updated":"2023-03-23T06:54:34Z","published":"2023-03-23T06:54:34Z","title":"PanoHead: Geometry-Aware 3D Full-Head Synthesis in 360$^{\\circ}$","summary":"  Synthesis and reconstruction of 3D human head has gained increasing interests\nin computer vision and computer graphics recently. Existing state-of-the-art 3D\ngenerative adversarial networks (GANs) for 3D human head synthesis are either\nlimited to near-frontal views or hard to preserve 3D consistency in large view\nangles. We propose PanoHead, the first 3D-aware generative model that enables\nhigh-quality view-consistent image synthesis of full heads in $360^\\circ$ with\ndiverse appearance and detailed geometry using only in-the-wild unstructured\nimages for training. At its core, we lift up the representation power of recent\n3D GANs and bridge the data alignment gap when training from in-the-wild images\nwith widely distributed views. Specifically, we propose a novel two-stage\nself-adaptive image alignment for robust 3D GAN training. We further introduce\na tri-grid neural volume representation that effectively addresses front-face\nand back-head feature entanglement rooted in the widely-adopted tri-plane\nformulation. Our method instills prior knowledge of 2D image segmentation in\nadversarial learning of 3D neural scene structures, enabling compositable head\nsynthesis in diverse backgrounds. Benefiting from these designs, our method\nsignificantly outperforms previous 3D GANs, generating high-quality 3D heads\nwith accurate geometry and diverse appearances, even with long wavy and afro\nhairstyles, renderable from arbitrary poses. Furthermore, we show that our\nsystem can reconstruct full 3D heads from single input images for personalized\nrealistic 3D avatars.\n","authors":["Sizhe An","Hongyi Xu","Yichun Shi","Guoxian Song","Umit Ogras","Linjie Luo"],"pdf_url":"https://arxiv.org/pdf/2303.13071v1.pdf","comment":"CVPR 2023. Project Page:https://sizhean.github.io/panohead"},{"id":"http://arxiv.org/abs/2303.13069v1","updated":"2023-03-23T06:53:14Z","published":"2023-03-23T06:53:14Z","title":"Human Guided Ground-truth Generation for Realistic Image\n  Super-resolution","summary":"  How to generate the ground-truth (GT) image is a critical issue for training\nrealistic image super-resolution (Real-ISR) models. Existing methods mostly\ntake a set of high-resolution (HR) images as GTs and apply various degradations\nto simulate their low-resolution (LR) counterparts. Though great progress has\nbeen achieved, such an LR-HR pair generation scheme has several limitations.\nFirst, the perceptual quality of HR images may not be high enough, limiting the\nquality of Real-ISR outputs. Second, existing schemes do not consider much\nhuman perception in GT generation, and the trained models tend to produce\nover-smoothed results or unpleasant artifacts. With the above considerations,\nwe propose a human guided GT generation scheme. We first elaborately train\nmultiple image enhancement models to improve the perceptual quality of HR\nimages, and enable one LR image having multiple HR counterparts. Human subjects\nare then involved to annotate the high quality regions among the enhanced HR\nimages as GTs, and label the regions with unpleasant artifacts as negative\nsamples. A human guided GT image dataset with both positive and negative\nsamples is then constructed, and a loss function is proposed to train the\nReal-ISR models. Experiments show that the Real-ISR models trained on our\ndataset can produce perceptually more realistic results with less artifacts.\nDataset and codes can be found at https://github.com/ChrisDud0257/HGGT\n","authors":["Du Chen","Jie Liang","Xindong Zhang","Ming Liu","Hui Zeng","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.13069v1.pdf","comment":"10 pages. Already accpted by 2023 IEEE/CVF Conference on Computer\n  Vision and Pattern Recognition (CVPR)"},{"id":"http://arxiv.org/abs/2303.11048v2","updated":"2023-03-23T06:21:43Z","published":"2023-03-20T11:59:23Z","title":"Revisiting Transformer for Point Cloud-based 3D Scene Graph Generation","summary":"  In this paper, we propose the semantic graph Transformer (SGT) for 3D scene\ngraph generation. The task aims to parse a cloud point-based scene into a\nsemantic structural graph, with the core challenge of modeling the complex\nglobal structure. Existing methods based on graph convolutional networks (GCNs)\nsuffer from the over-smoothing dilemma and could only propagate information\nfrom limited neighboring nodes. In contrast, our SGT uses Transformer layers as\nthe base building block to allow global information passing, with two types of\nproposed Transformer layers tailored for the 3D scene graph generation task.\nSpecifically, we introduce the graph embedding layer to best utilize the global\ninformation in graph edges while maintaining comparable computation costs.\nAdditionally, we propose the semantic injection layer to leverage categorical\ntext labels and visual object knowledge. We benchmark our SGT on the\nestablished 3DSSG benchmark and achieve a 35.9% absolute improvement in\nrelationship prediction's R@50 and an 80.4% boost on the subset with complex\nscenes over the state-of-the-art. Our analyses further show SGT's superiority\nin the long-tailed and zero-shot scenarios. We will release the code and model.\n","authors":["Changsheng Lv","Mengshi Qi","Xia Li","Zhengyuan Yang","Huadong Ma"],"pdf_url":"https://arxiv.org/pdf/2303.11048v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13062v1","updated":"2023-03-23T06:17:23Z","published":"2023-03-23T06:17:23Z","title":"SIEDOB: Semantic Image Editing by Disentangling Object and Background","summary":"  Semantic image editing provides users with a flexible tool to modify a given\nimage guided by a corresponding segmentation map. In this task, the features of\nthe foreground objects and the backgrounds are quite different. However, all\nprevious methods handle backgrounds and objects as a whole using a monolithic\nmodel. Consequently, they remain limited in processing content-rich images and\nsuffer from generating unrealistic objects and texture-inconsistent\nbackgrounds. To address this issue, we propose a novel paradigm,\n\\textbf{S}emantic \\textbf{I}mage \\textbf{E}diting by \\textbf{D}isentangling\n\\textbf{O}bject and \\textbf{B}ackground (\\textbf{SIEDOB}), the core idea of\nwhich is to explicitly leverages several heterogeneous subnetworks for objects\nand backgrounds. First, SIEDOB disassembles the edited input into background\nregions and instance-level objects. Then, we feed them into the dedicated\ngenerators. Finally, all synthesized parts are embedded in their original\nlocations and utilize a fusion network to obtain a harmonized result. Moreover,\nto produce high-quality edited images, we propose some innovative designs,\nincluding Semantic-Aware Self-Propagation Module, Boundary-Anchored Patch\nDiscriminator, and Style-Diversity Object Generator, and integrate them into\nSIEDOB. We conduct extensive experiments on Cityscapes and ADE20K-Room datasets\nand exhibit that our method remarkably outperforms the baselines, especially in\nsynthesizing realistic and diverse objects and texture-consistent backgrounds.\n","authors":["Wuyang Luo","Su Yang","Xinjian Zhang","Weishan Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.13062v1.pdf","comment":"CVPR 2023 highlight paper"},{"id":"http://arxiv.org/abs/2303.13060v1","updated":"2023-03-23T06:16:14Z","published":"2023-03-23T06:16:14Z","title":"DiffPattern: Layout Pattern Generation via Discrete Diffusion","summary":"  Deep generative models dominate the existing literature in layout pattern\ngeneration. However, leaving the guarantee of legality to an inexplicable\nneural network could be problematic in several applications. In this paper, we\npropose \\tool{DiffPattern} to generate reliable layout patterns.\n\\tool{DiffPattern} introduces a novel diverse topology generation method via a\ndiscrete diffusion model with compute-efficiently lossless layout pattern\nrepresentation. Then a white-box pattern assessment is utilized to generate\nlegal patterns given desired design rules. Our experiments on several benchmark\nsettings show that \\tool{DiffPattern} significantly outperforms existing\nbaselines and is capable of synthesizing reliable layout patterns.\n","authors":["Zixiao Wang","Yunheng Shen","Wenqian Zhao","Yang Bai","Guojin Chen","Farzan Farnia","Bei Yu"],"pdf_url":"https://arxiv.org/pdf/2303.13060v1.pdf","comment":"DAC2023 Accepted"},{"id":"http://arxiv.org/abs/2303.10610v2","updated":"2023-03-23T06:12:48Z","published":"2023-03-19T09:15:45Z","title":"DiffMIC: Dual-Guidance Diffusion Network for Medical Image\n  Classification","summary":"  Diffusion Probabilistic Models have recently shown remarkable performance in\ngenerative image modeling, attracting significant attention in the computer\nvision community. However, while a substantial amount of diffusion-based\nresearch has focused on generative tasks, few studies have applied diffusion\nmodels to general medical image classification. In this paper, we propose the\nfirst diffusion-based model (named DiffMIC) to address general medical image\nclassification by eliminating unexpected noise and perturbations in medical\nimages and robustly capturing semantic representation. To achieve this goal, we\ndevise a dual conditional guidance strategy that conditions each diffusion step\nwith multiple granularities to improve step-wise regional attention.\nFurthermore, we propose learning the mutual information in each granularity by\nenforcing Maximum-Mean Discrepancy regularization during the diffusion forward\nprocess. We evaluate the effectiveness of our DiffMIC on three medical\nclassification tasks with different image modalities, including placental\nmaturity grading on ultrasound images, skin lesion classification using\ndermatoscopic images, and diabetic retinopathy grading using fundus images. Our\nexperimental results demonstrate that DiffMIC outperforms state-of-the-art\nmethods by a significant margin, indicating the universality and effectiveness\nof the proposed model. Our code will be publicly available at\nhttps://github.com/scott-yjyang/DiffMIC.\n","authors":["Yijun Yang","Huazhu Fu","Angelica I. Aviles-Rivero","Carola-Bibiane Schönlieb","Lei Zhu"],"pdf_url":"https://arxiv.org/pdf/2303.10610v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.00304v3","updated":"2023-03-23T05:59:24Z","published":"2023-03-01T08:00:46Z","title":"Renderable Neural Radiance Map for Visual Navigation","summary":"  We propose a novel type of map for visual navigation, a renderable neural\nradiance map (RNR-Map), which is designed to contain the overall visual\ninformation of a 3D environment. The RNR-Map has a grid form and consists of\nlatent codes at each pixel. These latent codes are embedded from image\nobservations, and can be converted to the neural radiance field which enables\nimage rendering given a camera pose. The recorded latent codes implicitly\ncontain visual information about the environment, which makes the RNR-Map\nvisually descriptive. This visual information in RNR-Map can be a useful\nguideline for visual localization and navigation. We develop localization and\nnavigation frameworks that can effectively utilize the RNR-Map. We evaluate the\nproposed frameworks on camera tracking, visual localization, and image-goal\nnavigation. Experimental results show that the RNR-Map-based localization\nframework can find the target location based on a single query image with fast\nspeed and competitive accuracy compared to other baselines. Also, this\nlocalization framework is robust to environmental changes, and even finds the\nmost visually similar places when a query image from a different environment is\ngiven. The proposed navigation framework outperforms the existing image-goal\nnavigation methods in difficult scenarios, under odometry and actuation noises.\nThe navigation framework shows 65.7% success rate in curved scenarios of the\nNRNS dataset, which is an improvement of 18.6% over the current\nstate-of-the-art. Project page: https://rllab-snu.github.io/projects/RNR-Map/\n","authors":["Obin Kwon","Jeongho Park","Songhwai Oh"],"pdf_url":"https://arxiv.org/pdf/2303.00304v3.pdf","comment":"Preprint version. CVPR 2023 accepted, highlight paper. Project page:\n  https://rllab-snu.github.io/projects/RNR-Map/"},{"id":"http://arxiv.org/abs/2303.13051v1","updated":"2023-03-23T05:53:34Z","published":"2023-03-23T05:53:34Z","title":"Hierarchical Semantic Contrast for Scene-aware Video Anomaly Detection","summary":"  Increasing scene-awareness is a key challenge in video anomaly detection\n(VAD). In this work, we propose a hierarchical semantic contrast (HSC) method\nto learn a scene-aware VAD model from normal videos. We first incorporate\nforeground object and background scene features with high-level semantics by\ntaking advantage of pre-trained video parsing models. Then, building upon the\nautoencoder-based reconstruction framework, we introduce both scene-level and\nobject-level contrastive learning to enforce the encoded latent features to be\ncompact within the same semantic classes while being separable across different\nclasses. This hierarchical semantic contrast strategy helps to deal with the\ndiversity of normal patterns and also increases their discrimination ability.\nMoreover, for the sake of tackling rare normal activities, we design a\nskeleton-based motion augmentation to increase samples and refine the model\nfurther. Extensive experiments on three public datasets and scene-dependent\nmixture datasets validate the effectiveness of our proposed method.\n","authors":["Shengyang Sun","Xiaojin Gong"],"pdf_url":"https://arxiv.org/pdf/2303.13051v1.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2210.06096v2","updated":"2023-03-23T05:50:55Z","published":"2022-10-12T11:19:55Z","title":"Masked Motion Encoding for Self-Supervised Video Representation Learning","summary":"  How to learn discriminative video representation from unlabeled videos is\nchallenging but crucial for video analysis. The latest attempts seek to learn a\nrepresentation model by predicting the appearance contents in the masked\nregions. However, simply masking and recovering appearance contents may not be\nsufficient to model temporal clues as the appearance contents can be easily\nreconstructed from a single frame. To overcome this limitation, we present\nMasked Motion Encoding (MME), a new pre-training paradigm that reconstructs\nboth appearance and motion information to explore temporal clues. In MME, we\nfocus on addressing two critical challenges to improve the representation\nperformance: 1) how to well represent the possible long-term motion across\nmultiple frames; and 2) how to obtain fine-grained temporal clues from sparsely\nsampled videos. Motivated by the fact that human is able to recognize an action\nby tracking objects' position changes and shape changes, we propose to\nreconstruct a motion trajectory that represents these two kinds of change in\nthe masked regions. Besides, given the sparse video input, we enforce the model\nto reconstruct dense motion trajectories in both spatial and temporal\ndimensions. Pre-trained with our MME paradigm, the model is able to anticipate\nlong-term and fine-grained motion details. Code is available at\nhttps://github.com/XinyuSun/MME.\n","authors":["Xinyu Sun","Peihao Chen","Liangwei Chen","Changhao Li","Thomas H. Li","Mingkui Tan","Chuang Gan"],"pdf_url":"https://arxiv.org/pdf/2210.06096v2.pdf","comment":"CVPR 2023 camera-ready version"},{"id":"http://arxiv.org/abs/2303.12670v2","updated":"2023-03-23T05:41:37Z","published":"2023-03-22T15:48:23Z","title":"Correlational Image Modeling for Self-Supervised Visual Pre-Training","summary":"  We introduce Correlational Image Modeling (CIM), a novel and surprisingly\neffective approach to self-supervised visual pre-training. Our CIM performs a\nsimple pretext task: we randomly crop image regions (exemplars) from an input\nimage (context) and predict correlation maps between the exemplars and the\ncontext. Three key designs enable correlational image modeling as a nontrivial\nand meaningful self-supervisory task. First, to generate useful\nexemplar-context pairs, we consider cropping image regions with various scales,\nshapes, rotations, and transformations. Second, we employ a bootstrap learning\nframework that involves online and target encoders. During pre-training, the\nformer takes exemplars as inputs while the latter converts the context. Third,\nwe model the output correlation maps via a simple cross-attention block, within\nwhich the context serves as queries and the exemplars offer values and keys. We\nshow that CIM performs on par or better than the current state of the art on\nself-supervised and transfer benchmarks.\n","authors":["Wei Li","Jiahao Xie","Chen Change Loy"],"pdf_url":"https://arxiv.org/pdf/2303.12670v2.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2301.01482v3","updated":"2023-03-23T05:34:15Z","published":"2023-01-04T08:22:34Z","title":"Adjust Sample Imbalance and Exclude Similar Object in Underwater Object\n  Tracking","summary":"  Although modern trackers have competitive performance when dealing with\nunderwater image degradation, there are still two problems when applying them\nto Underwater Object Tracking (UOT). On the one hand, the single object tracker\nis trained on the open-air datasets, which means that the tracker has a serious\nsample imbalance between underwater objects and open-air objects when applied\nto UOT. On the other hand, underwater targets such as fish and dolphins usually\nhave a similar appearance, it is challenging for the model itself to\ndiscriminate the weak discriminative features. The existing detection-based\npost processing is hard to distinguish the tracked target among similar\nobjects. In this paper, we propose UOSTrack, which consists of Underwater\nimages and Open-air sequences Hybrid Training (UOHT) and Motion-based Post\nProcessing (MBPP). UOHT is designed to adjust the sample imbalance underwater\ntracker. Specifically, Underwater Object Detection (UOD) image is converted\ninto imag pairs through customized data augmentation, so that the tracker has\nmore underwater domain training samples and learn the feature expression of\nunderwater objects. MBPP is proposed to exclude similar objects around the\ntarget. Specifically, it uses the estimation box predicted by the Kalman Filter\nand candidate boxes in each frame to reconfirm the target that is hidden in the\ncandidate area when the target is lost. UOSTrack has an average performance\nimprovement of 3.5% over OSTrack on Similar Object challenge of the UOT100 and\nUTB180 datasets. The average performance improvement of UOSTrack on UOT100 and\nUTB180 is 1% and 3%, respectively. Experiments on two UOT benchmarks\ndemonstrate the effectiveness of UOHT and MBPP, and the generalization and\napplicability of MBPP for UOT.\n","authors":["Yunfeng Li","Bo Wang","Ye Li","Wei Huo","Zhuoyan Liu"],"pdf_url":"https://arxiv.org/pdf/2301.01482v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13043v1","updated":"2023-03-23T05:17:05Z","published":"2023-03-23T05:17:05Z","title":"Top-Down Visual Attention from Analysis by Synthesis","summary":"  Current attention algorithms (e.g., self-attention) are stimulus-driven and\nhighlight all the salient objects in an image. However, intelligent agents like\nhumans often guide their attention based on the high-level task at hand,\nfocusing only on task-related objects. This ability of task-guided top-down\nattention provides task-adaptive representation and helps the model generalize\nto various tasks. In this paper, we consider top-down attention from a classic\nAnalysis-by-Synthesis (AbS) perspective of vision. Prior work indicates a\nfunctional equivalence between visual attention and sparse reconstruction; we\nshow that an AbS visual system that optimizes a similar sparse reconstruction\nobjective modulated by a goal-directed top-down signal naturally simulates\ntop-down attention. We further propose Analysis-by-Synthesis Vision Transformer\n(AbSViT), which is a top-down modulated ViT model that variationally\napproximates AbS, and achieves controllable top-down attention. For real-world\napplications, AbSViT consistently improves over baselines on Vision-Language\ntasks such as VQA and zero-shot retrieval where language guides the top-down\nattention. AbSViT can also serve as a general backbone, improving performance\non classification, semantic segmentation, and model robustness.\n","authors":["Baifeng Shi","Trevor Darrell","Xin Wang"],"pdf_url":"https://arxiv.org/pdf/2303.13043v1.pdf","comment":"CVPR2023 highlight; Project page:\n  https://sites.google.com/view/absvit"},{"id":"http://arxiv.org/abs/2303.13040v1","updated":"2023-03-23T05:10:22Z","published":"2023-03-23T05:10:22Z","title":"Open-Vocabulary Object Detection using Pseudo Caption Labels","summary":"  Recent open-vocabulary detection methods aim to detect novel objects by\ndistilling knowledge from vision-language models (VLMs) trained on a vast\namount of image-text pairs. To improve the effectiveness of these methods,\nresearchers have utilized datasets with a large vocabulary that contains a\nlarge number of object classes, under the assumption that such data will enable\nmodels to extract comprehensive knowledge on the relationships between various\nobjects and better generalize to unseen object classes. In this study, we argue\nthat more fine-grained labels are necessary to extract richer knowledge about\nnovel objects, including object attributes and relationships, in addition to\ntheir names. To address this challenge, we propose a simple and effective\nmethod named Pseudo Caption Labeling (PCL), which utilizes an image captioning\nmodel to generate captions that describe object instances from diverse\nperspectives. The resulting pseudo caption labels offer dense samples for\nknowledge distillation. On the LVIS benchmark, our best model trained on the\nde-duplicated VisualGenome dataset achieves an AP of 34.5 and an APr of 30.6,\ncomparable to the state-of-the-art performance. PCL's simplicity and\nflexibility are other notable features, as it is a straightforward\npre-processing technique that can be used with any image captioning model\nwithout imposing any restrictions on model architecture or training process.\n","authors":["Han-Cheol Cho","Won Young Jhoo","Wooyoung Kang","Byungseok Roh"],"pdf_url":"https://arxiv.org/pdf/2303.13040v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12145v2","updated":"2023-03-23T04:54:28Z","published":"2023-03-21T19:02:36Z","title":"Efficient Feature Distillation for Zero-shot Detection","summary":"  The large-scale vision-language models (e.g., CLIP) are leveraged by\ndifferent methods to detect unseen objects. However, most of these works\nrequire additional captions or images for training, which is not feasible in\nthe context of zero-shot detection. In contrast, the distillation-based method\nis an extra-data-free method, but it has its limitations. Specifically,\nexisting work creates distillation regions that are biased to the base\ncategories, which limits the distillation of novel category information and\nharms the distillation efficiency. Furthermore, directly using the raw feature\nfrom CLIP for distillation neglects the domain gap between the training data of\nCLIP and the detection datasets, which makes it difficult to learn the mapping\nfrom the image region to the vision-language feature space - an essential\ncomponent for detecting unseen objects. As a result, existing\ndistillation-based methods require an excessively long training schedule. To\nsolve these problems, we propose Efficient feature distillation for Zero-Shot\nDetection (EZSD). Firstly, EZSD adapts the CLIP's feature space to the target\ndetection domain by re-normalizing CLIP to bridge the domain gap; Secondly,\nEZSD uses CLIP to generate distillation proposals with potential novel\ninstances, to avoid the distillation being overly biased to the base\ncategories. Finally, EZSD takes advantage of semantic meaning for regression to\nfurther improve the model performance. As a result, EZSD achieves\nstate-of-the-art performance in the COCO zero-shot benchmark with a much\nshorter training schedule and outperforms previous work by 4% in LVIS overall\nsetting with 1/10 training time.\n","authors":["Zhuoming Liu","Xuefeng Hu","Ram Nevatia"],"pdf_url":"https://arxiv.org/pdf/2303.12145v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.07291v5","updated":"2023-03-23T04:44:42Z","published":"2022-02-15T10:17:02Z","title":"Exploring Discontinuity for Video Frame Interpolation","summary":"  Video frame interpolation (VFI) is the task that synthesizes the intermediate\nframe given two consecutive frames. Most of the previous studies have focused\non appropriate frame warping operations and refinement modules for the warped\nframes. These studies have been conducted on natural videos containing only\ncontinuous motions. However, many practical videos contain various unnatural\nobjects with discontinuous motions such as logos, user interfaces and\nsubtitles. We propose three techniques to make the existing deep learning-based\nVFI architectures robust to these elements. First is a novel data augmentation\nstrategy called figure-text mixing (FTM) which can make the models learn\ndiscontinuous motions during training stage without any extra dataset. Second,\nwe propose a simple but effective module that predicts a map called\ndiscontinuity map (D-map), which densely distinguishes between areas of\ncontinuous and discontinuous motions. Lastly, we propose loss functions to give\nsupervisions of the discontinuous motion areas which can be applied along with\nFTM and D-map. We additionally collect a special test benchmark called\nGraphical Discontinuous Motion (GDM) dataset consisting of some mobile games\nand chatting videos. Applied to the various state-of-the-art VFI networks, our\nmethod significantly improves the interpolation qualities on the videos from\nnot only GDM dataset, but also the existing benchmarks containing only\ncontinuous motions such as Vimeo90K, UCF101, and DAVIS.\n","authors":["Sangjin Lee","Hyeongmin Lee","Chajin Shin","Hanbin Son","Sangyoun Lee"],"pdf_url":"https://arxiv.org/pdf/2202.07291v5.pdf","comment":"highlight at CVPR 2023 (10% of accepted papers)"},{"id":"http://arxiv.org/abs/2303.13033v1","updated":"2023-03-23T04:41:44Z","published":"2023-03-23T04:41:44Z","title":"Federated Uncertainty-Aware Aggregation for Fundus Diabetic Retinopathy\n  Staging","summary":"  Deep learning models have shown promising performance in the field of\ndiabetic retinopathy (DR) staging. However, collaboratively training a DR\nstaging model across multiple institutions remains a challenge due to non-iid\ndata, client reliability, and confidence evaluation of the prediction. To\naddress these issues, we propose a novel federated uncertainty-aware\naggregation paradigm (FedUAA), which considers the reliability of each client\nand produces a confidence estimation for the DR staging. In our FedUAA, an\naggregated encoder is shared by all clients for learning a global\nrepresentation of fundus images, while a novel temperature-warmed uncertainty\nhead (TWEU) is utilized for each client for local personalized staging\ncriteria. Our TWEU employs an evidential deep layer to produce the uncertainty\nscore with the DR staging results for client reliability evaluation.\nFurthermore, we developed a novel uncertainty-aware weighting module (UAW) to\ndynamically adjust the weights of model aggregation based on the uncertainty\nscore distribution of each client. In our experiments, we collect five publicly\navailable datasets from different institutions to conduct a dataset for\nfederated DR staging to satisfy the real non-iid condition. The experimental\nresults demonstrate that our FedUAA achieves better DR staging performance with\nhigher reliability compared to other federated learning methods. Our proposed\nFedUAA paradigm effectively addresses the challenges of collaboratively\ntraining DR staging models across multiple institutions, and provides a robust\nand reliable solution for the deployment of DR diagnosis models in real-world\nclinical scenarios.\n","authors":["Meng Wang","Lianyu Wang","Xinxing Xu","Ke Zou","Yiming Qian","Rick Siow Mong Goh","Yong Liu","Huazhu Fu"],"pdf_url":"https://arxiv.org/pdf/2303.13033v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13031v1","updated":"2023-03-23T04:40:33Z","published":"2023-03-23T04:40:33Z","title":"Learning a Practical SDR-to-HDRTV Up-conversion using New Dataset and\n  Degradation Models","summary":"  In media industry, the demand of SDR-to-HDRTV up-conversion arises when users\npossess HDR-WCG (high dynamic range-wide color gamut) TVs while most\noff-the-shelf footage is still in SDR (standard dynamic range). The research\ncommunity has started tackling this low-level vision task by learning-based\napproaches. When applied to real SDR, yet, current methods tend to produce dim\nand desaturated result, making nearly no improvement on viewing experience.\nDifferent from other network-oriented methods, we attribute such deficiency to\ntraining set (HDR-SDR pair). Consequently, we propose new HDRTV dataset (dubbed\nHDRTV4K) and new HDR-to-SDR degradation models. Then, it's used to train a\nluminance-segmented network (LSN) consisting of a global mapping trunk, and two\nTransformer branches on bright and dark luminance range. We also update\nassessment criteria by tailored metrics and subjective experiment. Finally,\nablation studies are conducted to prove the effectiveness. Our work is\navailable at: https://github.com/AndreGuo/HDRTVDM.\n","authors":["Cheng Guo","Leidong Fan","Ziyu Xue","and Xiuhua Jiang"],"pdf_url":"https://arxiv.org/pdf/2303.13031v1.pdf","comment":"Accepted by CVPR2023"},{"id":"http://arxiv.org/abs/2209.01540v3","updated":"2023-03-23T04:34:40Z","published":"2022-09-04T06:30:32Z","title":"An Empirical Study of End-to-End Video-Language Transformers with Masked\n  Visual Modeling","summary":"  Masked visual modeling (MVM) has been recently proven effective for visual\npre-training. While similar reconstructive objectives on video inputs (e.g.,\nmasked frame modeling) have been explored in video-language (VidL)\npre-training, previous studies fail to find a truly effective MVM strategy that\ncan largely benefit the downstream performance. In this work, we systematically\nexamine the potential of MVM in the context of VidL learning. Specifically, we\nbase our study on a fully end-to-end VIdeO-LanguagE Transformer (VIOLET), where\nthe supervision from MVM training can be backpropagated to the video pixel\nspace. In total, eight different reconstructive targets of MVM are explored,\nfrom low-level pixel values and oriented gradients to high-level depth maps,\noptical flow, discrete visual tokens, and latent visual features. We conduct\ncomprehensive experiments and provide insights into the factors leading to\neffective MVM training, resulting in an enhanced model VIOLETv2. Empirically,\nwe show VIOLETv2 pre-trained with MVM objective achieves notable improvements\non 13 VidL benchmarks, ranging from video question answering, video captioning,\nto text-to-video retrieval.\n","authors":["Tsu-Jui Fu","Linjie Li","Zhe Gan","Kevin Lin","William Yang Wang","Lijuan Wang","Zicheng Liu"],"pdf_url":"https://arxiv.org/pdf/2209.01540v3.pdf","comment":"CVPR'23; the first two authors contributed equally; code is available\n  at https://github.com/tsujuifu/pytorch_empirical-mvm"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2303.13484v1","updated":"2023-03-23T17:50:44Z","published":"2023-03-23T17:50:44Z","title":"Why and When: Understanding System Initiative during Conversational\n  Collaborative Search","summary":"  In the last decade, conversational search has attracted considerable\nattention. However, most research has focused on systems that can support a\n\\emph{single} searcher. In this paper, we explore how systems can support\n\\emph{multiple} searchers collaborating over an instant messaging platform\n(i.e., Slack). We present a ``Wizard of Oz'' study in which 27 participant\npairs collaborated on three information-seeking tasks over Slack. Participants\nwere unable to search on their own and had to gather information by interacting\nwith a \\emph{searchbot} directly from the Slack channel. The role of the\nsearchbot was played by a reference librarian. Conversational search systems\nmust be capable of engaging in \\emph{mixed-initiative} interaction by taking\nand relinquishing control of the conversation to fulfill different objectives.\nDiscourse analysis research suggests that conversational agents can take\n\\emph{two} levels of initiative: dialog- and task-level initiative. Agents take\ndialog-level initiative to establish mutual belief between agents and\ntask-level initiative to influence the goals of the other agents. During the\nstudy, participants were exposed to three experimental conditions in which the\nsearchbot could take different levels of initiative: (1) no initiative, (2)\nonly dialog-level initiative, and (3) both dialog- and task-level initiative.\nIn this paper, we focus on understanding the Wizard's actions. Specifically, we\nfocus on understanding the Wizard's motivations for taking initiative and their\nrationale for the timing of each intervention. To gain insights about the\nWizard's actions, we conducted a stimulated recall interview with the Wizard.\nWe present findings from a qualitative analysis of this interview data and\ndiscuss implications for designing conversational search systems to support\ncollaborative search.\n","authors":["Sandeep Avula","Bogeum Choi","Jaime Arguello"],"pdf_url":"https://arxiv.org/pdf/2303.13484v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13419v1","updated":"2023-03-23T16:40:00Z","published":"2023-03-23T16:40:00Z","title":"Modular Retrieval for Generalization and Interpretation","summary":"  New retrieval tasks have always been emerging, thus urging the development of\nnew retrieval models. However, instantiating a retrieval model for each new\nretrieval task is resource-intensive and time-consuming, especially for a\nretrieval model that employs a large-scale pre-trained language model. To\naddress this issue, we shift to a novel retrieval paradigm called modular\nretrieval, which aims to solve new retrieval tasks by instead composing\nmultiple existing retrieval modules. Built upon the paradigm, we propose a\nretrieval model with modular prompt tuning named REMOP. It constructs retrieval\nmodules subject to task attributes with deep prompt tuning, and yields\nretrieval models subject to tasks with module composition. We validate that,\nREMOP inherently with modularity not only has appealing generalizability and\ninterpretability in preliminary explorations, but also achieves comparable\nperformance to state-of-the-art retrieval models on a zero-shot retrieval\nbenchmark.\\footnote{Our code is available at\n\\url{https://github.com/FreedomIntelligence/REMOP}}\n","authors":["Juhao Liang","Chen Zhang","Zhengyang Tang","Jie Fu","Dawei Song","Benyou Wang"],"pdf_url":"https://arxiv.org/pdf/2303.13419v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2303.13416v1","updated":"2023-03-23T16:38:18Z","published":"2023-03-23T16:38:18Z","title":"A Unified Framework for Learned Sparse Retrieval","summary":"  Learned sparse retrieval (LSR) is a family of first-stage retrieval methods\nthat are trained to generate sparse lexical representations of queries and\ndocuments for use with an inverted index. Many LSR methods have been recently\nintroduced, with Splade models achieving state-of-the-art performance on\nMSMarco. Despite similarities in their model architectures, many LSR methods\nshow substantial differences in effectiveness and efficiency. Differences in\nthe experimental setups and configurations used make it difficult to compare\nthe methods and derive insights. In this work, we analyze existing LSR methods\nand identify key components to establish an LSR framework that unifies all LSR\nmethods under the same perspective. We then reproduce all prominent methods\nusing a common codebase and re-train them in the same environment, which allows\nus to quantify how components of the framework affect effectiveness and\nefficiency. We find that (1) including document term weighting is most\nimportant for a method's effectiveness, (2) including query weighting has a\nsmall positive impact, and (3) document expansion and query expansion have a\ncancellation effect. As a result, we show how removing query expansion from a\nstate-of-the-art model can reduce latency significantly while maintaining\neffectiveness on MSMarco and TripClick benchmarks. Our code is publicly\navailable at https://github.com/thongnt99/learned-sparse-retrieval\n","authors":["Thong Nguyen","Sean MacAvaney","Andrew Yates"],"pdf_url":"https://arxiv.org/pdf/2303.13416v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13284v1","updated":"2023-03-23T14:06:26Z","published":"2023-03-23T14:06:26Z","title":"GETT-QA: Graph Embedding based T2T Transformer for Knowledge Graph\n  Question Answering","summary":"  In this work, we present an end-to-end Knowledge Graph Question Answering\n(KGQA) system named GETT-QA. GETT-QA uses T5, a popular text-to-text\npre-trained language model. The model takes a question in natural language as\ninput and produces a simpler form of the intended SPARQL query. In the simpler\nform, the model does not directly produce entity and relation IDs. Instead, it\nproduces corresponding entity and relation labels. The labels are grounded to\nKG entity and relation IDs in a subsequent step. To further improve the\nresults, we instruct the model to produce a truncated version of the KG\nembedding for each entity. The truncated KG embedding enables a finer search\nfor disambiguation purposes. We find that T5 is able to learn the truncated KG\nembeddings without any change of loss function, improving KGQA performance. As\na result, we report strong results for LC-QuAD 2.0 and SimpleQuestions-Wikidata\ndatasets on end-to-end KGQA over Wikidata.\n","authors":["Debayan Banerjee","Pranav Ajit Nair","Ricardo Usbeck","Chris Biemann"],"pdf_url":"https://arxiv.org/pdf/2303.13284v1.pdf","comment":"16 pages single column format accepted at ESWC 2023 research track"},{"id":"http://arxiv.org/abs/2303.13220v1","updated":"2023-03-23T12:34:30Z","published":"2023-03-23T12:34:30Z","title":"Parameter-Efficient Sparse Retrievers and Rerankers using Adapters","summary":"  Parameter-Efficient transfer learning with Adapters have been studied in\nNatural Language Processing (NLP) as an alternative to full fine-tuning.\nAdapters are memory-efficient and scale well with downstream tasks by training\nsmall bottle-neck layers added between transformer layers while keeping the\nlarge pretrained language model (PLMs) frozen. In spite of showing promising\nresults in NLP, these methods are under-explored in Information Retrieval.\nWhile previous studies have only experimented with dense retriever or in a\ncross lingual retrieval scenario, in this paper we aim to complete the picture\non the use of adapters in IR. First, we study adapters for SPLADE, a sparse\nretriever, for which adapters not only retain the efficiency and effectiveness\notherwise achieved by finetuning, but are memory-efficient and orders of\nmagnitude lighter to train. We observe that Adapters-SPLADE not only optimizes\njust 2\\% of training parameters, but outperforms fully fine-tuned counterpart\nand existing parameter-efficient dense IR models on IR benchmark datasets.\nSecondly, we address domain adaptation of neural retrieval thanks to adapters\non cross-domain BEIR datasets and TripClick. Finally, we also consider\nknowledge sharing between rerankers and first stage rankers. Overall, our study\ncomplete the examination of adapters for neural IR\n","authors":["Vaishali Pal","Carlos Lassance","Hervé Déjean","Stéphane Clinchant"],"pdf_url":"https://arxiv.org/pdf/2303.13220v1.pdf","comment":"accepted at ECIR'23"},{"id":"http://arxiv.org/abs/2303.13091v1","updated":"2023-03-23T08:11:42Z","published":"2023-03-23T08:11:42Z","title":"Limits of Predictability in Top-N Recommendation","summary":"  Top-N recommendation aims to recommend each consumer a small set of N items\nfrom a large collection of items, and its accuracy is one of the most common\nindexes to evaluate the performance of a recommendation system. While a large\nnumber of algorithms are proposed to push the Top-N accuracy by learning the\nuser preference from their history purchase data, a predictability question is\nnaturally raised - whether there is an upper limit of such Top-N accuracy. This\nwork investigates such predictability by studying the degree of regularity from\na specific set of user behavior data. Quantifying the predictability of Top-N\nrecommendations requires simultaneously quantifying the limits on the accuracy\nof the N behaviors with the highest probability. This greatly increases the\ndifficulty of the problem. To achieve this, we firstly excavate the\nassociations among N behaviors with the highest probability and describe the\nuser behavior distribution based on the information theory. Then, we adopt the\nFano inequality to scale and obtain the Top-N predictability. Extensive\nexperiments are conducted on the real-world data where significant improvements\nare observed compared to the state-of-the-art methods. We have not only\ncompleted the predictability calculation for N targets but also obtained\npredictability that is much closer to the true value than existing methods. We\nexpect our results to assist these research areas where the quantitative\nrequirement of Top-N predictability is required.\n","authors":["En Xu","Zhiwen Yu","Ying Zhang","Bin Guo","Lina Yao"],"pdf_url":"https://arxiv.org/pdf/2303.13091v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12973v1","updated":"2023-03-23T00:42:48Z","published":"2023-03-23T00:42:48Z","title":"Uncertainty Calibration for Counterfactual Propensity Estimation in\n  Recommendation","summary":"  In recommendation systems, a large portion of the ratings are missing due to\nthe selection biases, which is known as Missing Not At Random. The\ncounterfactual inverse propensity scoring (IPS) was used to weight the\nimputation error of every observed rating. Although effective in multiple\nscenarios, we argue that the performance of IPS estimation is limited due to\nthe uncertainty miscalibration of propensity estimation. In this paper, we\npropose the uncertainty calibration for the propensity estimation in\nrecommendation systems with multiple representative uncertainty calibration\ntechniques. Theoretical analysis on the bias and generalization bound shows the\nsuperiority of the calibrated IPS estimator over the uncalibrated one.\nExperimental results on the coat and yahoo datasets shows that the uncertainty\ncalibration is improved and hence brings the better recommendation results.\n","authors":["Wenbo Hu","Xin Sun","Qiang liu","Shu Wu"],"pdf_url":"https://arxiv.org/pdf/2303.12973v1.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2303.13519v1","updated":"2023-03-23T17:59:54Z","published":"2023-03-23T17:59:54Z","title":"Learning and Verification of Task Structure in Instructional Videos","summary":"  Given the enormous number of instructional videos available online, learning\na diverse array of multi-step task models from videos is an appealing goal. We\nintroduce a new pre-trained video model, VideoTaskformer, focused on\nrepresenting the semantics and structure of instructional videos. We pre-train\nVideoTaskformer using a simple and effective objective: predicting weakly\nsupervised textual labels for steps that are randomly masked out from an\ninstructional video (masked step modeling). Compared to prior work which learns\nstep representations locally, our approach involves learning them globally,\nleveraging video of the entire surrounding task as context. From these learned\nrepresentations, we can verify if an unseen video correctly executes a given\ntask, as well as forecast which steps are likely to be taken after a given\nstep. We introduce two new benchmarks for detecting mistakes in instructional\nvideos, to verify if there is an anomalous step and if steps are executed in\nthe right order. We also introduce a long-term forecasting benchmark, where the\ngoal is to predict long-range future steps from a given step. Our method\noutperforms previous baselines on these tasks, and we believe the tasks will be\na valuable way for the community to measure the quality of step\nrepresentations. Additionally, we evaluate VideoTaskformer on 3 existing\nbenchmarks -- procedural activity recognition, step classification, and step\nforecasting -- and demonstrate on each that our method outperforms existing\nbaselines and achieves new state-of-the-art performance.\n","authors":["Medhini Narasimhan","Licheng Yu","Sean Bell","Ning Zhang","Trevor Darrell"],"pdf_url":"https://arxiv.org/pdf/2303.13519v1.pdf","comment":"Wesbite at https://medhini.github.io/task_structure"},{"id":"http://arxiv.org/abs/2303.13518v1","updated":"2023-03-23T17:59:53Z","published":"2023-03-23T17:59:53Z","title":"Three ways to improve feature alignment for open vocabulary detection","summary":"  The core problem in zero-shot open vocabulary detection is how to align\nvisual and text features, so that the detector performs well on unseen classes.\nPrevious approaches train the feature pyramid and detection head from scratch,\nwhich breaks the vision-text feature alignment established during pretraining,\nand struggles to prevent the language model from forgetting unseen classes.\n  We propose three methods to alleviate these issues. Firstly, a simple scheme\nis used to augment the text embeddings which prevents overfitting to a small\nnumber of classes seen during training, while simultaneously saving memory and\ncomputation. Secondly, the feature pyramid network and the detection head are\nmodified to include trainable gated shortcuts, which encourages vision-text\nfeature alignment and guarantees it at the start of detection training.\nFinally, a self-training approach is used to leverage a larger corpus of\nimage-text pairs thus improving detection performance on classes with no human\nannotated bounding boxes.\n  Our three methods are evaluated on the zero-shot version of the LVIS\nbenchmark, each of them showing clear and significant benefits. Our final\nnetwork achieves the new stateof-the-art on the mAP-all metric and demonstrates\ncompetitive performance for mAP-rare, as well as superior transfer to COCO and\nObjects365.\n","authors":["Relja Arandjelović","Alex Andonian","Arthur Mensch","Olivier J. Hénaff","Jean-Baptiste Alayrac","Andrew Zisserman"],"pdf_url":"https://arxiv.org/pdf/2303.13518v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13516v1","updated":"2023-03-23T17:59:42Z","published":"2023-03-23T17:59:42Z","title":"Ablating Concepts in Text-to-Image Diffusion Models","summary":"  Large-scale text-to-image diffusion models can generate high-fidelity images\nwith powerful compositional ability. However, these models are typically\ntrained on an enormous amount of Internet data, often containing copyrighted\nmaterial, licensed images, and personal photos. Furthermore, they have been\nfound to replicate the style of various living artists or memorize exact\ntraining samples. How can we remove such copyrighted concepts or images without\nretraining the model from scratch? To achieve this goal, we propose an\nefficient method of ablating concepts in the pretrained model, i.e., preventing\nthe generation of a target concept. Our algorithm learns to match the image\ndistribution for a target style, instance, or text prompt we wish to ablate to\nthe distribution corresponding to an anchor concept. This prevents the model\nfrom generating target concepts given its text condition. Extensive experiments\nshow that our method can successfully prevent the generation of the ablated\nconcept while preserving closely related concepts in the model.\n","authors":["Nupur Kumari","Bingliang Zhang","Sheng-Yu Wang","Eli Shechtman","Richard Zhang","Jun-Yan Zhu"],"pdf_url":"https://arxiv.org/pdf/2303.13516v1.pdf","comment":"project website: https://www.cs.cmu.edu/~concept-ablation/"},{"id":"http://arxiv.org/abs/2303.13515v1","updated":"2023-03-23T17:59:40Z","published":"2023-03-23T17:59:40Z","title":"Persistent Nature: A Generative Model of Unbounded 3D Worlds","summary":"  Despite increasingly realistic image quality, recent 3D image generative\nmodels often operate on 3D volumes of fixed extent with limited camera motions.\nWe investigate the task of unconditionally synthesizing unbounded nature\nscenes, enabling arbitrarily large camera motion while maintaining a persistent\n3D world model. Our scene representation consists of an extendable, planar\nscene layout grid, which can be rendered from arbitrary camera poses via a 3D\ndecoder and volume rendering, and a panoramic skydome. Based on this\nrepresentation, we learn a generative world model solely from single-view\ninternet photos. Our method enables simulating long flights through 3D\nlandscapes, while maintaining global scene consistency--for instance, returning\nto the starting point yields the same view of the scene. Our approach enables\nscene extrapolation beyond the fixed bounds of current 3D generative models,\nwhile also supporting a persistent, camera-independent world representation\nthat stands in contrast to auto-regressive 3D prediction models. Our project\npage: https://chail.github.io/persistent-nature/.\n","authors":["Lucy Chai","Richard Tucker","Zhengqi Li","Phillip Isola","Noah Snavely"],"pdf_url":"https://arxiv.org/pdf/2303.13515v1.pdf","comment":"CVPR camera ready version, project page:\n  https://chail.github.io/persistent-nature/"},{"id":"http://arxiv.org/abs/2303.13511v1","updated":"2023-03-23T17:59:10Z","published":"2023-03-23T17:59:10Z","title":"Neural Preset for Color Style Transfer","summary":"  In this paper, we present a Neural Preset technique to address the\nlimitations of existing color style transfer methods, including visual\nartifacts, vast memory requirement, and slow style switching speed. Our method\nis based on two core designs. First, we propose Deterministic Neural Color\nMapping (DNCM) to consistently operate on each pixel via an image-adaptive\ncolor mapping matrix, avoiding artifacts and supporting high-resolution inputs\nwith a small memory footprint. Second, we develop a two-stage pipeline by\ndividing the task into color normalization and stylization, which allows\nefficient style switching by extracting color styles as presets and reusing\nthem on normalized input images. Due to the unavailability of pairwise\ndatasets, we describe how to train Neural Preset via a self-supervised\nstrategy. Various advantages of Neural Preset over existing methods are\ndemonstrated through comprehensive evaluations. Besides, we show that our\ntrained model can naturally support multiple applications without fine-tuning,\nincluding low-light image enhancement, underwater image correction, image\ndehazing, and image harmonization.\n","authors":["Zhanghan Ke","Yuhao Liu","Lei Zhu","Nanxuan Zhao","Rynson W. H. Lau"],"pdf_url":"https://arxiv.org/pdf/2303.13511v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13506v1","updated":"2023-03-23T17:58:43Z","published":"2023-03-23T17:58:43Z","title":"The Quantization Model of Neural Scaling","summary":"  We propose the $\\textit{Quantization Model}$ of neural scaling laws,\nexplaining both the observed power law dropoff of loss with model and data\nsize, and also the sudden emergence of new capabilities with scale. We derive\nthis model from what we call the $\\textit{Quantization Hypothesis}$, where\nlearned network capabilities are quantized into discrete chunks\n($\\textit{quanta}$). We show that when quanta are learned in order of\ndecreasing use frequency, then a power law in use frequencies explains observed\npower law scaling of loss. We validate this prediction on toy datasets, then\nstudy how scaling curves decompose for large language models. Using language\nmodel internals, we auto-discover diverse model capabilities (quanta) and find\ntentative evidence that the distribution over corresponding subproblems in the\nprediction of natural text is compatible with the power law predicted from the\nneural scaling exponent as predicted from our theory.\n","authors":["Eric J. Michaud","Ziming Liu","Uzay Girit","Max Tegmark"],"pdf_url":"https://arxiv.org/pdf/2303.13506v1.pdf","comment":"24 pages, 15 figures"},{"id":"http://arxiv.org/abs/2303.13501v1","updated":"2023-03-23T17:57:28Z","published":"2023-03-23T17:57:28Z","title":"Chordal Averaging on Flag Manifolds and Its Applications","summary":"  This paper presents a new, provably-convergent algorithm for computing the\nflag-mean and flag-median of a set of points on a flag manifold under the\nchordal metric. The flag manifold is a mathematical space consisting of flags,\nwhich are sequences of nested subspaces of a vector space that increase in\ndimension. The flag manifold is a superset of a wide range of known matrix\ngroups, including Stiefel and Grassmanians, making it a general object that is\nuseful in a wide variety computer vision problems.\n  To tackle the challenge of computing first order flag statistics, we first\ntransform the problem into one that involves auxiliary variables constrained to\nthe Stiefel manifold. The Stiefel manifold is a space of orthogonal frames, and\nleveraging the numerical stability and efficiency of Stiefel-manifold\noptimization enables us to compute the flag-mean effectively. Through a series\nof experiments, we show the competence of our method in Grassmann and rotation\naveraging, as well as principal component analysis.\n","authors":["Nathan Mankovich","Tolga Birdal"],"pdf_url":"https://arxiv.org/pdf/2303.13501v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13500v1","updated":"2023-03-23T17:57:09Z","published":"2023-03-23T17:57:09Z","title":"A Closer Look at Model Adaptation using Feature Distortion and\n  Simplicity Bias","summary":"  Advances in the expressivity of pretrained models have increased interest in\nthe design of adaptation protocols which enable safe and effective transfer\nlearning. Going beyond conventional linear probing (LP) and fine tuning (FT)\nstrategies, protocols that can effectively control feature distortion, i.e.,\nthe failure to update features orthogonal to the in-distribution, have been\nfound to achieve improved out-of-distribution generalization (OOD). In order to\nlimit this distortion, the LP+FT protocol, which first learns a linear probe\nand then uses this initialization for subsequent FT, was proposed. However, in\nthis paper, we find when adaptation protocols (LP, FT, LP+FT) are also\nevaluated on a variety of safety objectives (e.g., calibration, robustness,\netc.), a complementary perspective to feature distortion is helpful to explain\nprotocol behavior. To this end, we study the susceptibility of protocols to\nsimplicity bias (SB), i.e. the well-known propensity of deep neural networks to\nrely upon simple features, as SB has recently been shown to underlie several\nproblems in robust generalization. Using a synthetic dataset, we demonstrate\nthe susceptibility of existing protocols to SB. Given the strong effectiveness\nof LP+FT, we then propose modified linear probes that help mitigate SB, and\nlead to better initializations for subsequent FT. We verify the effectiveness\nof the proposed LP+FT variants for decreasing SB in a controlled setting, and\ntheir ability to improve OOD generalization and safety on three adaptation\ndatasets.\n","authors":["Puja Trivedi","Danai Koutra","Jayaraman J. Thiagarajan"],"pdf_url":"https://arxiv.org/pdf/2303.13500v1.pdf","comment":"Accepted to ICLR 2023 as notable-25% (spotlight)"},{"id":"http://arxiv.org/abs/2208.05845v2","updated":"2023-03-23T17:56:52Z","published":"2022-08-11T14:28:21Z","title":"A Comprehensive Analysis of AI Biases in DeepFake Detection With\n  Massively Annotated Databases","summary":"  In recent years, image and video manipulations with Deepfake have become a\nsevere concern for security and society. Many detection models and datasets\nhave been proposed to detect Deepfake data reliably. However, there is an\nincreased concern that these models and training databases might be biased and,\nthus, cause Deepfake detectors to fail. In this work, we investigate the bias\nissue caused by public Deepfake datasets by (a) providing large-scale\ndemographic and non-demographic attribute annotations of 47 different\nattributes for five popular Deepfake datasets and (b) comprehensively analysing\nAI-bias of three state-of-the-art Deepfake detection backbone models on these\ndatasets. The investigation analyses the influence of a large variety of\ndistinctive attributes (from over 65M labels) on the detection performance,\nincluding demographic (age, gender, ethnicity) and non-demographic (hair, skin,\naccessories, etc.) information. The results indicate that investigated\ndatabases lack diversity and, more importantly, show that the utilised Deepfake\ndetection backbone models are strongly biased towards many investigated\nattributes. The Deepfake detection backbone methods, which are trained with\nbiased datasets, might output incorrect detection results, thereby leading to\ngeneralisability, fairness, and security issues. We hope that the findings of\nthis study and the annotation databases will help to evaluate and mitigate bias\nin future Deepfake detection techniques. The annotation datasets are publicly\navailable.\n","authors":["Ying Xu","Philipp Terhörst","Kiran Raja","Marius Pedersen"],"pdf_url":"https://arxiv.org/pdf/2208.05845v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13496v1","updated":"2023-03-23T17:56:12Z","published":"2023-03-23T17:56:12Z","title":"The effectiveness of MAE pre-pretraining for billion-scale pretraining","summary":"  This paper revisits the standard pretrain-then-finetune paradigm used in\ncomputer vision for visual recognition tasks. Typically, state-of-the-art\nfoundation models are pretrained using large scale (weakly) supervised datasets\nwith billions of images. We introduce an additional pre-pretraining stage that\nis simple and uses the self-supervised MAE technique to initialize the model.\nWhile MAE has only been shown to scale with the size of models, we find that it\nscales with the size of the training dataset as well. Thus, our MAE-based\npre-pretraining scales with both model and data size making it applicable for\ntraining foundation models. Pre-pretraining consistently improves both the\nmodel convergence and the downstream transfer performance across a range of\nmodel scales (millions to billions of parameters), and dataset sizes (millions\nto billions of images). We measure the effectiveness of pre-pretraining on 10\ndifferent visual recognition tasks spanning image classification, video\nrecognition, object detection, low-shot classification and zero-shot\nrecognition. Our largest model achieves new state-of-the-art results on\niNaturalist-18 (91.3%), 1-shot ImageNet-1k (62.1%), and zero-shot transfer on\nFood-101 (96.0%). Our study reveals that model initialization plays a\nsignificant role, even for web-scale pretraining with billions of images.\n","authors":["Mannat Singh","Quentin Duval","Kalyan Vasudev Alwala","Haoqi Fan","Vaibhav Aggarwal","Aaron Adcock","Armand Joulin","Piotr Dollár","Christoph Feichtenhofer","Ross Girshick","Rohit Girdhar","Ishan Misra"],"pdf_url":"https://arxiv.org/pdf/2303.13496v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13489v1","updated":"2023-03-23T17:53:44Z","published":"2023-03-23T17:53:44Z","title":"Boosting Reinforcement Learning and Planning with Demonstrations: A\n  Survey","summary":"  Although reinforcement learning has seen tremendous success recently, this\nkind of trial-and-error learning can be impractical or inefficient in complex\nenvironments. The use of demonstrations, on the other hand, enables agents to\nbenefit from expert knowledge rather than having to discover the best action to\ntake through exploration. In this survey, we discuss the advantages of using\ndemonstrations in sequential decision making, various ways to apply\ndemonstrations in learning-based decision making paradigms (for example,\nreinforcement learning and planning in the learned models), and how to collect\nthe demonstrations in various scenarios. Additionally, we exemplify a practical\npipeline for generating and utilizing demonstrations in the recently proposed\nManiSkill robot learning benchmark.\n","authors":["Tongzhou Mu","Hao Su"],"pdf_url":"https://arxiv.org/pdf/2303.13489v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13482v1","updated":"2023-03-23T17:50:09Z","published":"2023-03-23T17:50:09Z","title":"TactoFind: A Tactile Only System for Object Retrieval","summary":"  We study the problem of object retrieval in scenarios where visual sensing is\nabsent, object shapes are unknown beforehand and objects can move freely, like\ngrabbing objects out of a drawer. Successful solutions require localizing free\nobjects, identifying specific object instances, and then grasping the\nidentified objects, only using touch feedback. Unlike vision, where cameras can\nobserve the entire scene, touch sensors are local and only observe parts of the\nscene that are in contact with the manipulator. Moreover, information gathering\nvia touch sensors necessitates applying forces on the touched surface which may\ndisturb the scene itself. Reasoning with touch, therefore, requires careful\nexploration and integration of information over time -- a challenge we tackle.\nWe present a system capable of using sparse tactile feedback from fingertip\ntouch sensors on a dexterous hand to localize, identify and grasp novel objects\nwithout any visual feedback. Videos are available at\nhttps://taochenshh.github.io/projects/tactofind.\n","authors":["Sameer Pai","Tao Chen","Megha Tippur","Edward Adelson","Abhishek Gupta","Pulkit Agrawal"],"pdf_url":"https://arxiv.org/pdf/2303.13482v1.pdf","comment":"Accepted in ICRA 2023"},{"id":"http://arxiv.org/abs/2102.04518v2","updated":"2023-03-23T17:38:09Z","published":"2021-02-08T20:36:41Z","title":"A* Search Without Expansions: Learning Heuristic Functions with Deep\n  Q-Networks","summary":"  Efficiently solving problems with large action spaces using A* search has\nbeen of importance to the artificial intelligence community for decades. This\nis because the computation and memory requirements of A* search grow linearly\nwith the size of the action space. This burden becomes even more apparent when\nA* search uses a heuristic function learned by computationally expensive\nfunction approximators, such as deep neural networks. To address this problem,\nwe introduce Q* search, a search algorithm that uses deep Q-networks to guide\nsearch in order to take advantage of the fact that the sum of the transition\ncosts and heuristic values of the children of a node can be computed with a\nsingle forward pass through a deep Q-network without explicitly generating\nthose children. This significantly reduces computation time and requires only\none node to be generated per iteration. We use Q* search to solve the Rubik's\ncube when formulated with a large action space that includes 1872 meta-actions\nand find that this 157-fold increase in the size of the action space incurs\nless than a 4-fold increase in computation time and less than a 3-fold increase\nin number of nodes generated when performing Q* search. Furthermore, Q* search\nis up to 129 times faster and generates up to 1288 times fewer nodes than A*\nsearch. Finally, although obtaining admissible heuristic functions from deep\nneural networks is an ongoing area of research, we prove that Q* search is\nguaranteed to find a shortest path given a heuristic function that neither\noverestimates the cost of a shortest path nor underestimates the transition\ncost.\n","authors":["Forest Agostinelli","Alexander Shmakov","Stephen McAleer","Roy Fox","Pierre Baldi"],"pdf_url":"https://arxiv.org/pdf/2102.04518v2.pdf","comment":"Added theoretical results to show that Q* search is an admissible\n  search algorithm. Added comparisons to deferred heuristic evaluation. Added\n  experiments with Lights Out and the 35-Pancake puzzle"},{"id":"http://arxiv.org/abs/2303.13462v1","updated":"2023-03-23T17:32:20Z","published":"2023-03-23T17:32:20Z","title":"Generalization with quantum geometry for learning unitaries","summary":"  Generalization is the ability of quantum machine learning models to make\naccurate predictions on new data by learning from training data. Here, we\nintroduce the data quantum Fisher information metric (DQFIM) to determine when\na model can generalize. For variational learning of unitaries, the DQFIM\nquantifies the amount of circuit parameters and training data needed to\nsuccessfully train and generalize. We apply the DQFIM to explain when a\nconstant number of training states and polynomial number of parameters are\nsufficient for generalization. Further, we can improve generalization by\nremoving symmetries from training data. Finally, we show that\nout-of-distribution generalization, where training and testing data are drawn\nfrom different data distributions, can be better than using the same\ndistribution. Our work opens up new approaches to improve generalization in\nquantum machine learning.\n","authors":["Tobias Haug","M. S. Kim"],"pdf_url":"https://arxiv.org/pdf/2303.13462v1.pdf","comment":"16 pages, 13 figures"},{"id":"http://arxiv.org/abs/2212.08049v5","updated":"2023-03-23T17:26:44Z","published":"2022-12-15T18:55:23Z","title":"Sliced Optimal Partial Transport","summary":"  Optimal transport (OT) has become exceedingly popular in machine learning,\ndata science, and computer vision. The core assumption in the OT problem is the\nequal total amount of mass in source and target measures, which limits its\napplication. Optimal Partial Transport (OPT) is a recently proposed solution to\nthis limitation. Similar to the OT problem, the computation of OPT relies on\nsolving a linear programming problem (often in high dimensions), which can\nbecome computationally prohibitive. In this paper, we propose an efficient\nalgorithm for calculating the OPT problem between two non-negative measures in\none dimension. Next, following the idea of sliced OT distances, we utilize\nslicing to define the sliced OPT distance. Finally, we demonstrate the\ncomputational and accuracy benefits of the sliced OPT-based method in various\nnumerical experiments. In particular, we show an application of our proposed\nSliced-OPT in noisy point cloud registration.\n","authors":["Yikun Bai","Bernard Schmitzer","Mathew Thorpe","Soheil Kolouri"],"pdf_url":"https://arxiv.org/pdf/2212.08049v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13458v1","updated":"2023-03-23T17:26:12Z","published":"2023-03-23T17:26:12Z","title":"Optimization Dynamics of Equivariant and Augmented Neural Networks","summary":"  We investigate the optimization of multilayer perceptrons on symmetric data.\nWe compare the strategy of constraining the architecture to be equivariant to\nthat of using augmentation. We show that, under natural assumptions on the loss\nand non-linearities, the sets of equivariant stationary points are identical\nfor the two strategies, and that the set of equivariant layers is invariant\nunder the gradient flow for augmented models. Finally, we show that stationary\npoints may be unstable for augmented training although they are stable for the\nequivariant models\n","authors":["Axel Flinth","Fredrik Ohlsson"],"pdf_url":"https://arxiv.org/pdf/2303.13458v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.11912v2","updated":"2023-03-23T17:24:04Z","published":"2022-05-24T09:17:27Z","title":"Physics-Embedded Neural Networks: Graph Neural PDE Solvers with Mixed\n  Boundary Conditions","summary":"  Graph neural network (GNN) is a promising approach to learning and predicting\nphysical phenomena described in boundary value problems, such as partial\ndifferential equations (PDEs) with boundary conditions. However, existing\nmodels inadequately treat boundary conditions essential for the reliable\nprediction of such problems. In addition, because of the locally connected\nnature of GNNs, it is difficult to accurately predict the state after a long\ntime, where interaction between vertices tends to be global. We present our\napproach termed physics-embedded neural networks that considers boundary\nconditions and predicts the state after a long time using an implicit method.\nIt is built based on an E(n)-equivariant GNN, resulting in high generalization\nperformance on various shapes. We demonstrate that our model learns flow\nphenomena in complex shapes and outperforms a well-optimized classical solver\nand a state-of-the-art machine learning model in speed-accuracy trade-off.\nTherefore, our model can be a useful standard for realizing reliable, fast, and\naccurate GNN-based PDE solvers. The code is available at\nhttps://github.com/yellowshippo/penn-neurips2022.\n","authors":["Masanobu Horie","Naoto Mitsume"],"pdf_url":"https://arxiv.org/pdf/2205.11912v2.pdf","comment":"NeurIPS 2022"},{"id":"http://arxiv.org/abs/2303.13452v1","updated":"2023-03-23T17:19:26Z","published":"2023-03-23T17:19:26Z","title":"Human Behavior in the Time of COVID-19: Learning from Big Data","summary":"  Since the World Health Organization (WHO) characterized COVID-19 as a\npandemic in March 2020, there have been over 600 million confirmed cases of\nCOVID-19 and more than six million deaths as of October 2022. The relationship\nbetween the COVID-19 pandemic and human behavior is complicated. On one hand,\nhuman behavior is found to shape the spread of the disease. On the other hand,\nthe pandemic has impacted and even changed human behavior in almost every\naspect. To provide a holistic understanding of the complex interplay between\nhuman behavior and the COVID-19 pandemic, researchers have been employing big\ndata techniques such as natural language processing, computer vision, audio\nsignal processing, frequent pattern mining, and machine learning. In this\nstudy, we present an overview of the existing studies on using big data\ntechniques to study human behavior in the time of the COVID-19 pandemic. In\nparticular, we categorize these studies into three groups - using big data to\nmeasure, model, and leverage human behavior, respectively. The related tasks,\ndata, and methods are summarized accordingly. To provide more insights into how\nto fight the COVID-19 pandemic and future global catastrophes, we further\ndiscuss challenges and potential opportunities.\n","authors":["Hanjia Lyu","Arsal Imtiaz","Yufei Zhao","Jiebo Luo"],"pdf_url":"https://arxiv.org/pdf/2303.13452v1.pdf","comment":"Accepted for publication in the Horizons in Big Data 2022 article\n  collection of Frontiers in Big Data"},{"id":"http://arxiv.org/abs/2303.13450v1","updated":"2023-03-23T17:17:29Z","published":"2023-03-23T17:17:29Z","title":"Set-the-Scene: Global-Local Training for Generating Controllable NeRF\n  Scenes","summary":"  Recent breakthroughs in text-guided image generation have led to remarkable\nprogress in the field of 3D synthesis from text. By optimizing neural radiance\nfields (NeRF) directly from text, recent methods are able to produce remarkable\nresults. Yet, these methods are limited in their control of each object's\nplacement or appearance, as they represent the scene as a whole. This can be a\nmajor issue in scenarios that require refining or manipulating objects in the\nscene. To remedy this deficit, we propose a novel GlobalLocal training\nframework for synthesizing a 3D scene using object proxies. A proxy represents\nthe object's placement in the generated scene and optionally defines its coarse\ngeometry. The key to our approach is to represent each object as an independent\nNeRF. We alternate between optimizing each NeRF on its own and as part of the\nfull scene. Thus, a complete representation of each object can be learned,\nwhile also creating a harmonious scene with style and lighting match. We show\nthat using proxies allows a wide variety of editing options, such as adjusting\nthe placement of each independent object, removing objects from a scene, or\nrefining an object. Our results show that Set-the-Scene offers a powerful\nsolution for scene synthesis and manipulation, filling a crucial gap in\ncontrollable text-to-3D synthesis.\n","authors":["Dana Cohen-Bar","Elad Richardson","Gal Metzer","Raja Giryes","Daniel Cohen-Or"],"pdf_url":"https://arxiv.org/pdf/2303.13450v1.pdf","comment":"project page at https://danacohen95.github.io/Set-the-Scene/"},{"id":"http://arxiv.org/abs/2112.11947v3","updated":"2023-03-23T17:03:23Z","published":"2021-12-22T15:14:50Z","title":"Evaluating the Robustness of Deep Reinforcement Learning for Autonomous\n  Policies in a Multi-agent Urban Driving Environment","summary":"  Deep reinforcement learning is actively used for training autonomous car\npolicies in a simulated driving environment. Due to the large availability of\nvarious reinforcement learning algorithms and the lack of their systematic\ncomparison across different driving scenarios, we are unsure of which ones are\nmore effective for training autonomous car software in single-agent as well as\nmulti-agent driving environments. A benchmarking framework for the comparison\nof deep reinforcement learning in a vision-based autonomous driving will open\nup the possibilities for training better autonomous car driving policies. To\naddress these challenges, we provide an open and reusable benchmarking\nframework for systematic evaluation and comparative analysis of deep\nreinforcement learning algorithms for autonomous driving in a single- and\nmulti-agent environment. Using the framework, we perform a comparative study of\ndiscrete and continuous action space deep reinforcement learning algorithms. We\nalso propose a comprehensive multi-objective reward function designed for the\nevaluation of deep reinforcement learning-based autonomous driving agents. We\nrun the experiments in a vision-only high-fidelity urban driving simulated\nenvironments. The results indicate that only some of the deep reinforcement\nlearning algorithms perform consistently better across single and multi-agent\nscenarios when trained in various multi-agent-only environment settings. For\nexample, A3C- and TD3-based autonomous cars perform comparatively better in\nterms of more robust actions and minimal driving errors in both single and\nmulti-agent scenarios. We conclude that different deep reinforcement learning\nalgorithms exhibit different driving and testing performance in different\nscenarios, which underlines the need for their systematic comparative analysis.\nThe benchmarking framework proposed in this paper facilitates such a\ncomparison.\n","authors":["Aizaz Sharif","Dusica Marijan"],"pdf_url":"https://arxiv.org/pdf/2112.11947v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.09967v2","updated":"2023-03-23T16:53:13Z","published":"2022-12-20T02:45:09Z","title":"Learning Subgrid-scale Models with Neural Ordinary Differential\n  Equations","summary":"  We propose a new approach to learning the subgrid-scale model when simulating\npartial differential equations (PDEs) solved by the method of lines and their\nrepresentation in chaotic ordinary differential equations, based on neural\nordinary differential equations (NODEs). Solving systems with fine temporal and\nspatial grid scales is an ongoing computational challenge, and closure models\nare generally difficult to tune. Machine learning approaches have increased the\naccuracy and efficiency of computational fluid dynamics solvers. In this\napproach neural networks are used to learn the coarse- to fine-grid map, which\ncan be viewed as subgrid-scale parameterization. We propose a strategy that\nuses the NODE and partial knowledge to learn the source dynamics at a\ncontinuous level. Our method inherits the advantages of NODEs and can be used\nto parameterize subgrid scales, approximate coupling operators, and improve the\nefficiency of low-order solvers. Numerical results with the two-scale Lorenz 96\nODE, the convection-diffusion PDE, and the viscous Burgers' PDE are used to\nillustrate this approach.\n","authors":["Shinhoo Kang","Emil M. Constantinescu"],"pdf_url":"https://arxiv.org/pdf/2212.09967v2.pdf","comment":"27 pages, 20 figures, 2 tables"},{"id":"http://arxiv.org/abs/1909.12913v5","updated":"2023-03-23T16:43:29Z","published":"2019-09-18T15:46:48Z","title":"Student Engagement Detection Using Emotion Analysis, Eye Tracking and\n  Head Movement with Machine Learning","summary":"  With the increase of distance learning, in general, and e-learning, in\nparticular, having a system capable of determining the engagement of students\nis of primordial importance, and one of the biggest challenges, both for\nteachers, researchers and policy makers. Here, we present a system to detect\nthe engagement level of the students. It uses only information provided by the\ntypical built-in web-camera present in a laptop computer, and was designed to\nwork in real time. We combine information about the movements of the eyes and\nhead, and facial emotions to produce a concentration index with three classes\nof engagement: \"very engaged\", \"nominally engaged\" and \"not engaged at all\".\nThe system was tested in a typical e-learning scenario, and the results show\nthat it correctly identifies each period of time where students were \"very\nengaged\", \"nominally engaged\" and \"not engaged at all\". Additionally, the\nresults also show that the students with best scores also have higher\nconcentration indexes.\n","authors":["Prabin Sharma","Shubham Joshi","Subash Gautam","Sneha Maharjan","Salik Ram Khanal","Manuel Cabral Reis","João Barroso","Vítor Manuel de Jesus Filipe"],"pdf_url":"https://arxiv.org/pdf/1909.12913v5.pdf","comment":"9 pages, 9 Figures, 2 tables"},{"id":"http://arxiv.org/abs/2301.13734v2","updated":"2023-03-23T16:42:20Z","published":"2023-01-31T16:12:31Z","title":"Improving Monte Carlo Evaluation with Offline Data","summary":"  Monte Carlo (MC) methods are the most widely used methods to estimate the\nperformance of a policy. Given an interested policy, MC methods give estimates\nby repeatedly running this policy to collect samples and taking the average of\nthe outcomes. Samples collected during this process are called online samples.\nTo get an accurate estimate, MC methods consume massive online samples. When\nonline samples are expensive, e.g., online recommendations and inventory\nmanagement, we want to reduce the number of online samples while achieving the\nsame estimate accuracy. To this end, we use off-policy MC methods that evaluate\nthe interested policy by running a different policy called behavior policy. We\ndesign a tailored behavior policy such that the variance of the off-policy MC\nestimator is provably smaller than the ordinary MC estimator. Importantly, this\ntailored behavior policy can be efficiently learned from existing offline data,\ni,e., previously logged data, which are much cheaper than online samples. With\nreduced variance, our off-policy MC method requires fewer online samples to\nevaluate the performance of a policy compared with the ordinary MC method.\nMoreover, our off-policy MC estimator is always unbiased.\n","authors":["Shuze Liu","Shangtong Zhang"],"pdf_url":"https://arxiv.org/pdf/2301.13734v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13418v1","updated":"2023-03-23T16:39:31Z","published":"2023-03-23T16:39:31Z","title":"GiveMeLabeledIssues: An Open Source Issue Recommendation System","summary":"  Developers often struggle to navigate an Open Source Software (OSS) project's\nissue-tracking system and find a suitable task. Proper issue labeling can aid\ntask selection, but current tools are limited to classifying the issues\naccording to their type (e.g., bug, question, good first issue, feature, etc.).\nIn contrast, this paper presents a tool (GiveMeLabeledIssues) that mines\nproject repositories and labels issues based on the skills required to solve\nthem. We leverage the domain of the APIs involved in the solution (e.g., User\nInterface (UI), Test, Databases (DB), etc.) as a proxy for the required skills.\nGiveMeLabeledIssues facilitates matching developers' skills to tasks, reducing\nthe burden on project maintainers. The tool obtained a precision of 83.9% when\npredicting the API domains involved in the issues. The replication package\ncontains instructions on executing the tool and including new projects. A demo\nvideo is available at https://www.youtube.com/watch?v=ic2quUue7i8\n","authors":["Joseph Vargovich","Fabio Santos","Jacob Penney","Marco A. Gerosa","Igor Steinmacher"],"pdf_url":"https://arxiv.org/pdf/2303.13418v1.pdf","comment":"MSR Data and Tool Showcase 2023"},{"id":"http://arxiv.org/abs/2209.06203v4","updated":"2023-03-23T16:29:28Z","published":"2022-09-13T17:56:13Z","title":"Normalizing Flows for Interventional Density Estimation","summary":"  Existing machine learning methods for causal inference usually estimate\nquantities expressed via the mean of potential outcomes (e.g., average\ntreatment effect). However, such quantities do not capture the full information\nabout the distribution of potential outcomes. In this work, we estimate the\ndensity of potential outcomes after interventions from observational data. For\nthis, we propose a novel, fully-parametric deep learning method called\nInterventional Normalizing Flows. Specifically, we combine two normalizing\nflows, namely (i) a nuisance flow for estimating nuisance parameters and (ii) a\ntarget flow for a parametric estimation of the density of potential outcomes.\nWe further develop a tractable optimization objective based on a one-step bias\ncorrection for an efficient and doubly robust estimation of the target flow\nparameters. As a result our Interventional Normalizing Flows offer a properly\nnormalized density estimator. Across various experiments, we demonstrate that\nour Interventional Normalizing Flows are expressive and highly effective, and\nscale well with both sample size and high-dimensional confounding. To the best\nof our knowledge, our Interventional Normalizing Flows are the first proper\nfully-parametric, deep learning method for density estimation of potential\noutcomes.\n","authors":["Valentyn Melnychuk","Dennis Frauen","Stefan Feuerriegel"],"pdf_url":"https://arxiv.org/pdf/2209.06203v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13408v1","updated":"2023-03-23T16:29:27Z","published":"2023-03-23T16:29:27Z","title":"Paraphrasing evades detectors of AI-generated text, but retrieval is an\n  effective defense","summary":"  To detect the deployment of large language models for malicious use cases\n(e.g., fake content creation or academic plagiarism), several approaches have\nrecently been proposed for identifying AI-generated text via watermarks or\nstatistical irregularities. How robust are these detection algorithms to\nparaphrases of AI-generated text? To stress test these detectors, we first\ntrain an 11B parameter paraphrase generation model (DIPPER) that can paraphrase\nparagraphs, optionally leveraging surrounding text (e.g., user-written prompts)\nas context. DIPPER also uses scalar knobs to control the amount of lexical\ndiversity and reordering in the paraphrases. Paraphrasing text generated by\nthree large language models (including GPT3.5-davinci-003) with DIPPER\nsuccessfully evades several detectors, including watermarking, GPTZero,\nDetectGPT, and OpenAI's text classifier. For example, DIPPER drops the\ndetection accuracy of DetectGPT from 70.3% to 4.6% (at a constant false\npositive rate of 1%), without appreciably modifying the input semantics. To\nincrease the robustness of AI-generated text detection to paraphrase attacks,\nwe introduce a simple defense that relies on retrieving semantically-similar\ngenerations and must be maintained by a language model API provider. Given a\ncandidate text, our algorithm searches a database of sequences previously\ngenerated by the API, looking for sequences that match the candidate text\nwithin a certain threshold. We empirically verify our defense using a database\nof 15M generations from a fine-tuned T5-XXL model and find that it can detect\n80% to 97% of paraphrased generations across different settings, while only\nclassifying 1% of human-written sequences as AI-generated. We will open source\nour code, model and data for future research.\n","authors":["Kalpesh Krishna","Yixiao Song","Marzena Karpinska","John Wieting","Mohit Iyyer"],"pdf_url":"https://arxiv.org/pdf/2303.13408v1.pdf","comment":"Preprint (27 pages). Code, models, data will be added to\n  https://github.com/martiansideofthemoon/ai-detection-paraphrases"},{"id":"http://arxiv.org/abs/2301.07784v2","updated":"2023-03-23T16:29:23Z","published":"2023-01-18T20:54:40Z","title":"Sample-Efficient Multi-Objective Learning via Generalized Policy\n  Improvement Prioritization","summary":"  Multi-objective reinforcement learning (MORL) algorithms tackle sequential\ndecision problems where agents may have different preferences over (possibly\nconflicting) reward functions. Such algorithms often learn a set of policies\n(each optimized for a particular agent preference) that can later be used to\nsolve problems with novel preferences. We introduce a novel algorithm that uses\nGeneralized Policy Improvement (GPI) to define principled, formally-derived\nprioritization schemes that improve sample-efficient learning. They implement\nactive-learning strategies by which the agent can (i) identify the most\npromising preferences/objectives to train on at each moment, to more rapidly\nsolve a given MORL problem; and (ii) identify which previous experiences are\nmost relevant when learning a policy for a particular agent preference, via a\nnovel Dyna-style MORL method. We prove our algorithm is guaranteed to always\nconverge to an optimal solution in a finite number of steps, or an\n$\\epsilon$-optimal solution (for a bounded $\\epsilon$) if the agent is limited\nand can only identify possibly sub-optimal policies. We also prove that our\nmethod monotonically improves the quality of its partial solutions while\nlearning. Finally, we introduce a bound that characterizes the maximum utility\nloss (with respect to the optimal solution) incurred by the partial solutions\ncomputed by our method throughout learning. We empirically show that our method\noutperforms state-of-the-art MORL algorithms in challenging multi-objective\ntasks, both with discrete and continuous state and action spaces.\n","authors":["Lucas N. Alegre","Ana L. C. Bazzan","Diederik M. Roijers","Ann Nowé","Bruno C. da Silva"],"pdf_url":"https://arxiv.org/pdf/2301.07784v2.pdf","comment":"Accepted to AAMAS 2023"},{"id":"http://arxiv.org/abs/2303.13407v1","updated":"2023-03-23T16:28:26Z","published":"2023-03-23T16:28:26Z","title":"Adaptive Endpointing with Deep Contextual Multi-armed Bandits","summary":"  Current endpointing (EP) solutions learn in a supervised framework, which\ndoes not allow the model to incorporate feedback and improve in an online\nsetting. Also, it is a common practice to utilize costly grid-search to find\nthe best configuration for an endpointing model. In this paper, we aim to\nprovide a solution for adaptive endpointing by proposing an efficient method\nfor choosing an optimal endpointing configuration given utterance-level audio\nfeatures in an online setting, while avoiding hyperparameter grid-search. Our\nmethod does not require ground truth labels, and only uses online learning from\nreward signals without requiring annotated labels. Specifically, we propose a\ndeep contextual multi-armed bandit-based approach, which combines the\nrepresentational power of neural networks with the action exploration behavior\nof Thompson modeling algorithms. We compare our approach to several baselines,\nand show that our deep bandit models also succeed in reducing early cutoff\nerrors while maintaining low latency.\n","authors":["Do June Min","Andreas Stolcke","Anirudh Raju","Colin Vaz","Di He","Venkatesh Ravichandran","Viet Anh Trinh"],"pdf_url":"https://arxiv.org/pdf/2303.13407v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13405v1","updated":"2023-03-23T16:28:15Z","published":"2023-03-23T16:28:15Z","title":"SC-MIL: Supervised Contrastive Multiple Instance Learning for Imbalanced\n  Classification in Pathology","summary":"  Multiple Instance learning (MIL) models have been extensively used in\npathology to predict biomarkers and risk-stratify patients from gigapixel-sized\nimages. Machine learning problems in medical imaging often deal with rare\ndiseases, making it important for these models to work in a label-imbalanced\nsetting. Furthermore, these imbalances can occur in out-of-distribution (OOD)\ndatasets when the models are deployed in the real-world. We leverage the idea\nthat decoupling feature and classifier learning can lead to improved decision\nboundaries for label imbalanced datasets. To this end, we investigate the\nintegration of supervised contrastive learning with multiple instance learning\n(SC-MIL). Specifically, we propose a joint-training MIL framework in the\npresence of label imbalance that progressively transitions from learning\nbag-level representations to optimal classifier learning. We perform\nexperiments with different imbalance settings for two well-studied problems in\ncancer pathology: subtyping of non-small cell lung cancer and subtyping of\nrenal cell carcinoma. SC-MIL provides large and consistent improvements over\nother techniques on both in-distribution (ID) and OOD held-out sets across\nmultiple imbalanced settings.\n","authors":["Dinkar Juyal","Siddhant Shingi","Syed Ashar Javed","Harshith Padigela","Chintan Shah","Anand Sampat","Archit Khosla","John Abel","Amaro Taylor-Weiner"],"pdf_url":"https://arxiv.org/pdf/2303.13405v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13401v1","updated":"2023-03-23T16:22:59Z","published":"2023-03-23T16:22:59Z","title":"Optimization and Optimizers for Adversarial Robustness","summary":"  Empirical robustness evaluation (RE) of deep learning models against\nadversarial perturbations entails solving nontrivial constrained optimization\nproblems. Existing numerical algorithms that are commonly used to solve them in\npractice predominantly rely on projected gradient, and mostly handle\nperturbations modeled by the $\\ell_1$, $\\ell_2$ and $\\ell_\\infty$ distances. In\nthis paper, we introduce a novel algorithmic framework that blends a\ngeneral-purpose constrained-optimization solver PyGRANSO with Constraint\nFolding (PWCF), which can add more reliability and generality to the\nstate-of-the-art RE packages, e.g., AutoAttack. Regarding reliability, PWCF\nprovides solutions with stationarity measures and feasibility tests to assess\nthe solution quality. For generality, PWCF can handle perturbation models that\nare typically inaccessible to the existing projected gradient methods; the main\nrequirement is the distance metric to be almost everywhere differentiable.\nTaking advantage of PWCF and other existing numerical algorithms, we further\nexplore the distinct patterns in the solutions found for solving these\noptimization problems using various combinations of losses, perturbation\nmodels, and optimization algorithms. We then discuss the implications of these\npatterns on the current robustness evaluation and adversarial training.\n","authors":["Hengyue Liang","Buyun Liang","Le Peng","Ying Cui","Tim Mitchell","Ju Sun"],"pdf_url":"https://arxiv.org/pdf/2303.13401v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2106.06312v4","updated":"2023-03-23T16:19:38Z","published":"2021-06-11T11:09:53Z","title":"A Coupled Design of Exploiting Record Similarity for Practical Vertical\n  Federated Learning","summary":"  Federated learning is a learning paradigm to enable collaborative learning\nacross different parties without revealing raw data. Notably, vertical\nfederated learning (VFL), where parties share the same set of samples but only\nhold partial features, has a wide range of real-world applications. However,\nmost existing studies in VFL disregard the \"record linkage\" process. They\ndesign algorithms either assuming the data from different parties can be\nexactly linked or simply linking each record with its most similar neighboring\nrecord. These approaches may fail to capture the key features from other less\nsimilar records. Moreover, such improper linkage cannot be corrected by\ntraining since existing approaches provide no feedback on linkage during\ntraining. In this paper, we design a novel coupled training paradigm, FedSim,\nthat integrates one-to-many linkage into the training process. Besides enabling\nVFL in many real-world applications with fuzzy identifiers, FedSim also\nachieves better performance in traditional VFL tasks. Moreover, we\ntheoretically analyze the additional privacy risk incurred by sharing\nsimilarities. Our experiments on eight datasets with various similarity metrics\nshow that FedSim outperforms other state-of-the-art baselines. The codes of\nFedSim are available at https://github.com/Xtra-Computing/FedSim.\n","authors":["Zhaomin Wu","Qinbin Li","Bingsheng He"],"pdf_url":"https://arxiv.org/pdf/2106.06312v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.00024v2","updated":"2023-03-23T16:18:11Z","published":"2022-10-31T18:00:02Z","title":"A robust estimator of mutual information for deep learning\n  interpretability","summary":"  We develop the use of mutual information (MI), a well-established metric in\ninformation theory, to interpret the inner workings of deep learning models. To\naccurately estimate MI from a finite number of samples, we present GMM-MI\n(pronounced $``$Jimmie$\"$), an algorithm based on Gaussian mixture models that\ncan be applied to both discrete and continuous settings. GMM-MI is\ncomputationally efficient, robust to the choice of hyperparameters and provides\nthe uncertainty on the MI estimate due to the finite sample size. We\nextensively validate GMM-MI on toy data for which the ground truth MI is known,\ncomparing its performance against established mutual information estimators. We\nthen demonstrate the use of our MI estimator in the context of representation\nlearning, working with synthetic data and physical datasets describing highly\nnon-linear processes. We train deep learning models to encode high-dimensional\ndata within a meaningful compressed (latent) representation, and use GMM-MI to\nquantify both the level of disentanglement between the latent variables, and\ntheir association with relevant physical quantities, thus unlocking the\ninterpretability of the latent representation. We make GMM-MI publicly\navailable.\n","authors":["Davide Piras","Hiranya V. Peiris","Andrew Pontzen","Luisa Lucie-Smith","Ningyuan Guo","Brian Nord"],"pdf_url":"https://arxiv.org/pdf/2211.00024v2.pdf","comment":"30 pages, 8 figures. Minor changes to match version accepted for\n  publication in Machine Learning: Science and Technology. GMM-MI available at\n  https://github.com/dpiras/GMM-MI"},{"id":"http://arxiv.org/abs/2303.13391v1","updated":"2023-03-23T16:07:31Z","published":"2023-03-23T16:07:31Z","title":"Xplainer: From X-Ray Observations to Explainable Zero-Shot Diagnosis","summary":"  Automated diagnosis prediction from medical images is a valuable resource to\nsupport clinical decision-making. However, such systems usually need to be\ntrained on large amounts of annotated data, which often is scarce in the\nmedical domain. Zero-shot methods address this challenge by allowing a flexible\nadaption to new settings with different clinical findings without relying on\nlabeled data. Further, to integrate automated diagnosis in the clinical\nworkflow, methods should be transparent and explainable, increasing medical\nprofessionals' trust and facilitating correctness verification. In this work,\nwe introduce Xplainer, a novel framework for explainable zero-shot diagnosis in\nthe clinical setting. Xplainer adapts the classification-by-description\napproach of contrastive vision-language models to the multi-label medical\ndiagnosis task. Specifically, instead of directly predicting a diagnosis, we\nprompt the model to classify the existence of descriptive observations, which a\nradiologist would look for on an X-Ray scan, and use the descriptor\nprobabilities to estimate the likelihood of a diagnosis. Our model is\nexplainable by design, as the final diagnosis prediction is directly based on\nthe prediction of the underlying descriptors. We evaluate Xplainer on two chest\nX-ray datasets, CheXpert and ChestX-ray14, and demonstrate its effectiveness in\nimproving the performance and explainability of zero-shot diagnosis. Our\nresults suggest that Xplainer provides a more detailed understanding of the\ndecision-making process and can be a valuable tool for clinical diagnosis.\n","authors":["Chantal Pellegrini","Matthias Keicher","Ege Özsoy","Petra Jiraskova","Rickmer Braren","Nassir Navab"],"pdf_url":"https://arxiv.org/pdf/2303.13391v1.pdf","comment":"9 pages, 2 figures, 6 tables"},{"id":"http://arxiv.org/abs/2303.13386v1","updated":"2023-03-23T15:58:41Z","published":"2023-03-23T15:58:41Z","title":"Compositional Zero-Shot Domain Transfer with Text-to-Text Models","summary":"  Label scarcity is a bottleneck for improving task performance in specialised\ndomains. We propose a novel compositional transfer learning framework (DoT5 -\ndomain compositional zero-shot T5) for zero-shot domain transfer. Without\naccess to in-domain labels, DoT5 jointly learns domain knowledge (from MLM of\nunlabelled in-domain free text) and task knowledge (from task training on more\nreadily available general-domain data) in a multi-task manner. To improve the\ntransferability of task training, we design a strategy named NLGU: we\nsimultaneously train NLG for in-domain label-to-data generation which enables\ndata augmentation for self-finetuning and NLU for label prediction. We evaluate\nDoT5 on the biomedical domain and the resource-lean subdomain of radiology,\nfocusing on NLI, text summarisation and embedding learning. DoT5 demonstrates\nthe effectiveness of compositional transfer learning through multi-task\nlearning. In particular, DoT5 outperforms the current SOTA in zero-shot\ntransfer by over 7 absolute points in accuracy on RadNLI. We validate DoT5 with\nablations and a case study demonstrating its ability to solve challenging NLI\nexamples requiring in-domain expertise.\n","authors":["Fangyu Liu","Qianchu Liu","Shruthi Bannur","Fernando Pérez-García","Naoto Usuyama","Sheng Zhang","Tristan Naumann","Aditya Nori","Hoifung Poon","Javier Alvarez-Valle","Ozan Oktay","Stephanie L. Hyland"],"pdf_url":"https://arxiv.org/pdf/2303.13386v1.pdf","comment":"Accepted at TACL, pre-MIT Press publication version. 16 pages, 4\n  figures"},{"id":"http://arxiv.org/abs/2210.14064v3","updated":"2023-03-23T15:45:41Z","published":"2022-10-25T14:45:15Z","title":"Learning Low Dimensional State Spaces with Overparameterized Recurrent\n  Neural Nets","summary":"  Overparameterization in deep learning typically refers to settings where a\ntrained neural network (NN) has representational capacity to fit the training\ndata in many ways, some of which generalize well, while others do not. In the\ncase of Recurrent Neural Networks (RNNs), there exists an additional layer of\noverparameterization, in the sense that a model may exhibit many solutions that\ngeneralize well for sequence lengths seen in training, some of which\nextrapolate to longer sequences, while others do not. Numerous works have\nstudied the tendency of Gradient Descent (GD) to fit overparameterized NNs with\nsolutions that generalize well. On the other hand, its tendency to fit\noverparameterized RNNs with solutions that extrapolate has been discovered only\nrecently and is far less understood. In this paper, we analyze the\nextrapolation properties of GD when applied to overparameterized linear RNNs.\nIn contrast to recent arguments suggesting an implicit bias towards short-term\nmemory, we provide theoretical evidence for learning low-dimensional state\nspaces, which can also model long-term memory. Our result relies on a dynamical\ncharacterization which shows that GD (with small step size and near-zero\ninitialization) strives to maintain a certain form of balancedness, as well as\non tools developed in the context of the moment problem from statistics\n(recovery of a probability distribution from its moments). Experiments\ncorroborate our theory, demonstrating extrapolation via learning\nlow-dimensional state spaces with both linear and non-linear RNNs.\n","authors":["Edo Cohen-Karlik","Itamar Menuhin-Gruman","Raja Giryes","Nadav Cohen","Amir Globerson"],"pdf_url":"https://arxiv.org/pdf/2210.14064v3.pdf","comment":"Accepted to ICLR 2023, 9 pages, 2 figures plus supplementary"},{"id":"http://arxiv.org/abs/2302.04126v2","updated":"2023-03-23T15:45:10Z","published":"2023-02-08T15:24:17Z","title":"Predicting the performance of hybrid ventilation in buildings using a\n  multivariate attention-based biLSTM Encoder-Decoder neural network","summary":"  Hybrid ventilation is an energy-efficient solution to provide fresh air for\nmost climates, given that it has a reliable control system. To operate such\nsystems optimally, a high-fidelity control-oriented modesl is required. It\nshould enable near-real time forecast of the indoor air temperature based on\noperational conditions such as window opening and HVAC operating schedules.\nHowever, physics-based control-oriented models (i.e., white-box models) are\nlabour-intensive and computationally expensive. Alternatively, black-box models\nbased on artificial neural networks can be trained to be good estimators for\nbuilding dynamics. This paper investigates the capabilities of a deep neural\nnetwork (DNN), which is a multivariate multi-head attention-based long\nshort-term memory (LSTM) encoder-decoder neural network, to predict indoor air\ntemperature when windows are opened or closed. Training and test data are\ngenerated from a detailed multi-zone office building model (EnergyPlus).\nPseudo-random signals are used for the indoor air temperature setpoints and\nwindow opening instances. The results indicate that the DNN is able to\naccurately predict the indoor air temperature of five zones whenever windows\nare opened or closed. The prediction error plateaus after the 24th step ahead\nprediction (6 hr ahead prediction).\n","authors":["Gaurav Chaudhary","Hicham Johra","Laurent Georges","Bjørn Austbø"],"pdf_url":"https://arxiv.org/pdf/2302.04126v2.pdf","comment":"11 pages, 8 figures"},{"id":"http://arxiv.org/abs/2303.13363v1","updated":"2023-03-23T15:37:17Z","published":"2023-03-23T15:37:17Z","title":"FS-Real: Towards Real-World Cross-Device Federated Learning","summary":"  Federated Learning (FL) aims to train high-quality models in collaboration\nwith distributed clients while not uploading their local data, which attracts\nincreasing attention in both academia and industry. However, there is still a\nconsiderable gap between the flourishing FL research and real-world scenarios,\nmainly caused by the characteristics of heterogeneous devices and its scales.\nMost existing works conduct evaluations with homogeneous devices, which are\nmismatched with the diversity and variability of heterogeneous devices in\nreal-world scenarios. Moreover, it is challenging to conduct research and\ndevelopment at scale with heterogeneous devices due to limited resources and\ncomplex software stacks. These two key factors are important yet underexplored\nin FL research as they directly impact the FL training dynamics and final\nperformance, making the effectiveness and usability of FL algorithms unclear.\nTo bridge the gap, in this paper, we propose an efficient and scalable\nprototyping system for real-world cross-device FL, FS-Real. It supports\nheterogeneous device runtime, contains parallelism and robustness enhanced FL\nserver, and provides implementations and extensibility for advanced FL utility\nfeatures such as personalization, communication compression and asynchronous\naggregation. To demonstrate the usability and efficiency of FS-Real, we conduct\nextensive experiments with various device distributions, quantify and analyze\nthe effect of the heterogeneous device and various scales, and further provide\ninsights and open discussions about real-world FL scenarios. Our system is\nreleased to help to pave the way for further real-world FL research and broad\napplications involving diverse devices and scales.\n","authors":["Daoyuan Chen","Dawei Gao","Yuexiang Xie","Xuchen Pan","Zitao Li","Yaliang Li","Bolin Ding","Jingren Zhou"],"pdf_url":"https://arxiv.org/pdf/2303.13363v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13340v1","updated":"2023-03-23T15:20:05Z","published":"2023-03-23T15:20:05Z","title":"Increasing Textual Context Size Boosts Medical Image-Text Matching","summary":"  This short technical report demonstrates a simple technique that yields state\nof the art results in medical image-text matching tasks. We analyze the use of\nOpenAI's CLIP, a general image-text matching model, and observe that CLIP's\nlimited textual input size has negative impact on downstream performance in the\nmedical domain where encoding longer textual contexts is often required. We\nthus train and release ClipMD, which is trained with a simple sliding window\ntechnique to encode textual captions. ClipMD was tested on two medical\nimage-text datasets and compared with other image-text matching models. The\nresults show that ClipMD outperforms other models on both datasets by a large\nmargin. We make our code and pretrained model publicly available.\n","authors":["Idan Glassberg","Tom Hope"],"pdf_url":"https://arxiv.org/pdf/2303.13340v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.12693v2","updated":"2023-03-23T15:17:22Z","published":"2022-09-23T11:37:02Z","title":"Leveraging the Potential of Novel Data in Power Line Communication of\n  Electricity Grids","summary":"  Electricity grids have become an essential part of daily life, even if they\nare often not noticed in everyday life. We usually only become particularly\naware of this dependence by the time the electricity grid is no longer\navailable. However, significant changes, such as the transition to renewable\nenergy (photovoltaic, wind turbines, etc.) and an increasing number of energy\nconsumers with complex load profiles (electric vehicles, home battery systems,\netc.), pose new challenges for the electricity grid. To address these\nchallenges, we propose two first-of-its-kind datasets based on measurements in\na broadband powerline communications (PLC) infrastructure. Both datasets FiN-1\nand FiN-2, were collected during real practical use in a part of the German\nlow-voltage grid that supplies around 4.4 million people and show more than 13\nbillion datapoints collected by more than 5100 sensors. In addition, we present\ndifferent use cases in asset management, grid state visualization, forecasting,\npredictive maintenance, and novelty detection to highlight the benefits of\nthese types of data. For these applications, we particularly highlight the use\nof novel machine learning architectures to extract rich information from\nreal-world data that cannot be captured using traditional approaches. By\npublishing the first large-scale real-world dataset, we aim to shed light on\nthe previously largely unrecognized potential of PLC data and emphasize\nmachine-learning-based research in low-voltage distribution networks by\npresenting a variety of different use cases.\n","authors":["Christoph Balada","Max Bondorf","Sheraz Ahmed","Andreas Dengela","Markus Zdrallek"],"pdf_url":"https://arxiv.org/pdf/2209.12693v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13336v1","updated":"2023-03-23T15:17:15Z","published":"2023-03-23T15:17:15Z","title":"Audio Diffusion Model for Speech Synthesis: A Survey on Text To Speech\n  and Speech Enhancement in Generative AI","summary":"  Generative AI has demonstrated impressive performance in various fields,\namong which speech synthesis is an interesting direction. With the diffusion\nmodel as the most popular generative model, numerous works have attempted two\nactive tasks: text to speech and speech enhancement. This work conducts a\nsurvey on audio diffusion model, which is complementary to existing surveys\nthat either lack the recent progress of diffusion-based speech synthesis or\nhighlight an overall picture of applying diffusion model in multiple fields.\nSpecifically, this work first briefly introduces the background of audio and\ndiffusion model. As for the text-to-speech task, we divide the methods into\nthree categories based on the stage where diffusion model is adopted: acoustic\nmodel, vocoder and end-to-end framework. Moreover, we categorize various speech\nenhancement tasks by either certain signals are removed or added into the input\nspeech. Comparisons of experimental results and discussions are also covered in\nthis survey.\n","authors":["Chenshuang Zhang","Chaoning Zhang","Sheng Zheng","Mengchun Zhang","Maryam Qamar","Sung-Ho Bae","In So Kweon"],"pdf_url":"https://arxiv.org/pdf/2303.13336v1.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2002.02247v2","updated":"2023-03-23T15:13:14Z","published":"2020-02-06T13:25:26Z","title":"Almost Sure Convergence of Dropout Algorithms for Neural Networks","summary":"  We investigate the convergence and convergence rate of stochastic training\nalgorithms for Neural Networks (NNs) that have been inspired by Dropout (Hinton\net al., 2012). With the goal of avoiding overfitting during training of NNs,\ndropout algorithms consist in practice of multiplying the weight matrices of a\nNN componentwise by independently drawn random matrices with $\\{0, 1 \\}$-valued\nentries during each iteration of Stochastic Gradient Descent (SGD). This paper\npresents a probability theoretical proof that for fully-connected NNs with\ndifferentiable, polynomially bounded activation functions, if we project the\nweights onto a compact set when using a dropout algorithm, then the weights of\nthe NN converge to a unique stationary point of a projected system of Ordinary\nDifferential Equations (ODEs). After this general convergence guarantee, we go\non to investigate the convergence rate of dropout. Firstly, we obtain generic\nsample complexity bounds for finding $\\epsilon$-stationary points of smooth\nnonconvex functions using SGD with dropout that explicitly depend on the\ndropout probability. Secondly, we obtain an upper bound on the rate of\nconvergence of Gradient Descent (GD) on the limiting ODEs of dropout algorithms\nfor NNs with the shape of arborescences of arbitrary depth and with linear\nactivation functions. The latter bound shows that for an algorithm such as\nDropout or Dropconnect (Wan et al., 2013), the convergence rate can be impaired\nexponentially by the depth of the arborescence. In contrast, we experimentally\nobserve no such dependence for wide NNs with just a few dropout layers. We also\nprovide a heuristic argument for this observation. Our results suggest that\nthere is a change of scale of the effect of the dropout probability in the\nconvergence rate that depends on the relative size of the width of the NN\ncompared to its depth.\n","authors":["Albert Senen-Cerda","Jaron Sanders"],"pdf_url":"https://arxiv.org/pdf/2002.02247v2.pdf","comment":"52 pages, 3 figures. Added results pertaining to the convergence rate\n  of Dropout SGD to $\\epsilon$-stationary points and numerical experiments.\n  Updated the introduction, conclusion and appendix. Changed format to\n  one-column text"},{"id":"http://arxiv.org/abs/2210.17216v2","updated":"2023-03-23T15:10:19Z","published":"2022-10-31T10:55:30Z","title":"Symmetries, flat minima, and the conserved quantities of gradient flow","summary":"  Empirical studies of the loss landscape of deep networks have revealed that\nmany local minima are connected through low-loss valleys. Yet, little is known\nabout the theoretical origin of such valleys. We present a general framework\nfor finding continuous symmetries in the parameter space, which carve out\nlow-loss valleys. Our framework uses equivariances of the activation functions\nand can be applied to different layer architectures. To generalize this\nframework to nonlinear neural networks, we introduce a novel set of nonlinear,\ndata-dependent symmetries. These symmetries can transform a trained model such\nthat it performs similarly on new samples, which allows ensemble building that\nimproves robustness under certain adversarial attacks. We then show that\nconserved quantities associated with linear symmetries can be used to define\ncoordinates along low-loss valleys. The conserved quantities help reveal that\nusing common initialization methods, gradient flow only explores a small part\nof the global minimum. By relating conserved quantities to convergence rate and\nsharpness of the minimum, we provide insights on how initialization impacts\nconvergence and generalizability.\n","authors":["Bo Zhao","Iordan Ganev","Robin Walters","Rose Yu","Nima Dehmamy"],"pdf_url":"https://arxiv.org/pdf/2210.17216v2.pdf","comment":"To appear at ICLR 2023"},{"id":"http://arxiv.org/abs/2303.13326v1","updated":"2023-03-23T15:05:16Z","published":"2023-03-23T15:05:16Z","title":"Decentralized Adversarial Training over Graphs","summary":"  The vulnerability of machine learning models to adversarial attacks has been\nattracting considerable attention in recent years. Most existing studies focus\non the behavior of stand-alone single-agent learners. In comparison, this work\nstudies adversarial training over graphs, where individual agents are subjected\nto perturbations of varied strength levels across space. It is expected that\ninteractions by linked agents, and the heterogeneity of the attack models that\nare possible over the graph, can help enhance robustness in view of the\ncoordination power of the group. Using a min-max formulation of diffusion\nlearning, we develop a decentralized adversarial training framework for\nmulti-agent systems. We analyze the convergence properties of the proposed\nscheme for both convex and non-convex environments, and illustrate the enhanced\nrobustness to adversarial attacks.\n","authors":["Ying Cao","Elsa Rizk","Stefan Vlaski","Ali H. Sayed"],"pdf_url":"https://arxiv.org/pdf/2303.13326v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2303.01936"},{"id":"http://arxiv.org/abs/2303.13323v1","updated":"2023-03-23T15:01:52Z","published":"2023-03-23T15:01:52Z","title":"Deep Generative Multi-Agent Imitation Model as a Computational Benchmark\n  for Evaluating Human Performance in Complex Interactive Tasks: A Case Study\n  in Football","summary":"  Evaluating the performance of human is a common need across many\napplications, such as in engineering and sports. When evaluating human\nperformance in completing complex and interactive tasks, the most common way is\nto use a metric having been proved efficient for that context, or to use\nsubjective measurement techniques. However, this can be an error prone and\nunreliable process since static metrics cannot capture all the complex contexts\nassociated with such tasks and biases exist in subjective measurement. The\nobjective of our research is to create data-driven AI agents as computational\nbenchmarks to evaluate human performance in solving difficult tasks involving\nmultiple humans and contextual factors. We demonstrate this within the context\nof football performance analysis. We train a generative model based on\nConditional Variational Recurrent Neural Network (VRNN) Model on a large player\nand ball tracking dataset. The trained model is used to imitate the\ninteractions between two teams and predict the performance from each team. Then\nthe trained Conditional VRNN Model is used as a benchmark to evaluate team\nperformance. The experimental results on Premier League football dataset\ndemonstrates the usefulness of our method to existing state-of-the-art static\nmetric used in football analytics.\n","authors":["Chaoyi Gu","Varuna De Silva"],"pdf_url":"https://arxiv.org/pdf/2303.13323v1.pdf","comment":"8 pages, 10 figures"},{"id":"http://arxiv.org/abs/2303.13299v1","updated":"2023-03-23T14:35:37Z","published":"2023-03-23T14:35:37Z","title":"Reckoning with the Disagreement Problem: Explanation Consensus as a\n  Training Objective","summary":"  As neural networks increasingly make critical decisions in high-stakes\nsettings, monitoring and explaining their behavior in an understandable and\ntrustworthy manner is a necessity. One commonly used type of explainer is post\nhoc feature attribution, a family of methods for giving each feature in an\ninput a score corresponding to its influence on a model's output. A major\nlimitation of this family of explainers in practice is that they can disagree\non which features are more important than others. Our contribution in this\npaper is a method of training models with this disagreement problem in mind. We\ndo this by introducing a Post hoc Explainer Agreement Regularization (PEAR)\nloss term alongside the standard term corresponding to accuracy, an additional\nterm that measures the difference in feature attribution between a pair of\nexplainers. We observe on three datasets that we can train a model with this\nloss term to improve explanation consensus on unseen data, and see improved\nconsensus between explainers other than those used in the loss term. We examine\nthe trade-off between improved consensus and model performance. And finally, we\nstudy the influence our method has on feature attribution explanations.\n","authors":["Avi Schwarzschild","Max Cembalest","Karthik Rao","Keegan Hines","John Dickerson"],"pdf_url":"https://arxiv.org/pdf/2303.13299v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13297v1","updated":"2023-03-23T14:27:49Z","published":"2023-03-23T14:27:49Z","title":"Improving Generalization with Domain Convex Game","summary":"  Domain generalization (DG) tends to alleviate the poor generalization\ncapability of deep neural networks by learning model with multiple source\ndomains. A classical solution to DG is domain augmentation, the common belief\nof which is that diversifying source domains will be conducive to the\nout-of-distribution generalization. However, these claims are understood\nintuitively, rather than mathematically. Our explorations empirically reveal\nthat the correlation between model generalization and the diversity of domains\nmay be not strictly positive, which limits the effectiveness of domain\naugmentation. This work therefore aim to guarantee and further enhance the\nvalidity of this strand. To this end, we propose a new perspective on DG that\nrecasts it as a convex game between domains. We first encourage each\ndiversified domain to enhance model generalization by elaborately designing a\nregularization term based on supermodularity. Meanwhile, a sample filter is\nconstructed to eliminate low-quality samples, thereby avoiding the impact of\npotentially harmful information. Our framework presents a new avenue for the\nformal analysis of DG, heuristic analysis and extensive experiments demonstrate\nthe rationality and effectiveness.\n","authors":["Fangrui Lv","Jian Liang","Shuang Li","Jinming Zhang","Di Liu"],"pdf_url":"https://arxiv.org/pdf/2303.13297v1.pdf","comment":"accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2103.10427v4","updated":"2023-03-23T14:21:02Z","published":"2021-03-18T17:58:02Z","title":"The Low-Rank Simplicity Bias in Deep Networks","summary":"  Modern deep neural networks are highly over-parameterized compared to the\ndata on which they are trained, yet they often generalize remarkably well. A\nflurry of recent work has asked: why do deep networks not overfit to their\ntraining data? In this work, we make a series of empirical observations that\ninvestigate and extend the hypothesis that deeper networks are inductively\nbiased to find solutions with lower effective rank embeddings. We conjecture\nthat this bias exists because the volume of functions that maps to low\neffective rank embedding increases with depth. We show empirically that our\nclaim holds true on finite width linear and non-linear models on practical\nlearning paradigms and show that on natural data, these are often the solutions\nthat generalize well. We then show that the simplicity bias exists at both\ninitialization and after training and is resilient to hyper-parameters and\nlearning methods. We further demonstrate how linear over-parameterization of\ndeep non-linear models can be used to induce low-rank bias, improving\ngeneralization performance on CIFAR and ImageNet without changing the modeling\ncapacity.\n","authors":["Minyoung Huh","Hossein Mobahi","Richard Zhang","Brian Cheung","Pulkit Agrawal","Phillip Isola"],"pdf_url":"https://arxiv.org/pdf/2103.10427v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.10404v2","updated":"2023-03-23T14:14:00Z","published":"2022-09-21T14:51:42Z","title":"GP-net: Flexible Viewpoint Grasp Proposal","summary":"  We present the Grasp Proposal Network (GP-net), a Convolutional Neural\nNetwork model which can generate 6-DOF grasps from flexible viewpoints, e.g. as\nexperienced by mobile manipulators. To train GP-net, we synthetically generate\na dataset containing depth-images and ground-truth grasp information. In\nreal-world experiments we use the EGAD! grasping benchmark to evaluate GP-net\nagainst two commonly used algorithms, the Volumetric Grasping Network (VGN) and\nthe Grasp Pose Detection package (GPD), on a PAL TIAGo mobile manipulator. In\ncontrast to the state-of-the-art methods in robotic grasping, GP-net can be\nused for grasping objects from flexible, unknown viewpoints without the need to\ndefine the workspace and achieves a grasp success of 51.8% compared to 51.1%\nfor VGN and 33.6% for GPD. We provide a ROS package along with our code and\npre-trained models at https://aucoroboticsmu.github.io/GP-net/.\n","authors":["Anna Konrad","John McDonald","Rudi Villing"],"pdf_url":"https://arxiv.org/pdf/2209.10404v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.03792v3","updated":"2023-03-23T14:02:44Z","published":"2022-06-08T10:17:40Z","title":"Utilising the CLT Structure in Stochastic Gradient based Sampling :\n  Improved Analysis and Faster Algorithms","summary":"  We consider stochastic approximations of sampling algorithms, such as\nStochastic Gradient Langevin Dynamics (SGLD) and the Random Batch Method (RBM)\nfor Interacting Particle Dynamcs (IPD). We observe that the noise introduced by\nthe stochastic approximation is nearly Gaussian due to the Central Limit\nTheorem (CLT) while the driving Brownian motion is exactly Gaussian. We harness\nthis structure to absorb the stochastic approximation error inside the\ndiffusion process, and obtain improved convergence guarantees for these\nalgorithms. For SGLD, we prove the first stable convergence rate in KL\ndivergence without requiring uniform warm start, assuming the target density\nsatisfies a Log-Sobolev Inequality. Our result implies superior first-order\noracle complexity compared to prior works, under significantly milder\nassumptions. We also prove the first guarantees for SGLD under even weaker\nconditions such as H\\\"{o}lder smoothness and Poincare Inequality, thus bridging\nthe gap between the state-of-the-art guarantees for LMC and SGLD. Our analysis\nmotivates a new algorithm called covariance correction, which corrects for the\nadditional noise introduced by the stochastic approximation by rescaling the\nstrength of the diffusion. Finally, we apply our techniques to analyze RBM, and\nsignificantly improve upon the guarantees in prior works (such as removing\nexponential dependence on horizon), under minimal assumptions.\n","authors":["Aniket Das","Dheeraj Nagaraj","Anant Raj"],"pdf_url":"https://arxiv.org/pdf/2206.03792v3.pdf","comment":"Version 3 reduces the computational complexity of Covariance\n  Correction and relaxes the almost-sure noise growth assumption for smooth\n  SGLD"},{"id":"http://arxiv.org/abs/2208.07743v2","updated":"2023-03-23T13:54:52Z","published":"2022-08-16T13:33:19Z","title":"Langevin Diffusion Variational Inference","summary":"  Many methods that build powerful variational distributions based on\nunadjusted Langevin transitions exist. Most of these were developed using a\nwide range of different approaches and techniques. Unfortunately, the lack of a\nunified analysis and derivation makes developing new methods and reasoning\nabout existing ones a challenging task. We address this giving a single\nanalysis that unifies and generalizes these existing techniques. The main idea\nis to augment the target and variational by numerically simulating the\nunderdamped Langevin diffusion process and its time reversal. The benefits of\nthis approach are twofold: it provides a unified formulation for many existing\nmethods, and it simplifies the development of new ones. In fact, using our\nformulation we propose a new method that combines the strengths of previously\nexisting algorithms; it uses underdamped Langevin transitions and powerful\naugmentations parameterized by a score network. Our empirical evaluation shows\nthat our proposed method consistently outperforms relevant baselines in a wide\nrange of tasks.\n","authors":["Tomas Geffner","Justin Domke"],"pdf_url":"https://arxiv.org/pdf/2208.07743v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.01117v2","updated":"2023-03-23T13:42:27Z","published":"2022-10-03T17:58:04Z","title":"Omnigrok: Grokking Beyond Algorithmic Data","summary":"  Grokking, the unusual phenomenon for algorithmic datasets where\ngeneralization happens long after overfitting the training data, has remained\nelusive. We aim to understand grokking by analyzing the loss landscapes of\nneural networks, identifying the mismatch between training and test losses as\nthe cause for grokking. We refer to this as the \"LU mechanism\" because training\nand test losses (against model weight norm) typically resemble \"L\" and \"U\",\nrespectively. This simple mechanism can nicely explain many aspects of\ngrokking: data size dependence, weight decay dependence, the emergence of\nrepresentations, etc. Guided by the intuitive picture, we are able to induce\ngrokking on tasks involving images, language and molecules. In the reverse\ndirection, we are able to eliminate grokking for algorithmic datasets. We\nattribute the dramatic nature of grokking for algorithmic datasets to\nrepresentation learning.\n","authors":["Ziming Liu","Eric J. Michaud","Max Tegmark"],"pdf_url":"https://arxiv.org/pdf/2210.01117v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.11777v2","updated":"2023-03-23T13:28:58Z","published":"2023-01-27T15:30:25Z","title":"Interpreting learning in biological neural networks as zero-order\n  optimization method","summary":"  Recently, significant progress has been made regarding the statistical\nunderstanding of artificial neural networks (ANNs). ANNs are motivated by the\nfunctioning of the brain, but differ in several crucial aspects. In particular,\nthe locality in the updating rule of the connection parameters in biological\nneural networks (BNNs) makes it biologically implausible that the learning of\nthe brain is based on gradient descent. In this work, we look at the brain as a\nstatistical method for supervised learning. The main contribution is to relate\nthe local updating rule of the connection parameters in BNNs to a zero-order\noptimization method. It is shown that the expected values of the iterates\nimplement a modification of gradient descent.\n","authors":["Johannes Schmidt-Hieber"],"pdf_url":"https://arxiv.org/pdf/2301.11777v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13228v1","updated":"2023-03-23T12:59:37Z","published":"2023-03-23T12:59:37Z","title":"Enriching Neural Network Training Dataset to Improve Worst-Case\n  Performance Guarantees","summary":"  Machine learning algorithms, especially Neural Networks (NNs), are a valuable\ntool used to approximate non-linear relationships, like the AC-Optimal Power\nFlow (AC-OPF), with considerable accuracy -- and achieving a speedup of several\norders of magnitude when deployed for use. Often in power systems literature,\nthe NNs are trained with a fixed dataset generated prior to the training\nprocess. In this paper, we show that adapting the NN training dataset during\ntraining can improve the NN performance and substantially reduce its worst-case\nviolations. This paper proposes an algorithm that identifies and enriches the\ntraining dataset with critical datapoints that reduce the worst-case violations\nand deliver a neural network with improved worst-case performance guarantees.\nWe demonstrate the performance of our algorithm in four test power systems,\nranging from 39-buses to 162-buses.\n","authors":["Rahul Nellikkath","Spyros Chatzivasileiadis"],"pdf_url":"https://arxiv.org/pdf/2303.13228v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2212.10930"},{"id":"http://arxiv.org/abs/2303.07150v2","updated":"2023-03-23T12:49:39Z","published":"2023-03-13T14:23:39Z","title":"Multi PILOT: Learned Feasible Multiple Acquisition Trajectories for\n  Dynamic MRI","summary":"  Dynamic Magnetic Resonance Imaging (MRI) is known to be a powerful and\nreliable technique for the dynamic imaging of internal organs and tissues,\nmaking it a leading diagnostic tool. A major difficulty in using MRI in this\nsetting is the relatively long acquisition time (and, hence, increased cost)\nrequired for imaging in high spatio-temporal resolution, leading to the\nappearance of related motion artifacts and decrease in resolution. Compressed\nSensing (CS) techniques have become a common tool to reduce MRI acquisition\ntime by subsampling images in the k-space according to some acquisition\ntrajectory. Several studies have particularly focused on applying deep learning\ntechniques to learn these acquisition trajectories in order to attain better\nimage reconstruction, rather than using some predefined set of trajectories. To\nthe best of our knowledge, learning acquisition trajectories has been only\nexplored in the context of static MRI. In this study, we consider acquisition\ntrajectory learning in the dynamic imaging setting. We design an end-to-end\npipeline for the joint optimization of multiple per-frame acquisition\ntrajectories along with a reconstruction neural network, and demonstrate\nimproved image reconstruction quality in shorter acquisition times. The code\nfor reproducing all experiments is accessible at\nhttps://github.com/tamirshor7/MultiPILOT.\n","authors":["Tamir Shor","Tomer Weiss","Dor Noti","Alex Bronstein"],"pdf_url":"https://arxiv.org/pdf/2303.07150v2.pdf","comment":"Accepted For MIDL 2023"},{"id":"http://arxiv.org/abs/2012.10315v5","updated":"2023-03-23T12:48:40Z","published":"2020-12-18T16:00:08Z","title":"Kernel Methods for Unobserved Confounding: Negative Controls, Proxies,\n  and Instruments","summary":"  Negative control is a strategy for learning the causal relationship between\ntreatment and outcome in the presence of unmeasured confounding. The treatment\neffect can nonetheless be identified if two auxiliary variables are available:\na negative control treatment (which has no effect on the actual outcome), and a\nnegative control outcome (which is not affected by the actual treatment). These\nauxiliary variables can also be viewed as proxies for a traditional set of\ncontrol variables, and they bear resemblance to instrumental variables. I\npropose a family of algorithms based on kernel ridge regression for learning\nnonparametric treatment effects with negative controls. Examples include dose\nresponse curves, dose response curves with distribution shift, and\nheterogeneous treatment effects. Data may be discrete or continuous, and low,\nhigh, or infinite dimensional. I prove uniform consistency and provide finite\nsample rates of convergence. I estimate the dose response curve of cigarette\nsmoking on infant birth weight adjusting for unobserved confounding due to\nhousehold income, using a data set of singleton births in the state of\nPennsylvania between 1989 and 1991.\n","authors":["Rahul Singh"],"pdf_url":"https://arxiv.org/pdf/2012.10315v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.01047v2","updated":"2023-03-23T12:30:35Z","published":"2023-02-02T12:21:10Z","title":"Real-Time Evaluation in Online Continual Learning: A New Hope","summary":"  Current evaluations of Continual Learning (CL) methods typically assume that\nthere is no constraint on training time and computation. This is an unrealistic\nassumption for any real-world setting, which motivates us to propose: a\npractical real-time evaluation of continual learning, in which the stream does\nnot wait for the model to complete training before revealing the next data for\npredictions. To do this, we evaluate current CL methods with respect to their\ncomputational costs. We conduct extensive experiments on CLOC, a large-scale\ndataset containing 39 million time-stamped images with geolocation labels. We\nshow that a simple baseline outperforms state-of-the-art CL methods under this\nevaluation, questioning the applicability of existing methods in realistic\nsettings. In addition, we explore various CL components commonly used in the\nliterature, including memory sampling strategies and regularization approaches.\nWe find that all considered methods fail to be competitive against our simple\nbaseline. This surprisingly suggests that the majority of existing CL\nliterature is tailored to a specific class of streams that is not practical. We\nhope that the evaluation we provide will be the first step towards a paradigm\nshift to consider the computational cost in the development of online continual\nlearning methods.\n","authors":["Yasir Ghunaim","Adel Bibi","Kumail Alhamoud","Motasem Alfarra","Hasan Abed Al Kader Hammoud","Ameya Prabhu","Philip H. S. Torr","Bernard Ghanem"],"pdf_url":"https://arxiv.org/pdf/2302.01047v2.pdf","comment":"Accepted at CVPR'23 as Highlight (Top 2.5%)"},{"id":"http://arxiv.org/abs/2303.13211v1","updated":"2023-03-23T12:11:24Z","published":"2023-03-23T12:11:24Z","title":"Don't FREAK Out: A Frequency-Inspired Approach to Detecting Backdoor\n  Poisoned Samples in DNNs","summary":"  In this paper we investigate the frequency sensitivity of Deep Neural\nNetworks (DNNs) when presented with clean samples versus poisoned samples. Our\nanalysis shows significant disparities in frequency sensitivity between these\ntwo types of samples. Building on these findings, we propose FREAK, a\nfrequency-based poisoned sample detection algorithm that is simple yet\neffective. Our experimental results demonstrate the efficacy of FREAK not only\nagainst frequency backdoor attacks but also against some spatial attacks. Our\nwork is just the first step in leveraging these insights. We believe that our\nanalysis and proposed defense mechanism will provide a foundation for future\nresearch and development of backdoor defenses.\n","authors":["Hasan Abed Al Kader Hammoud","Adel Bibi","Philip H. S. Torr","Bernard Ghanem"],"pdf_url":"https://arxiv.org/pdf/2303.13211v1.pdf","comment":"Accepted at CVPRW (The Art of Robustness)"},{"id":"http://arxiv.org/abs/2206.05564v2","updated":"2023-03-23T11:50:36Z","published":"2022-06-11T16:57:23Z","title":"gDDIM: Generalized denoising diffusion implicit models","summary":"  Our goal is to extend the denoising diffusion implicit model (DDIM) to\ngeneral diffusion models~(DMs) besides isotropic diffusions. Instead of\nconstructing a non-Markov noising process as in the original DDIM, we examine\nthe mechanism of DDIM from a numerical perspective. We discover that the DDIM\ncan be obtained by using some specific approximations of the score when solving\nthe corresponding stochastic differential equation. We present an\ninterpretation of the accelerating effects of DDIM that also explains the\nadvantages of a deterministic sampling scheme over the stochastic one for fast\nsampling. Building on this insight, we extend DDIM to general DMs, coined\ngeneralized DDIM (gDDIM), with a small but delicate modification in\nparameterizing the score network. We validate gDDIM in two non-isotropic DMs:\nBlurring diffusion model (BDM) and Critically-damped Langevin diffusion model\n(CLD). We observe more than 20 times acceleration in BDM. In the CLD, a\ndiffusion model by augmenting the diffusion process with velocity, our\nalgorithm achieves an FID score of 2.26, on CIFAR10, with only 50 number of\nscore function evaluations~(NFEs) and an FID score of 2.86 with only 27 NFEs.\nCode is available at https://github.com/qsh-zh/gDDIM\n","authors":["Qinsheng Zhang","Molei Tao","Yongxin Chen"],"pdf_url":"https://arxiv.org/pdf/2206.05564v2.pdf","comment":"26 pages, 9 figures, implementation https://github.com/qsh-zh/gDDIM"},{"id":"http://arxiv.org/abs/2209.04747v4","updated":"2023-03-23T11:42:58Z","published":"2022-09-10T22:00:30Z","title":"Diffusion Models in Vision: A Survey","summary":"  Denoising diffusion models represent a recent emerging topic in computer\nvision, demonstrating remarkable results in the area of generative modeling. A\ndiffusion model is a deep generative model that is based on two stages, a\nforward diffusion stage and a reverse diffusion stage. In the forward diffusion\nstage, the input data is gradually perturbed over several steps by adding\nGaussian noise. In the reverse stage, a model is tasked at recovering the\noriginal input data by learning to gradually reverse the diffusion process,\nstep by step. Diffusion models are widely appreciated for the quality and\ndiversity of the generated samples, despite their known computational burdens,\ni.e. low speeds due to the high number of steps involved during sampling. In\nthis survey, we provide a comprehensive review of articles on denoising\ndiffusion models applied in vision, comprising both theoretical and practical\ncontributions in the field. First, we identify and present three generic\ndiffusion modeling frameworks, which are based on denoising diffusion\nprobabilistic models, noise conditioned score networks, and stochastic\ndifferential equations. We further discuss the relations between diffusion\nmodels and other deep generative models, including variational auto-encoders,\ngenerative adversarial networks, energy-based models, autoregressive models and\nnormalizing flows. Then, we introduce a multi-perspective categorization of\ndiffusion models applied in computer vision. Finally, we illustrate the current\nlimitations of diffusion models and envision some interesting directions for\nfuture research.\n","authors":["Florinel-Alin Croitoru","Vlad Hondru","Radu Tudor Ionescu","Mubarak Shah"],"pdf_url":"https://arxiv.org/pdf/2209.04747v4.pdf","comment":"Accepted in IEEE Transactions on Pattern Analysis and Machine\n  Intelligence. 25 pages, 3 figures"},{"id":"http://arxiv.org/abs/2301.01217v4","updated":"2023-03-23T11:29:03Z","published":"2022-12-31T04:26:25Z","title":"Unlearnable Clusters: Towards Label-agnostic Unlearnable Examples","summary":"  There is a growing interest in developing unlearnable examples (UEs) against\nvisual privacy leaks on the Internet. UEs are training samples added with\ninvisible but unlearnable noise, which have been found can prevent unauthorized\ntraining of machine learning models. UEs typically are generated via a bilevel\noptimization framework with a surrogate model to remove (minimize) errors from\nthe original samples, and then applied to protect the data against unknown\ntarget models. However, existing UE generation methods all rely on an ideal\nassumption called label-consistency, where the hackers and protectors are\nassumed to hold the same label for a given sample. In this work, we propose and\npromote a more practical label-agnostic setting, where the hackers may exploit\nthe protected data quite differently from the protectors. E.g., a m-class\nunlearnable dataset held by the protector may be exploited by the hacker as a\nn-class dataset. Existing UE generation methods are rendered ineffective in\nthis challenging setting. To tackle this challenge, we present a novel\ntechnique called Unlearnable Clusters (UCs) to generate label-agnostic\nunlearnable examples with cluster-wise perturbations. Furthermore, we propose\nto leverage VisionandLanguage Pre-trained Models (VLPMs) like CLIP as the\nsurrogate model to improve the transferability of the crafted UCs to diverse\ndomains. We empirically verify the effectiveness of our proposed approach under\na variety of settings with different datasets, target models, and even\ncommercial platforms Microsoft Azure and Baidu PaddlePaddle. Code is available\nat \\url{https://github.com/jiamingzhang94/Unlearnable-Clusters}.\n","authors":["Jiaming Zhang","Xingjun Ma","Qi Yi","Jitao Sang","Yu-Gang Jiang","Yaowei Wang","Changsheng Xu"],"pdf_url":"https://arxiv.org/pdf/2301.01217v4.pdf","comment":"CVPR2023"},{"id":"http://arxiv.org/abs/2303.13177v1","updated":"2023-03-23T11:16:33Z","published":"2023-03-23T11:16:33Z","title":"It is all Connected: A New Graph Formulation for Spatio-Temporal\n  Forecasting","summary":"  With an ever-increasing number of sensors in modern society, spatio-temporal\ntime series forecasting has become a de facto tool to make informed decisions\nabout the future. Most spatio-temporal forecasting models typically comprise\ndistinct components that learn spatial and temporal dependencies. A common\nmethodology employs some Graph Neural Network (GNN) to capture relations\nbetween spatial locations, while another network, such as a recurrent neural\nnetwork (RNN), learns temporal correlations. By representing every recorded\nsample as its own node in a graph, rather than all measurements for a\nparticular location as a single node, temporal and spatial information is\nencoded in a similar manner. In this setting, GNNs can now directly learn both\ntemporal and spatial dependencies, jointly, while also alleviating the need for\nadditional temporal networks. Furthermore, the framework does not require\naligned measurements along the temporal dimension, meaning that it also\nnaturally facilitates irregular time series, different sampling frequencies or\nmissing data, without the need for data imputation. To evaluate the proposed\nmethodology, we consider wind speed forecasting as a case study, where our\nproposed framework outperformed other spatio-temporal models using GNNs with\neither Transformer or LSTM networks as temporal update functions.\n","authors":["Lars Ødegaard Bentsen","Narada Dilp Warakagoda","Roy Stenbro","Paal Engelstad"],"pdf_url":"https://arxiv.org/pdf/2303.13177v1.pdf","comment":"Pre-print"},{"id":"http://arxiv.org/abs/2206.08289v4","updated":"2023-03-23T10:54:32Z","published":"2022-06-16T16:46:32Z","title":"Switchable Representation Learning Framework with Self-compatibility","summary":"  Real-world visual search systems involve deployments on multiple platforms\nwith different computing and storage resources. Deploying a unified model that\nsuits the minimal-constrain platforms leads to limited accuracy. It is expected\nto deploy models with different capacities adapting to the resource\nconstraints, which requires features extracted by these models to be aligned in\nthe metric space. The method to achieve feature alignments is called\n``compatible learning''. Existing research mainly focuses on the one-to-one\ncompatible paradigm, which is limited in learning compatibility among multiple\nmodels. We propose a Switchable representation learning Framework with\nSelf-Compatibility (SFSC). SFSC generates a series of compatible sub-models\nwith different capacities through one training process. The optimization of\nsub-models faces gradients conflict, and we mitigate this problem from the\nperspective of the magnitude and direction. We adjust the priorities of\nsub-models dynamically through uncertainty estimation to co-optimize sub-models\nproperly. Besides, the gradients with conflicting directions are projected to\navoid mutual interference. SFSC achieves state-of-the-art performance on the\nevaluated datasets.\n","authors":["Shengsen Wu","Yan Bai","Yihang Lou","Xiongkun Linghu","Jianzhong He","Ling-Yu Duan"],"pdf_url":"https://arxiv.org/pdf/2206.08289v4.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2303.13166v1","updated":"2023-03-23T10:36:10Z","published":"2023-03-23T10:36:10Z","title":"Take 5: Interpretable Image Classification with a Handful of Features","summary":"  Deep Neural Networks use thousands of mostly incomprehensible features to\nidentify a single class, a decision no human can follow. We propose an\ninterpretable sparse and low dimensional final decision layer in a deep neural\nnetwork with measurable aspects of interpretability and demonstrate it on\nfine-grained image classification. We argue that a human can only understand\nthe decision of a machine learning model, if the features are interpretable and\nonly very few of them are used for a single decision. For that matter, the\nfinal layer has to be sparse and, to make interpreting the features feasible,\nlow dimensional. We call a model with a Sparse Low-Dimensional Decision\nSLDD-Model. We show that a SLDD-Model is easier to interpret locally and\nglobally than a dense high-dimensional decision layer while being able to\nmaintain competitive accuracy. Additionally, we propose a loss function that\nimproves a model's feature diversity and accuracy. Our more interpretable\nSLDD-Model only uses 5 out of just 50 features per class, while maintaining 97%\nto 100% of the accuracy on four common benchmark datasets compared to the\nbaseline model with 2048 features.\n","authors":["Thomas Norrenbrock","Marco Rudolph","Bodo Rosenhahn"],"pdf_url":"https://arxiv.org/pdf/2303.13166v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13157v1","updated":"2023-03-23T10:18:06Z","published":"2023-03-23T10:18:06Z","title":"Adiabatic replay for continual learning","summary":"  Conventional replay-based approaches to continual learning (CL) require, for\neach learning phase with new data, the replay of samples representing all of\nthe previously learned knowledge in order to avoid catastrophic forgetting.\nSince the amount of learned knowledge grows over time in CL problems,\ngenerative replay spends an increasing amount of time just re-learning what is\nalready known. In this proof-of-concept study, we propose a replay-based CL\nstrategy that we term adiabatic replay (AR), which derives its efficiency from\nthe (reasonable) assumption that each new learning phase is adiabatic, i.e.,\nrepresents only a small addition to existing knowledge. Each new learning phase\ntriggers a sampling process that selectively replays, from the body of existing\nknowledge, just such samples that are similar to the new data, in contrast to\nreplaying all of it. Complete replay is not required since AR represents the\ndata distribution by GMMs, which are capable of selectively updating their\ninternal representation only where data statistics have changed. As long as\nadditions are adiabatic, the amount of to-be-replayed samples need not to\ndepend on the amount of previously acquired knowledge at all. We verify\nexperimentally that AR is superior to state-of-the-art deep generative replay\nusing VAEs.\n","authors":["Alexander Krawczyk","Alexander Gepperth"],"pdf_url":"https://arxiv.org/pdf/2303.13157v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.11143v3","updated":"2023-03-23T09:42:26Z","published":"2022-07-12T06:59:13Z","title":"Towards Global Optimality in Cooperative MARL with the Transformation\n  And Distillation Framework","summary":"  Decentralized execution is one core demand in cooperative multi-agent\nreinforcement learning (MARL). Recently, most popular MARL algorithms have\nadopted decentralized policies to enable decentralized execution and use\ngradient descent as their optimizer. However, there is hardly any theoretical\nanalysis of these algorithms taking the optimization method into consideration,\nand we find that various popular MARL algorithms with decentralized policies\nare suboptimal in toy tasks when gradient descent is chosen as their\noptimization method. In this paper, we theoretically analyze two common classes\nof algorithms with decentralized policies -- multi-agent policy gradient\nmethods and value-decomposition methods to prove their suboptimality when\ngradient descent is used. In addition, we propose the Transformation And\nDistillation (TAD) framework, which reformulates a multi-agent MDP as a special\nsingle-agent MDP with a sequential structure and enables decentralized\nexecution by distilling the learned policy on the derived ``single-agent\" MDP.\nThis approach uses a two-stage learning paradigm to address the optimization\nproblem in cooperative MARL, maintaining its performance guarantee.\nEmpirically, we implement TAD-PPO based on PPO, which can theoretically perform\noptimal policy learning in the finite multi-agent MDPs and shows significant\noutperformance on a large set of cooperative multi-agent tasks.\n","authors":["Jianing Ye","Chenghao Li","Jianhao Wang","Chongjie Zhang"],"pdf_url":"https://arxiv.org/pdf/2207.11143v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13137v1","updated":"2023-03-23T09:38:52Z","published":"2023-03-23T09:38:52Z","title":"FedGH: Heterogeneous Federated Learning with Generalized Global Header","summary":"  Federated learning (FL) is an emerging machine learning paradigm that allows\nmultiple parties to train a shared model collaboratively in a\nprivacy-preserving manner. Existing horizontal FL methods generally assume that\nthe FL server and clients hold the same model structure. However, due to system\nheterogeneity and the need for personalization, enabling clients to hold models\nwith diverse structures has become an important direction. Existing\nmodel-heterogeneous FL approaches often require publicly available datasets and\nincur high communication and/or computational costs, which limit their\nperformances. To address these limitations, we propose the Federated Global\nprediction Header (FedGH) approach. It is a communication and\ncomputation-efficient model-heterogeneous FL framework which trains a shared\ngeneralized global prediction header with representations extracted by\nheterogeneous extractors for clients' models at the FL server. The trained\ngeneralized global prediction header learns from different clients. The\nacquired global knowledge is then transferred to clients to substitute each\nclient's local prediction header. We derive the non-convex convergence rate of\nFedGH. Extensive experiments on two real-world datasets demonstrate that FedGH\nachieves significantly more advantageous performance in both model-homogeneous\nand -heterogeneous FL scenarios compared to seven state-of-the-art personalized\nFL models, beating the best-performing baseline by up to 8.87% (for\nmodel-homogeneous FL) and 1.83% (for model-heterogeneous FL) in terms of\naverage test accuracy, while saving up to 85.53% of communication overhead.\n","authors":["Liping Yi","Gang Wang","Xiaoguang Liu","Zhuan Shi","Han Yu"],"pdf_url":"https://arxiv.org/pdf/2303.13137v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13123v1","updated":"2023-03-23T09:23:57Z","published":"2023-03-23T09:23:57Z","title":"Laplacian Segmentation Networks: Improved Epistemic Uncertainty from\n  Spatial Aleatoric Uncertainty","summary":"  Out of distribution (OOD) medical images are frequently encountered, e.g.\nbecause of site- or scanner differences, or image corruption. OOD images come\nwith a risk of incorrect image segmentation, potentially negatively affecting\ndownstream diagnoses or treatment. To ensure robustness to such incorrect\nsegmentations, we propose Laplacian Segmentation Networks (LSN) that jointly\nmodel epistemic (model) and aleatoric (data) uncertainty in image segmentation.\nWe capture data uncertainty with a spatially correlated logit distribution. For\nmodel uncertainty, we propose the first Laplace approximation of the weight\nposterior that scales to large neural networks with skip connections that have\nhigh-dimensional outputs. Empirically, we demonstrate that modelling spatial\npixel correlation allows the Laplacian Segmentation Network to successfully\nassign high epistemic uncertainty to out-of-distribution objects appearing\nwithin images.\n","authors":["Kilian Zepf","Selma Wanna","Marco Miani","Juston Moore","Jes Frellsen","Søren Hauberg","Aasa Feragen","Frederik Warburg"],"pdf_url":"https://arxiv.org/pdf/2303.13123v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10365v2","updated":"2023-03-23T09:19:56Z","published":"2023-03-18T08:48:16Z","title":"CroSel: Cross Selection of Confident Pseudo Labels for Partial-Label\n  Learning","summary":"  Partial-label learning (PLL) is an important weakly supervised learning\nproblem, which allows each training example to have a candidate label set\ninstead of a single ground-truth label. Identification-based methods have been\nwidely explored to tackle label ambiguity issues in PLL, which regard the true\nlabel as a latent variable to be identified. However, identifying the true\nlabels accurately and completely remains challenging, causing noise in pseudo\nlabels during model training. In this paper, we propose a new method called\nCroSel, which leverages historical prediction information from models to\nidentify true labels for most training examples. First, we introduce a cross\nselection strategy, which enables two deep models to select true labels of\npartially labeled data for each other. Besides, we propose a novel consistent\nregularization term called co-mix to avoid sample waste and tiny noise caused\nby false selection. In this way, CroSel can pick out the true labels of most\nexamples with high precision. Extensive experiments demonstrate the superiority\nof CroSel, which consistently outperforms previous state-of-the-art methods on\nbenchmark datasets. Additionally, our method achieves over 90\\% accuracy and\nquantity for selecting true labels on CIFAR-type datasets under various\nsettings.\n","authors":["Shiyu Tian","Hongxin Wei","Yiqun Wang","Lei Feng"],"pdf_url":"https://arxiv.org/pdf/2303.10365v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13117v1","updated":"2023-03-23T09:07:30Z","published":"2023-03-23T09:07:30Z","title":"RLOR: A Flexible Framework of Deep Reinforcement Learning for Operation\n  Research","summary":"  Reinforcement learning has been applied in operation research and has shown\npromise in solving large combinatorial optimization problems. However, existing\nworks focus on developing neural network architectures for certain problems.\nThese works lack the flexibility to incorporate recent advances in\nreinforcement learning, as well as the flexibility of customizing model\narchitectures for operation research problems. In this work, we analyze the\nend-to-end autoregressive models for vehicle routing problems and show that\nthese models can benefit from the recent advances in reinforcement learning\nwith a careful re-implementation of the model architecture. In particular, we\nre-implemented the Attention Model and trained it with Proximal Policy\nOptimization (PPO) in CleanRL, showing at least 8 times speed up in training\ntime. We hereby introduce RLOR, a flexible framework for Deep Reinforcement\nLearning for Operation Research. We believe that a flexible framework is key to\ndeveloping deep reinforcement learning models for operation research problems.\nThe code of our work is publicly available at https://github.com/cpwan/RLOR.\n","authors":["Ching Pui Wan","Tung Li","Jason Min Wang"],"pdf_url":"https://arxiv.org/pdf/2303.13117v1.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2303.11789v2","updated":"2023-03-23T09:05:07Z","published":"2023-03-20T08:37:08Z","title":"Random Inverse Problems Over Graphs: Decentralized Online Learning","summary":"  We establish a framework of random inverse problems with real-time\nobservations over graphs, and present a decentralized online learning algorithm\nbased on online data streams, which unifies the distributed parameter\nestimation in Hilbert space and the least mean square problem in reproducing\nkernel Hilbert space (RKHS-LMS). We transform the algorithm convergence into\nthe asymptotic stability of randomly time-varying difference equations in\nHilbert space with L2-bounded martingale difference terms and develop the L2\n-asymptotic stability theory. It is shown that if the network graph is\nconnected and the sequence of forward operators satisfies the\ninfinitedimensional spatio-temporal persistence of excitation condition, then\nthe estimates of all nodes are mean square and almost surely strongly\nconsistent. By equivalently transferring the distributed learning problem in\nRKHS to the random inverse problem over graphs, we propose a decentralized\nonline learning algorithm in RKHS based on non-stationary and non-independent\nonline data streams, and prove that the algorithm is mean square and almost\nsurely strongly consistent if the operators induced by the random input data\nsatisfy the infinite-dimensional spatio-temporal persistence of excitation\ncondition.\n","authors":["Tao Li","Xiwei Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.11789v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13113v1","updated":"2023-03-23T09:00:38Z","published":"2023-03-23T09:00:38Z","title":"Adaptive Regularization for Class-Incremental Learning","summary":"  Class-Incremental Learning updates a deep classifier with new categories\nwhile maintaining the previously observed class accuracy. Regularizing the\nneural network weights is a common method to prevent forgetting previously\nlearned classes while learning novel ones. However, existing regularizers use a\nconstant magnitude throughout the learning sessions, which may not reflect the\nvarying levels of difficulty of the tasks encountered during incremental\nlearning. This study investigates the necessity of adaptive regularization in\nClass-Incremental Learning, which dynamically adjusts the regularization\nstrength according to the complexity of the task at hand. We propose a Bayesian\nOptimization-based approach to automatically determine the optimal\nregularization magnitude for each learning task. Our experiments on two\ndatasets via two regularizers demonstrate the importance of adaptive\nregularization for achieving accurate and less forgetful visual incremental\nlearning.\n","authors":["Elif Ceren Gok","Murat Onur Yildirim","Mert Kilickaya","Joaquin Vanschoren"],"pdf_url":"https://arxiv.org/pdf/2303.13113v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13102v1","updated":"2023-03-23T08:35:56Z","published":"2023-03-23T08:35:56Z","title":"Keypoint-Guided Optimal Transport","summary":"  Existing Optimal Transport (OT) methods mainly derive the optimal transport\nplan/matching under the criterion of transport cost/distance minimization,\nwhich may cause incorrect matching in some cases. In many applications,\nannotating a few matched keypoints across domains is reasonable or even\neffortless in annotation burden. It is valuable to investigate how to leverage\nthe annotated keypoints to guide the correct matching in OT. In this paper, we\npropose a novel KeyPoint-Guided model by ReLation preservation (KPG-RL) that\nsearches for the optimal matching (i.e., transport plan) guided by the\nkeypoints in OT. To impose the keypoints in OT, first, we propose a mask-based\nconstraint of the transport plan that preserves the matching of keypoint pairs.\nSecond, we propose to preserve the relation of each data point to the keypoints\nto guide the matching. The proposed KPG-RL model can be solved by Sinkhorn's\nalgorithm and is applicable even when distributions are supported in different\nspaces. We further utilize the relation preservation constraint in the\nKantorovich Problem and Gromov-Wasserstein model to impose the guidance of\nkeypoints in them. Meanwhile, the proposed KPG-RL model is extended to the\npartial OT setting. Moreover, we deduce the dual formulation of the KPG-RL\nmodel, which is solved using deep learning techniques. Based on the learned\ntransport plan from dual KPG-RL, we propose a novel manifold barycentric\nprojection to transport source data to the target domain. As applications, we\napply the proposed KPG-RL model to the heterogeneous domain adaptation and\nimage-to-image translation. Experiments verified the effectiveness of the\nproposed approach.\n","authors":["Xiang Gu","Yucheng Yang","Wei Zeng","Jian Sun","Zongben Xu"],"pdf_url":"https://arxiv.org/pdf/2303.13102v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2211.02420v2","updated":"2023-03-23T08:31:34Z","published":"2022-11-04T12:54:06Z","title":"Spatially Selective Deep Non-linear Filters for Speaker Extraction","summary":"  In a scenario with multiple persons talking simultaneously, the spatial\ncharacteristics of the signals are the most distinct feature for extracting the\ntarget signal. In this work, we develop a deep joint spatial-spectral\nnon-linear filter that can be steered in an arbitrary target direction. For\nthis we propose a simple and effective conditioning mechanism, which sets the\ninitial state of the filter's recurrent layers based on the target direction.\nWe show that this scheme is more effective than the baseline approach and\nincreases the flexibility of the filter at no performance cost. The resulting\nspatially selective non-linear filters can also be used for speech separation\nof an arbitrary number of speakers and enable very accurate multi-speaker\nlocalization as we demonstrate in this paper.\n","authors":["Kristina Tesch","Timo Gerkmann"],"pdf_url":"https://arxiv.org/pdf/2211.02420v2.pdf","comment":"\\copyright 2023 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works"},{"id":"http://arxiv.org/abs/2303.13093v1","updated":"2023-03-23T08:17:10Z","published":"2023-03-23T08:17:10Z","title":"The Probabilistic Stability of Stochastic Gradient Descent","summary":"  A fundamental open problem in deep learning theory is how to define and\nunderstand the stability of stochastic gradient descent (SGD) close to a fixed\npoint. Conventional literature relies on the convergence of statistical\nmoments, esp., the variance, of the parameters to quantify the stability. We\nrevisit the definition of stability for SGD and use the \\textit{convergence in\nprobability} condition to define the \\textit{probabilistic stability} of SGD.\nThe proposed stability directly answers a fundamental question in deep learning\ntheory: how SGD selects a meaningful solution for a neural network from an\nenormous number of solutions that may overfit badly. To achieve this, we show\nthat only under the lens of probabilistic stability does SGD exhibit rich and\npractically relevant phases of learning, such as the phases of the complete\nloss of stability, incorrect learning, convergence to low-rank saddles, and\ncorrect learning. When applied to a neural network, these phase diagrams imply\nthat SGD prefers low-rank saddles when the underlying gradient is noisy,\nthereby improving the learning performance. This result is in sharp contrast to\nthe conventional wisdom that SGD prefers flatter minima to sharp ones, which we\nfind insufficient to explain the experimental data. We also prove that the\nprobabilistic stability of SGD can be quantified by the Lyapunov exponents of\nthe SGD dynamics, which can easily be measured in practice. Our work\npotentially opens a new venue for addressing the fundamental question of how\nthe learning algorithm affects the learning outcome in deep learning.\n","authors":["Liu Ziyin","Botao Li","Tomer Galanti","Masahito Ueda"],"pdf_url":"https://arxiv.org/pdf/2303.13093v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2201.12358v2","updated":"2023-03-23T08:16:42Z","published":"2022-01-28T10:06:39Z","title":"Hypothesis Testing for Unknown Dynamical Systems and System Anomaly\n  Detection via Autoencoders","summary":"  We study the hypothesis testing problem for unknown dynamical systems. More\nspecifically, we observe sequential input and output data from a dynamical\nsystem with unknown parameters, and we aim to determine whether the collected\ndata is from a null distribution. Such a problem can have many applications.\nHere we formulate anomaly detection as hypothesis testing where the anomaly is\ndefined through the alternative hypothesis. Consequently, hypothesis testing\nalgorithms can detect faults in real-world systems such as robots, weather,\nenergy systems, and stock markets. Although recent works achieved\nstate-of-the-art performances in these tasks with deep learning models, we show\nthat a careful analysis using hypothesis testing and graphical models can not\nonly justify the effectiveness of autoencoder models, but also lead to a novel\nneural network design, termed DyAD (DYnamical system Anomaly Detection), with\nimproved performances. We then show that DyAD achieves state-of-the-art\nperformance on several existing datasets and a new dataset on battery anomaly\ndetection in electric vehicles.\n","authors":["Haowei He","Jingzhao Zhang","Yanan Wang","Benben Jiang","Shaobo Huang","Chen Wang","Yang Zhang","Xuebing Han","Dongxu Guo","Guannan He","Minggao Ouyang"],"pdf_url":"https://arxiv.org/pdf/2201.12358v2.pdf","comment":"16 pages, 6 figures"},{"id":"http://arxiv.org/abs/2303.13089v1","updated":"2023-03-23T08:06:10Z","published":"2023-03-23T08:06:10Z","title":"Box-Level Active Detection","summary":"  Active learning selects informative samples for annotation within budget,\nwhich has proven efficient recently on object detection. However, the widely\nused active detection benchmarks conduct image-level evaluation, which is\nunrealistic in human workload estimation and biased towards crowded images.\nFurthermore, existing methods still perform image-level annotation, but equally\nscoring all targets within the same image incurs waste of budget and redundant\nlabels. Having revealed above problems and limitations, we introduce a\nbox-level active detection framework that controls a box-based budget per\ncycle, prioritizes informative targets and avoids redundancy for fair\ncomparison and efficient application.\n  Under the proposed box-level setting, we devise a novel pipeline, namely\nComplementary Pseudo Active Strategy (ComPAS). It exploits both human\nannotations and the model intelligence in a complementary fashion: an efficient\ninput-end committee queries labels for informative objects only; meantime\nwell-learned targets are identified by the model and compensated with\npseudo-labels. ComPAS consistently outperforms 10 competitors under 4 settings\nin a unified codebase. With supervision from labeled data only, it achieves\n100% supervised performance of VOC0712 with merely 19% box annotations. On the\nCOCO dataset, it yields up to 4.3% mAP improvement over the second-best method.\nComPAS also supports training with the unlabeled pool, where it surpasses 90%\nCOCO supervised performance with 85% label reduction. Our source code is\npublicly available at https://github.com/lyumengyao/blad.\n","authors":["Mengyao Lyu","Jundong Zhou","Hui Chen","Yijie Huang","Dongdong Yu","Yaqian Li","Yandong Guo","Yuchen Guo","Liuyu Xiang","Guiguang Ding"],"pdf_url":"https://arxiv.org/pdf/2303.13089v1.pdf","comment":"CVPR 2023 highlight"},{"id":"http://arxiv.org/abs/2303.12497v2","updated":"2023-03-23T07:49:14Z","published":"2023-03-22T12:09:12Z","title":"Lower Bound on the Bayesian Risk via Information Measures","summary":"  This paper focuses on parameter estimation and introduces a new method for\nlower bounding the Bayesian risk. The method allows for the use of virtually\n\\emph{any} information measure, including R\\'enyi's $\\alpha$,\n$\\varphi$-Divergences, and Sibson's $\\alpha$-Mutual Information. The approach\nconsiders divergences as functionals of measures and exploits the duality\nbetween spaces of measures and spaces of functions. In particular, we show that\none can lower bound the risk with any information measure by upper bounding its\ndual via Markov's inequality. We are thus able to provide estimator-independent\nimpossibility results thanks to the Data-Processing Inequalities that\ndivergences satisfy. The results are then applied to settings of interest\ninvolving both discrete and continuous parameters, including the\n``Hide-and-Seek'' problem, and compared to the state-of-the-art techniques. An\nimportant observation is that the behaviour of the lower bound in the number of\nsamples is influenced by the choice of the information measure. We leverage\nthis by introducing a new divergence inspired by the ``Hockey-Stick''\nDivergence, which is demonstrated empirically to provide the largest\nlower-bound across all considered settings. If the observations are subject to\nprivatisation, stronger impossibility results can be obtained via Strong\nData-Processing Inequalities. The paper also discusses some generalisations and\nalternative directions.\n","authors":["Amedeo Roberto Esposito","Adrien Vandenbroucque","Michael Gastpar"],"pdf_url":"https://arxiv.org/pdf/2303.12497v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.05018v2","updated":"2023-03-23T07:20:34Z","published":"2023-03-09T03:40:34Z","title":"Improved Regret Bounds for Online Kernel Selection under Bandit Feedback","summary":"  In this paper, we improve the regret bound for online kernel selection under\nbandit feedback. Previous algorithm enjoys a $O((\\Vert\nf\\Vert^2_{\\mathcal{H}_i}+1)K^{\\frac{1}{3}}T^{\\frac{2}{3}})$ expected bound for\nLipschitz loss functions. We prove two types of regret bounds improving the\nprevious bound. For smooth loss functions, we propose an algorithm with a\n$O(U^{\\frac{2}{3}}K^{-\\frac{1}{3}}(\\sum^K_{i=1}L_T(f^\\ast_i))^{\\frac{2}{3}})$\nexpected bound where $L_T(f^\\ast_i)$ is the cumulative losses of optimal\nhypothesis in $\\mathbb{H}_{i}=\\{f\\in\\mathcal{H}_i:\\Vert\nf\\Vert_{\\mathcal{H}_i}\\leq U\\}$. The data-dependent bound keeps the previous\nworst-case bound and is smaller if most of candidate kernels match well with\nthe data. For Lipschitz loss functions, we propose an algorithm with a\n$O(U\\sqrt{KT}\\ln^{\\frac{2}{3}}{T})$ expected bound asymptotically improving the\nprevious bound. We apply the two algorithms to online kernel selection with\ntime constraint and prove new regret bounds matching or improving the previous\n$O(\\sqrt{T\\ln{K}} +\\Vert\nf\\Vert^2_{\\mathcal{H}_i}\\max\\{\\sqrt{T},\\frac{T}{\\sqrt{\\mathcal{R}}}\\})$\nexpected bound where $\\mathcal{R}$ is the time budget. Finally, we empirically\nverify our algorithms on online regression and classification tasks.\n","authors":["Junfan Li","Shizhong Liao"],"pdf_url":"https://arxiv.org/pdf/2303.05018v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13080v1","updated":"2023-03-23T07:18:08Z","published":"2023-03-23T07:18:08Z","title":"MSAT: Biologically Inspired Multi-Stage Adaptive Threshold for\n  Conversion of Spiking Neural Networks","summary":"  Spiking Neural Networks (SNNs) can do inference with low power consumption\ndue to their spike sparsity. ANN-SNN conversion is an efficient way to achieve\ndeep SNNs by converting well-trained Artificial Neural Networks (ANNs).\nHowever, the existing methods commonly use constant threshold for conversion,\nwhich prevents neurons from rapidly delivering spikes to deeper layers and\ncauses high time delay. In addition, the same response for different inputs may\nresult in information loss during the information transmission. Inspired by the\nbiological model mechanism, we propose a multi-stage adaptive threshold (MSAT).\nSpecifically, for each neuron, the dynamic threshold varies with firing history\nand input properties and is positively correlated with the average membrane\npotential and negatively correlated with the rate of depolarization. The\nself-adaptation to membrane potential and input allows a timely adjustment of\nthe threshold to fire spike faster and transmit more information. Moreover, we\nanalyze the Spikes of Inactivated Neurons error which is pervasive in early\ntime steps and propose spike confidence accordingly as a measurement of\nconfidence about the neurons that correctly deliver spikes. We use such spike\nconfidence in early time steps to determine whether to elicit spike to\nalleviate this error. Combined with the proposed method, we examine the\nperformance on non-trivial datasets CIFAR-10, CIFAR-100, and ImageNet. We also\nconduct sentiment classification and speech recognition experiments on the IDBM\nand Google speech commands datasets respectively. Experiments show\nnear-lossless and lower latency ANN-SNN conversion. To the best of our\nknowledge, this is the first time to build a biologically inspired multi-stage\nadaptive threshold for converted SNN, with comparable performance to\nstate-of-the-art methods while improving energy efficiency.\n","authors":["Xiang He","Yang Li","Dongcheng Zhao","Qingqun Kong","Yi Zeng"],"pdf_url":"https://arxiv.org/pdf/2303.13080v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08431v2","updated":"2023-03-23T07:02:17Z","published":"2023-03-15T08:08:02Z","title":"Policy Gradient Converges to the Globally Optimal Policy for Nearly\n  Linear-Quadratic Regulators","summary":"  Nonlinear control systems with partial information to the decision maker are\nprevalent in a variety of applications. As a step toward studying such\nnonlinear systems, this work explores reinforcement learning methods for\nfinding the optimal policy in the nearly linear-quadratic regulator systems. In\nparticular, we consider a dynamic system that combines linear and nonlinear\ncomponents, and is governed by a policy with the same structure. Assuming that\nthe nonlinear component comprises kernels with small Lipschitz coefficients, we\ncharacterize the optimization landscape of the cost function. Although the cost\nfunction is nonconvex in general, we establish the local strong convexity and\nsmoothness in the vicinity of the global optimizer. Additionally, we propose an\ninitialization mechanism to leverage these properties. Building on the\ndevelopments, we design a policy gradient algorithm that is guaranteed to\nconverge to the globally optimal policy with a linear rate.\n","authors":["Yinbin Han","Meisam Razaviyayn","Renyuan Xu"],"pdf_url":"https://arxiv.org/pdf/2303.08431v2.pdf","comment":"32 pages"},{"id":"http://arxiv.org/abs/2112.15287v5","updated":"2023-03-23T06:44:25Z","published":"2021-12-31T03:59:37Z","title":"Distributed Random Reshuffling over Networks","summary":"  In this paper, we consider distributed optimization problems where $n$\nagents, each possessing a local cost function, collaboratively minimize the\naverage of the local cost functions over a connected network. To solve the\nproblem, we propose a distributed random reshuffling (D-RR) algorithm that\ninvokes the random reshuffling (RR) update in each agent. We show that D-RR\ninherits favorable characteristics of RR for both smooth strongly convex and\nsmooth nonconvex objective functions. In particular, for smooth strongly convex\nobjective functions, D-RR achieves $\\mathcal{O}(1/T^2)$ rate of convergence\n(where $T$ counts epoch number) in terms of the squared distance between the\niterate and the global minimizer. When the objective function is assumed to be\nsmooth nonconvex, we show that D-RR drives the squared norm of gradient to $0$\nat a rate of $\\mathcal{O}(1/T^{2/3})$. These convergence results match those of\ncentralized RR (up to constant factors) and outperform the distributed\nstochastic gradient descent (DSGD) algorithm if we run a relatively large\nnumber of epochs. Finally, we conduct a set of numerical experiments to\nillustrate the efficiency of the proposed D-RR method on both strongly convex\nand nonconvex distributed optimization problems.\n","authors":["Kun Huang","Xiao Li","Andre Milzarek","Shi Pu","Junwen Qiu"],"pdf_url":"https://arxiv.org/pdf/2112.15287v5.pdf","comment":"20 pages, 13 figures"},{"id":"http://arxiv.org/abs/2208.08888v3","updated":"2023-03-23T06:35:45Z","published":"2022-08-15T12:33:09Z","title":"POCS-based Clustering Algorithm","summary":"  A novel clustering technique based on the projection onto convex set (POCS)\nmethod, called POCS-based clustering algorithm, is proposed in this paper. The\nproposed POCS-based clustering algorithm exploits a parallel projection method\nof POCS to find appropriate cluster prototypes in the feature space. The\nalgorithm considers each data point as a convex set and projects the cluster\nprototypes parallelly to the member data points. The projections are convexly\ncombined to minimize the objective function for data clustering purpose. The\nperformance of the proposed POCS-based clustering algorithm is verified through\nexperiments on various synthetic datasets. The experimental results show that\nthe proposed POCS-based clustering algorithm is competitive and efficient in\nterms of clustering error and execution speed when compared with other\nconventional clustering methods including Fuzzy C-Means (FCM) and K-means\nclustering algorithms.\n","authors":["Le-Anh Tran","Henock M. Deberneh","Truong-Dong Do","Thanh-Dat Nguyen","My-Ha Le","Dong-Chul Park"],"pdf_url":"https://arxiv.org/pdf/2208.08888v3.pdf","comment":"6 pages, 4 figures, IWIS 2022"},{"id":"http://arxiv.org/abs/2303.13056v1","updated":"2023-03-23T06:04:36Z","published":"2023-03-23T06:04:36Z","title":"Predicting the Initial Conditions of the Universe using Deep Learning","summary":"  Finding the initial conditions that led to the current state of the universe\nis challenging because it involves searching over a vast input space of initial\nconditions, along with modeling their evolution via tools such as N-body\nsimulations which are computationally expensive. Deep learning has emerged as\nan alternate modeling tool that can learn the mapping between the linear input\nof an N-body simulation and the final nonlinear displacements at redshift zero,\nwhich can significantly accelerate the forward modeling. However, this does not\nhelp reduce the search space for initial conditions. In this paper, we\ndemonstrate for the first time that a deep learning model can be trained for\nthe reverse mapping. We train a V-Net based convolutional neural network, which\noutputs the linear displacement of an N-body system, given the current time\nnonlinear displacement and the cosmological parameters of the system. We\ndemonstrate that this neural network accurately recovers the initial linear\ndisplacement field over a wide range of scales ($<1$-$2\\%$ error up to nearly\n$k = 1\\ \\mathrm{Mpc}^{-1}\\,h$), despite the ill-defined nature of the inverse\nproblem at smaller scales. Specifically, smaller scales are dominated by\nnonlinear effects which makes the backward dynamics much more susceptible to\nnumerical and computational errors leading to highly divergent backward\ntrajectories and a one-to-many backward mapping. The results of our method\nmotivate that neural network based models can act as good approximators of the\ninitial linear states and their predictions can serve as good starting points\nfor sampling-based methods to infer the initial states of the universe.\n","authors":["Vaibhav Jindal","Drew Jamieson","Albert Liang","Aarti Singh","Shirley Ho"],"pdf_url":"https://arxiv.org/pdf/2303.13056v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13055v1","updated":"2023-03-23T05:59:56Z","published":"2023-03-23T05:59:56Z","title":"Reimagining Application User Interface (UI) Design using Deep Learning\n  Methods: Challenges and Opportunities","summary":"  In this paper, we present a review of the recent work in deep learning\nmethods for user interface design. The survey encompasses well known deep\nlearning techniques (deep neural networks, convolutional neural networks,\nrecurrent neural networks, autoencoders, and generative adversarial networks)\nand datasets widely used to design user interface applications. We highlight\nimportant problems and emerging research frontiers in this field. We believe\nthat the use of deep learning for user interface design automation tasks could\nbe one of the high potential fields for the advancement of the software\ndevelopment industry.\n","authors":["Subtain Malik","Muhammad Tariq Saeed","Marya Jabeen Zia","Shahzad Rasool","Liaquat Ali Khan","Mian Ilyas Ahmed"],"pdf_url":"https://arxiv.org/pdf/2303.13055v1.pdf","comment":"A review paper on studies of UI design techniques and deep learning"},{"id":"http://arxiv.org/abs/2303.12414v2","updated":"2023-03-23T05:59:55Z","published":"2023-03-22T09:23:29Z","title":"Delay-Aware Hierarchical Federated Learning","summary":"  Federated learning has gained popularity as a means of training models\ndistributed across the wireless edge. The paper introduces delay-aware\nfederated learning (DFL) to improve the efficiency of distributed machine\nlearning (ML) model training by addressing communication delays between edge\nand cloud. DFL employs multiple stochastic gradient descent iterations on\ndevice datasets during each global aggregation interval and intermittently\naggregates model parameters through edge servers in local subnetworks. The\ncloud server synchronizes the local models with the global deployed model\ncomputed via a local-global combiner at global synchronization. The convergence\nbehavior of DFL is theoretically investigated under a generalized data\nheterogeneity metric. A set of conditions is obtained to achieve the sub-linear\nconvergence rate of O(1/k). Based on these findings, an adaptive control\nalgorithm is developed for DFL, implementing policies to mitigate energy\nconsumption and edge-to-cloud communication latency while aiming for a\nsublinear convergence rate. Numerical evaluations show DFL's superior\nperformance in terms of faster global model convergence, reduced resource\nconsumption, and robustness against communication delays compared to existing\nFL algorithms. In summary, this proposed method offers improved efficiency and\nsatisfactory results when dealing with both convex and non-convex loss\nfunctions.\n","authors":["Frank Po-Chen Lin","Seyyedali Hosseinalipour","Christopher Brinton","Nicolò Michelusi"],"pdf_url":"https://arxiv.org/pdf/2303.12414v2.pdf","comment":"A condensed version of this paper was presented at IEEE Globecom 2020"},{"id":"http://arxiv.org/abs/2210.01919v2","updated":"2023-03-23T05:32:44Z","published":"2022-10-04T21:42:06Z","title":"Convex and Nonconvex Sublinear Regression with Application to\n  Data-driven Learning of Reach Sets","summary":"  We consider estimating a compact set from finite data by approximating the\nsupport function of that set via sublinear regression. Support functions\nuniquely characterize a compact set up to closure of convexification, and are\nsublinear (convex as well as positive homogeneous of degree one). Conversely,\nany sublinear function is the support function of a compact set. We leverage\nthis property to transcribe the task of learning a compact set to that of\nlearning its support function. We propose two algorithms to perform the\nsublinear regression, one via convex and another via nonconvex programming. The\nconvex programming approach involves solving a quadratic program (QP). The\nnonconvex programming approach involves training a input sublinear neural\nnetwork. We illustrate the proposed methods via numerical examples on learning\nthe reach sets of controlled dynamics subject to set-valued input uncertainties\nfrom trajectory data.\n","authors":["Shadi Haddad","Abhishek Halder"],"pdf_url":"https://arxiv.org/pdf/2210.01919v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13047v1","updated":"2023-03-23T05:27:32Z","published":"2023-03-23T05:27:32Z","title":"Towards Better Dynamic Graph Learning: New Architecture and Unified\n  Library","summary":"  We propose DyGFormer, a new Transformer-based architecture for dynamic graph\nlearning that solely learns from the sequences of nodes' historical first-hop\ninteractions. DyGFormer incorporates two distinct designs: a neighbor\nco-occurrence encoding scheme that explores the correlations of the source node\nand destination node based on their sequences; a patching technique that\ndivides each sequence into multiple patches and feeds them to Transformer,\nallowing the model to effectively and efficiently benefit from longer\nhistories. We also introduce DyGLib, a unified library with standard training\npipelines, extensible coding interfaces, and comprehensive evaluating protocols\nto promote reproducible, scalable, and credible dynamic graph learning\nresearch. By performing extensive experiments on thirteen datasets from various\ndomains for transductive/inductive dynamic link prediction and dynamic node\nclassification tasks, we observe that: DyGFormer achieves state-of-the-art\nperformance on most of the datasets, demonstrating the effectiveness of\ncapturing nodes' correlations and long-term temporal dependencies; the results\nof baselines vary across different datasets and some findings are inconsistent\nwith previous reports, which may be caused by their diverse pipelines and\nproblematic implementations. We hope our work can provide new insights and\nfacilitate the development of the dynamic graph learning field. All the\nresources including datasets, data loaders, algorithms, and executing scripts\nare publicly available at https://github.com/yule-BUAA/DyGLib.\n","authors":["Le Yu","Leilei Sun","Bowen Du","Weifeng Lv"],"pdf_url":"https://arxiv.org/pdf/2303.13047v1.pdf","comment":"24 pages"},{"id":"http://arxiv.org/abs/2303.13035v1","updated":"2023-03-23T04:47:46Z","published":"2023-03-23T04:47:46Z","title":"SPeC: A Soft Prompt-Based Calibration on Mitigating Performance\n  Variability in Clinical Notes Summarization","summary":"  Electronic health records (EHRs) store an extensive array of patient\ninformation, encompassing medical histories, diagnoses, treatments, and test\noutcomes. These records are crucial for enabling healthcare providers to make\nwell-informed decisions regarding patient care. Summarizing clinical notes\nfurther assists healthcare professionals in pinpointing potential health risks\nand making better-informed decisions. This process contributes to reducing\nerrors and enhancing patient outcomes by ensuring providers have access to the\nmost pertinent and current patient data. Recent research has shown that\nincorporating prompts with large language models (LLMs) substantially boosts\nthe efficacy of summarization tasks. However, we show that this approach also\nleads to increased output variance, resulting in notably divergent outputs even\nwhen prompts share similar meanings. To tackle this challenge, we introduce a\nmodel-agnostic Soft Prompt-Based Calibration (SPeC) pipeline that employs soft\nprompts to diminish variance while preserving the advantages of prompt-based\nsummarization. Experimental findings on multiple clinical note tasks and LLMs\nindicate that our method not only bolsters performance but also effectively\ncurbs variance for various LLMs, providing a more uniform and dependable\nsolution for summarizing vital medical information.\n","authors":["Yu-Neng Chuang","Ruixiang Tang","Xiaoqian Jiang","Xia Hu"],"pdf_url":"https://arxiv.org/pdf/2303.13035v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13034v1","updated":"2023-03-23T04:46:49Z","published":"2023-03-23T04:46:49Z","title":"Preference-Aware Constrained Multi-Objective Bayesian Optimization","summary":"  This paper addresses the problem of constrained multi-objective optimization\nover black-box objective functions with practitioner-specified preferences over\nthe objectives when a large fraction of the input space is infeasible (i.e.,\nviolates constraints). This problem arises in many engineering design problems\nincluding analog circuits and electric power system design. Our overall goal is\nto approximate the optimal Pareto set over the small fraction of feasible input\ndesigns. The key challenges include the huge size of the design space, multiple\nobjectives and large number of constraints, and the small fraction of feasible\ninput designs which can be identified only after performing expensive\nsimulations. We propose a novel and efficient preference-aware constrained\nmulti-objective Bayesian optimization approach referred to as PAC-MOO to\naddress these challenges. The key idea is to learn surrogate models for both\noutput objectives and constraints, and select the candidate input for\nevaluation in each iteration that maximizes the information gained about the\noptimal constrained Pareto front while factoring in the preferences over\nobjectives. Our experiments on two real-world analog circuit design\noptimization problems demonstrate the efficacy of PAC-MOO over prior methods.\n","authors":["Alaleh Ahmadianshalchi","Syrine Belakaria","Janardhan Rao Doppa"],"pdf_url":"https://arxiv.org/pdf/2303.13034v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2110.06980"},{"id":"http://arxiv.org/abs/2303.00141v2","updated":"2023-03-23T04:26:19Z","published":"2023-03-01T00:13:52Z","title":"Containing a spread through sequential learning: to exploit or to\n  explore?","summary":"  The spread of an undesirable contact process, such as an infectious disease\n(e.g. COVID-19), is contained through testing and isolation of infected nodes.\nThe temporal and spatial evolution of the process (along with containment\nthrough isolation) render such detection as fundamentally different from active\nsearch detection strategies. In this work, through an active learning approach,\nwe design testing and isolation strategies to contain the spread and minimize\nthe cumulative infections under a given test budget. We prove that the\nobjective can be optimized, with performance guarantees, by greedily selecting\nthe nodes to test. We further design reward-based methodologies that\neffectively minimize an upper bound on the cumulative infections and are\ncomputationally more tractable in large networks. These policies, however, need\nknowledge about the nodes' infection probabilities which are dynamically\nchanging and have to be learned by sequential testing. We develop a\nmessage-passing framework for this purpose and, building on that, show novel\ntradeoffs between exploitation of knowledge through reward-based heuristics and\nexploration of the unknown through a carefully designed probabilistic testing.\nThe tradeoffs are fundamentally distinct from the classical counterparts under\nactive search or multi-armed bandit problems (MABs). We provably show the\nnecessity of exploration in a stylized network and show through simulations\nthat exploration can outperform exploitation in various synthetic and real-data\nnetworks depending on the parameters of the network and the spread.\n","authors":["Xingran Chen","Hesam Nikpey","Jungyeol Kim","Saswati Sarkar","Shirin Saeedi-Bidokhti"],"pdf_url":"https://arxiv.org/pdf/2303.00141v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13024v1","updated":"2023-03-23T04:16:00Z","published":"2023-03-23T04:16:00Z","title":"Self-Supervised Clustering of Multivariate Time-Series Data for\n  Identifying TBI Physiological States","summary":"  Determining clinically relevant physiological states from multivariate time\nseries data with missing values is essential for providing appropriate\ntreatment for acute conditions such as Traumatic Brain Injury (TBI),\nrespiratory failure, and heart failure. Utilizing non-temporal clustering or\ndata imputation and aggregation techniques may lead to loss of valuable\ninformation and biased analyses. In our study, we apply the SLAC-Time\nalgorithm, an innovative self-supervision-based approach that maintains data\nintegrity by avoiding imputation or aggregation, offering a more useful\nrepresentation of acute patient states. By using SLAC-Time to cluster data in a\nlarge research dataset, we identified three distinct TBI physiological states\nand their specific feature profiles. We employed various clustering evaluation\nmetrics and incorporated input from a clinical domain expert to validate and\ninterpret the identified physiological states. Further, we discovered how\nspecific clinical events and interventions can influence patient states and\nstate transitions.\n","authors":["Hamid Ghaderi","Brandon Foreman","Amin Nayebi","Sindhu Tipirneni","Chandan K. Reddy","Vignesh Subbian"],"pdf_url":"https://arxiv.org/pdf/2303.13024v1.pdf","comment":"10 pages, 7 figures, 2 tables"},{"id":"http://arxiv.org/abs/2303.13022v1","updated":"2023-03-23T04:12:07Z","published":"2023-03-23T04:12:07Z","title":"ENVIDR: Implicit Differentiable Renderer with Neural Environment\n  Lighting","summary":"  Recent advances in neural rendering have shown great potential for\nreconstructing scenes from multiview images. However, accurately representing\nobjects with glossy surfaces remains a challenge for existing methods. In this\nwork, we introduce ENVIDR, a rendering and modeling framework for high-quality\nrendering and reconstruction of surfaces with challenging specular reflections.\nTo achieve this, we first propose a novel neural renderer with decomposed\nrendering components to learn the interaction between surface and environment\nlighting. This renderer is trained using existing physically based renderers\nand is decoupled from actual scene representations. We then propose an\nSDF-based neural surface model that leverages this learned neural renderer to\nrepresent general scenes. Our model additionally synthesizes indirect\nilluminations caused by inter-reflections from shiny surfaces by marching\nsurface-reflected rays. We demonstrate that our method outperforms state-of-art\nmethods on challenging shiny scenes, providing high-quality rendering of\nspecular reflections while also enabling material editing and scene relighting.\n","authors":["Ruofan Liang","Huiting Chen","Chunlin Li","Fan Chen","Selvakumar Panneer","Nandita Vijaykumar"],"pdf_url":"https://arxiv.org/pdf/2303.13022v1.pdf","comment":"Project page: https://nexuslrf.github.io/ENVIDR/"},{"id":"http://arxiv.org/abs/2211.08573v5","updated":"2023-03-23T03:41:06Z","published":"2022-11-15T23:35:15Z","title":"Realization of Causal Representation Learning to Adjust Confounding Bias\n  in Latent Space","summary":"  Causal DAGs(Directed Acyclic Graphs) are usually considered in a 2D plane.\nEdges indicate causal effects' directions and imply their corresponding\ntime-passings. Due to the natural restriction of statistical models, effect\nestimation is usually approximated by averaging the individuals' correlations,\ni.e., observational changes over a specific time. However, in the context of\nMachine Learning on large-scale questions with complex DAGs, such slight biases\ncan snowball to distort global models - More importantly, it has practically\nimpeded the development of AI, for instance, the weak generalizability of\ncausal models. In this paper, we redefine causal DAG as \\emph{do-DAG}, in which\nvariables' values are no longer time-stamp-dependent, and timelines can be seen\nas axes. By geometric explanation of multi-dimensional do-DAG, we identify the\n\\emph{Causal Representation Bias} and its necessary factors, differentiated\nfrom common confounding biases. Accordingly, a DL(Deep Learning)-based\nframework will be proposed as the general solution, along with a realization\nmethod and experiments to verify its feasibility.\n","authors":["Jia Li","Xiang Li","Xiaowei Jia","Michael Steinbach","Vipin Kumar"],"pdf_url":"https://arxiv.org/pdf/2211.08573v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13015v1","updated":"2023-03-23T03:39:12Z","published":"2023-03-23T03:39:12Z","title":"Failure-tolerant Distributed Learning for Anomaly Detection in Wireless\n  Networks","summary":"  The analysis of distributed techniques is often focused upon their\nefficiency, without considering their robustness (or lack thereof). Such a\nconsideration is particularly important when devices or central servers can\nfail, which can potentially cripple distributed systems. When such failures\narise in wireless communications networks, important services that they\nuse/provide (like anomaly detection) can be left inoperable and can result in a\ncascade of security problems. In this paper, we present a novel method to\naddress these risks by combining both flat- and star-topologies, combining the\nperformance and reliability benefits of both. We refer to this method as\n\"Tol-FL\", due to its increased failure-tolerance as compared to the technique\nof Federated Learning. Our approach both limits device failure risks while\noutperforming prior methods by up to 8% in terms of anomaly detection AUROC in\na range of realistic settings that consider client as well as server failure,\nall while reducing communication costs. This performance demonstrates that\nTol-FL is a highly suitable method for distributed model training for anomaly\ndetection, especially in the domain of wireless networks.\n","authors":["Marc Katzef","Andrew C. Cullen","Tansu Alpcan","Christopher Leckie","Justin Kopacz"],"pdf_url":"https://arxiv.org/pdf/2303.13015v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.11488v3","updated":"2023-03-23T03:27:40Z","published":"2022-06-23T06:02:33Z","title":"On the Importance and Applicability of Pre-Training for Federated\n  Learning","summary":"  Pre-training is prevalent in nowadays deep learning to improve the learned\nmodel's performance. However, in the literature on federated learning (FL),\nneural networks are mostly initialized with random weights. These attract our\ninterest in conducting a systematic study to explore pre-training for FL.\nAcross multiple visual recognition benchmarks, we found that pre-training can\nnot only improve FL, but also close its accuracy gap to the counterpart\ncentralized learning, especially in the challenging cases of non-IID clients'\ndata. To make our findings applicable to situations where pre-trained models\nare not directly available, we explore pre-training with synthetic data or even\nwith clients' data in a decentralized manner, and found that they can already\nimprove FL notably. Interestingly, many of the techniques we explore are\ncomplementary to each other to further boost the performance, and we view this\nas a critical result toward scaling up deep FL for real-world applications. We\nconclude our paper with an attempt to understand the effect of pre-training on\nFL. We found that pre-training enables the learned global models under\ndifferent clients' data conditions to converge to the same loss basin, and\nmakes global aggregation in FL more stable. Nevertheless, pre-training seems to\nnot alleviate local model drifting, a fundamental problem in FL under non-IID\ndata.\n","authors":["Hong-You Chen","Cheng-Hao Tu","Ziwei Li","Han-Wei Shen","Wei-Lun Chao"],"pdf_url":"https://arxiv.org/pdf/2206.11488v3.pdf","comment":"Accepted to ICLR 2023"},{"id":"http://arxiv.org/abs/2303.13010v1","updated":"2023-03-23T03:13:04Z","published":"2023-03-23T03:13:04Z","title":"Semantic Image Attack for Visual Model Diagnosis","summary":"  In practice, metric analysis on a specific train and test dataset does not\nguarantee reliable or fair ML models. This is partially due to the fact that\nobtaining a balanced, diverse, and perfectly labeled dataset is typically\nexpensive, time-consuming, and error-prone. Rather than relying on a carefully\ndesigned test set to assess ML models' failures, fairness, or robustness, this\npaper proposes Semantic Image Attack (SIA), a method based on the adversarial\nattack that provides semantic adversarial images to allow model diagnosis,\ninterpretability, and robustness. Traditional adversarial training is a popular\nmethodology for robustifying ML models against attacks. However, existing\nadversarial methods do not combine the two aspects that enable the\ninterpretation and analysis of the model's flaws: semantic traceability and\nperceptual quality. SIA combines the two features via iterative gradient ascent\non a predefined semantic attribute space and the image space. We illustrate the\nvalidity of our approach in three scenarios for keypoint detection and\nclassification. (1) Model diagnosis: SIA generates a histogram of attributes\nthat highlights the semantic vulnerability of the ML model (i.e., attributes\nthat make the model fail). (2) Stronger attacks: SIA generates adversarial\nexamples with visually interpretable attributes that lead to higher attack\nsuccess rates than baseline methods. The adversarial training on SIA improves\nthe transferable robustness across different gradient-based attacks. (3)\nRobustness to imbalanced datasets: we use SIA to augment the underrepresented\nclasses, which outperforms strong augmentation and re-balancing baselines.\n","authors":["Jinqi Luo","Zhaoning Wang","Chen Henry Wu","Dong Huang","Fernando De la Torre"],"pdf_url":"https://arxiv.org/pdf/2303.13010v1.pdf","comment":"Initial version submitted to NeurIPS 2022"},{"id":"http://arxiv.org/abs/2303.13006v1","updated":"2023-03-23T03:02:09Z","published":"2023-03-23T03:02:09Z","title":"Controllable Inversion of Black-Box Face-Recognition Models via\n  Diffusion","summary":"  Face recognition models embed a face image into a low-dimensional identity\nvector containing abstract encodings of identity-specific facial features that\nallow individuals to be distinguished from one another. We tackle the\nchallenging task of inverting the latent space of pre-trained face recognition\nmodels without full model access (i.e. black-box setting). A variety of methods\nhave been proposed in literature for this task, but they have serious\nshortcomings such as a lack of realistic outputs, long inference times, and\nstrong requirements for the data set and accessibility of the face recognition\nmodel. Through an analysis of the black-box inversion problem, we show that the\nconditional diffusion model loss naturally emerges and that we can effectively\nsample from the inverse distribution even without an identity-specific loss.\nOur method, named identity denoising diffusion probabilistic model (ID3PM),\nleverages the stochastic nature of the denoising diffusion process to produce\nhigh-quality, identity-preserving face images with various backgrounds,\nlighting, poses, and expressions. We demonstrate state-of-the-art performance\nin terms of identity preservation and diversity both qualitatively and\nquantitatively. Our method is the first black-box face recognition model\ninversion method that offers intuitive control over the generation process and\ndoes not suffer from any of the common shortcomings from competing methods.\n","authors":["Manuel Kansy","Anton Raël","Graziana Mignone","Jacek Naruniec","Christopher Schroers","Markus Gross","Romann M. Weber"],"pdf_url":"https://arxiv.org/pdf/2303.13006v1.pdf","comment":"34 pages. Preprint. Under review"},{"id":"http://arxiv.org/abs/2303.13004v1","updated":"2023-03-23T02:58:14Z","published":"2023-03-23T02:58:14Z","title":"Adversarially Contrastive Estimation of Conditional Neural Processes","summary":"  Conditional Neural Processes~(CNPs) formulate distributions over functions\nand generate function observations with exact conditional likelihoods. CNPs,\nhowever, have limited expressivity for high-dimensional observations, since\ntheir predictive distribution is factorized into a product of unconstrained\n(typically) Gaussian outputs. Previously, this could be handled using latent\nvariables or autoregressive likelihood, but at the expense of intractable\ntraining and quadratically increased complexity. Instead, we propose\ncalibrating CNPs with an adversarial training scheme besides regular maximum\nlikelihood estimates. Specifically, we train an energy-based model (EBM) with\nnoise contrastive estimation, which enforces EBM to identify true observations\nfrom the generations of CNP. In this way, CNP must generate predictions closer\nto the ground-truth to fool EBM, instead of merely optimizing with respect to\nthe fixed-form likelihood. From generative function reconstruction to\ndownstream regression and classification tasks, we demonstrate that our method\nfits mainstream CNP members, showing effectiveness when unconstrained Gaussian\nlikelihood is defined, requiring minimal computation overhead while preserving\nfoundation properties of CNPs.\n","authors":["Zesheng Ye","Jing Du","Lina Yao"],"pdf_url":"https://arxiv.org/pdf/2303.13004v1.pdf","comment":"29 pages"},{"id":"http://arxiv.org/abs/2303.13003v1","updated":"2023-03-23T02:55:50Z","published":"2023-03-23T02:55:50Z","title":"Benchmarking the Reliability of Post-training Quantization: a Particular\n  Focus on Worst-case Performance","summary":"  Post-training quantization (PTQ) is a popular method for compressing deep\nneural networks (DNNs) without modifying their original architecture or\ntraining procedures. Despite its effectiveness and convenience, the reliability\nof PTQ methods in the presence of some extrem cases such as distribution shift\nand data noise remains largely unexplored. This paper first investigates this\nproblem on various commonly-used PTQ methods. We aim to answer several research\nquestions related to the influence of calibration set distribution variations,\ncalibration paradigm selection, and data augmentation or sampling strategies on\nPTQ reliability. A systematic evaluation process is conducted across a wide\nrange of tasks and commonly-used PTQ paradigms. The results show that most\nexisting PTQ methods are not reliable enough in term of the worst-case group\nperformance, highlighting the need for more robust methods. Our findings\nprovide insights for developing PTQ methods that can effectively handle\ndistribution shift scenarios and enable the deployment of quantized DNNs in\nreal-world applications.\n","authors":["Zhihang Yuan","Jiawei Liu","Jiaxiang Wu","Dawei Yang","Qiang Wu","Guangyu Sun","Wenyu Liu","Xinggang Wang","Bingzhe Wu"],"pdf_url":"https://arxiv.org/pdf/2303.13003v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13002v1","updated":"2023-03-23T02:51:50Z","published":"2023-03-23T02:51:50Z","title":"Planning Goals for Exploration","summary":"  Dropped into an unknown environment, what should an agent do to quickly learn\nabout the environment and how to accomplish diverse tasks within it? We address\nthis question within the goal-conditioned reinforcement learning paradigm, by\nidentifying how the agent should set its goals at training time to maximize\nexploration. We propose \"Planning Exploratory Goals\" (PEG), a method that sets\ngoals for each training episode to directly optimize an intrinsic exploration\nreward. PEG first chooses goal commands such that the agent's goal-conditioned\npolicy, at its current level of training, will end up in states with high\nexploration potential. It then launches an exploration policy starting at those\npromising states. To enable this direct optimization, PEG learns world models\nand adapts sampling-based planning algorithms to \"plan goal commands\". In\nchallenging simulated robotics environments including a multi-legged ant robot\nin a maze, and a robot arm on a cluttered tabletop, PEG exploration enables\nmore efficient and effective training of goal-conditioned policies relative to\nbaselines and ablations. Our ant successfully navigates a long maze, and the\nrobot arm successfully builds a stack of three blocks upon command. Website:\nhttps://penn-pal-lab.github.io/peg/\n","authors":["Edward S. Hu","Richard Chang","Oleh Rybkin","Dinesh Jayaraman"],"pdf_url":"https://arxiv.org/pdf/2303.13002v1.pdf","comment":"Camera Ready version for ICLR2023 Spotlight"},{"id":"http://arxiv.org/abs/2303.12999v1","updated":"2023-03-23T02:42:10Z","published":"2023-03-23T02:42:10Z","title":"Automated Federated Learning in Mobile Edge Networks -- Fast Adaptation\n  and Convergence","summary":"  Federated Learning (FL) can be used in mobile edge networks to train machine\nlearning models in a distributed manner. Recently, FL has been interpreted\nwithin a Model-Agnostic Meta-Learning (MAML) framework, which brings FL\nsignificant advantages in fast adaptation and convergence over heterogeneous\ndatasets. However, existing research simply combines MAML and FL without\nexplicitly addressing how much benefit MAML brings to FL and how to maximize\nsuch benefit over mobile edge networks. In this paper, we quantify the benefit\nfrom two aspects: optimizing FL hyperparameters (i.e., sampled data size and\nthe number of communication rounds) and resource allocation (i.e., transmit\npower) in mobile edge networks. Specifically, we formulate the MAML-based FL\ndesign as an overall learning time minimization problem, under the constraints\nof model accuracy and energy consumption. Facilitated by the convergence\nanalysis of MAML-based FL, we decompose the formulated problem and then solve\nit using analytical solutions and the coordinate descent method. With the\nobtained FL hyperparameters and resource allocation, we design a MAML-based FL\nalgorithm, called Automated Federated Learning (AutoFL), that is able to\nconduct fast adaptation and convergence. Extensive experimental results verify\nthat AutoFL outperforms other benchmark algorithms regarding the learning time\nand convergence performance.\n","authors":["Chaoqun You","Kun Guo","Gang Feng","Peng Yang","Tony Q. S. Quek"],"pdf_url":"https://arxiv.org/pdf/2303.12999v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12992v1","updated":"2023-03-23T02:09:18Z","published":"2023-03-23T02:09:18Z","title":"A Survey of Historical Learning: Learning Models with Learning History","summary":"  New knowledge originates from the old. The various types of elements,\ndeposited in the training history, are a large amount of wealth for improving\nlearning deep models. In this survey, we comprehensively review and summarize\nthe topic--``Historical Learning: Learning Models with Learning History'',\nwhich learns better neural models with the help of their learning history\nduring its optimization, from three detailed aspects: Historical Type (what),\nFunctional Part (where) and Storage Form (how). To our best knowledge, it is\nthe first survey that systematically studies the methodologies which make use\nof various historical statistics when training deep neural networks. The\ndiscussions with related topics like recurrent/memory networks, ensemble\nlearning, and reinforcement learning are demonstrated. We also expose future\nchallenges of this topic and encourage the community to pay attention to the\nthink of historical learning principles when designing algorithms. The paper\nlist related to historical learning is available at\n\\url{https://github.com/Martinser/Awesome-Historical-Learning.}\n","authors":["Xiang Li","Ge Wu","Lingfeng Yang","Wenhai Wang","Renjie Song","Jian Yang"],"pdf_url":"https://arxiv.org/pdf/2303.12992v1.pdf","comment":"Xiang Li and Ge Wu have equal contributions"},{"id":"http://arxiv.org/abs/2303.12982v1","updated":"2023-03-23T01:19:41Z","published":"2023-03-23T01:19:41Z","title":"Fault Prognosis of Turbofan Engines: Eventual Failure Prediction and\n  Remaining Useful Life Estimation","summary":"  In the era of industrial big data, prognostics and health management is\nessential to improve the prediction of future failures to minimize inventory,\nmaintenance, and human costs. Used for the 2021 PHM Data Challenge, the new\nCommercial Modular Aero-Propulsion System Simulation dataset from NASA is an\nopen-source benchmark containing simulated turbofan engine units flown under\nrealistic flight conditions. Deep learning approaches implemented previously\nfor this application attempt to predict the remaining useful life of the engine\nunits, but have not utilized labeled failure mode information, impeding\npractical usage and explainability. To address these limitations, a new\nprognostics approach is formulated with a customized loss function to\nsimultaneously predict the current health state, the eventual failing\ncomponent(s), and the remaining useful life. The proposed method incorporates\nprincipal component analysis to orthogonalize statistical time-domain features,\nwhich are inputs into supervised regressors such as random forests, extreme\nrandom forests, XGBoost, and artificial neural networks. The highest performing\nalgorithm, ANN-Flux, achieves AUROC and AUPR scores exceeding 0.95 for each\nclassification. In addition, ANN-Flux reduces the remaining useful life RMSE by\n38% for the same test split of the dataset compared to past work, with\nsignificantly less computational cost.\n","authors":["Joseph Cohen","Xun Huan","Jun Ni"],"pdf_url":"https://arxiv.org/pdf/2303.12982v1.pdf","comment":"Preprint with 10 pages, 5 figures. Submitted to International Journal\n  of Prognostics and Health Management (IJPHM)"},{"id":"http://arxiv.org/abs/2303.12981v1","updated":"2023-03-23T01:14:36Z","published":"2023-03-23T01:14:36Z","title":"Connected Superlevel Set in (Deep) Reinforcement Learning and its\n  Application to Minimax Theorems","summary":"  The aim of this paper is to improve the understanding of the optimization\nlandscape for policy optimization problems in reinforcement learning.\nSpecifically, we show that the superlevel set of the objective function with\nrespect to the policy parameter is always a connected set both in the tabular\nsetting and under policies represented by a class of neural networks. In\naddition, we show that the optimization objective as a function of the policy\nparameter and reward satisfies a stronger \"equiconnectedness\" property. To our\nbest knowledge, these are novel and previously unknown discoveries.\n  We present an application of the connectedness of these superlevel sets to\nthe derivation of minimax theorems for robust reinforcement learning. We show\nthat any minimax optimization program which is convex on one side and is\nequiconnected on the other side observes the minimax equality (i.e. has a Nash\nequilibrium). We find that this exact structure is exhibited by an interesting\nrobust reinforcement learning problem under an adversarial reward attack, and\nthe validity of its minimax equality immediately follows. This is the first\ntime such a result is established in the literature.\n","authors":["Sihan Zeng","Thinh T. Doan","Justin Romberg"],"pdf_url":"https://arxiv.org/pdf/2303.12981v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.12440v3","updated":"2023-03-23T01:07:14Z","published":"2022-12-23T16:14:53Z","title":"HAC-Net: A Hybrid Attention-Based Convolutional Neural Network for\n  Highly Accurate Protein-Ligand Binding Affinity Prediction","summary":"  Applying deep learning concepts from image detection and graph theory has\ngreatly advanced protein-ligand binding affinity prediction, a challenge with\nenormous ramifications for both drug discovery and protein engineering. We\nbuild upon these advances by designing a novel deep learning architecture\nconsisting of a 3-dimensional convolutional neural network utilizing\nchannel-wise attention and two graph convolutional networks utilizing\nattention-based aggregation of node features. HAC-Net (Hybrid Attention-Based\nConvolutional Neural Network) obtains state-of-the-art results on the PDBbind\nv.2016 core set, the most widely recognized benchmark in the field. We\nextensively assess the generalizability of our model using multiple train-test\nsplits, each of which maximizes differences between either protein structures,\nprotein sequences, or ligand extended-connectivity fingerprints of complexes in\nthe training and test sets. Furthermore, we perform 10-fold cross-validation\nwith a similarity cutoff between SMILES strings of ligands in the training and\ntest sets, and also evaluate the performance of HAC-Net on lower-quality data.\nWe envision that this model can be extended to a broad range of supervised\nlearning problems related to structure-based biomolecular property prediction.\nAll of our software is available as open source at\nhttps://github.com/gregory-kyro/HAC-Net/, and the HACNet Python package is\navailable through PyPI.\n","authors":["Gregory W. Kyro","Rafael I. Brent","Victor S. Batista"],"pdf_url":"https://arxiv.org/pdf/2212.12440v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.11241v2","updated":"2023-03-23T00:49:44Z","published":"2023-01-26T17:25:45Z","title":"On the Convergence of No-Regret Learning Dynamics in Time-Varying Games","summary":"  Most of the literature on learning in games has focused on the restrictive\nsetting where the underlying repeated game does not change over time. Much less\nis known about the convergence of no-regret learning algorithms in dynamic\nmultiagent settings. In this paper, we characterize the convergence of\noptimistic gradient descent (OGD) in time-varying games. Our framework yields\nsharp convergence bounds for the equilibrium gap of OGD in zero-sum games\nparameterized on natural variation measures of the sequence of games, subsuming\nknown results for static games. Furthermore, we establish improved second-order\nvariation bounds under strong convexity-concavity, as long as each game is\nrepeated multiple times. Our results also apply to time-varying general-sum\nmulti-player games via a bilinear formulation of correlated equilibria, which\nhas novel implications for meta-learning and for obtaining refined\nvariation-dependent regret bounds, addressing questions left open in prior\npapers. Finally, we leverage our framework to also provide new insights on\ndynamic regret guarantees in static games.\n","authors":["Ioannis Anagnostides","Ioannis Panageas","Gabriele Farina","Tuomas Sandholm"],"pdf_url":"https://arxiv.org/pdf/2301.11241v2.pdf","comment":"V2 clarifies connections with prior work"},{"id":"http://arxiv.org/abs/2303.12964v1","updated":"2023-03-23T00:11:17Z","published":"2023-03-23T00:11:17Z","title":"Continuous Indeterminate Probability Neural Network","summary":"  This paper introduces a general model called CIPNN - Continuous Indeterminate\nProbability Neural Network, and this model is based on IPNN, which is used for\ndiscrete latent random variables. Currently, posterior of continuous latent\nvariables is regarded as intractable, with the new theory proposed by IPNN this\nproblem can be solved. Our contributions are Four-fold. First, we derive the\nanalytical solution of the posterior calculation of continuous latent random\nvariables and propose a general classification model (CIPNN). Second, we\npropose a general auto-encoder called CIPAE - Continuous Indeterminate\nProbability Auto-Encoder, the decoder part is not a neural network and uses a\nfully probabilistic inference model for the first time. Third, we propose a new\nmethod to visualize the latent random variables, we use one of N dimensional\nlatent variables as a decoder to reconstruct the input image, which can work\neven for classification tasks, in this way, we can see what each latent\nvariable has learned. Fourth, IPNN has shown great classification capability,\nCIPNN has pushed this classification capability to infinity. Theoretical\nadvantages are reflected in experimental results.\n","authors":["Tao Yang"],"pdf_url":"https://arxiv.org/pdf/2303.12964v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2303.12963v1","updated":"2023-03-23T00:03:07Z","published":"2023-03-23T00:03:07Z","title":"Forecast-Aware Model Driven LSTM","summary":"  Poor air quality can have a significant impact on human health. The National\nOceanic and Atmospheric Administration (NOAA) air quality forecasting guidance\nis challenged by the increasing presence of extreme air quality events due to\nextreme weather events such as wild fires and heatwaves. These extreme air\nquality events further affect human health. Traditional methods used to correct\nmodel bias make assumptions about linearity and the underlying distribution.\nExtreme air quality events tend to occur without a strong signal leading up to\nthe event and this behavior tends to cause existing methods to either under or\nover compensate for the bias. Deep learning holds promise for air quality\nforecasting in the presence of extreme air quality events due to its ability to\ngeneralize and learn nonlinear problems. However, in the presence of these\nanomalous air quality events, standard deep network approaches that use a\nsingle network for generalizing to future forecasts, may not always provide the\nbest performance even with a full feature-set including geography and\nmeteorology. In this work we describe a method that combines unsupervised\nlearning and a forecast-aware bi-directional LSTM network to perform bias\ncorrection for operational air quality forecasting using AirNow station data\nfor ozone and PM2.5 in the continental US. Using an unsupervised clustering\nmethod trained on station geographical features such as latitude and longitude,\nurbanization, and elevation, the learned clusters direct training by\npartitioning the training data for the LSTM networks. LSTMs are forecast-aware\nand implemented using a unique way to perform learning forward and backwards in\ntime across forecasting days. When comparing the RMSE of the forecast model to\nthe RMSE of the bias corrected model, the bias corrected model shows\nsignificant improvement (27\\% lower RMSE for ozone) over the base forecast.\n","authors":["Sophia Hamer","Jennifer Sleeman","Ivanka Stajner"],"pdf_url":"https://arxiv.org/pdf/2303.12963v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2303.13471v1","updated":"2023-03-23T17:43:11Z","published":"2023-03-23T17:43:11Z","title":"Egocentric Audio-Visual Object Localization","summary":"  Humans naturally perceive surrounding scenes by unifying sound and sight in a\nfirst-person view. Likewise, machines are advanced to approach human\nintelligence by learning with multisensory inputs from an egocentric\nperspective. In this paper, we explore the challenging egocentric audio-visual\nobject localization task and observe that 1) egomotion commonly exists in\nfirst-person recordings, even within a short duration; 2) The out-of-view sound\ncomponents can be created while wearers shift their attention. To address the\nfirst problem, we propose a geometry-aware temporal aggregation module to\nhandle the egomotion explicitly. The effect of egomotion is mitigated by\nestimating the temporal geometry transformation and exploiting it to update\nvisual representations. Moreover, we propose a cascaded feature enhancement\nmodule to tackle the second issue. It improves cross-modal localization\nrobustness by disentangling visually-indicated audio representation. During\ntraining, we take advantage of the naturally available audio-visual temporal\nsynchronization as the ``free'' self-supervision to avoid costly labeling. We\nalso annotate and create the Epic Sounding Object dataset for evaluation\npurposes. Extensive experiments show that our method achieves state-of-the-art\nlocalization performance in egocentric videos and can be generalized to diverse\naudio-visual scenes.\n","authors":["Chao Huang","Yapeng Tian","Anurag Kumar","Chenliang Xu"],"pdf_url":"https://arxiv.org/pdf/2303.13471v1.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2303.13397v1","updated":"2023-03-23T16:15:18Z","published":"2023-03-23T16:15:18Z","title":"DDT: A Diffusion-Driven Transformer-based Framework for Human Mesh\n  Recovery from a Video","summary":"  Human mesh recovery (HMR) provides rich human body information for various\nreal-world applications such as gaming, human-computer interaction, and virtual\nreality. Compared to single image-based methods, video-based methods can\nutilize temporal information to further improve performance by incorporating\nhuman body motion priors. However, many-to-many approaches such as VIBE suffer\nfrom motion smoothness and temporal inconsistency. While many-to-one approaches\nsuch as TCMR and MPS-Net rely on the future frames, which is non-causal and\ntime inefficient during inference. To address these challenges, a novel\nDiffusion-Driven Transformer-based framework (DDT) for video-based HMR is\npresented. DDT is designed to decode specific motion patterns from the input\nsequence, enhancing motion smoothness and temporal consistency. As a\nmany-to-many approach, the decoder of our DDT outputs the human mesh of all the\nframes, making DDT more viable for real-world applications where time\nefficiency is crucial and a causal model is desired. Extensive experiments are\nconducted on the widely used datasets (Human3.6M, MPI-INF-3DHP, and 3DPW),\nwhich demonstrated the effectiveness and efficiency of our DDT.\n","authors":["Ce Zheng","Guo-Jun Qi","Chen Chen"],"pdf_url":"https://arxiv.org/pdf/2303.13397v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13371v1","updated":"2023-03-23T15:42:05Z","published":"2023-03-23T15:42:05Z","title":"Plug-and-Play Regulators for Image-Text Matching","summary":"  Exploiting fine-grained correspondence and visual-semantic alignments has\nshown great potential in image-text matching. Generally, recent approaches\nfirst employ a cross-modal attention unit to capture latent region-word\ninteractions, and then integrate all the alignments to obtain the final\nsimilarity. However, most of them adopt one-time forward association or\naggregation strategies with complex architectures or additional information,\nwhile ignoring the regulation ability of network feedback. In this paper, we\ndevelop two simple but quite effective regulators which efficiently encode the\nmessage output to automatically contextualize and aggregate cross-modal\nrepresentations. Specifically, we propose (i) a Recurrent Correspondence\nRegulator (RCR) which facilitates the cross-modal attention unit progressively\nwith adaptive attention factors to capture more flexible correspondence, and\n(ii) a Recurrent Aggregation Regulator (RAR) which adjusts the aggregation\nweights repeatedly to increasingly emphasize important alignments and dilute\nunimportant ones. Besides, it is interesting that RCR and RAR are\nplug-and-play: both of them can be incorporated into many frameworks based on\ncross-modal interaction to obtain significant benefits, and their cooperation\nachieves further improvements. Extensive experiments on MSCOCO and Flickr30K\ndatasets validate that they can bring an impressive and consistent R@1 gain on\nmultiple models, confirming the general effectiveness and generalization\nability of the proposed methods. Code and pre-trained models are available at:\nhttps://github.com/Paranioar/RCAR.\n","authors":["Haiwen Diao","Ying Zhang","Wei Liu","Xiang Ruan","Huchuan Lu"],"pdf_url":"https://arxiv.org/pdf/2303.13371v1.pdf","comment":"13 pages, 9 figures, Accepted by TIP2023"},{"id":"http://arxiv.org/abs/2303.13357v1","updated":"2023-03-23T15:36:12Z","published":"2023-03-23T15:36:12Z","title":"POTTER: Pooling Attention Transformer for Efficient Human Mesh Recovery","summary":"  Transformer architectures have achieved SOTA performance on the human mesh\nrecovery (HMR) from monocular images. However, the performance gain has come at\nthe cost of substantial memory and computational overhead. A lightweight and\nefficient model to reconstruct accurate human mesh is needed for real-world\napplications. In this paper, we propose a pure transformer architecture named\nPOoling aTtention TransformER (POTTER) for the HMR task from single images.\nObserving that the conventional attention module is memory and computationally\nexpensive, we propose an efficient pooling attention module, which\nsignificantly reduces the memory and computational cost without sacrificing\nperformance. Furthermore, we design a new transformer architecture by\nintegrating a High-Resolution (HR) stream for the HMR task. The high-resolution\nlocal and global features from the HR stream can be utilized for recovering\nmore accurate human mesh. Our POTTER outperforms the SOTA method METRO by only\nrequiring 7% of total parameters and 14% of the Multiply-Accumulate Operations\non the Human3.6M (PA-MPJPE metric) and 3DPW (all three metrics) datasets. The\nproject webpage is https://zczcwh.github.io/potter_page.\n","authors":["Ce Zheng","Xianpeng Liu","Guo-Jun Qi","Chen Chen"],"pdf_url":"https://arxiv.org/pdf/2303.13357v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.13336v1","updated":"2023-03-23T15:17:15Z","published":"2023-03-23T15:17:15Z","title":"Audio Diffusion Model for Speech Synthesis: A Survey on Text To Speech\n  and Speech Enhancement in Generative AI","summary":"  Generative AI has demonstrated impressive performance in various fields,\namong which speech synthesis is an interesting direction. With the diffusion\nmodel as the most popular generative model, numerous works have attempted two\nactive tasks: text to speech and speech enhancement. This work conducts a\nsurvey on audio diffusion model, which is complementary to existing surveys\nthat either lack the recent progress of diffusion-based speech synthesis or\nhighlight an overall picture of applying diffusion model in multiple fields.\nSpecifically, this work first briefly introduces the background of audio and\ndiffusion model. As for the text-to-speech task, we divide the methods into\nthree categories based on the stage where diffusion model is adopted: acoustic\nmodel, vocoder and end-to-end framework. Moreover, we categorize various speech\nenhancement tasks by either certain signals are removed or added into the input\nspeech. Comparisons of experimental results and discussions are also covered in\nthis survey.\n","authors":["Chenshuang Zhang","Chaoning Zhang","Sheng Zheng","Mengchun Zhang","Maryam Qamar","Sung-Ho Bae","In So Kweon"],"pdf_url":"https://arxiv.org/pdf/2303.13336v1.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2303.13079v1","updated":"2023-03-23T07:15:28Z","published":"2023-03-23T07:15:28Z","title":"Implementation of communication media around a mixed reality experience\n  with HoloLens headset, as part of a digitalization of a nutrition workshop","summary":"  The release of Microsoft's HoloLens headset addresses new types of issues\nthat would have been difficult to design without such a hardware. This\nsemi-transparent visor headset allows the user who wears it to view the\nprojection of 3D virtual objects placed in its real environment. The user can\nalso interact with these 3D objects, which can interact with each other. The\nframework of this new technology is called mixed reality. We had the\nopportunity to numerically transform a conventional human nutrition workshop\nfor patients waiting for bariatric surgery by developing a software called\nHOLO_NUTRI using the HoloLens headset. Unlike our experience of user and\nconventional programmer specialized in the development of interactive 3D\ngraphics applications, we realized that such a mixed reality experience\nrequired specific programming concepts quite different from those of\nconventional software or those of virtual reality applications, but above all\nrequired a thorough reflection about communication for users. In this article,\nwe propose to explain our design of communication (graphic supports, tutorials\nof use of material, explanatory videos), a step which was crucial for the good\nprogress of our project. The software was used by thirty patients from Le\nPuy-en-Velay Hospital during 10 sessions of one hour and a half during which\npatients had to take in hand the headset and software HOLO_NUTRI. We also\nproposed a series of questions to patients to have an assessment of both the\nadequacy and the importance of this communication approach for such experience.\nAs the mixed reality technology is very recent but the number of applications\nbased on it significantly increases, the reflection on the implementation of\nthe elements of communication described in this article (videos, exercise of\nlearning for the use of the headset, communication leaflet, etc.) can help\ndevelopers of such applications.\n","authors":["Owen Kevin Appadoo","Hugo Rositi","Sylvie Valarier","Marie-Claire Ombret","Émilie Gadéa","Christine Barret-Grimault","Christophe Lohou"],"pdf_url":"https://arxiv.org/pdf/2303.13079v1.pdf","comment":"19 pages, 14 figures"},{"id":"http://arxiv.org/abs/2303.13031v1","updated":"2023-03-23T04:40:33Z","published":"2023-03-23T04:40:33Z","title":"Learning a Practical SDR-to-HDRTV Up-conversion using New Dataset and\n  Degradation Models","summary":"  In media industry, the demand of SDR-to-HDRTV up-conversion arises when users\npossess HDR-WCG (high dynamic range-wide color gamut) TVs while most\noff-the-shelf footage is still in SDR (standard dynamic range). The research\ncommunity has started tackling this low-level vision task by learning-based\napproaches. When applied to real SDR, yet, current methods tend to produce dim\nand desaturated result, making nearly no improvement on viewing experience.\nDifferent from other network-oriented methods, we attribute such deficiency to\ntraining set (HDR-SDR pair). Consequently, we propose new HDRTV dataset (dubbed\nHDRTV4K) and new HDR-to-SDR degradation models. Then, it's used to train a\nluminance-segmented network (LSN) consisting of a global mapping trunk, and two\nTransformer branches on bright and dark luminance range. We also update\nassessment criteria by tailored metrics and subjective experiment. Finally,\nablation studies are conducted to prove the effectiveness. Our work is\navailable at: https://github.com/AndreGuo/HDRTVDM.\n","authors":["Cheng Guo","Leidong Fan","Ziyu Xue","and Xiuhua Jiang"],"pdf_url":"https://arxiv.org/pdf/2303.13031v1.pdf","comment":"Accepted by CVPR2023"}]}}